[{"title": "55221698", "vertexSet": [[{"sent_id": 1, "name": "tf.keras.layers.stackedrnncells", "pos": [79, 91]}], [{"sent_id": 1, "name": "tf.nn.rnn_cell.multirnncell", "pos": [53, 68]}]], "sents": ["It seems like recent changes in tensorflow API makes it similar to the keras, also, new tutorials focused on the keras-like solutions.", "If you need some \"old-style\" tensorflow stacked LSTM, you can use tf.nn.rnn_cell.MultiRNNCell (now it's deprecated and replaced with tf.keras.layers.StackedRNNCells):", "<code>Code Snippet</code>.", "Now you should define some loss, but it's not clear which loss you're planning to use, so i omit this part (based on some features, it could be sigmoid_cross_entropy_with_logits (after all, it's not runnable example, but I can provide one for some standart dataset like MNIST or so, if you need):", "<code>Code Snippet</code>.", "Initialization and train (simplified):", "<code>Code Snippet</code>.", "Notice please if you don't need stack deprecated tensorflow layers and require some solution for 2.0 version."], "sent_idxs": [101, 2009, 3849, 2066, 3522, 3431, 1999, 23435, 12314, 17928, 3084, 2009, 2714, 2000, 1996, 17710, 8180, 1010, 2036, 1010, 2047, 14924, 26340, 4208, 2006, 1996, 17710, 8180, 1011, 2066, 7300, 1012, 102, 101, 2065, 2017, 2342, 2070, 1000, 2214, 1011, 2806, 1000, 23435, 12314, 16934, 1048, 3367, 2213, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 4800, 6826, 5897, 3363, 1006, 2085, 2009, 1005, 1055, 2139, 28139, 12921, 1998, 2999, 2007, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 16934, 6826, 5897, 12718, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 2017, 2323, 9375, 2070, 3279, 1010, 2021, 2009, 1005, 1055, 2025, 3154, 2029, 3279, 2017, 1005, 2128, 4041, 2000, 2224, 1010, 2061, 1045, 18168, 4183, 2023, 2112, 1006, 2241, 2006, 2070, 2838, 1010, 2009, 2071, 2022, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 2044, 2035, 1010, 2009, 1005, 1055, 2025, 2448, 22966, 2742, 1010, 2021, 1045, 2064, 3073, 2028, 2005, 2070, 3233, 8445, 2951, 13462, 2066, 24098, 2923, 2030, 2061, 1010, 2065, 2017, 2342, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3988, 3989, 1998, 3345, 1006, 11038, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5060, 3531, 2065, 2017, 2123, 1005, 1056, 2342, 9991, 2139, 28139, 12921, 23435, 12314, 9014, 1998, 5478, 2070, 5576, 2005, 1016, 1012, 1014, 2544, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 33, 94, 108, 193, 207, 217, 231, 258], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42539696", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.rnn", "pos": [1, 9]}], [{"sent_id": 0, "name": "tf.nn.static_rnn", "pos": [12, 22]}, {"sent_id": 1, "name": "tf.nn.static_rnn", "pos": [39, 49]}, {"sent_id": 1, "name": "tf.nn.static_rnn", "pos": [76, 86]}], [{"sent_id": 1, "name": "tf.contrib.rnn.static_rnn", "pos": [55, 69]}]], "sents": ["tf.nn.rnn is equivalent to tf.nn.static_rnn.", "Note: before version 1.2 of TensorFlow, the namespace tf.nn.static_rnn did not exist, but only tf.contrib.rnn.static_rnn (which is now an alias for tf.nn.static_rnn)."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 2003, 5662, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 1012, 102, 101, 3602, 1024, 2077, 2544, 1015, 1012, 1016, 1997, 23435, 12314, 1010, 1996, 3415, 15327, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 2106, 2025, 4839, 1010, 2021, 2069, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 10763, 1035, 29300, 2078, 1006, 2029, 2003, 2085, 2019, 14593, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}, {"r": "S1", "h": 1, "t": 2, "evidence": [1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [1]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 24, 89], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0]}, {"title": "53902375", "vertexSet": [[{"sent_id": 2, "name": "tf.add_to_collection", "pos": [41, 49]}, {"sent_id": 7, "name": "tf.add_to_collection", "pos": [201, 209]}], [{"sent_id": 0, "name": "tf.group", "pos": [1, 5]}, {"sent_id": 7, "name": "tf.group", "pos": [196, 200]}, {"sent_id": 8, "name": "tf.group", "pos": [306, 310]}], [{"sent_id": 6, "name": "tf.tensor", "pos": [143, 147]}, {"sent_id": 6, "name": "tf.tensor", "pos": [165, 169]}], [{"sent_id": 5, "name": "tf.operation", "pos": [116, 120]}], [{"sent_id": 4, "name": "tf.get_collection", "pos": [98, 104]}, {"sent_id": 6, "name": "tf.get_collection", "pos": [134, 140]}, {"sent_id": 7, "name": "tf.get_collection", "pos": [210, 216]}, {"sent_id": 7, "name": "tf.get_collection", "pos": [244, 250]}]], "sents": ["tf.group creates an operation inside the computational graph that once evaluated executes all the tensors in the group:", "<code>Code Snippet</code>.", "tf.add_to_collection instead, creates a group of operations not inside the computational graph, but only in the python script.", "<code>Code Snippet</code>.", "You can see this by looking at the description of op and `tf.get_collection('coll'):", "op: <tf.Operation 'group_deps' type=NoOp>.", "tf.get_collection: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'Const_1:0' shape=() dtype=int32>].", "In your example, using tf.group or tf.add_to_collection + tf.get_collection is the same: you just need all the operations executed in parallel, hence sess.run(op) and sess.run(tf.get_collection('coll')) have the same behaviour.", "But in the case of the export of a computational graph (that's just an example to make you understand a possible scenario), you can't rely upon a python list, hence you have to use tf.group"], "sent_idxs": [101, 1056, 2546, 1012, 2177, 9005, 2019, 3169, 2503, 1996, 15078, 10629, 2008, 2320, 16330, 15389, 2015, 2035, 1996, 23435, 2015, 1999, 1996, 2177, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 2612, 1010, 9005, 1037, 2177, 1997, 3136, 2025, 2503, 1996, 15078, 10629, 1010, 2021, 2069, 1999, 1996, 18750, 5896, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2156, 2023, 2011, 2559, 2012, 1996, 6412, 1997, 6728, 1998, 1036, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1005, 8902, 2140, 1005, 1007, 1024, 102, 101, 6728, 1024, 1026, 1056, 2546, 1012, 3169, 1005, 2177, 1035, 2139, 4523, 1005, 2828, 1027, 2053, 7361, 1028, 1012, 102, 101, 1056, 2546, 1012, 2131, 1035, 3074, 1024, 1031, 1026, 1056, 2546, 1012, 23435, 1005, 9530, 3367, 1024, 1014, 1005, 4338, 1027, 1006, 1007, 26718, 18863, 1027, 20014, 16703, 1028, 1010, 1026, 1056, 2546, 1012, 23435, 1005, 9530, 3367, 1035, 1015, 1024, 1014, 1005, 4338, 1027, 1006, 1007, 26718, 18863, 1027, 20014, 16703, 1028, 1033, 1012, 102, 101, 1999, 2115, 2742, 1010, 2478, 1056, 2546, 1012, 2177, 2030, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1009, 1056, 2546, 1012, 2131, 1035, 3074, 2003, 1996, 2168, 1024, 2017, 2074, 2342, 2035, 1996, 3136, 6472, 1999, 5903, 1010, 6516, 7367, 4757, 1012, 2448, 1006, 6728, 1007, 1998, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1005, 8902, 2140, 1005, 1007, 1007, 2031, 1996, 2168, 9164, 1012, 102, 101, 2021, 1999, 1996, 2553, 1997, 1996, 9167, 1997, 1037, 15078, 10629, 1006, 2008, 1005, 1055, 2074, 2019, 2742, 2000, 2191, 2017, 3305, 1037, 2825, 11967, 1007, 1010, 2017, 2064, 1005, 1056, 11160, 2588, 1037, 18750, 2862, 1010, 6516, 2017, 2031, 2000, 2224, 1056, 2546, 1012, 2177, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 7, 8]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 26, 40, 70, 84, 112, 133, 190, 263, 311], "sent_pos": [0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0]}, {"title": "44809583", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.moments", "pos": [1, 8]}], [{"sent_id": 0, "name": "tf.nn.fused_batch_norm", "pos": [15, 26]}]], "sents": ["tf.nn.moments is computing the sample variance, whereas tf.nn.fused_batch_norm is computing the unbiased variance estimator.", "The difference between the two is a factor n/n-1, where n is your sample size.", "The code applying this factor can be found here.", "Note however that while the returned variance is the unbiased estimated, the saved variance used for the moving average is the biased one.", "In your example, your sample size is 4*4*1=16 and you will notice that var2 = var1 * 16/15.", "When you take a much larger sample, you see that differences between var1 and var2 becomes smaller.", "That would probably be worth mentioning in the documentation."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 5312, 2003, 9798, 1996, 7099, 23284, 1010, 6168, 1056, 2546, 1012, 1050, 2078, 1012, 19660, 1035, 14108, 1035, 13373, 2003, 9798, 1996, 4895, 11607, 6924, 23284, 9765, 9581, 4263, 1012, 102, 101, 1996, 4489, 2090, 1996, 2048, 2003, 1037, 5387, 1050, 1013, 1050, 1011, 1015, 1010, 2073, 1050, 2003, 2115, 7099, 2946, 1012, 102, 101, 1996, 3642, 11243, 2023, 5387, 2064, 2022, 2179, 2182, 1012, 102, 101, 3602, 2174, 2008, 2096, 1996, 2513, 23284, 2003, 1996, 4895, 11607, 6924, 4358, 1010, 1996, 5552, 23284, 2109, 2005, 1996, 3048, 2779, 2003, 1996, 25352, 2028, 1012, 102, 101, 1999, 2115, 2742, 1010, 2115, 7099, 2946, 2003, 1018, 1008, 1018, 1008, 1015, 1027, 2385, 1998, 2017, 2097, 5060, 2008, 13075, 2475, 1027, 13075, 2487, 1008, 2385, 1013, 2321, 1012, 102, 101, 2043, 2017, 2202, 1037, 2172, 3469, 7099, 1010, 2017, 2156, 2008, 5966, 2090, 13075, 2487, 1998, 13075, 2475, 4150, 3760, 1012, 102, 101, 2008, 2052, 2763, 2022, 4276, 18625, 1999, 1996, 12653, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 38, 61, 73, 102, 134, 157, 169], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45186929", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib.learn.estimator", "pos": [89, 101]}, {"sent_id": 4, "name": "tf.contrib.learn.estimator", "pos": [188, 200]}], [{"sent_id": 1, "name": "tf.estimator.estimator", "pos": [30, 40]}, {"sent_id": 2, "name": "tf.estimator.estimator", "pos": [110, 120]}], [{"sent_id": 1, "name": "tf.estimator.estimatorspec", "pos": [47, 59]}]], "sents": ["I wondered the same and cannot give a definitive answer, but I have a few educated guesses that might help you:", "It seems that tf.estimator.Estimator together with a model function that returns tf.estimator.EstimatorSpec is the most current one that is used in the newer examples and the one to be used in new code.", "My guess now is that the tf.contrib.learn.Estimator is an early prototype that got replaced by the tf.estimator.Estimator.", "According to the docs everything in tf.contrib is unstable API that may change at any time and it looks like the tf.estimator module is the stable API that \u201cevolved\u201d from the tf.contrib.learn module.", "I assume that the authors just forgot to mark tf.contrib.learn.Estimator as deprecated and that it wasn't removed yet so existing code won't break."], "sent_idxs": [101, 1045, 4999, 1996, 2168, 1998, 3685, 2507, 1037, 15764, 3437, 1010, 2021, 1045, 2031, 1037, 2261, 5161, 3984, 2229, 2008, 2453, 2393, 2017, 1024, 102, 101, 2009, 3849, 2008, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 2362, 2007, 1037, 2944, 3853, 2008, 5651, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 6591, 5051, 2278, 2003, 1996, 2087, 2783, 2028, 2008, 2003, 2109, 1999, 1996, 10947, 4973, 1998, 1996, 2028, 2000, 2022, 2109, 1999, 2047, 3642, 1012, 102, 101, 2026, 3984, 2085, 2003, 2008, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 9765, 9581, 4263, 2003, 2019, 2220, 8773, 2008, 2288, 2999, 2011, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1012, 102, 101, 2429, 2000, 1996, 9986, 2015, 2673, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 2003, 14480, 17928, 2008, 2089, 2689, 2012, 2151, 2051, 1998, 2009, 3504, 2066, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 11336, 2003, 1996, 6540, 17928, 2008, 1523, 7964, 1524, 2013, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 11336, 1012, 102, 101, 1045, 7868, 2008, 1996, 6048, 2074, 9471, 2000, 2928, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 9765, 9581, 4263, 2004, 2139, 28139, 12921, 1998, 2008, 2009, 2347, 1005, 1056, 3718, 2664, 2061, 4493, 3642, 2180, 1005, 1056, 3338, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 82, 122, 178, 221], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53197786", "vertexSet": [[{"sent_id": 0, "name": "tf.metrics", "pos": [5, 10]}], [{"sent_id": 1, "name": "tf.contrib.metrics", "pos": [43, 52]}], [{"sent_id": 1, "name": "tf.keras.metrics", "pos": [53, 61]}]], "sents": ["The metrics in tf.metrics are stateful; they create variables to accumulate partial results in, so you shouldn't expect them to auto-reset.", "Instead use the metrics in tf.contrib.metrics or tf.keras.metrics and session.run the ops to reset them accordingly."], "sent_idxs": [101, 1996, 12046, 2015, 1999, 1056, 2546, 1012, 12046, 2015, 2024, 2110, 3993, 1025, 2027, 3443, 10857, 2000, 27598, 7704, 3463, 1999, 1010, 2061, 2017, 5807, 1005, 1056, 5987, 2068, 2000, 8285, 1011, 25141, 1012, 102, 101, 2612, 2224, 1996, 12046, 2015, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 12046, 2015, 2030, 1056, 2546, 1012, 17710, 8180, 1012, 12046, 2015, 1998, 5219, 1012, 2448, 1996, 23092, 2000, 25141, 2068, 11914, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 36, 73], "sent_pos": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34596212", "vertexSet": [[{"sent_id": 3, "name": "tf.fifoqueue", "pos": [80, 87]}], [{"sent_id": 3, "name": "tf.randomshufflequeue", "pos": [104, 112]}], [{"sent_id": 14, "name": "tf.train", "pos": [395, 399]}, {"sent_id": 14, "name": "tf.train", "pos": [402, 406]}], [{"sent_id": 7, "name": "tf.placeholder", "pos": [234, 239]}], [{"sent_id": 14, "name": "tf.train.coordinator", "pos": [395, 401]}], [{"sent_id": 14, "name": "tf.train.queuerunner", "pos": [402, 409]}]], "sents": ["This is a common use case, and most implementations use TensorFlow's queues to decouple the preprocessing code from the training code.", "There is a tutorial on how to use queues, but the main steps are as follows:", "Define a queue, q, that will buffer the preprocessed data.", "TensorFlow supports the simple tf.FIFOQueue that produces elements in the order they were enqueued, and the more advanced tf.RandomShuffleQueue that produces elements in a random order.", "A queue element is a tuple of one or more tensors (which can have different types and shapes).", "All queues support single-element (enqueue, dequeue) and batch (enqueue_many, dequeue_many) operations, but to use the batch operations you must specify the shapes of each tensor in a queue element when constructing the queue.", "Build a subgraph that enqueues preprocessed elements into the queue.", "One way to do this would be to define some tf.placeholder() ops for tensors corresponding to a single input example, then pass them to q.enqueue().", "(If your preprocessing produces a batch at once, you should use q.enqueue_many() instead.)", "You might also include TensorFlow ops in this subgraph.", "Build a subgraph that performs training.", "This will look like a regular TensorFlow graph, but will get its input by calling q.dequeue_many(BATCH_SIZE).", "Start your session.", "Create one or more threads that execute your preprocessing logic, then execute the enqueue op, feeding in the preprocessed data.", "You may find the tf.train.Coordinator and tf.train.QueueRunner utility classes useful for this.", "Run your training graph (optimizer, etc.)", "as normal.", "EDIT: Here's a simple load_and_enqueue() function and code fragment to get you started:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2003, 1037, 2691, 2224, 2553, 1010, 1998, 2087, 24977, 2224, 23435, 12314, 1005, 1055, 24240, 2015, 2000, 21933, 6279, 2571, 1996, 17463, 3217, 9623, 7741, 3642, 2013, 1996, 2731, 3642, 1012, 102, 101, 2045, 2003, 1037, 14924, 4818, 2006, 2129, 2000, 2224, 24240, 2015, 1010, 2021, 1996, 2364, 4084, 2024, 2004, 4076, 1024, 102, 101, 9375, 1037, 24240, 1010, 1053, 1010, 2008, 2097, 17698, 1996, 17463, 3217, 9623, 6924, 2951, 1012, 102, 101, 23435, 12314, 6753, 1996, 3722, 1056, 2546, 1012, 10882, 14876, 4226, 5657, 2008, 7137, 3787, 1999, 1996, 2344, 2027, 2020, 4372, 4226, 5657, 2094, 1010, 1998, 1996, 2062, 3935, 1056, 2546, 1012, 6721, 14235, 18142, 4226, 5657, 2008, 7137, 3787, 1999, 1037, 6721, 2344, 1012, 102, 101, 1037, 24240, 5783, 2003, 1037, 10722, 10814, 1997, 2028, 2030, 2062, 23435, 2015, 1006, 2029, 2064, 2031, 2367, 4127, 1998, 10466, 1007, 1012, 102, 101, 2035, 24240, 2015, 2490, 2309, 1011, 5783, 1006, 4372, 4226, 5657, 1010, 2139, 4226, 5657, 1007, 1998, 14108, 1006, 4372, 4226, 5657, 1035, 2116, 1010, 2139, 4226, 5657, 1035, 2116, 1007, 3136, 1010, 2021, 2000, 2224, 1996, 14108, 3136, 2017, 2442, 20648, 1996, 10466, 1997, 2169, 23435, 1999, 1037, 24240, 5783, 2043, 15696, 1996, 24240, 1012, 102, 101, 3857, 1037, 4942, 14413, 2008, 4372, 4226, 15808, 17463, 3217, 9623, 6924, 3787, 2046, 1996, 24240, 1012, 102, 101, 2028, 2126, 2000, 2079, 2023, 2052, 2022, 2000, 9375, 2070, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23092, 2005, 23435, 2015, 7978, 2000, 1037, 2309, 7953, 2742, 1010, 2059, 3413, 2068, 2000, 1053, 1012, 4372, 4226, 5657, 1006, 1007, 1012, 102, 101, 1006, 2065, 2115, 17463, 3217, 9623, 7741, 7137, 1037, 14108, 2012, 2320, 1010, 2017, 2323, 2224, 1053, 1012, 4372, 4226, 5657, 1035, 2116, 1006, 1007, 2612, 1012, 1007, 102, 101, 2017, 2453, 2036, 2421, 23435, 12314, 23092, 1999, 2023, 4942, 14413, 1012, 102, 101, 3857, 1037, 4942, 14413, 2008, 10438, 2731, 1012, 102, 101, 2023, 2097, 2298, 2066, 1037, 3180, 23435, 12314, 10629, 1010, 2021, 2097, 2131, 2049, 7953, 2011, 4214, 1053, 1012, 2139, 4226, 5657, 1035, 2116, 1006, 14108, 1035, 2946, 1007, 1012, 102, 101, 2707, 2115, 5219, 1012, 102, 101, 3443, 2028, 2030, 2062, 16457, 2008, 15389, 2115, 17463, 3217, 9623, 7741, 7961, 1010, 2059, 15389, 1996, 4372, 4226, 5657, 6728, 1010, 8521, 1999, 1996, 17463, 3217, 9623, 6924, 2951, 1012, 102, 101, 2017, 2089, 2424, 1996, 1056, 2546, 1012, 3345, 1012, 10669, 1998, 1056, 2546, 1012, 3345, 1012, 24240, 23195, 9710, 4280, 6179, 2005, 2023, 1012, 102, 101, 2448, 2115, 2731, 10629, 1006, 23569, 27605, 6290, 1010, 4385, 1012, 1007, 102, 101, 2004, 3671, 1012, 102, 101, 10086, 1024, 2182, 1005, 1055, 1037, 3722, 7170, 1035, 1998, 1035, 4372, 4226, 5657, 1006, 1007, 3853, 1998, 3642, 15778, 2000, 2131, 2017, 2318, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 34, 56, 74, 121, 146, 204, 223, 265, 295, 309, 319, 351, 357, 390, 416, 430, 435, 462, 476], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39712860", "vertexSet": [[{"sent_id": 0, "name": "tf.reshape", "pos": [18, 24]}, {"sent_id": 1, "name": "tf.reshape", "pos": [37, 43]}, {"sent_id": 2, "name": "tf.reshape", "pos": [62, 68]}], [{"sent_id": 6, "name": "tf.image", "pos": [205, 209]}], [{"sent_id": 6, "name": "tf.image.decode_jpeg", "pos": [205, 215]}]], "sents": ["This looks like a confusion between the Tensor.set_shape() method and the tf.reshape() operator.", "In this case, you should use tf.reshape() because you are changing the shape of the pool and y tensors:", "The tf.reshape(tensor, shape) operator takes a tensor of any shape, and returns a tensor with the given shape, as long as they have the same number of elements.", "This operator should be used to change the shape of the input tensor.", "The tensor.set_shape(shape) method takes a tensor that might have a partially known or unknown shape, and asserts to TensorFlow that it actually has the given shape.", "This method should be used to provide more information about the shape of a particular tensor.", "It can be used, e.g., when you take the output of an operator that has a data-dependent output shape (such as tf.image.decode_jpeg()) and assert that it has a static shape (e.g.", "based on knowledge about the sizes of images in your dataset).", "In your program, you should replace the calls to set_shape() with something like the following:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 3504, 2066, 1037, 6724, 2090, 1996, 23435, 1012, 2275, 1035, 4338, 1006, 1007, 4118, 1998, 1996, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 6872, 1012, 102, 101, 1999, 2023, 2553, 1010, 2017, 2323, 2224, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 2138, 2017, 2024, 5278, 1996, 4338, 1997, 1996, 4770, 1998, 1061, 23435, 2015, 1024, 102, 101, 1996, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 23435, 1010, 4338, 1007, 6872, 3138, 1037, 23435, 1997, 2151, 4338, 1010, 1998, 5651, 1037, 23435, 2007, 1996, 2445, 4338, 1010, 2004, 2146, 2004, 2027, 2031, 1996, 2168, 2193, 1997, 3787, 1012, 102, 101, 2023, 6872, 2323, 2022, 2109, 2000, 2689, 1996, 4338, 1997, 1996, 7953, 23435, 1012, 102, 101, 1996, 23435, 1012, 2275, 1035, 4338, 1006, 4338, 1007, 4118, 3138, 1037, 23435, 2008, 2453, 2031, 1037, 6822, 2124, 2030, 4242, 4338, 1010, 1998, 19514, 2000, 23435, 12314, 2008, 2009, 2941, 2038, 1996, 2445, 4338, 1012, 102, 101, 2023, 4118, 2323, 2022, 2109, 2000, 3073, 2062, 2592, 2055, 1996, 4338, 1997, 1037, 3327, 23435, 1012, 102, 101, 2009, 2064, 2022, 2109, 1010, 1041, 1012, 1043, 1012, 1010, 2043, 2017, 2202, 1996, 6434, 1997, 2019, 6872, 2008, 2038, 1037, 2951, 1011, 7790, 6434, 4338, 1006, 2107, 2004, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 1006, 1007, 1007, 1998, 20865, 2008, 2009, 2038, 1037, 10763, 4338, 1006, 1041, 1012, 1043, 1012, 102, 101, 2241, 2006, 3716, 2055, 1996, 10826, 1997, 4871, 1999, 2115, 2951, 13462, 1007, 1012, 102, 101, 1999, 2115, 2565, 1010, 2017, 2323, 5672, 1996, 4455, 2000, 2275, 1035, 4338, 1006, 1007, 2007, 2242, 2066, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 60, 102, 118, 156, 175, 232, 248, 271, 285], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50068955", "vertexSet": [[{"sent_id": 0, "name": "tf.metrics.auc", "pos": [4, 12]}, {"sent_id": 3, "name": "tf.metrics.auc", "pos": [94, 102]}, {"sent_id": 12, "name": "tf.metrics.auc", "pos": [335, 343]}], [{"sent_id": 1, "name": "tf.keras.metrics.auc", "pos": [60, 71]}], [{"sent_id": 1, "name": "tf.compat.v1.metrics.auc", "pos": [23, 38]}], [{"sent_id": 5, "name": "tf.one_hot", "pos": [177, 183]}]], "sents": ["Please note the tf.metrics.auc() mentioned below is obsolete.", "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC.", "Sorry, don't have time to refactor the code below.", "You can use tf.metrics.auc() for this purpose.", "Please note you need the one-hot encoded labels and the predictions for this, and you also need to run the update_op it returns if you're trying to accumulate the AUC over multiple sess.run() commands, see separate section below.", "In your code, you create y_one_hot with tf.one_hot(), and you'd put all this right after accuracy maybe:", "<code>Code Snippet</code>.", "Before you start the training loop, you need to initialize the local variables auc creates as well, maybe right after init.run():", "<code>Code Snippet</code>.", "and then when you run the accuracy, you also need to run the auc with accuracy in sess.run() instead of .eval() like this (untested):", "<code>Code Snippet</code>.", "Accumulation over multiple batches.", "If you do want to use the accumulation feature of tf.metrics.auc() then you also need to take care of resetting the accumulation once you want to start a new calculation.", "For that to happen, you need to collect the local variables created.", "So create the auc like this:", "<code>Code Snippet</code>.", "And when you're done with accumulation, reset the internal variables of auc like this:", "<code>Code Snippet</code>.", "And you also need to make sure to run the auc_update_op each time you run sess.run() like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 3531, 3602, 1996, 1056, 2546, 1012, 12046, 2015, 1012, 8740, 2278, 1006, 1007, 3855, 2917, 2003, 15832, 1012, 102, 101, 2045, 2003, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 12046, 2015, 1012, 8740, 2278, 2004, 1037, 4530, 1011, 1999, 7681, 1010, 2021, 2130, 2008, 2003, 2139, 28139, 12921, 2085, 1010, 1998, 1996, 12832, 2003, 2000, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 12046, 2015, 1012, 8740, 2278, 1012, 102, 101, 3374, 1010, 2123, 1005, 1056, 2031, 2051, 2000, 25416, 18908, 2953, 1996, 3642, 2917, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 12046, 2015, 1012, 8740, 2278, 1006, 1007, 2005, 2023, 3800, 1012, 102, 101, 3531, 3602, 2017, 2342, 1996, 2028, 1011, 2980, 12359, 10873, 1998, 1996, 20932, 2005, 2023, 1010, 1998, 2017, 2036, 2342, 2000, 2448, 1996, 10651, 1035, 6728, 2009, 5651, 2065, 2017, 1005, 2128, 2667, 2000, 27598, 1996, 8740, 2278, 2058, 3674, 7367, 4757, 1012, 2448, 1006, 1007, 10954, 1010, 2156, 3584, 2930, 2917, 1012, 102, 101, 1999, 2115, 3642, 1010, 2017, 3443, 1061, 1035, 2028, 1035, 2980, 2007, 1056, 2546, 1012, 2028, 1035, 2980, 1006, 1007, 1010, 1998, 2017, 1005, 1040, 2404, 2035, 2023, 2157, 2044, 10640, 2672, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2077, 2017, 2707, 1996, 2731, 7077, 1010, 2017, 2342, 2000, 3988, 4697, 1996, 2334, 10857, 8740, 2278, 9005, 2004, 2092, 1010, 2672, 2157, 2044, 1999, 4183, 1012, 2448, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2059, 2043, 2017, 2448, 1996, 10640, 1010, 2017, 2036, 2342, 2000, 2448, 1996, 8740, 2278, 2007, 10640, 1999, 7367, 4757, 1012, 2448, 1006, 1007, 2612, 1997, 1012, 9345, 2140, 1006, 1007, 2066, 2023, 1006, 4895, 22199, 2098, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 20299, 2058, 3674, 14108, 2229, 1012, 102, 101, 2065, 2017, 2079, 2215, 2000, 2224, 1996, 20299, 3444, 1997, 1056, 2546, 1012, 12046, 2015, 1012, 8740, 2278, 1006, 1007, 2059, 2017, 2036, 2342, 2000, 2202, 2729, 1997, 25141, 3436, 1996, 20299, 2320, 2017, 2215, 2000, 2707, 1037, 2047, 17208, 1012, 102, 101, 2005, 2008, 2000, 4148, 1010, 2017, 2342, 2000, 8145, 1996, 2334, 10857, 2580, 1012, 102, 101, 2061, 3443, 1996, 8740, 2278, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2043, 2017, 1005, 2128, 2589, 2007, 20299, 1010, 25141, 1996, 4722, 10857, 1997, 8740, 2278, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2017, 2036, 2342, 2000, 2191, 2469, 2000, 2448, 1996, 8740, 2278, 1035, 10651, 1035, 6728, 2169, 2051, 2017, 2448, 7367, 4757, 1012, 2448, 1006, 1007, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 2, "evidence": [1]}], "na_triple": [[0, 3], [1, 3], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 20, 73, 90, 109, 164, 199, 213, 246, 260, 302, 316, 324, 367, 383, 393, 407, 428, 442, 473, 487], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48347687", "vertexSet": [[{"sent_id": 0, "name": "tf.image.decode_image", "pos": [10, 19]}], [{"sent_id": 0, "name": "tf.image.decode_jpeg", "pos": [21, 31]}]], "sents": ["The issue comes from the fact that you use tf.image.decode_image instead of tf.image.decode_jpeg.", "The first one doesn't return any shape because of some issues described here and here.", "I've written a more extensive answer here."], "sent_idxs": [101, 1996, 3277, 3310, 2013, 1996, 2755, 2008, 2017, 2224, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 3746, 2612, 1997, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 1012, 102, 101, 1996, 2034, 2028, 2987, 1005, 1056, 2709, 2151, 4338, 2138, 1997, 2070, 3314, 2649, 2182, 1998, 2182, 1012, 102, 101, 1045, 1005, 2310, 2517, 1037, 2062, 4866, 3437, 2182, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 33, 53, 65], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41483033", "vertexSet": [[{"sent_id": 0, "name": "tf.summary.filewriter", "pos": [15, 22]}, {"sent_id": 3, "name": "tf.summary.filewriter", "pos": [56, 63]}], [{"sent_id": 0, "name": "tf.train.summarywriter", "pos": [1, 8]}]], "sents": ["tf.train.SummaryWriter is deprecated, instead use tf.summary.FileWriter.", "\u21b3 Adding Summaries to Event Files", "It will be removed after 2016-11-30.", "Instructions for updating: Please switch to tf.summary.FileWriter.", "The interface and behavior is the same; this is just a rename.", "<TF Official Migration Page> \u2733\ufe0e includes all current deprecated/renamed functions \u2733\ufe0e"], "sent_idxs": [101, 1056, 2546, 1012, 3345, 1012, 12654, 15994, 2003, 2139, 28139, 12921, 1010, 2612, 2224, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1012, 102, 101, 100, 5815, 7680, 7849, 3111, 2000, 2724, 6764, 102, 101, 2009, 2097, 2022, 3718, 2044, 2355, 1011, 2340, 1011, 2382, 1012, 102, 101, 8128, 2005, 2039, 16616, 1024, 3531, 6942, 2000, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1012, 102, 101, 1996, 8278, 1998, 5248, 2003, 1996, 2168, 1025, 2023, 2003, 2074, 1037, 14916, 14074, 1012, 102, 101, 1026, 1056, 2546, 2880, 9230, 3931, 1028, 100, 2950, 2035, 2783, 2139, 28139, 12921, 1013, 4096, 4972, 100, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 4]}], "na_triple": [], "sent_ends": [0, 24, 34, 47, 65, 82, 102], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48006315", "vertexSet": [[{"sent_id": 6, "name": "tf.layers.batch_normalization", "pos": [142, 151]}], [{"sent_id": 9, "name": "tf.contrib.layers.batch_norm", "pos": [210, 222]}], [{"sent_id": 1, "name": "tf.nn.batch_normalization", "pos": [25, 35]}, {"sent_id": 5, "name": "tf.nn.batch_normalization", "pos": [119, 129]}, {"sent_id": 12, "name": "tf.nn.batch_normalization", "pos": [304, 314]}, {"sent_id": 13, "name": "tf.nn.batch_normalization", "pos": [357, 367]}], [{"sent_id": 11, "name": "tf.nn.batch_norm_with_global_normalization", "pos": [273, 289]}], [{"sent_id": 3, "name": "tf.nn.fused_batch_norm", "pos": [60, 71]}]], "sents": ["Just to add to the list, there're several more ways to do batch-norm in tensorflow:", "tf.nn.batch_normalization is a low-level op.", "The caller is responsible to handle mean and variance tensors themselves..", "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.", "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.", "tf.nn.batch_normalization accepts tensors of any rank greater than 1..", "tf.layers.batch_normalization is a high-level wrapper over the previous ops.", "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.", "Usually, this should be the default choice for you..", "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).", "The use of it is not recommended because it may be dropped in the future releases..", "tf.nn.batch_norm_with_global_normalization is another deprecated op.", "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future..", "Finally, there's also Keras layer keras.layers.BatchNormalization, which in case of tensorflow backend invokes tf.nn.batch_normalization.."], "sent_idxs": [101, 2074, 2000, 5587, 2000, 1996, 2862, 1010, 2045, 1005, 2128, 2195, 2062, 3971, 2000, 2079, 14108, 1011, 13373, 1999, 23435, 12314, 1024, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 2003, 1037, 2659, 1011, 2504, 6728, 1012, 102, 101, 1996, 20587, 2003, 3625, 2000, 5047, 2812, 1998, 23284, 23435, 2015, 3209, 1012, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 19660, 1035, 14108, 1035, 13373, 2003, 2178, 2659, 1011, 2504, 6728, 1010, 2714, 2000, 1996, 3025, 2028, 1012, 102, 101, 1996, 4489, 2003, 2008, 2009, 1005, 1055, 23569, 27605, 5422, 2005, 1018, 2094, 7953, 23435, 2015, 1010, 2029, 2003, 1996, 5156, 2553, 1999, 9530, 6767, 7630, 3508, 2389, 15756, 6125, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 13385, 23435, 2015, 1997, 2151, 4635, 3618, 2084, 1015, 1012, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 14108, 1035, 3671, 3989, 2003, 1037, 2152, 1011, 2504, 10236, 4842, 2058, 1996, 3025, 23092, 1012, 102, 101, 1996, 5221, 4489, 2003, 2008, 2009, 3138, 2729, 1997, 4526, 1998, 6605, 1996, 2770, 2812, 1998, 23284, 23435, 2015, 1010, 1998, 4455, 1037, 3435, 19660, 6728, 2043, 2825, 1012, 102, 101, 2788, 1010, 2023, 2323, 2022, 1996, 12398, 3601, 2005, 2017, 1012, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 14108, 1035, 13373, 2003, 1996, 2220, 7375, 1997, 14108, 13373, 1010, 2077, 2009, 1005, 1055, 3852, 2000, 1996, 4563, 17928, 1006, 1045, 1012, 1041, 1012, 1010, 1056, 2546, 1012, 9014, 1007, 1012, 102, 101, 1996, 2224, 1997, 2009, 2003, 2025, 6749, 2138, 2009, 2089, 2022, 3333, 1999, 1996, 2925, 7085, 1012, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 13373, 1035, 2007, 1035, 3795, 1035, 3671, 3989, 2003, 2178, 2139, 28139, 12921, 6728, 1012, 102, 101, 2747, 1010, 10284, 1996, 2655, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 1010, 2021, 3497, 2000, 2022, 3333, 1999, 1996, 2925, 1012, 1012, 102, 101, 2633, 1010, 2045, 1005, 1055, 2036, 17710, 8180, 6741, 17710, 8180, 1012, 9014, 1012, 14108, 12131, 9067, 3989, 1010, 2029, 1999, 2553, 1997, 23435, 12314, 2067, 10497, 1999, 6767, 9681, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 1012, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 6, 7, 8, 9, 10]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 6, 7, 8, 9, 10]}, {"r": "S1", "h": 0, "t": 3, "evidence": [0, 6, 7, 8, 11, 12]}, {"r": "S1", "h": 3, "t": 0, "evidence": [0, 6, 7, 8, 11, 12]}, {"r": "S1", "h": 1, "t": 3, "evidence": [0, 9, 10, 11, 12]}, {"r": "S1", "h": 3, "t": 1, "evidence": [0, 9, 10, 11, 12]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1, 2, 6, 7, 8]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1, 2, 6, 7, 8]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1, 2, 9, 10]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1, 2, 9, 10]}, {"r": "S1", "h": 2, "t": 3, "evidence": [0, 1, 2, 11, 12]}, {"r": "S1", "h": 3, "t": 2, "evidence": [0, 1, 2, 11, 12]}, {"r": "S1", "h": 2, "t": 4, "evidence": [0, 1, 2, 3, 4, 5]}, {"r": "S1", "h": 4, "t": 2, "evidence": [0, 1, 2, 3, 4, 5]}, {"r": "S1", "h": 4, "t": 0, "evidence": [0, 3, 4, 6, 7, 8]}, {"r": "S1", "h": 0, "t": 4, "evidence": [0, 3, 4, 6, 7, 8]}, {"r": "S1", "h": 4, "t": 1, "evidence": [0, 3, 4, 9, 10]}, {"r": "S1", "h": 1, "t": 4, "evidence": [0, 3, 4, 9, 10]}, {"r": "S1", "h": 4, "t": 3, "evidence": [0, 3, 4, 11, 12]}, {"r": "S1", "h": 3, "t": 4, "evidence": [0, 3, 4, 11, 12]}], "na_triple": [], "sent_ends": [0, 24, 43, 59, 85, 118, 141, 164, 195, 209, 252, 272, 297, 326, 370], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0]}, {"title": "44654244", "vertexSet": [[{"sent_id": 5, "name": "tf.contrib.learn.estimator", "pos": [106, 118]}], [{"sent_id": 7, "name": "tf.contrib.learn.linearestimator", "pos": [174, 187]}]], "sents": ["It sounds like you're talking about on-line training, i.e.", "continuously train a model with incoming data while simultaneously using it.", "You're right in that you should be able to pick up where you left off and just feed in new data.", "What you'll need is a way to save and load the variables between training sessions.", "You can use a tf.Saver to do this in \"raw\" tensorflow.", "You can also use a tf.contrib.learn.Estimator to do this for you.", "You just give it a model_fn that constructs your model, and a model_dir to save the model in, and it will take care of the rest.", "Of course, there's already a linear model in tf.contrib.learn.LinearEstimator.", "With estimators, you'd just call fit(...) whenever you have new data and it will load your variables and continue running the training steps you've defined."], "sent_idxs": [101, 2009, 4165, 2066, 2017, 1005, 2128, 3331, 2055, 2006, 1011, 2240, 2731, 1010, 1045, 1012, 1041, 1012, 102, 101, 10843, 3345, 1037, 2944, 2007, 14932, 2951, 2096, 7453, 2478, 2009, 1012, 102, 101, 2017, 1005, 2128, 2157, 1999, 2008, 2017, 2323, 2022, 2583, 2000, 4060, 2039, 2073, 2017, 2187, 2125, 1998, 2074, 5438, 1999, 2047, 2951, 1012, 102, 101, 2054, 2017, 1005, 2222, 2342, 2003, 1037, 2126, 2000, 3828, 1998, 7170, 1996, 10857, 2090, 2731, 6521, 1012, 102, 101, 2017, 2064, 2224, 1037, 1056, 2546, 1012, 3828, 2099, 2000, 2079, 2023, 1999, 1000, 6315, 1000, 23435, 12314, 1012, 102, 101, 2017, 2064, 2036, 2224, 1037, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 9765, 9581, 4263, 2000, 2079, 2023, 2005, 2017, 1012, 102, 101, 2017, 2074, 2507, 2009, 1037, 2944, 1035, 1042, 2078, 2008, 9570, 2015, 2115, 2944, 1010, 1998, 1037, 2944, 1035, 16101, 2000, 3828, 1996, 2944, 1999, 1010, 1998, 2009, 2097, 2202, 2729, 1997, 1996, 2717, 1012, 102, 101, 1997, 2607, 1010, 2045, 1005, 1055, 2525, 1037, 7399, 2944, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7399, 4355, 9581, 4263, 1012, 102, 101, 2007, 9765, 9581, 6591, 1010, 2017, 1005, 1040, 2074, 2655, 4906, 1006, 1012, 1012, 1012, 1007, 7188, 2017, 2031, 2047, 2951, 1998, 2009, 2097, 7170, 2115, 10857, 1998, 3613, 2770, 1996, 2731, 4084, 2017, 1005, 2310, 4225, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [5, 6, 7, 8]}, {"r": "S1", "h": 1, "t": 0, "evidence": [5, 6, 7, 8]}], "na_triple": [], "sent_ends": [0, 19, 33, 59, 79, 100, 125, 162, 189, 229], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52055189", "vertexSet": [[{"sent_id": 0, "name": "tf.metrics.average_precision_at_k", "pos": [21, 34]}], [{"sent_id": 0, "name": "tf.metrics.sparse_average_precision_at_k", "pos": [2, 17]}, {"sent_id": 5, "name": "tf.metrics.sparse_average_precision_at_k", "pos": [199, 214]}, {"sent_id": 6, "name": "tf.metrics.sparse_average_precision_at_k", "pos": [262, 277]}], [{"sent_id": 6, "name": "tf.nn", "pos": [225, 230]}], [{"sent_id": 6, "name": "tf.nn.top_k", "pos": [225, 234]}], [{"sent_id": 5, "name": "tf.local_variables", "pos": [177, 183]}]], "sents": ["The tf.metrics.sparse_average_precision_at_k will be replaced by tf.metrics.average_precision_at_k.", "And by browsing the code in tensorflow, you will find that when your inputs are y_true and y_pred, this function will actually transform the y_pred to y_pred_idx, by using top_k function.", "y_true is a tensor of shape (batch_size, num_labels), and y_pred is of shape (batch_size, num_classes)", "You can also see some discussion in this issue, and this example comes from this issue.", "<code>Code Snippet</code>.", "This line stream_vars = [i for i in tf.local_variables()] helps you see the two local_variables which is created in this tf.metrics.sparse_average_precision_at_k function.", "This line tmp_rank = tf.nn.top_k(y_pred,3) in order to helps you understand by changing the value of k ,the prediction index which is used in tf.metrics.sparse_average_precision_at_k .", "You can change the value of k to see the different result, and the tmp_rank represents the index which is used in calculating  the average precision.", "For example:\nwhen k=1, only the first batch match the label, so the average precision at 1 result will be 1/6 = 0.16666666.", "When k=2, the third batch will also match the label, so the average precision at 2 result will be (1+(1/2))/6=0.25."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 12046, 2015, 1012, 20288, 1035, 2779, 1035, 11718, 1035, 2012, 1035, 1047, 2097, 2022, 2999, 2011, 1056, 2546, 1012, 12046, 2015, 1012, 2779, 1035, 11718, 1035, 2012, 1035, 1047, 1012, 102, 101, 1998, 2011, 11347, 2075, 1996, 3642, 1999, 23435, 12314, 1010, 2017, 2097, 2424, 2008, 2043, 2115, 20407, 2024, 1061, 1035, 2995, 1998, 1061, 1035, 3653, 2094, 1010, 2023, 3853, 2097, 2941, 10938, 1996, 1061, 1035, 3653, 2094, 2000, 1061, 1035, 3653, 2094, 1035, 8909, 2595, 1010, 2011, 2478, 2327, 1035, 1047, 3853, 1012, 102, 101, 1061, 1035, 2995, 2003, 1037, 23435, 1997, 4338, 1006, 14108, 1035, 2946, 1010, 16371, 2213, 1035, 10873, 1007, 1010, 1998, 1061, 1035, 3653, 2094, 2003, 1997, 4338, 1006, 14108, 1035, 2946, 1010, 16371, 2213, 1035, 4280, 1007, 102, 101, 2017, 2064, 2036, 2156, 2070, 6594, 1999, 2023, 3277, 1010, 1998, 2023, 2742, 3310, 2013, 2023, 3277, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2240, 5460, 1035, 13075, 2015, 1027, 1031, 1045, 2005, 1045, 1999, 1056, 2546, 1012, 2334, 1035, 10857, 1006, 1007, 1033, 7126, 2017, 2156, 1996, 2048, 2334, 1035, 10857, 2029, 2003, 2580, 1999, 2023, 1056, 2546, 1012, 12046, 2015, 1012, 20288, 1035, 2779, 1035, 11718, 1035, 2012, 1035, 1047, 3853, 1012, 102, 101, 2023, 2240, 1056, 8737, 1035, 4635, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 2327, 1035, 1047, 1006, 1061, 1035, 3653, 2094, 1010, 1017, 1007, 1999, 2344, 2000, 7126, 2017, 3305, 2011, 5278, 1996, 3643, 1997, 1047, 1010, 1996, 17547, 5950, 2029, 2003, 2109, 1999, 1056, 2546, 1012, 12046, 2015, 1012, 20288, 1035, 2779, 1035, 11718, 1035, 2012, 1035, 1047, 1012, 102, 101, 2017, 2064, 2689, 1996, 3643, 1997, 1047, 2000, 2156, 1996, 2367, 2765, 1010, 1998, 1996, 1056, 8737, 1035, 4635, 5836, 1996, 5950, 2029, 2003, 2109, 1999, 20177, 1996, 2779, 11718, 1012, 102, 101, 2005, 2742, 1024, 2043, 1047, 1027, 1015, 1010, 2069, 1996, 2034, 14108, 2674, 1996, 3830, 1010, 2061, 1996, 2779, 11718, 2012, 1015, 2765, 2097, 2022, 1015, 1013, 1020, 1027, 1014, 1012, 27407, 28756, 28756, 1012, 102, 101, 2043, 1047, 1027, 1016, 1010, 1996, 2353, 14108, 2097, 2036, 2674, 1996, 3830, 1010, 2061, 1996, 2779, 11718, 2012, 1016, 2765, 2097, 2022, 1006, 1015, 1009, 1006, 1015, 1013, 1016, 1007, 1007, 1013, 1020, 1027, 1014, 1012, 2423, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 36, 91, 130, 150, 164, 217, 279, 312, 349, 390], "sent_pos": [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37603765", "vertexSet": [[{"sent_id": 7, "name": "tf.get_variable", "pos": [145, 151]}], [{"sent_id": 4, "name": "tf.variable", "pos": [92, 96]}, {"sent_id": 6, "name": "tf.variable", "pos": [129, 133]}, {"sent_id": 7, "name": "tf.variable", "pos": [155, 159]}], [{"sent_id": 4, "name": "tf.variable_scope", "pos": [92, 98]}, {"sent_id": 6, "name": "tf.variable_scope", "pos": [129, 135]}]], "sents": ["From your output, it looks like each tower is creating a new set of variables:", "<code>Code Snippet</code>.", "Note that there are twice as many variables (16 vs. 8) in the output for the second tower, and the first 8 variables have no gradients in the second tower.", "There are two potential fixes:", "Ensure that your TOWER_LOSS() function includes with tf.variable_scope(...): blocks.", "See  the cifar10.inference() function for an example.", "You need to create a tf.variable_scope() to use variable sharing.", "Use tf.get_variable() instead of tf.Variable() to ensure that the variables are shared where possible.", "Here's an example of how this is done in CIFAR-10.", "For more details on sharing variables, see to how-to guide."], "sent_idxs": [101, 2013, 2115, 6434, 1010, 2009, 3504, 2066, 2169, 3578, 2003, 4526, 1037, 2047, 2275, 1997, 10857, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 2045, 2024, 3807, 2004, 2116, 10857, 1006, 2385, 5443, 1012, 1022, 1007, 1999, 1996, 6434, 2005, 1996, 2117, 3578, 1010, 1998, 1996, 2034, 1022, 10857, 2031, 2053, 17978, 2015, 1999, 1996, 2117, 3578, 1012, 102, 101, 2045, 2024, 2048, 4022, 8081, 2229, 1024, 102, 101, 5676, 2008, 2115, 3578, 1035, 3279, 1006, 1007, 3853, 2950, 2007, 1056, 2546, 1012, 8023, 1035, 9531, 1006, 1012, 1012, 1012, 1007, 1024, 5991, 1012, 102, 101, 2156, 1996, 25022, 14971, 10790, 1012, 28937, 1006, 1007, 3853, 2005, 2019, 2742, 1012, 102, 101, 2017, 2342, 2000, 3443, 1037, 1056, 2546, 1012, 8023, 1035, 9531, 1006, 1007, 2000, 2224, 8023, 6631, 1012, 102, 101, 2224, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 2612, 1997, 1056, 2546, 1012, 8023, 1006, 1007, 2000, 5676, 2008, 1996, 10857, 2024, 4207, 2073, 2825, 1012, 102, 101, 2182, 1005, 1055, 2019, 2742, 1997, 2129, 2023, 2003, 2589, 1999, 25022, 14971, 1011, 2184, 1012, 102, 101, 2005, 2062, 4751, 2006, 6631, 10857, 1010, 2156, 2000, 2129, 1011, 2000, 5009, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [7]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 33, 71, 80, 107, 123, 143, 172, 190, 206], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50049491", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.layer", "pos": [6, 13]}], [{"sent_id": 0, "name": "tf.layers", "pos": [1, 5]}], [{"sent_id": 3, "name": "tf.keras.model", "pos": [154, 161]}], [{"sent_id": 3, "name": "tf.keras.layers", "pos": [146, 153]}]], "sents": ["tf.layers and tf.keras.layer classes are generally interchangeable and in fact at head (and thus by the next release - 1.9), the former actually inherits from the latter.", "TensorFlow is moving towards consolidating on tf.keras APIs for constructing models as that makes state ownership more explicit (e.g., parameters are \"owned\" by the Layer object, as opposed to the functional style where all model parameters are put in a \"collection\" associated with the complete graph).", "This style works well for both eager execution and graph construction (support for eager execution is improving with every release).", "I'd recommend using tf.keras.layers and tf.keras.Model.", "Some examples that you may find useful:", "MNIST in the tensorflow/models repository.", "The programmer's guide.", "Other eager execution samples (where the exact same model definition works for both graph execution and eager execution)..", "Not all existing TensorFlow examples have been moved to this style, but they slowly will.", "Hope that helps."], "sent_idxs": [101, 1056, 2546, 1012, 9014, 1998, 1056, 2546, 1012, 17710, 8180, 1012, 6741, 4280, 2024, 3227, 8989, 3085, 1998, 1999, 2755, 2012, 2132, 1006, 1998, 2947, 2011, 1996, 2279, 2713, 1011, 1015, 1012, 1023, 1007, 1010, 1996, 2280, 2941, 22490, 2015, 2013, 1996, 3732, 1012, 102, 101, 23435, 12314, 2003, 3048, 2875, 9530, 19454, 8524, 3436, 2006, 1056, 2546, 1012, 17710, 8180, 17928, 2015, 2005, 15696, 4275, 2004, 2008, 3084, 2110, 6095, 2062, 13216, 1006, 1041, 1012, 1043, 1012, 1010, 11709, 2024, 1000, 3079, 1000, 2011, 1996, 6741, 4874, 1010, 2004, 4941, 2000, 1996, 8360, 2806, 2073, 2035, 2944, 11709, 2024, 2404, 1999, 1037, 1000, 3074, 1000, 3378, 2007, 1996, 3143, 10629, 1007, 1012, 102, 101, 2023, 2806, 2573, 2092, 2005, 2119, 9461, 7781, 1998, 10629, 2810, 1006, 2490, 2005, 9461, 7781, 2003, 9229, 2007, 2296, 2713, 1007, 1012, 102, 101, 1045, 1005, 1040, 16755, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1998, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1012, 102, 101, 2070, 4973, 2008, 2017, 2089, 2424, 6179, 1024, 102, 101, 24098, 2923, 1999, 1996, 23435, 12314, 1013, 4275, 22409, 1012, 102, 101, 1996, 20273, 1005, 1055, 5009, 1012, 102, 101, 2060, 9461, 7781, 8168, 1006, 2073, 1996, 6635, 2168, 2944, 6210, 2573, 2005, 2119, 10629, 7781, 1998, 9461, 7781, 1007, 1012, 1012, 102, 101, 2025, 2035, 4493, 23435, 12314, 4973, 2031, 2042, 2333, 2000, 2023, 2806, 1010, 2021, 2027, 3254, 2097, 1012, 102, 101, 3246, 2008, 7126, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 46, 115, 140, 163, 173, 185, 193, 217, 237, 243], "sent_pos": [0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58574655", "vertexSet": [[{"sent_id": 4, "name": "tf.compat.v1.variable_scope", "pos": [93, 106]}], [{"sent_id": 2, "name": "tf.variable_scope", "pos": [38, 44]}], [{"sent_id": 1, "name": "tf.math", "pos": [28, 32]}]], "sents": ["TensorFlow 2.0 cleaned up some of the APIs.", "Mathematical functions such as squared_difference() are now under tf.math.", "There is no tf.variable_scope() in TensorFlow 2.0.", "I suggest reading this post with examples on how to migrate your code to TF2.", "If you want your code to be compatible with older versions of TensorFlow, you can use tf.compat.v1.variable_scope()"], "sent_idxs": [101, 23435, 12314, 1016, 1012, 1014, 12176, 2039, 2070, 1997, 1996, 17928, 2015, 1012, 102, 101, 8045, 4972, 2107, 2004, 19942, 1035, 4489, 1006, 1007, 2024, 2085, 2104, 1056, 2546, 1012, 8785, 1012, 102, 101, 2045, 2003, 2053, 1056, 2546, 1012, 8023, 1035, 9531, 1006, 1007, 1999, 23435, 12314, 1016, 1012, 1014, 1012, 102, 101, 1045, 6592, 3752, 2023, 2695, 2007, 4973, 2006, 2129, 2000, 22806, 2115, 3642, 2000, 1056, 2546, 2475, 1012, 102, 101, 2065, 2017, 2215, 2115, 3642, 2000, 2022, 11892, 2007, 3080, 4617, 1997, 23435, 12314, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 8023, 1035, 9531, 1006, 1007, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2, 4]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 34, 54, 74, 109], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}, {"title": "55796211", "vertexSet": [[{"sent_id": 5, "name": "tf.sparsetensor", "pos": [75, 81]}], [{"sent_id": 0, "name": "tf.sparse_to_dense", "pos": [11, 19]}, {"sent_id": 4, "name": "tf.sparse_to_dense", "pos": [56, 64]}], [{"sent_id": 5, "name": "tf.sparse.to_dense", "pos": [84, 92]}]], "sents": ["A much better way to accomplish this is to use tf.sparse_to_dense.", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "However, tf.sparse_to_dense is deprecated recently.", "Thus, use tf.SparseTensor and then use tf.sparse.to_dense to get the same result as above"], "sent_idxs": [101, 1037, 2172, 2488, 2126, 2000, 14570, 2023, 2003, 2000, 2224, 1056, 2546, 1012, 20288, 1035, 2000, 1035, 9742, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 1010, 1056, 2546, 1012, 20288, 1035, 2000, 1035, 9742, 2003, 2139, 28139, 12921, 3728, 1012, 102, 101, 2947, 1010, 2224, 1056, 2546, 1012, 20288, 25808, 2953, 1998, 2059, 2224, 1056, 2546, 1012, 20288, 1012, 2000, 1035, 9742, 2000, 2131, 1996, 2168, 2765, 2004, 2682, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [4, 5]}, {"r": "S1", "h": 0, "t": 1, "evidence": [4, 5]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 35, 39, 53, 71, 100], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37624060", "vertexSet": [[{"sent_id": 0, "name": "tf.tensor", "pos": [7, 11]}], [{"sent_id": 1, "name": "tf.variable", "pos": [39, 43]}, {"sent_id": 2, "name": "tf.variable", "pos": [72, 76]}], [{"sent_id": 11, "name": "tf.assign", "pos": [267, 271]}], [{"sent_id": 9, "name": "tf.import_graph_def", "pos": [223, 231]}], [{"sent_id": 2, "name": "tf.trainable_variables", "pos": [82, 89]}]], "sents": ["Most TensorFlow tensors (tf.Tensor objects) are immutable, so you cannot simply assign a value to them.", "However, if you created the tensor as a tf.Variable, you can assign a value to it by calling Variable.assign().", "The code you have unnecessarily converts a tf.Variable object (from the list of tf.trainable_variables()) into a string name.", "Instead, you can do the following:", "<code>Code Snippet</code>.", "However, according to your comment, you have multiple graphs (i.e.", "self.graph and graph are different), so the general solution I wrote above won't work.", "In this case, you have two options:", "Get the variable by name in the other graph (N.B.", "this will only work if graph_2.get_collection('trainable_variables') has been populated; it won't work if you used tf.import_graph_def() to build the graph):", "<code>Code Snippet</code>.", "Get the tensor by name in the other graph, and use tf.assign():", "<code>Code Snippet</code>."], "sent_idxs": [101, 2087, 23435, 12314, 23435, 2015, 1006, 1056, 2546, 1012, 23435, 5200, 1007, 2024, 10047, 28120, 3085, 1010, 2061, 2017, 3685, 3432, 23911, 1037, 3643, 2000, 2068, 1012, 102, 101, 2174, 1010, 2065, 2017, 2580, 1996, 23435, 2004, 1037, 1056, 2546, 1012, 8023, 1010, 2017, 2064, 23911, 1037, 3643, 2000, 2009, 2011, 4214, 8023, 1012, 23911, 1006, 1007, 1012, 102, 101, 1996, 3642, 2017, 2031, 4895, 2638, 9623, 22740, 2135, 19884, 1037, 1056, 2546, 1012, 8023, 4874, 1006, 2013, 1996, 2862, 1997, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 1007, 2046, 1037, 5164, 2171, 1012, 102, 101, 2612, 1010, 2017, 2064, 2079, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 1010, 2429, 2000, 2115, 7615, 1010, 2017, 2031, 3674, 19287, 1006, 1045, 1012, 1041, 1012, 102, 101, 2969, 1012, 10629, 1998, 10629, 2024, 2367, 1007, 1010, 2061, 1996, 2236, 5576, 1045, 2626, 2682, 2180, 1005, 1056, 2147, 1012, 102, 101, 1999, 2023, 2553, 1010, 2017, 2031, 2048, 7047, 1024, 102, 101, 2131, 1996, 8023, 2011, 2171, 1999, 1996, 2060, 10629, 1006, 1050, 1012, 1038, 1012, 102, 101, 2023, 2097, 2069, 2147, 2065, 10629, 1035, 1016, 1012, 2131, 1035, 3074, 1006, 1005, 3345, 3085, 1035, 10857, 1005, 1007, 2038, 2042, 10357, 1025, 2009, 2180, 1005, 1056, 2147, 2065, 2017, 2109, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 2000, 3857, 1996, 10629, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2131, 1996, 23435, 2011, 2171, 1999, 1996, 2060, 10629, 1010, 1998, 2224, 1056, 2546, 1012, 23911, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 29, 60, 98, 108, 122, 140, 163, 174, 190, 240, 254, 275, 289], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62771771", "vertexSet": [[{"sent_id": 3, "name": "tf.keras.layers.lstm", "pos": [106, 117]}], [{"sent_id": 2, "name": " tf.nn.dymanic_rnn", "pos": [62, 74]}], [{"sent_id": 4, "name": "tf.keras.layers.lstmcell", "pos": [163, 175]}]], "sents": ["Why do you use the LSTM cell from compat.v1?", "I would imagine this leads to compatibility issues.", "Most importantly, those \"pure Tensorflow\" RNN cells are not made to be used with the keras RNN anyway -- they were used with tf.nn.dymanic_rnn for example, which is now deprecated and also found only in the compat.v1 module.", "I would recommend that you simply use tf.keras.layers.LSTM directly as it's much faster anyway -- it allows for the use of highly optimized GPU kernels.", "Alternatively, you can replace the compat.v1.LSTMCell with a tf.keras.layers.LSTMCell and put this into the RNN."], "sent_idxs": [101, 2339, 2079, 2017, 2224, 1996, 1048, 3367, 2213, 3526, 2013, 4012, 4502, 2102, 1012, 1058, 2487, 1029, 102, 101, 1045, 2052, 5674, 2023, 5260, 2000, 21778, 3314, 1012, 102, 101, 2087, 14780, 1010, 2216, 1000, 5760, 23435, 12314, 1000, 29300, 2078, 4442, 2024, 2025, 2081, 2000, 2022, 2109, 2007, 1996, 17710, 8180, 29300, 2078, 4312, 1011, 1011, 2027, 2020, 2109, 2007, 1056, 2546, 1012, 1050, 2078, 1012, 1040, 17906, 2594, 1035, 29300, 2078, 2005, 2742, 1010, 2029, 2003, 2085, 2139, 28139, 12921, 1998, 2036, 2179, 2069, 1999, 1996, 4012, 4502, 2102, 1012, 1058, 2487, 11336, 1012, 102, 101, 1045, 2052, 16755, 2008, 2017, 3432, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 1048, 3367, 2213, 3495, 2004, 2009, 1005, 1055, 2172, 5514, 4312, 1011, 1011, 2009, 4473, 2005, 1996, 2224, 1997, 3811, 23569, 27605, 5422, 14246, 2226, 16293, 2015, 1012, 102, 101, 14084, 1010, 2017, 2064, 5672, 1996, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 1048, 3367, 12458, 5349, 2007, 1037, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 1048, 3367, 12458, 5349, 1998, 2404, 2023, 2046, 1996, 29300, 2078, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2, 3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 30, 98, 143, 184], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48618541", "vertexSet": [[{"sent_id": 6, "name": "tf.float64", "pos": [140, 145]}], [{"sent_id": 3, "name": "tf.matmul", "pos": [74, 80]}], [{"sent_id": 1, "name": "tf.float32", "pos": [22, 27]}]], "sents": ["I just compare the results by tensorflow and numpy.", "Since you used dtype=tf.float32 for X and y, I will use np.float32 for the numpy example as follows:", "<code>Code Snippet</code>.", "Now let's try to compare the results by tf.matmul(XT, X) (tensorflow) and X.T.dot(X) (numpy):", "<code>Code Snippet</code>.", "So this is the problem of float's precision.", "If you change the precision to tf.float64 and np.float64, you will have the same result for theta."], "sent_idxs": [101, 1045, 2074, 12826, 1996, 3463, 2011, 23435, 12314, 1998, 16371, 8737, 2100, 1012, 102, 101, 2144, 2017, 2109, 26718, 18863, 1027, 1056, 2546, 1012, 14257, 16703, 2005, 1060, 1998, 1061, 1010, 1045, 2097, 2224, 27937, 1012, 14257, 16703, 2005, 1996, 16371, 8737, 2100, 2742, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 2292, 1005, 1055, 3046, 2000, 12826, 1996, 3463, 2011, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 2102, 1010, 1060, 1007, 1006, 23435, 12314, 1007, 1998, 1060, 1012, 1056, 1012, 11089, 1006, 1060, 1007, 1006, 16371, 8737, 2100, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2061, 2023, 2003, 1996, 3291, 1997, 14257, 1005, 1055, 11718, 1012, 102, 101, 2065, 2017, 2689, 1996, 11718, 2000, 1056, 2546, 1012, 14257, 21084, 1998, 27937, 1012, 14257, 21084, 1010, 2017, 2097, 2031, 1996, 2168, 2765, 2005, 23963, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 49, 63, 106, 120, 133, 161], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55318851", "vertexSet": [[{"sent_id": 4, "name": "tf.compat.v1.logging", "pos": [111, 122]}], [{"sent_id": 0, "name": "tf.logging", "pos": [1, 5]}, {"sent_id": 3, "name": "tf.logging", "pos": [85, 89]}, {"sent_id": 4, "name": "tf.logging", "pos": [106, 110]}], [{"sent_id": 2, "name": "tf.math", "pos": [76, 80]}]], "sents": ["tf.logging was for Logging and Summary Operations and in TF 2.0 it has been removed in favor of the open-source absl-py, and to make the main tf.", "* namespace has functions that will be used more often.", "In TF.2 lesser used functions are gone or moved into sub-packages like tf.math", "So instead of tf.logging you could:", "tf_upgrade_v2 will upgrade script and changes tf.logging to tf.compat.v1.logging.", "Python logging module can be used instead.", "Import absl-py library."], "sent_idxs": [101, 1056, 2546, 1012, 15899, 2001, 2005, 15899, 1998, 12654, 3136, 1998, 1999, 1056, 2546, 1016, 1012, 1014, 2009, 2038, 2042, 3718, 1999, 5684, 1997, 1996, 2330, 1011, 3120, 14689, 2140, 1011, 1052, 2100, 1010, 1998, 2000, 2191, 1996, 2364, 1056, 2546, 1012, 102, 101, 1008, 3415, 15327, 2038, 4972, 2008, 2097, 2022, 2109, 2062, 2411, 1012, 102, 101, 1999, 1056, 2546, 1012, 1016, 8276, 2109, 4972, 2024, 2908, 2030, 2333, 2046, 4942, 1011, 14555, 2066, 1056, 2546, 1012, 8785, 102, 101, 2061, 2612, 1997, 1056, 2546, 1012, 15899, 2017, 2071, 1024, 102, 101, 1056, 2546, 1035, 12200, 1035, 1058, 2475, 2097, 12200, 5896, 1998, 3431, 1056, 2546, 1012, 15899, 2000, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 15899, 1012, 102, 101, 18750, 15899, 11336, 2064, 2022, 2109, 2612, 1012, 102, 101, 12324, 14689, 2140, 1011, 1052, 2100, 3075, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 44, 58, 81, 93, 124, 134, 144], "sent_pos": [0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35530400", "vertexSet": [[{"sent_id": 0, "name": "tf.decode_csv", "pos": [25, 33]}], [{"sent_id": 0, "name": "tf.read_file", "pos": [2, 8]}], [{"sent_id": 2, "name": "tf.textlinereader", "pos": [74, 81]}], [{"sent_id": 9, "name": "tf.train", "pos": [221, 225]}], [{"sent_id": 9, "name": "tf.train.batch", "pos": [221, 227]}]], "sents": ["The tf.read_file() op reads the entire contents of the given file into a single string, whereas tf.decode_csv() op expects each element of its input to be a single record (i.e.", "one line).", "Therefore you need something that reads one line at a time, which the tf.TextLineReader supports.", "Using a reader is slightly more complicated than using a simple op, because it's designed for reading large multi-file datasets with a lot of flexibility in how the files are chosen.", "You can see the tutorial \nfor a complete explanation, but the following example code should help to get you started:", "<code>Code Snippet</code>.", "Now col1 and col2 represent single values.", "If you evaluate them, you'll get the contents of the next line:", "<code>Code Snippet</code>.", "If instead you want to batch the columns, you can use tf.train.batch():", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 3191, 1035, 5371, 1006, 1007, 6728, 9631, 1996, 2972, 8417, 1997, 1996, 2445, 5371, 2046, 1037, 2309, 5164, 1010, 6168, 1056, 2546, 1012, 21933, 3207, 1035, 20116, 2615, 1006, 1007, 6728, 24273, 2169, 5783, 1997, 2049, 7953, 2000, 2022, 1037, 2309, 2501, 1006, 1045, 1012, 1041, 1012, 102, 101, 2028, 2240, 1007, 1012, 102, 101, 3568, 2017, 2342, 2242, 2008, 9631, 2028, 2240, 2012, 1037, 2051, 1010, 2029, 1996, 1056, 2546, 1012, 3793, 20660, 13775, 2121, 6753, 1012, 102, 101, 2478, 1037, 8068, 2003, 3621, 2062, 8552, 2084, 2478, 1037, 3722, 6728, 1010, 2138, 2009, 1005, 1055, 2881, 2005, 3752, 2312, 4800, 1011, 5371, 2951, 13462, 2015, 2007, 1037, 2843, 1997, 16991, 1999, 2129, 1996, 6764, 2024, 4217, 1012, 102, 101, 2017, 2064, 2156, 1996, 14924, 4818, 2005, 1037, 3143, 7526, 1010, 2021, 1996, 2206, 2742, 3642, 2323, 2393, 2000, 2131, 2017, 2318, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 8902, 2487, 1998, 8902, 2475, 5050, 2309, 5300, 1012, 102, 101, 2065, 2017, 16157, 2068, 1010, 2017, 1005, 2222, 2131, 1996, 8417, 1997, 1996, 2279, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2612, 2017, 2215, 2000, 14108, 1996, 7753, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 3345, 1012, 14108, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 2]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 2]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 3], [1, 4], [2, 0], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 53, 59, 84, 125, 150, 164, 176, 194, 208, 231, 245], "sent_pos": [0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42965020", "vertexSet": [[{"sent_id": 0, "name": "tf.tensor", "pos": [24, 28]}], [{"sent_id": 5, "name": "tf.variable", "pos": [199, 203]}], [{"sent_id": 3, "name": "tf.session", "pos": [110, 114]}], [{"sent_id": 5, "name": "tf.fifoqueue", "pos": [217, 224]}]], "sents": ["For the sake of concreteness, I'm assuming that func1 and func2 in your example are tf.Tensor objects.", "In \"code 1\", the value of func1 will be computed once: the same value will be returned to the user and used to compute func2.", "In \"code 2\", the value of func1 will be computed twice: once in each call to sess.run().", "TensorFlow does not cache intermediate tensor values between calls to tf.Session.run().", "The reason for this is simple: in a typical neural network workload (training or inference), most intermediate values become invalid between runs of the graph, because they are a function of the input (which changes from step to step) and the current state (which changes during training).", "If you want to save a value for later use, you must explicitly assign it to a tf.Variable, or store it in some other stateful object, such as a tf.FIFOQueue."], "sent_idxs": [101, 2005, 1996, 8739, 1997, 5509, 2791, 1010, 1045, 1005, 1049, 10262, 2008, 4569, 2278, 2487, 1998, 4569, 2278, 2475, 1999, 2115, 2742, 2024, 1056, 2546, 1012, 23435, 5200, 1012, 102, 101, 1999, 1000, 3642, 1015, 1000, 1010, 1996, 3643, 1997, 4569, 2278, 2487, 2097, 2022, 24806, 2320, 1024, 1996, 2168, 3643, 2097, 2022, 2513, 2000, 1996, 5310, 1998, 2109, 2000, 24134, 4569, 2278, 2475, 1012, 102, 101, 1999, 1000, 3642, 1016, 1000, 1010, 1996, 3643, 1997, 4569, 2278, 2487, 2097, 2022, 24806, 3807, 1024, 2320, 1999, 2169, 2655, 2000, 7367, 4757, 1012, 2448, 1006, 1007, 1012, 102, 101, 23435, 12314, 2515, 2025, 17053, 7783, 23435, 5300, 2090, 4455, 2000, 1056, 2546, 1012, 5219, 1012, 2448, 1006, 1007, 1012, 102, 101, 1996, 3114, 2005, 2023, 2003, 3722, 1024, 1999, 1037, 5171, 15756, 2897, 2147, 11066, 1006, 2731, 2030, 28937, 1007, 1010, 2087, 7783, 5300, 2468, 19528, 2090, 3216, 1997, 1996, 10629, 1010, 2138, 2027, 2024, 1037, 3853, 1997, 1996, 7953, 1006, 2029, 3431, 2013, 3357, 2000, 3357, 1007, 1998, 1996, 2783, 2110, 1006, 2029, 3431, 2076, 2731, 1007, 1012, 102, 101, 2065, 2017, 2215, 2000, 3828, 1037, 3643, 2005, 2101, 2224, 1010, 2017, 2442, 12045, 23911, 2009, 2000, 1037, 1056, 2546, 1012, 8023, 1010, 2030, 3573, 2009, 1999, 2070, 2060, 2110, 3993, 4874, 1010, 2107, 2004, 1037, 1056, 2546, 1012, 10882, 14876, 4226, 5657, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 3, 4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 3, 4, 5]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 31, 67, 98, 120, 180, 226], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0]}, {"title": "63669319", "vertexSet": [[{"sent_id": 1, "name": "tf.autograph.to_graph", "pos": [23, 32]}], [{"sent_id": 2, "name": "tf.function", "pos": [53, 57]}, {"sent_id": 4, "name": "tf.function", "pos": [118, 122]}, {"sent_id": 5, "name": "tf.function", "pos": [147, 151]}]], "sents": ["@nessuno 's answer is excellent and it helps me a lot.", "While, actually the doc tf.autograph.to_graph explains the relation ship between autograpsh and tf.funciton directly:", "Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code.", "It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired.", "Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable.", "Internally, tf.function uses to_graph."], "sent_idxs": [101, 1030, 23384, 27819, 1005, 1055, 3437, 2003, 6581, 1998, 2009, 7126, 2033, 1037, 2843, 1012, 102, 101, 2096, 1010, 2941, 1996, 9986, 1056, 2546, 1012, 8285, 14413, 1012, 2000, 1035, 10629, 7607, 1996, 7189, 2911, 2090, 8285, 17643, 4523, 2232, 1998, 1056, 2546, 1012, 4569, 26243, 2239, 3495, 1024, 102, 101, 4406, 1056, 2546, 1012, 3853, 1010, 2000, 1035, 10629, 2003, 1037, 2659, 1011, 2504, 9099, 22090, 2099, 2008, 19884, 18750, 3642, 2000, 23435, 12314, 10629, 3642, 1012, 102, 101, 2009, 2515, 2025, 10408, 2151, 6187, 8450, 1010, 8023, 2968, 2030, 3443, 2151, 5025, 23092, 1010, 1998, 2003, 2190, 2109, 2073, 3618, 2491, 2058, 1996, 7013, 23435, 12314, 10629, 2003, 9059, 1012, 102, 101, 2178, 4489, 2013, 1056, 2546, 1012, 3853, 2003, 2008, 2000, 1035, 10629, 2097, 2025, 10236, 1996, 10629, 2046, 1037, 23435, 12314, 3853, 2030, 1037, 18750, 2655, 3085, 1012, 102, 101, 16058, 1010, 1056, 2546, 1012, 3853, 3594, 2000, 1035, 10629, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4, 5]}], "na_triple": [], "sent_ends": [0, 17, 51, 80, 114, 144, 157], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]}, {"title": "47143624", "vertexSet": [[{"sent_id": 6, "name": "tf.nn.batch_normalization", "pos": [134, 144]}], [{"sent_id": 6, "name": "tf.layers.batch_normalization", "pos": [118, 127]}]], "sents": ["The original batch-norm paper prescribes using the batch-norm before ReLU activation.", "But there is evidence that it's probably better to use batchnorm after the activation.", "Here's a comment on Keras GitHub by Francois Chollet:", "...", "I can guarantee that recent code written by Christian [Szegedy] \n  applies relu\n  before BN.", "It is still occasionally a topic of debate, though.", "To your second question: in tensorflow, you can use a high-level tf.layers.batch_normalization function, or a low-level tf.nn.batch_normalization."], "sent_idxs": [101, 1996, 2434, 14108, 1011, 13373, 3259, 3653, 29234, 2015, 2478, 1996, 14108, 1011, 13373, 2077, 2128, 7630, 13791, 1012, 102, 101, 2021, 2045, 2003, 3350, 2008, 2009, 1005, 1055, 2763, 2488, 2000, 2224, 14108, 12131, 2213, 2044, 1996, 13791, 1012, 102, 101, 2182, 1005, 1055, 1037, 7615, 2006, 17710, 8180, 21025, 2705, 12083, 2011, 8173, 16480, 22592, 1024, 102, 101, 1012, 1012, 1012, 102, 101, 1045, 2064, 11302, 2008, 3522, 3642, 2517, 2011, 3017, 1031, 1055, 4371, 5999, 2100, 1033, 12033, 2128, 7630, 2077, 24869, 1012, 102, 101, 2009, 2003, 2145, 5681, 1037, 8476, 1997, 5981, 1010, 2295, 1012, 102, 101, 2000, 2115, 2117, 3160, 1024, 1999, 23435, 12314, 1010, 2017, 2064, 2224, 1037, 2152, 1011, 2504, 1056, 2546, 1012, 9014, 1012, 14108, 1035, 3671, 3989, 3853, 1010, 2030, 1037, 2659, 1011, 2504, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [6]}, {"r": "S1", "h": 0, "t": 1, "evidence": [6]}], "na_triple": [], "sent_ends": [0, 21, 42, 60, 65, 88, 101, 146], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "59953445", "vertexSet": [[{"sent_id": 1, "name": "tf.random_normal", "pos": [27, 33]}], [{"sent_id": 1, "name": "tf.random.normal", "pos": [19, 25]}]], "sents": ["Tensorflow 2.0 comes with new aliases for random_normal.", "Using tf.random.normal instead of tf.random_normal should execute successfully."], "sent_idxs": [101, 23435, 12314, 1016, 1012, 1014, 3310, 2007, 2047, 14593, 2229, 2005, 6721, 1035, 3671, 1012, 102, 101, 2478, 1056, 2546, 1012, 6721, 1012, 3671, 2612, 1997, 1056, 2546, 1012, 6721, 1035, 3671, 2323, 15389, 5147, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 17, 38], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}, {"title": "42774074", "vertexSet": [[{"sent_id": 5, "name": "tf.nn.relu", "pos": [128, 136]}], [{"sent_id": 5, "name": "tf.contrib.layers.relu", "pos": [103, 114]}]], "sents": ["They are not the same thing.", "The latter is not an activation function but a fully_connected layer that has its activation function preset as nn.relu:", "<code>Code Snippet</code>.", "If you read the docs for contrib.layers, you'll find:", "Aliases for fully_connected which set a default activation function\n  are available: relu, relu6 and linear.", "Summarily, tf.contrib.layers.relu is an alias for a fully_connected layer with relu activation while tf.nn.relu is the REctified Linear Unit activation function itself."], "sent_idxs": [101, 2027, 2024, 2025, 1996, 2168, 2518, 1012, 102, 101, 1996, 3732, 2003, 2025, 2019, 13791, 3853, 2021, 1037, 3929, 1035, 4198, 6741, 2008, 2038, 2049, 13791, 3853, 3653, 13462, 2004, 1050, 2078, 1012, 2128, 7630, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 3191, 1996, 9986, 2015, 2005, 9530, 18886, 2497, 1012, 9014, 1010, 2017, 1005, 2222, 2424, 1024, 102, 101, 14593, 2229, 2005, 3929, 1035, 4198, 2029, 2275, 1037, 12398, 13791, 3853, 2024, 2800, 1024, 2128, 7630, 1010, 2128, 7630, 2575, 1998, 7399, 1012, 102, 101, 7680, 7849, 6588, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 2128, 7630, 2003, 2019, 14593, 2005, 1037, 3929, 1035, 4198, 6741, 2007, 2128, 7630, 13791, 2096, 1056, 2546, 1012, 1050, 2078, 1012, 2128, 7630, 2003, 1996, 28667, 3775, 10451, 7399, 3131, 13791, 3853, 2993, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 4, 5]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 4, 5]}], "na_triple": [], "sent_ends": [0, 9, 38, 52, 72, 98, 148], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50080463", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.sigmoid", "pos": [42, 51]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [9, 17]}], [{"sent_id": 5, "name": "tf.log", "pos": [125, 129]}, {"sent_id": 7, "name": "tf.log", "pos": [166, 170]}], [{"sent_id": 0, "name": "tf.matmul", "pos": [18, 24]}, {"sent_id": 2, "name": "tf.matmul", "pos": [52, 58]}], [{"sent_id": 5, "name": "tf.reduce_sum", "pos": [116, 122]}, {"sent_id": 7, "name": "tf.reduce_sum", "pos": [157, 163]}], [{"sent_id": 5, "name": "tf.reduce_mean", "pos": [108, 114]}, {"sent_id": 7, "name": "tf.reduce_mean", "pos": [149, 155]}]], "sents": ["You need to use - y_ = tf.nn.softmax(tf.matmul(X,W)+b)", "instead of :", "y_ = tf.nn.sigmoid(tf.matmul(X,W)+b)", "as the MNIST data set has multi-class labels (sigmoid is used in case of 2 classes).", "You may also need to add a small number to", "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(y_), reduction_indices=1))", "like -", "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(y_ + 1e-10), reduction_indices=1))", "in case the cost results in nan"], "sent_idxs": [101, 2017, 2342, 2000, 2224, 1011, 1061, 1035, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 1010, 1059, 1007, 1009, 1038, 1007, 102, 101, 2612, 1997, 1024, 102, 101, 1061, 1035, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1006, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 1010, 1059, 1007, 1009, 1038, 1007, 102, 101, 2004, 1996, 24098, 2923, 2951, 2275, 2038, 4800, 1011, 2465, 10873, 1006, 9033, 21693, 9314, 2003, 2109, 1999, 2553, 1997, 1016, 4280, 1007, 1012, 102, 101, 2017, 2089, 2036, 2342, 2000, 5587, 1037, 2235, 2193, 2000, 102, 101, 3465, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1011, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1061, 1008, 1056, 2546, 1012, 8833, 1006, 1061, 1035, 1007, 1010, 7312, 1035, 29299, 1027, 1015, 1007, 1007, 102, 101, 2066, 1011, 102, 101, 3465, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1011, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1061, 1008, 1056, 2546, 1012, 8833, 1006, 1061, 1035, 1009, 1015, 2063, 1011, 2184, 1007, 1010, 7312, 1035, 29299, 1027, 1015, 1007, 1007, 102, 101, 1999, 2553, 1996, 3465, 3463, 1999, 16660, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 33, 38, 67, 93, 105, 142, 146, 188, 197], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34243720", "vertexSet": [[{"sent_id": 7, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [179, 196]}, {"sent_id": 11, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [272, 289]}, {"sent_id": 12, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [345, 362]}], [{"sent_id": 14, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [435, 454]}], [{"sent_id": 2, "name": "tf.nn.softmax", "pos": [68, 76]}, {"sent_id": 7, "name": "tf.nn.softmax", "pos": [179, 187]}, {"sent_id": 11, "name": "tf.nn.softmax", "pos": [272, 280]}, {"sent_id": 12, "name": "tf.nn.softmax", "pos": [345, 353]}]], "sents": ["Logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear.", "It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities (you might have an input of 5).", "tf.nn.softmax produces just the result of applying the softmax function to an input tensor.", "The softmax \"squishes\" the inputs so that sum(input) = 1:  it's a way of normalizing.", "The shape of output of a softmax is the same as the input: it just normalizes the values.", "The outputs of softmax can be interpreted as probabilities.", "<code>Code Snippet</code>.", "In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).", "It's similar to the result of:", "<code>Code Snippet</code>.", "The cross entropy is a summary metric: it sums across the elements.", "The output of tf.nn.softmax_cross_entropy_with_logits on a shape [2,5] tensor is of shape [2,1] (the first dimension is treated as the batch).", "If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.", "Otherwise, you'll end up hacking it by adding little epsilons here and there.", "Edited 2016-02-07: \nIf you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.", "This function was added after release 0.6.0."], "sent_idxs": [101, 8833, 12762, 3432, 2965, 2008, 1996, 3853, 5748, 2006, 1996, 4895, 15782, 3709, 6434, 1997, 3041, 9014, 1998, 2008, 1996, 5816, 4094, 2000, 3305, 1996, 3197, 2003, 7399, 1012, 102, 101, 2009, 2965, 1010, 1999, 3327, 1010, 1996, 7680, 1997, 1996, 20407, 2089, 2025, 5020, 1015, 1010, 2008, 1996, 5300, 2024, 2025, 4013, 3676, 14680, 1006, 2017, 2453, 2031, 2019, 7953, 1997, 1019, 1007, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 7137, 2074, 1996, 2765, 1997, 11243, 1996, 3730, 17848, 3853, 2000, 2019, 7953, 23435, 1012, 102, 101, 1996, 3730, 17848, 1000, 5490, 27020, 15689, 1000, 1996, 20407, 2061, 2008, 7680, 1006, 7953, 1007, 1027, 1015, 1024, 2009, 1005, 1055, 1037, 2126, 1997, 3671, 6026, 1012, 102, 101, 1996, 4338, 1997, 6434, 1997, 1037, 3730, 17848, 2003, 1996, 2168, 2004, 1996, 7953, 1024, 2009, 2074, 3671, 10057, 1996, 5300, 1012, 102, 101, 1996, 27852, 1997, 3730, 17848, 2064, 2022, 10009, 2004, 4013, 3676, 14680, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 5688, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 24134, 2015, 1996, 2892, 23077, 1997, 1996, 2765, 2044, 11243, 1996, 3730, 17848, 3853, 1006, 2021, 2009, 2515, 2009, 2035, 2362, 1999, 1037, 2062, 8045, 2135, 6176, 2126, 1007, 1012, 102, 101, 2009, 1005, 1055, 2714, 2000, 1996, 2765, 1997, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2892, 23077, 2003, 1037, 12654, 12046, 1024, 2009, 20571, 2408, 1996, 3787, 1012, 102, 101, 1996, 6434, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2006, 1037, 4338, 1031, 1016, 1010, 1019, 1033, 23435, 2003, 1997, 4338, 1031, 1016, 1010, 1015, 1033, 1006, 1996, 2034, 9812, 2003, 5845, 2004, 1996, 14108, 1007, 1012, 102, 101, 2065, 2017, 2215, 2000, 2079, 20600, 2000, 18478, 1996, 2892, 23077, 1998, 2017, 1005, 2128, 3730, 17848, 2075, 2044, 2115, 2197, 6741, 1010, 2017, 2323, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2612, 1997, 2725, 2009, 4426, 1010, 2138, 2009, 4472, 15973, 2135, 14480, 3420, 3572, 1999, 1996, 8045, 2135, 2157, 2126, 1012, 102, 101, 4728, 1010, 2017, 1005, 2222, 2203, 2039, 23707, 2009, 2011, 5815, 2210, 28038, 2015, 2182, 1998, 2045, 1012, 102, 101, 5493, 2355, 1011, 6185, 1011, 5718, 1024, 2065, 2017, 2031, 2309, 1011, 2465, 10873, 1010, 2073, 2019, 4874, 2064, 2069, 7141, 2000, 2028, 2465, 1010, 2017, 2453, 2085, 5136, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2061, 2008, 2017, 2123, 1005, 1056, 2031, 2000, 10463, 2115, 10873, 2000, 1037, 9742, 2028, 1011, 2980, 9140, 1012, 102, 101, 2023, 3853, 2001, 2794, 2044, 2713, 1014, 1012, 1020, 1012, 1014, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [7, 10, 12, 14, 15]}, {"r": "S1", "h": 1, "t": 0, "evidence": [7, 10, 12, 14, 15]}, {"r": "S1", "h": 2, "t": 0, "evidence": [2, 3, 4, 5, 7, 10, 12]}, {"r": "S1", "h": 0, "t": 2, "evidence": [2, 3, 4, 5, 7, 10, 12]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 31, 67, 92, 122, 146, 161, 175, 227, 238, 252, 268, 318, 384, 404, 474, 488], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46525453", "vertexSet": [[{"sent_id": 3, "name": "tf.tensor", "pos": [56, 60]}], [{"sent_id": 4, "name": "tf.placeholder", "pos": [103, 108]}], [{"sent_id": 6, "name": "tf.train", "pos": [154, 158]}, {"sent_id": 8, "name": "tf.train", "pos": [202, 206]}], [{"sent_id": 8, "name": "tf.tfrecordreader", "pos": [190, 199]}], [{"sent_id": 6, "name": "tf.train.shuffle_batch", "pos": [154, 162]}, {"sent_id": 8, "name": "tf.train.shuffle_batch", "pos": [202, 210]}]], "sents": ["From the Session.run doc:", "The optional feed_dict argument allows the caller to override the\n  value of tensors in the graph.", "Each key in feed_dict can be one of the\n  following types:", "If the key is a tf.Tensor, the value may be a Python scalar, string,\n  list, or numpy ndarray that can be converted to the same dtype as that\n  tensor.", "Additionally, if the key is a tf.placeholder, the shape of the\n  value will be checked for compatibility with the placeholder.", "...", "So you are right: for X and Y (which are placeholders) you can't feed a tensor and tf.train.shuffle_batch is not designed to work with placeholders.", "You can follow one of two ways:", "get rid of placeholders and use tf.TFRecordReader in combination with tf.train.shuffle_batch, as suggested here.", "This way you'll have only tensors in your model and you won't need to \"feed\" anything additionally.", "batch and shuffle the data yourself in numpy and feed into placeholders.", "This takes just several lines of code, so I find it easier, though both paths are valid.", "Take also into account performance considerations."], "sent_idxs": [101, 2013, 1996, 5219, 1012, 2448, 9986, 1024, 102, 101, 1996, 11887, 5438, 1035, 4487, 6593, 6685, 4473, 1996, 20587, 2000, 2058, 15637, 1996, 3643, 1997, 23435, 2015, 1999, 1996, 10629, 1012, 102, 101, 2169, 3145, 1999, 5438, 1035, 4487, 6593, 2064, 2022, 2028, 1997, 1996, 2206, 4127, 1024, 102, 101, 2065, 1996, 3145, 2003, 1037, 1056, 2546, 1012, 23435, 1010, 1996, 3643, 2089, 2022, 1037, 18750, 26743, 2099, 1010, 5164, 1010, 2862, 1010, 2030, 16371, 8737, 2100, 1050, 7662, 9447, 2008, 2064, 2022, 4991, 2000, 1996, 2168, 26718, 18863, 2004, 2008, 23435, 1012, 102, 101, 5678, 1010, 2065, 1996, 3145, 2003, 1037, 1056, 2546, 1012, 2173, 14528, 1010, 1996, 4338, 1997, 1996, 3643, 2097, 2022, 7039, 2005, 21778, 2007, 1996, 2173, 14528, 1012, 102, 101, 1012, 1012, 1012, 102, 101, 2061, 2017, 2024, 2157, 1024, 2005, 1060, 1998, 1061, 1006, 2029, 2024, 2173, 17794, 1007, 2017, 2064, 1005, 1056, 5438, 1037, 23435, 1998, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 2003, 2025, 2881, 2000, 2147, 2007, 2173, 17794, 1012, 102, 101, 2017, 2064, 3582, 2028, 1997, 2048, 3971, 1024, 102, 101, 2131, 9436, 1997, 2173, 17794, 1998, 2224, 1056, 2546, 1012, 1056, 19699, 8586, 8551, 16416, 4063, 1999, 5257, 2007, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1010, 2004, 4081, 2182, 1012, 102, 101, 2023, 2126, 2017, 1005, 2222, 2031, 2069, 23435, 2015, 1999, 2115, 2944, 1998, 2017, 2180, 1005, 1056, 2342, 2000, 1000, 5438, 1000, 2505, 5678, 1012, 102, 101, 14108, 1998, 23046, 1996, 2951, 4426, 1999, 16371, 8737, 2100, 1998, 5438, 2046, 2173, 17794, 1012, 102, 101, 2023, 3138, 2074, 2195, 3210, 1997, 3642, 1010, 2061, 1045, 2424, 2009, 6082, 1010, 2295, 2119, 10425, 2024, 9398, 1012, 102, 101, 2202, 2036, 2046, 4070, 2836, 16852, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 9, 33, 50, 95, 125, 130, 172, 182, 216, 243, 261, 283, 292], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60409372", "vertexSet": [[{"sent_id": 4, "name": "tf.layers", "pos": [108, 112]}], [{"sent_id": 5, "name": "tf.compat.v1.layers", "pos": [127, 138]}]], "sents": ["The code you're using was written in Tensorflow v1.x, and is not compatible as it is with Tensorflow v2.", "The easiest solution is probably to downgrade to a version of tensorflow v1 to run the code as it is.", "An other option would be to could follow this guide to migrate the code from v1 to v2.", "A third option would be to use the tf.compat module to get some retro-compatibility.", "For example, tf.layers does not exist anymore in Tensorflow v2.", "You can use tf.compat.v1.layers (see for example the Conv2D function) instead, but this is a temporary fix, as these functions will be removed in a future version."], "sent_idxs": [101, 1996, 3642, 2017, 1005, 2128, 2478, 2001, 2517, 1999, 23435, 12314, 1058, 2487, 1012, 1060, 1010, 1998, 2003, 2025, 11892, 2004, 2009, 2003, 2007, 23435, 12314, 1058, 2475, 1012, 102, 101, 1996, 25551, 5576, 2003, 2763, 2000, 2091, 24170, 2000, 1037, 2544, 1997, 23435, 12314, 1058, 2487, 2000, 2448, 1996, 3642, 2004, 2009, 2003, 1012, 102, 101, 2019, 2060, 5724, 2052, 2022, 2000, 2071, 3582, 2023, 5009, 2000, 22806, 1996, 3642, 2013, 1058, 2487, 2000, 1058, 2475, 1012, 102, 101, 1037, 2353, 5724, 2052, 2022, 2000, 2224, 1996, 1056, 2546, 1012, 4012, 4502, 2102, 11336, 2000, 2131, 2070, 22307, 1011, 21778, 1012, 102, 101, 2005, 2742, 1010, 1056, 2546, 1012, 9014, 2515, 2025, 4839, 4902, 1999, 23435, 12314, 1058, 2475, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 9014, 1006, 2156, 2005, 2742, 1996, 9530, 2615, 2475, 2094, 3853, 1007, 2612, 1010, 2021, 2023, 2003, 1037, 5741, 8081, 1010, 2004, 2122, 4972, 2097, 2022, 3718, 1999, 1037, 2925, 2544, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [4, 5]}], "na_triple": [], "sent_ends": [0, 31, 57, 80, 104, 123, 170], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50259637", "vertexSet": [[{"sent_id": 4, "name": "tf.layers.flatten", "pos": [84, 91]}, {"sent_id": 9, "name": "tf.layers.flatten", "pos": [226, 233]}], [{"sent_id": 9, "name": "tf.contrib.layers.flatten", "pos": [235, 246]}]], "sents": ["The flatten function in numpy does a complete array flattening, meaning that you end up with a single axis of data (1 dimension only).", "For example,", "<code>Code Snippet</code>.", "In the previous example, you end up with a 1-d array of 20 elements.", "In tensorflow, the flatten layer (tf.layers.flatten) preserves the batch axis (axis 0).", "In the previous example, with tensorflow, you would still have a shape of (5,4).", "In any case, there is no effect on training if you use flatten in an equivalent way.", "However, you should avoid using numpy when working with tensorflow, since almost all numpy operations have their tensorflow counterparts.", "Tensorflow and numpy rely on different runtime libraries and combining both could be runtime inefficient.", "Moreover, avoid using contrib package layers, when they already exist in the main package (use tf.layers.flatten instead of tf.contrib.layers.flatten).", "For a more general performance comparison between numpy and tensorflow, have a look at this question: Tensorflow vs. Numpy Performance"], "sent_idxs": [101, 1996, 4257, 6528, 3853, 1999, 16371, 8737, 2100, 2515, 1037, 3143, 9140, 4257, 6528, 2075, 1010, 3574, 2008, 2017, 2203, 2039, 2007, 1037, 2309, 8123, 1997, 2951, 1006, 1015, 9812, 2069, 1007, 1012, 102, 101, 2005, 2742, 1010, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 1996, 3025, 2742, 1010, 2017, 2203, 2039, 2007, 1037, 1015, 1011, 1040, 9140, 1997, 2322, 3787, 1012, 102, 101, 1999, 23435, 12314, 1010, 1996, 4257, 6528, 6741, 1006, 1056, 2546, 1012, 9014, 1012, 4257, 6528, 1007, 18536, 1996, 14108, 8123, 1006, 8123, 1014, 1007, 1012, 102, 101, 1999, 1996, 3025, 2742, 1010, 2007, 23435, 12314, 1010, 2017, 2052, 2145, 2031, 1037, 4338, 1997, 1006, 1019, 1010, 1018, 1007, 1012, 102, 101, 1999, 2151, 2553, 1010, 2045, 2003, 2053, 3466, 2006, 2731, 2065, 2017, 2224, 4257, 6528, 1999, 2019, 5662, 2126, 1012, 102, 101, 2174, 1010, 2017, 2323, 4468, 2478, 16371, 8737, 2100, 2043, 2551, 2007, 23435, 12314, 1010, 2144, 2471, 2035, 16371, 8737, 2100, 3136, 2031, 2037, 23435, 12314, 14562, 1012, 102, 101, 23435, 12314, 1998, 16371, 8737, 2100, 11160, 2006, 2367, 2448, 7292, 8860, 1998, 11566, 2119, 2071, 2022, 2448, 7292, 1999, 12879, 8873, 23402, 3372, 1012, 102, 101, 9308, 1010, 4468, 2478, 9530, 18886, 2497, 7427, 9014, 1010, 2043, 2027, 2525, 4839, 1999, 1996, 2364, 7427, 1006, 2224, 1056, 2546, 1012, 9014, 1012, 4257, 6528, 2612, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 4257, 6528, 1007, 1012, 102, 101, 2005, 1037, 2062, 2236, 2836, 7831, 2090, 16371, 8737, 2100, 1998, 23435, 12314, 1010, 2031, 1037, 2298, 2012, 2023, 3160, 1024, 23435, 12314, 5443, 1012, 16371, 8737, 2100, 2836, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [9]}, {"r": "S1", "h": 1, "t": 0, "evidence": [9]}], "na_triple": [], "sent_ends": [0, 35, 40, 54, 74, 102, 126, 148, 178, 205, 249, 280], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50259737", "vertexSet": [[{"sent_id": 0, "name": "tf.layers.flatten", "pos": [10, 17]}, {"sent_id": 7, "name": "tf.layers.flatten", "pos": [164, 171]}], [{"sent_id": 0, "name": "tf.contrib.layers.flatten", "pos": [19, 30]}]], "sents": ["The biggest difference between np.flatten and tf.layers.flatten (or tf.contrib.layers.flatten) is that numpy operations are applicable only to static nd arrays, while tensorflow operations can work with dynamic tensors.", "Dynamic in this case means that the exact shape will be known only at runtime (either training or testing).", "So my recommendation is pretty simple:", "If the input data is static numpy array, e.g.", "in pre-processing, use np.flatten.", "This avoids unnecessary overhead and returns numpy array as well..", "If the data is already a tensor, use any of the flatten ops provided by tensorflow.", "Between those, tf.layers.flatten is better choice since tf.layers API is more stable than tf.contrib.", "*.."], "sent_idxs": [101, 1996, 5221, 4489, 2090, 27937, 1012, 4257, 6528, 1998, 1056, 2546, 1012, 9014, 1012, 4257, 6528, 1006, 2030, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 4257, 6528, 1007, 2003, 2008, 16371, 8737, 2100, 3136, 2024, 12711, 2069, 2000, 10763, 1050, 2094, 27448, 1010, 2096, 23435, 12314, 3136, 2064, 2147, 2007, 8790, 23435, 2015, 1012, 102, 101, 8790, 1999, 2023, 2553, 2965, 2008, 1996, 6635, 4338, 2097, 2022, 2124, 2069, 2012, 2448, 7292, 1006, 2593, 2731, 2030, 5604, 1007, 1012, 102, 101, 2061, 2026, 12832, 2003, 3492, 3722, 1024, 102, 101, 2065, 1996, 7953, 2951, 2003, 10763, 16371, 8737, 2100, 9140, 1010, 1041, 1012, 1043, 1012, 102, 101, 1999, 3653, 1011, 6364, 1010, 2224, 27937, 1012, 4257, 6528, 1012, 102, 101, 2023, 26777, 14203, 8964, 1998, 5651, 16371, 8737, 2100, 9140, 2004, 2092, 1012, 1012, 102, 101, 2065, 1996, 2951, 2003, 2525, 1037, 23435, 1010, 2224, 2151, 1997, 1996, 4257, 6528, 23092, 3024, 2011, 23435, 12314, 1012, 102, 101, 2090, 2216, 1010, 1056, 2546, 1012, 9014, 1012, 4257, 6528, 2003, 2488, 3601, 2144, 1056, 2546, 1012, 9014, 17928, 2003, 2062, 6540, 2084, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 102, 101, 1008, 1012, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 6, 7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 6, 7]}], "na_triple": [], "sent_ends": [0, 58, 83, 92, 109, 122, 138, 160, 192, 197], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48340802", "vertexSet": [[{"sent_id": 0, "name": "tf.train.get_global_step", "pos": [9, 19]}], [{"sent_id": 0, "name": "tf.train.get_or_create_global_step", "pos": [23, 37]}], [{"sent_id": 2, "name": "tf.train.checkpointsaverhook", "pos": [60, 70]}]], "sents": ["You can get the current global step via tf.train.get_global_step() or via tf.train.get_or_create_global_step() function.", "The latter should be called before training starts.", "For the monitored session, add tf.train.CheckpointSaverHook to the hooks, which internally uses the defined global step tensor to save the model after every N steps."], "sent_idxs": [101, 2017, 2064, 2131, 1996, 2783, 3795, 3357, 3081, 1056, 2546, 1012, 3345, 1012, 2131, 1035, 3795, 1035, 3357, 1006, 1007, 2030, 3081, 1056, 2546, 1012, 3345, 1012, 2131, 1035, 2030, 1035, 3443, 1035, 3795, 1035, 3357, 1006, 1007, 3853, 1012, 102, 101, 1996, 3732, 2323, 2022, 2170, 2077, 2731, 4627, 1012, 102, 101, 2005, 1996, 17785, 5219, 1010, 5587, 1056, 2546, 1012, 3345, 1012, 26520, 3736, 6299, 6806, 6559, 2000, 1996, 18008, 1010, 2029, 16058, 3594, 1996, 4225, 3795, 3357, 23435, 2000, 3828, 1996, 2944, 2044, 2296, 1050, 4084, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 42, 53, 92], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47984274", "vertexSet": [[{"sent_id": 3, "name": "tf.nn.batch_normalization", "pos": [61, 71]}], [{"sent_id": 0, "name": "tf.layers.batch_normalization", "pos": [3, 12]}, {"sent_id": 5, "name": "tf.layers.batch_normalization", "pos": [102, 111]}], [{"sent_id": 1, "name": "tf.get_variable", "pos": [20, 26]}], [{"sent_id": 6, "name": "tf.nn.fused_batch_norm", "pos": [130, 141]}]], "sents": ["Simply use tf.layers.batch_normalization.", "It also creates variables via tf.get_variable(), hence they can be shared as well.", "In addition, it works seamlessly with tf.layers.conv* functions.", "Update: tf.nn.batch_normalization is fine too.", "It's a more low-level function that requires you manage mean and variance tensors yourself.", "In fact, tf.layers.batch_normalization is a wrapper over tf.nn.", "* functions, which also includes tf.nn.fused_batch_norm (a faster fused version)."], "sent_idxs": [101, 3432, 2224, 1056, 2546, 1012, 9014, 1012, 14108, 1035, 3671, 3989, 1012, 102, 101, 2009, 2036, 9005, 10857, 3081, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 1010, 6516, 2027, 2064, 2022, 4207, 2004, 2092, 1012, 102, 101, 1999, 2804, 1010, 2009, 2573, 25180, 10895, 2007, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 1008, 4972, 1012, 102, 101, 10651, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 2003, 2986, 2205, 1012, 102, 101, 2009, 1005, 1055, 1037, 2062, 2659, 1011, 2504, 3853, 2008, 5942, 2017, 6133, 2812, 1998, 23284, 23435, 2015, 4426, 1012, 102, 101, 1999, 2755, 1010, 1056, 2546, 1012, 9014, 1012, 14108, 1035, 3671, 3989, 2003, 1037, 10236, 4842, 2058, 1056, 2546, 1012, 1050, 2078, 1012, 102, 101, 1008, 4972, 1010, 2029, 2036, 2950, 1056, 2546, 1012, 1050, 2078, 1012, 19660, 1035, 14108, 1035, 13373, 1006, 1037, 5514, 19660, 2544, 1007, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 3, 4, 5, 6]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 3, 4, 5, 6]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 14, 38, 58, 76, 98, 123, 149], "sent_pos": [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51089448", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.layers.conv2d", "pos": [1, 13]}], [{"sent_id": 0, "name": "tf.layers.max_pooling2d", "pos": [22, 33]}]], "sents": ["tf.keras.layers.Conv2d is a tensorflow-keras layer while tf.layers.max_pooling2d is a tensorflow 'native layer'", "You cannot use a native layer directly within a Keras model, as it will be missing certain attributes required by the Keras API.", "However, it is possible to use native layer if wrapped within a tensorflow-keras Lambda layer.", "A link to the documentation for this is below.", "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"], "sent_idxs": [101, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2003, 1037, 23435, 12314, 1011, 17710, 8180, 6741, 2096, 1056, 2546, 1012, 9014, 1012, 4098, 1035, 4770, 2075, 2475, 2094, 2003, 1037, 23435, 12314, 1005, 3128, 6741, 1005, 102, 101, 2017, 3685, 2224, 1037, 3128, 6741, 3495, 2306, 1037, 17710, 8180, 2944, 1010, 2004, 2009, 2097, 2022, 4394, 3056, 12332, 3223, 2011, 1996, 17710, 8180, 17928, 1012, 102, 101, 2174, 1010, 2009, 2003, 2825, 2000, 2224, 3128, 6741, 2065, 5058, 2306, 1037, 23435, 12314, 1011, 17710, 8180, 23375, 6741, 1012, 102, 101, 1037, 4957, 2000, 1996, 12653, 2005, 2023, 2003, 2917, 1012, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 17710, 8180, 1013, 9014, 1013, 23375, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 42, 71, 94, 106, 135], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50048405", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.layers.dense", "pos": [18, 27]}], [{"sent_id": 0, "name": "tf.layers.dense", "pos": [29, 35]}, {"sent_id": 1, "name": "tf.layers.dense", "pos": [61, 67]}]], "sents": ["Looks like you're using the Dense layer from the wrong package: it should be tf.keras.layers.Dense rather than tf.layers.Dense.", "Note that though they have the same class name and lots of similar parameters, in fact they have nothing in common: tf.layers.Dense is a high-level tensorflow API, not related to keras.", "That's why you can't add them to classifier."], "sent_idxs": [101, 3504, 2066, 2017, 1005, 2128, 2478, 1996, 9742, 6741, 2013, 1996, 3308, 7427, 1024, 2009, 2323, 2022, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 2738, 2084, 1056, 2546, 1012, 9014, 1012, 9742, 1012, 102, 101, 3602, 2008, 2295, 2027, 2031, 1996, 2168, 2465, 2171, 1998, 7167, 1997, 2714, 11709, 1010, 1999, 2755, 2027, 2031, 2498, 1999, 2691, 1024, 1056, 2546, 1012, 9014, 1012, 9742, 2003, 1037, 2152, 1011, 2504, 23435, 12314, 17928, 1010, 2025, 3141, 2000, 17710, 8180, 1012, 102, 101, 2008, 1005, 1055, 2339, 2017, 2064, 1005, 1056, 5587, 2068, 2000, 2465, 18095, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 37, 83, 99], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50820539", "vertexSet": [[{"sent_id": 5, "name": "tf.matmul", "pos": [127, 133]}], [{"sent_id": 5, "name": "tf.nn.doftmax", "pos": [148, 157]}]], "sents": ["There are several problems with the modifications you made to the original code:", "You cannot use numpy operations in the middle of your Keras/TF graph.", "First because numpy will try to operate directly, while the inputs tensors will actually be evaluated/receive their value only at graph runtime.", "Second because Keras/TF won't be able to back-propagate through non-Keras/TF operations.", "You should replace the original tensorflow operations by their keras or keras.backend operations instead (e.g.", "tf.matmul() by keras.backend.batch_dot(),  tf.nn.doftmax() by keras.backend.softmax(), etc.)", "You are mixing Keras Layers (e.g.", "Conv2D) and Keras operations (e.g.", "np/keras.backend.reshape).", "Keras operations should be wrapped in a Lambda layer to be used along others.", "Since this custom layer has a trainable parameter (gamma), you would need to write your own custom layer, e.g.", ":", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2024, 2195, 3471, 2007, 1996, 12719, 2017, 2081, 2000, 1996, 2434, 3642, 1024, 102, 101, 2017, 3685, 2224, 16371, 8737, 2100, 3136, 1999, 1996, 2690, 1997, 2115, 17710, 8180, 1013, 1056, 2546, 10629, 1012, 102, 101, 2034, 2138, 16371, 8737, 2100, 2097, 3046, 2000, 5452, 3495, 1010, 2096, 1996, 20407, 23435, 2015, 2097, 2941, 2022, 16330, 1013, 4374, 2037, 3643, 2069, 2012, 10629, 2448, 7292, 1012, 102, 101, 2117, 2138, 17710, 8180, 1013, 1056, 2546, 2180, 1005, 1056, 2022, 2583, 2000, 2067, 1011, 17678, 16098, 2618, 2083, 2512, 1011, 17710, 8180, 1013, 1056, 2546, 3136, 1012, 102, 101, 2017, 2323, 5672, 1996, 2434, 23435, 12314, 3136, 2011, 2037, 17710, 8180, 2030, 17710, 8180, 1012, 2067, 10497, 3136, 2612, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1007, 2011, 17710, 8180, 1012, 2067, 10497, 1012, 14108, 1035, 11089, 1006, 1007, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 2079, 6199, 17848, 1006, 1007, 2011, 17710, 8180, 1012, 2067, 10497, 1012, 3730, 17848, 1006, 1007, 1010, 4385, 1012, 1007, 102, 101, 2017, 2024, 6809, 17710, 8180, 9014, 1006, 1041, 1012, 1043, 1012, 102, 101, 9530, 2615, 2475, 2094, 1007, 1998, 17710, 8180, 3136, 1006, 1041, 1012, 1043, 1012, 102, 101, 27937, 1013, 17710, 8180, 1012, 2067, 10497, 1012, 24501, 3270, 5051, 1007, 1012, 102, 101, 17710, 8180, 3136, 2323, 2022, 5058, 1999, 1037, 23375, 6741, 2000, 2022, 2109, 2247, 2500, 1012, 102, 101, 2144, 2023, 7661, 6741, 2038, 1037, 3345, 3085, 16381, 1006, 13091, 1007, 1010, 2017, 2052, 2342, 2000, 4339, 2115, 2219, 7661, 6741, 1010, 1041, 1012, 1043, 1012, 102, 101, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 16, 37, 69, 99, 126, 175, 188, 204, 219, 237, 266, 269, 283], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45960834", "vertexSet": [[{"sent_id": 8, "name": "tf.graphkeys.global_variables", "pos": [294, 304]}, {"sent_id": 11, "name": "tf.graphkeys.global_variables", "pos": [393, 403]}], [{"sent_id": 1, "name": "tf.graphkeys.model_variables", "pos": [56, 66]}, {"sent_id": 8, "name": "tf.graphkeys.model_variables", "pos": [305, 315]}, {"sent_id": 11, "name": "tf.graphkeys.model_variables", "pos": [378, 388]}], [{"sent_id": 8, "name": "tf.estimator", "pos": [259, 265]}], [{"sent_id": 7, "name": "tf.get_variable", "pos": [216, 222]}], [{"sent_id": 7, "name": "tf.add_to_collection", "pos": [227, 235]}], [{"sent_id": 8, "name": "tf.estimator.linearregressor", "pos": [259, 271]}]], "sents": ["EDIT: As Jason Ching points out, there have been some changes after this answer was posted.", "There are now the estimator methods get_variable_names and get_variable_value, and the estimator weights do not seem to be automatically added to tf.GraphKeys.MODEL_VARIABLES anymore.", "Estimators are designed to work basically as a black box, so there is no direct API to retrieve the weights.", "Even if, as in your case, you are the one defining the model (as opposed to using a preexisting model), you do not have a direct access to the parameters from the estimator object.", "That said, you can still retrieve the variables back through other means.", "If you know the names of the variables, one option is to simply get them from the graph object with get_operation_by_name or get_tensor_by_name.", "A more practical and general option is to use a collection.", "Either when you call tf.get_variable or after that, calling tf.add_to_collection, you can put the model variables under a common collection name for later retrieval.", "If you look at how a tf.estimator.LinearRegressor is actually built (search for the function linear_model in this module), all model variables are added to both tf.GraphKeys.GLOBAL_VARIABLES and tf.GraphKeys.MODEL_VARIABLES.", "This is (presumably, I haven't really checked) common to all the available canned estimators, so usually when using one of those you should be able to simply do:", "<code>Code Snippet</code>.", "It is preferable that you use tf.GraphKeys.MODEL_VARIABLES in this case instead of tf.GraphKeys.GLOBAL_VARIABLES, which has a more general purpose and is likely to contain other unrelated variables as well."], "sent_idxs": [101, 10086, 1024, 2004, 4463, 19992, 2685, 2041, 1010, 2045, 2031, 2042, 2070, 3431, 2044, 2023, 3437, 2001, 6866, 1012, 102, 101, 2045, 2024, 2085, 1996, 9765, 9581, 4263, 4725, 2131, 1035, 8023, 1035, 3415, 1998, 2131, 1035, 8023, 1035, 3643, 1010, 1998, 1996, 9765, 9581, 4263, 15871, 2079, 2025, 4025, 2000, 2022, 8073, 2794, 2000, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 2944, 1035, 10857, 4902, 1012, 102, 101, 9765, 9581, 6591, 2024, 2881, 2000, 2147, 10468, 2004, 1037, 2304, 3482, 1010, 2061, 2045, 2003, 2053, 3622, 17928, 2000, 12850, 1996, 15871, 1012, 102, 101, 2130, 2065, 1010, 2004, 1999, 2115, 2553, 1010, 2017, 2024, 1996, 2028, 12854, 1996, 2944, 1006, 2004, 4941, 2000, 2478, 1037, 3653, 10288, 2923, 2075, 2944, 1007, 1010, 2017, 2079, 2025, 2031, 1037, 3622, 3229, 2000, 1996, 11709, 2013, 1996, 9765, 9581, 4263, 4874, 1012, 102, 101, 2008, 2056, 1010, 2017, 2064, 2145, 12850, 1996, 10857, 2067, 2083, 2060, 2965, 1012, 102, 101, 2065, 2017, 2113, 1996, 3415, 1997, 1996, 10857, 1010, 2028, 5724, 2003, 2000, 3432, 2131, 2068, 2013, 1996, 10629, 4874, 2007, 2131, 1035, 3169, 1035, 2011, 1035, 2171, 2030, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1012, 102, 101, 1037, 2062, 6742, 1998, 2236, 5724, 2003, 2000, 2224, 1037, 3074, 1012, 102, 101, 2593, 2043, 2017, 2655, 1056, 2546, 1012, 2131, 1035, 8023, 2030, 2044, 2008, 1010, 4214, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1010, 2017, 2064, 2404, 1996, 2944, 10857, 2104, 1037, 2691, 3074, 2171, 2005, 2101, 26384, 1012, 102, 101, 2065, 2017, 2298, 2012, 2129, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 7399, 2890, 17603, 24137, 2099, 2003, 2941, 2328, 1006, 3945, 2005, 1996, 3853, 7399, 1035, 2944, 1999, 2023, 11336, 1007, 1010, 2035, 2944, 10857, 2024, 2794, 2000, 2119, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3795, 1035, 10857, 1998, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 2944, 1035, 10857, 1012, 102, 101, 2023, 2003, 1006, 10712, 1010, 1045, 4033, 1005, 1056, 2428, 7039, 1007, 2691, 2000, 2035, 1996, 2800, 27141, 9765, 9581, 6591, 1010, 2061, 2788, 2043, 2478, 2028, 1997, 2216, 2017, 2323, 2022, 2583, 2000, 3432, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2009, 2003, 9544, 3085, 2008, 2017, 2224, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 2944, 1035, 10857, 1999, 2023, 2553, 2612, 1997, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3795, 1035, 10857, 1010, 2029, 2038, 1037, 2062, 2236, 3800, 1998, 2003, 3497, 2000, 5383, 2060, 15142, 10857, 2004, 2092, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [11]}, {"r": "S1", "h": 0, "t": 1, "evidence": [11]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 21, 69, 95, 142, 158, 197, 211, 252, 317, 356, 370, 422], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41628828", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib.layers.fully_connected", "pos": [68, 80]}], [{"sent_id": 2, "name": "tf.contrib.layers.linear", "pos": [45, 55]}]], "sents": ["The tf.contrib.layers module has API documentation here.", "As you observed in your answer, the contrib APIs in TensorFlow are (especially) subject to change.", "The tf.contrib.layers.linear() function appears to have been removed, but you can use tf.contrib.layers.fully_connected(\u2026, activation_fn=None) to achieve the same effect."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 11336, 2038, 17928, 12653, 2182, 1012, 102, 101, 2004, 2017, 5159, 1999, 2115, 3437, 1010, 1996, 9530, 18886, 2497, 17928, 2015, 1999, 23435, 12314, 2024, 1006, 2926, 1007, 3395, 2000, 2689, 1012, 102, 101, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 7399, 1006, 1007, 3853, 3544, 2000, 2031, 2042, 3718, 1010, 2021, 2017, 2064, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 3929, 1035, 4198, 1006, 1529, 1010, 13791, 1035, 1042, 2078, 1027, 3904, 1007, 2000, 6162, 1996, 2168, 3466, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2]}], "na_triple": [], "sent_ends": [0, 17, 43, 97], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44881684", "vertexSet": [[{"sent_id": 7, "name": "tf.float32", "pos": [199, 204]}], [{"sent_id": 0, "name": "tf.nn", "pos": [5, 10]}], [{"sent_id": 4, "name": "tf.layers", "pos": [119, 123]}], [{"sent_id": 4, "name": "tf.layers.dense", "pos": [119, 125]}], [{"sent_id": 0, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [5, 22]}]], "sents": ["This is due to tf.nn.softmax_cross_entropy_with_logits:", "logits and labels must have the same shape [batch_size, num_classes] and the same dtype (either float16, float32, or float64).", "I suppose you could compute a loss with integer inputs.", "However, most of the time, this loss is minimized by gradient descent -- as you do -- which means inputs needs to encode real numbers to get arbitrary updates.", "The thing is that tf.layers.dense won't change the type of your input.", "So it will produce an integer output it its input is an integer.", "(At least if the activation is compatible with integers, such as relu -- a sigmoid would raise an error).", "What you probably wanted to do is provide integer inputs then do all computations in say tf.float32.", "To do this, cast your input first before providing it to dense:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2003, 2349, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1024, 102, 101, 8833, 12762, 1998, 10873, 2442, 2031, 1996, 2168, 4338, 1031, 14108, 1035, 2946, 1010, 16371, 2213, 1035, 4280, 1033, 1998, 1996, 2168, 26718, 18863, 1006, 2593, 14257, 16048, 1010, 14257, 16703, 1010, 2030, 14257, 21084, 1007, 1012, 102, 101, 1045, 6814, 2017, 2071, 24134, 1037, 3279, 2007, 16109, 20407, 1012, 102, 101, 2174, 1010, 2087, 1997, 1996, 2051, 1010, 2023, 3279, 2003, 18478, 2094, 2011, 17978, 6934, 1011, 1011, 2004, 2017, 2079, 1011, 1011, 2029, 2965, 20407, 3791, 2000, 4372, 16044, 2613, 3616, 2000, 2131, 15275, 14409, 1012, 102, 101, 1996, 2518, 2003, 2008, 1056, 2546, 1012, 9014, 1012, 9742, 2180, 1005, 1056, 2689, 1996, 2828, 1997, 2115, 7953, 1012, 102, 101, 2061, 2009, 2097, 3965, 2019, 16109, 6434, 2009, 2049, 7953, 2003, 2019, 16109, 1012, 102, 101, 1006, 2012, 2560, 2065, 1996, 13791, 2003, 11892, 2007, 24028, 1010, 2107, 2004, 2128, 7630, 1011, 1011, 1037, 9033, 21693, 9314, 2052, 5333, 2019, 7561, 1007, 1012, 102, 101, 2054, 2017, 2763, 2359, 2000, 2079, 2003, 3073, 16109, 20407, 2059, 2079, 2035, 22334, 2015, 1999, 2360, 1056, 2546, 1012, 14257, 16703, 1012, 102, 101, 2000, 2079, 2023, 1010, 3459, 2115, 7953, 2034, 2077, 4346, 2009, 2000, 9742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 24, 63, 76, 114, 136, 152, 181, 206, 222, 236], "sent_pos": [0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53791390", "vertexSet": [[{"sent_id": 0, "name": "tf.metrics.recall_at_top_k", "pos": [23, 36]}], [{"sent_id": 0, "name": "tf.metrics.recall_at_k", "pos": [11, 22]}], [{"sent_id": 2, "name": "tf.nn", "pos": [77, 82]}], [{"sent_id": 2, "name": "tf.nn.top_k", "pos": [77, 86]}]], "sents": ["First, let's talk about the difference between tf.metrics.recall_at_k and tf.metrics.recall_at_top_k.", "If you look at open source code, you will find precision_at_k is a simple wrapper around precision_at_top_k.", "precision_at_k applies tf.nn.top_k first, and then calls precision_at_top_k.", "Documentation shows that precision_at_k expects a float tensor of logits values, but precision_at_top_k expects integer tensor the predictions to be the indices of the top k classes.", "So if your value is a logit score values, you should use precision_at_k.", "There are also some mistakes in the calculation method.", "When you calculate recall@1 result for S1 and S2 should be (0 + 0)/2=0 (Since S1 has the highest top-1 score to T4 label not to the ground T1 and S2 has the highest top-1 score to T1 not to the Ground T3 label).", "<code>Code Snippet</code>.", "You can try to change the value of k to see the recall@k."], "sent_idxs": [101, 2034, 1010, 2292, 1005, 1055, 2831, 2055, 1996, 4489, 2090, 1056, 2546, 1012, 12046, 2015, 1012, 9131, 1035, 2012, 1035, 1047, 1998, 1056, 2546, 1012, 12046, 2015, 1012, 9131, 1035, 2012, 1035, 2327, 1035, 1047, 1012, 102, 101, 2065, 2017, 2298, 2012, 2330, 3120, 3642, 1010, 2017, 2097, 2424, 11718, 1035, 2012, 1035, 1047, 2003, 1037, 3722, 10236, 4842, 2105, 11718, 1035, 2012, 1035, 2327, 1035, 1047, 1012, 102, 101, 11718, 1035, 2012, 1035, 1047, 12033, 1056, 2546, 1012, 1050, 2078, 1012, 2327, 1035, 1047, 2034, 1010, 1998, 2059, 4455, 11718, 1035, 2012, 1035, 2327, 1035, 1047, 1012, 102, 101, 12653, 3065, 2008, 11718, 1035, 2012, 1035, 1047, 24273, 1037, 14257, 23435, 1997, 8833, 12762, 5300, 1010, 2021, 11718, 1035, 2012, 1035, 2327, 1035, 1047, 24273, 16109, 23435, 1996, 20932, 2000, 2022, 1996, 29299, 1997, 1996, 2327, 1047, 4280, 1012, 102, 101, 2061, 2065, 2115, 3643, 2003, 1037, 8833, 4183, 3556, 5300, 1010, 2017, 2323, 2224, 11718, 1035, 2012, 1035, 1047, 1012, 102, 101, 2045, 2024, 2036, 2070, 12051, 1999, 1996, 17208, 4118, 1012, 102, 101, 2043, 2017, 18422, 9131, 1030, 1015, 2765, 2005, 1055, 2487, 1998, 1055, 2475, 2323, 2022, 1006, 1014, 1009, 1014, 1007, 1013, 1016, 1027, 1014, 1006, 2144, 1055, 2487, 2038, 1996, 3284, 2327, 1011, 1015, 3556, 2000, 1056, 2549, 3830, 2025, 2000, 1996, 2598, 1056, 2487, 1998, 1055, 2475, 2038, 1996, 3284, 2327, 1011, 1015, 3556, 2000, 1056, 2487, 2025, 2000, 1996, 2598, 1056, 2509, 3830, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 3046, 2000, 2689, 1996, 3643, 1997, 1047, 2000, 2156, 1996, 9131, 1030, 1047, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 38, 70, 100, 142, 164, 176, 245, 259, 277], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62979584", "vertexSet": [[{"sent_id": 0, "name": "tf.logging.info", "pos": [1, 7]}], [{"sent_id": 2, "name": "tf.get_logger", "pos": [51, 58]}]], "sents": ["tf.logging.info('embedding_name: %s', FLAGS.embedding_dimension) is indeed an out-date way of doing this.", "It is no longer supported.", "You can use tf.get_logger as an alternative.", "Here is an working example.", "<code>Code Snippet</code>.", "outputs:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 15899, 1012, 18558, 1006, 1005, 7861, 8270, 4667, 1035, 2171, 1024, 1003, 1055, 1005, 1010, 9245, 1012, 7861, 8270, 4667, 1035, 9812, 1007, 2003, 5262, 2019, 2041, 1011, 3058, 2126, 1997, 2725, 2023, 1012, 102, 101, 2009, 2003, 2053, 2936, 3569, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 2131, 1035, 8833, 4590, 2004, 2019, 4522, 1012, 102, 101, 2182, 2003, 2019, 2551, 2742, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 27852, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 39, 47, 63, 71, 85, 89, 103], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48472420", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib.data.unbatch", "pos": [11, 23]}], [{"sent_id": 7, "name": "tf.data.dataset.unbatch", "pos": [150, 161]}]], "sents": ["I think you're just looking for the transformation tf.contrib.data.unbatch.", "This does exactly what you want:", "<code>Code Snippet</code>.", "From the documentation:", "If elements of the dataset are shaped [B, a0, a1, ...], where B may vary from element to element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].", "Edit for TF 2.0.", "(Thanks @DavidParks)", "From TF 2.0, you can use directly tf.data.Dataset.unbatch:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2228, 2017, 1005, 2128, 2074, 2559, 2005, 1996, 8651, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 4895, 14479, 2818, 1012, 102, 101, 2023, 2515, 3599, 2054, 2017, 2215, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2013, 1996, 12653, 1024, 102, 101, 2065, 3787, 1997, 1996, 2951, 13462, 2024, 5044, 1031, 1038, 1010, 1037, 2692, 1010, 17350, 1010, 1012, 1012, 1012, 1033, 1010, 2073, 1038, 2089, 8137, 2013, 5783, 2000, 5783, 1010, 2059, 2005, 2169, 5783, 1999, 1996, 2951, 13462, 1010, 1996, 4895, 14479, 7690, 2951, 13462, 2097, 5383, 1038, 5486, 3787, 1997, 4338, 1031, 1037, 2692, 1010, 17350, 1010, 1012, 1012, 1012, 1033, 1012, 102, 101, 10086, 2005, 1056, 2546, 1016, 1012, 1014, 1012, 102, 101, 1006, 4283, 1030, 2585, 14432, 2015, 1007, 102, 101, 2013, 1056, 2546, 1016, 1012, 1014, 1010, 2017, 2064, 2224, 3495, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 4895, 14479, 2818, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 4, 7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 4, 7]}], "na_triple": [], "sent_ends": [0, 25, 34, 48, 54, 119, 129, 138, 163, 177], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53633664", "vertexSet": [[{"sent_id": 4, "name": "tf.tile", "pos": [116, 120]}], [{"sent_id": 4, "name": "tf.stack", "pos": [107, 111]}], [{"sent_id": 2, "name": "tf.where", "pos": [39, 43]}], [{"sent_id": 6, "name": "tf.broadcast_to", "pos": [163, 169]}]], "sents": ["This are two quite different questions, and they should probably have been posted as such, but anyway.", "1)", "Yes, you need to manually broadcast all the inputs to [tf.where](https://www.tensorflow.org/api_docs/python/tf/where] if they are different.", "For what is worth, there is an (old) open issue about it, but so far implicit broadcasting it has not been implemented.", "You can use tf.stack like you suggest, although tf.tile would probably be more obvious (and may save memory, although I'm not sure how it is implemented really):", "<code>Code Snippet</code>.", "Or simply with tf.broadcast_to:", "<code>Code Snippet</code>.", "2)", "This is one way to do that:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2024, 2048, 3243, 2367, 3980, 1010, 1998, 2027, 2323, 2763, 2031, 2042, 6866, 2004, 2107, 1010, 2021, 4312, 1012, 102, 101, 1015, 1007, 102, 101, 2748, 1010, 2017, 2342, 2000, 21118, 3743, 2035, 1996, 20407, 2000, 1031, 1056, 2546, 1012, 2073, 1033, 1006, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 2073, 1033, 2065, 2027, 2024, 2367, 1012, 102, 101, 2005, 2054, 2003, 4276, 1010, 2045, 2003, 2019, 1006, 2214, 1007, 2330, 3277, 2055, 2009, 1010, 2021, 2061, 2521, 24655, 5062, 2009, 2038, 2025, 2042, 7528, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 9991, 2066, 2017, 6592, 1010, 2348, 1056, 2546, 1012, 14090, 2052, 2763, 2022, 2062, 5793, 1006, 1998, 2089, 3828, 3638, 1010, 2348, 1045, 1005, 1049, 2025, 2469, 2129, 2009, 2003, 7528, 2428, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 3432, 2007, 1056, 2546, 1012, 3743, 1035, 2000, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1016, 1007, 102, 101, 2023, 2003, 2028, 2126, 2000, 2079, 2008, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [4]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 22, 26, 74, 103, 145, 159, 171, 185, 189, 199, 213, 217, 231], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64080452", "vertexSet": [[{"sent_id": 3, "name": "tf.io.parse_example", "pos": [79, 88]}, {"sent_id": 3, "name": "tf.io.parse_example", "pos": [90, 99]}, {"sent_id": 4, "name": "tf.io.parse_example", "pos": [110, 119]}], [{"sent_id": 3, "name": "tf.io.parse_single_example", "pos": [54, 65]}], [{"sent_id": 1, "name": "tf.data", "pos": [13, 17]}], [{"sent_id": 1, "name": "tf.data.experimental", "pos": [13, 19]}], [{"sent_id": 1, "name": "tf.data.experimental.parse_example_dataset", "pos": [13, 27]}]], "sents": ["There is not much difference in both cases.", "In tf.data.experimental.parse_example_dataset the below apply function is retuned.", "<code>Code Snippet</code>.", "Where in tf.io.parse_single_example   Parses a serialized Example protos given in serialized which is similar to tf.io.parse_example, except tf.io.parse_example is serialized in batches.", "You can use tf.io.parse_example for performance advantage over batching samples."], "sent_idxs": [101, 2045, 2003, 2025, 2172, 4489, 1999, 2119, 3572, 1012, 102, 101, 1999, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 11968, 3366, 1035, 2742, 1035, 2951, 13462, 1996, 2917, 6611, 3853, 2003, 2128, 8525, 7228, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2073, 1999, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 2309, 1035, 2742, 11968, 8583, 1037, 27289, 2742, 15053, 2015, 2445, 1999, 27289, 2029, 2003, 2714, 2000, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 2742, 1010, 3272, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 2742, 2003, 27289, 1999, 14108, 2229, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 2742, 2005, 2836, 5056, 2058, 14108, 2075, 8168, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 11, 37, 51, 106, 128], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47249767", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib.learn.estimator", "pos": [8, 20]}], [{"sent_id": 0, "name": "tf.estimator.estimator", "pos": [23, 33]}, {"sent_id": 1, "name": "tf.estimator.estimator", "pos": [91, 101]}]], "sents": ["First up, you should stop using tf.contrib.learn.Estimator in favor of tf.estimator.Estimator, because contrib is an experimental module, and classes that have graduated to the core API (such es Estimator) automatically get deprecated.", "Now, back to your main question, you can create a distributed model and pass it via model_fn parameter of tf.estimator.Estimator.__init__.", "<code>Code Snippet</code>.", "The model above defines 6 layers with /device:GPU:1 placement and 3 other layers with /device:GPU:2 placement.", "The return value of my_model function should be an EstimatorSpec instance.", "A complete working example can be found in tensorflow examples."], "sent_idxs": [101, 2034, 2039, 1010, 2017, 2323, 2644, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 9765, 9581, 4263, 1999, 5684, 1997, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1010, 2138, 9530, 18886, 2497, 2003, 2019, 6388, 11336, 1010, 1998, 4280, 2008, 2031, 3852, 2000, 1996, 4563, 17928, 1006, 2107, 9686, 9765, 9581, 4263, 1007, 8073, 2131, 2139, 28139, 12921, 1012, 102, 101, 2085, 1010, 2067, 2000, 2115, 2364, 3160, 1010, 2017, 2064, 3443, 1037, 5500, 2944, 1998, 3413, 2009, 3081, 2944, 1035, 1042, 2078, 16381, 1997, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1012, 1035, 1035, 1999, 4183, 1035, 1035, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2944, 2682, 11859, 1020, 9014, 2007, 1013, 5080, 1024, 14246, 2226, 1024, 1015, 11073, 1998, 1017, 2060, 9014, 2007, 1013, 5080, 1024, 14246, 2226, 1024, 1016, 11073, 1012, 102, 101, 1996, 2709, 3643, 1997, 2026, 1035, 2944, 3853, 2323, 2022, 2019, 9765, 9581, 6591, 5051, 2278, 6013, 1012, 102, 101, 1037, 3143, 2551, 2742, 2064, 2022, 2179, 1999, 23435, 12314, 4973, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 66, 110, 124, 155, 175, 189], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45459557", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [7, 25]}], [{"sent_id": 0, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [27, 44]}]], "sents": ["I think that you should use tf.nn.sigmoid_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits because you use sigmoid and 1 neuron in output layer.", "Also you need to remove the sigmoid from the last layer in the create_model_linear \nand, you're not using your y label, accuracy has to be of the following form.", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2228, 2008, 2017, 2323, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2612, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2138, 2017, 2224, 9033, 21693, 9314, 1998, 1015, 11265, 21017, 1999, 6434, 6741, 1012, 102, 101, 2036, 2017, 2342, 2000, 6366, 1996, 9033, 21693, 9314, 2013, 1996, 2197, 6741, 1999, 1996, 3443, 1035, 2944, 1035, 7399, 1998, 1010, 2017, 1005, 2128, 2025, 2478, 2115, 1061, 3830, 1010, 10640, 2038, 2000, 2022, 1997, 1996, 2206, 2433, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 59, 101, 115], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46040638", "vertexSet": [[{"sent_id": 3, "name": "tf.nn.conv2d_transpose", "pos": [123, 136]}], [{"sent_id": 2, "name": "tf.layers.conv2d_transpose", "pos": [57, 69]}]], "sents": ["The problem is that a convolution with 'SAME' padding and a stride of 2 will have an output shape of 24*14 for all the following input shapes:", "48*27, 48*28, 47*27 and 47*28", "Therefore tf.layers.conv2d_transpose can not know the correct output shape by itself and it goes with the symmetric case of doubling each of the dimensions: 24*14 -> 48*28", "If you want to use a different output shape you can use the lower level deconvolution: tf.nn.conv2d_transpose", "Using it allows you to specify the output shape.", "Alternatively you can adjust your whole network such that the input shape is always a multiple of the stride.", "Then the deconvolution will always predict the correct output shape."], "sent_idxs": [101, 1996, 3291, 2003, 2008, 1037, 9530, 6767, 7630, 3508, 2007, 1005, 2168, 1005, 11687, 4667, 1998, 1037, 18045, 1997, 1016, 2097, 2031, 2019, 6434, 4338, 1997, 2484, 1008, 2403, 2005, 2035, 1996, 2206, 7953, 10466, 1024, 102, 101, 4466, 1008, 2676, 1010, 4466, 1008, 2654, 1010, 4700, 1008, 2676, 1998, 4700, 1008, 2654, 102, 101, 3568, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2064, 2025, 2113, 1996, 6149, 6434, 4338, 2011, 2993, 1998, 2009, 3632, 2007, 1996, 19490, 2553, 1997, 19383, 2169, 1997, 1996, 9646, 1024, 2484, 1008, 2403, 1011, 1028, 4466, 1008, 2654, 102, 101, 2065, 2017, 2215, 2000, 2224, 1037, 2367, 6434, 4338, 2017, 2064, 2224, 1996, 2896, 2504, 21933, 2078, 6767, 7630, 3508, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 102, 101, 2478, 2009, 4473, 2017, 2000, 20648, 1996, 6434, 4338, 1012, 102, 101, 14084, 2017, 2064, 14171, 2115, 2878, 2897, 2107, 2008, 1996, 7953, 4338, 2003, 2467, 1037, 3674, 1997, 1996, 18045, 1012, 102, 101, 2059, 1996, 21933, 2078, 6767, 7630, 3508, 2097, 2467, 16014, 1996, 6149, 6434, 4338, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 38, 55, 101, 137, 149, 171, 188], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40981831", "vertexSet": [[{"sent_id": 0, "name": "tf.map_fn", "pos": [5, 12]}], [{"sent_id": 3, "name": "tf.mul", "pos": [88, 93]}], [{"sent_id": 3, "name": "tf.reduce_sum", "pos": [110, 116]}]], "sents": ["The TensorFlow function tf.map_fn(fn, elems) allows you to apply a function (fn) to each slice of a tensor (elems).", "For example, you could express your program as follows:", "<code>Code Snippet</code>.", "It may also be possible to implement your operation more concisely using broadcasting on the tf.mul() operator, which uses NumPy broadcasting semantics, and the axis argument to tf.reduce_sum()."], "sent_idxs": [101, 1996, 23435, 12314, 3853, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1042, 2078, 1010, 3449, 6633, 2015, 1007, 4473, 2017, 2000, 6611, 1037, 3853, 1006, 1042, 2078, 1007, 2000, 2169, 14704, 1997, 1037, 23435, 1006, 3449, 6633, 2015, 1007, 1012, 102, 101, 2005, 2742, 1010, 2017, 2071, 4671, 2115, 2565, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2009, 2089, 2036, 2022, 2825, 2000, 10408, 2115, 3169, 2062, 9530, 18380, 2135, 2478, 5062, 2006, 1996, 1056, 2546, 1012, 14163, 2140, 1006, 1007, 6872, 1010, 2029, 3594, 16371, 8737, 2100, 5062, 28081, 1010, 1998, 1996, 8123, 6685, 2000, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 43, 56, 70, 120], "sent_pos": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0]}, {"title": "37377303", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.local_response_normalization", "pos": [13, 25]}], [{"sent_id": 0, "name": "tf.nn.lrn", "pos": [1, 9]}]], "sents": ["tf.nn.lrn is a short for tf.nn.local_response_normalization.", "Therefore, the documentation you may want to look at is: https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization"], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 1048, 6826, 2003, 1037, 2460, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 2334, 1035, 3433, 1035, 3671, 3989, 1012, 102, 101, 3568, 1010, 1996, 12653, 2017, 2089, 2215, 2000, 2298, 2012, 2003, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 1050, 2078, 1013, 2334, 1035, 3433, 1035, 3671, 3989, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 27, 71], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42932979", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.convolution", "pos": [33, 43]}], [{"sent_id": 1, "name": "tf.layers.conv2d", "pos": [17, 26]}, {"sent_id": 2, "name": "tf.layers.conv2d", "pos": [58, 67]}]], "sents": ["For convolution, they are the same.", "More precisely, tf.layers.conv2d (actually _Conv) uses tf.nn.convolution as the backend.", "You can follow the calling chain of: tf.layers.conv2d>Conv2D>Conv2D.apply()>_Conv>_Conv.apply()>_Layer.apply()>_Layer.\\__call__()>_Conv.call()>nn.convolution()..."], "sent_idxs": [101, 2005, 9530, 6767, 7630, 3508, 1010, 2027, 2024, 1996, 2168, 1012, 102, 101, 2062, 10785, 1010, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1006, 2941, 1035, 9530, 2615, 1007, 3594, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 6767, 7630, 3508, 2004, 1996, 2067, 10497, 1012, 102, 101, 2017, 2064, 3582, 1996, 4214, 4677, 1997, 1024, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1028, 9530, 2615, 2475, 2094, 1028, 9530, 2615, 2475, 2094, 1012, 6611, 1006, 1007, 1028, 1035, 9530, 2615, 1028, 1035, 9530, 2615, 1012, 6611, 1006, 1007, 1028, 1035, 6741, 1012, 6611, 1006, 1007, 1028, 1035, 6741, 1012, 1032, 1035, 1035, 2655, 1035, 1035, 1006, 1007, 1028, 1035, 9530, 2615, 1012, 2655, 1006, 1007, 1028, 1050, 2078, 1012, 9530, 6767, 7630, 3508, 1006, 1007, 1012, 1012, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 13, 49, 134], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51898852", "vertexSet": [[{"sent_id": 2, "name": "tf.train.get_global_step", "pos": [40, 50]}, {"sent_id": 3, "name": "tf.train.get_global_step", "pos": [77, 87]}], [{"sent_id": 8, "name": "tf.train.global_step", "pos": [179, 187]}], [{"sent_id": 7, "name": "tf.train.get_or_create_global_step", "pos": [142, 156]}]], "sents": ["That is usually called in TensorFlow the \"global step\", and it has some helper functions for it:", "<code>Code Snippet</code>.", "tf.train.get_global_step really just creates a variable, but making sure it is not trainable and adding it to the right collections.", "You also have tf.train.get_global_step so you can do:", "<code>Code Snippet</code>.", "And at some other point in the code retrieve the tensor like:", "<code>Code Snippet</code>.", "Moreover, you can use tf.train.get_or_create_global_step if you are not sure which part of the graph definition will go first.", "The is one more function, tf.train.global_step, but I don't think it serves any purpose currently, since it just runs the given tensor in the given session and returns its value as an integer."], "sent_idxs": [101, 2008, 2003, 2788, 2170, 1999, 23435, 12314, 1996, 1000, 3795, 3357, 1000, 1010, 1998, 2009, 2038, 2070, 2393, 2121, 4972, 2005, 2009, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 2131, 1035, 3795, 1035, 3357, 2428, 2074, 9005, 1037, 8023, 1010, 2021, 2437, 2469, 2009, 2003, 2025, 3345, 3085, 1998, 5815, 2009, 2000, 1996, 2157, 6407, 1012, 102, 101, 2017, 2036, 2031, 1056, 2546, 1012, 3345, 1012, 2131, 1035, 3795, 1035, 3357, 2061, 2017, 2064, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2012, 2070, 2060, 2391, 1999, 1996, 3642, 12850, 1996, 23435, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 9308, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 3345, 1012, 2131, 1035, 2030, 1035, 3443, 1035, 3795, 1035, 3357, 2065, 2017, 2024, 2025, 2469, 2029, 2112, 1997, 1996, 10629, 6210, 2097, 2175, 2034, 1012, 102, 101, 1996, 2003, 2028, 2062, 3853, 1010, 1056, 2546, 1012, 3345, 1012, 3795, 1035, 3357, 1010, 2021, 1045, 2123, 1005, 1056, 2228, 2009, 4240, 2151, 3800, 2747, 1010, 2144, 2009, 2074, 3216, 1996, 2445, 23435, 1999, 1996, 2445, 5219, 1998, 5651, 2049, 3643, 2004, 2019, 16109, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 8]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 8]}, {"r": "S1", "h": 0, "t": 2, "evidence": [2, 7]}, {"r": "S1", "h": 2, "t": 0, "evidence": [2, 7]}, {"r": "S1", "h": 2, "t": 1, "evidence": [7, 8]}, {"r": "S1", "h": 1, "t": 2, "evidence": [7, 8]}], "na_triple": [], "sent_ends": [0, 25, 39, 73, 93, 107, 122, 136, 172, 220], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59454480", "vertexSet": [[{"sent_id": 3, "name": "tf.keras.losses.sparsecategoricalcrossentropy", "pos": [48, 64]}], [{"sent_id": 4, "name": "tf.keras.losses.categoricalcrossentropy", "pos": [75, 89]}], [{"sent_id": 4, "name": "tf.keras.losses.binarycrossentropy", "pos": [103, 116]}]], "sents": ["first of all:", "Your convergence problems may be due to \"incorrect\" loss function.", "tf.keras supports a variety of losses that depend on the shape of your input labels.", "Try different possibilities like \ntf.keras.losses.SparseCategoricalCrossentropy if your labels are one-hot vectors.", "tf.keras.losses.CategoricalCrossentropy if your lables are 1,2,3...\nor tf.keras.losses.BinaryCrossentropy if your labels are just 0,1.", "Honestly, this part of tf.keras is a bit tricky and some settings like that might need tuning.", "Second of all - this part:", "<code>Code Snippet</code>.", "assuming Xtst is your test set you want to scale it based on your training set.", "So the correct scaling would be just", "<code>Code Snippet</code>.", "Hope this helps!"], "sent_idxs": [101, 2034, 1997, 2035, 1024, 102, 101, 2115, 19143, 3471, 2089, 2022, 2349, 2000, 1000, 16542, 1000, 3279, 3853, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 6753, 1037, 3528, 1997, 6409, 2008, 12530, 2006, 1996, 4338, 1997, 2115, 7953, 10873, 1012, 102, 101, 3046, 2367, 12020, 2066, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 20288, 16280, 20255, 7476, 16458, 4765, 18981, 2100, 2065, 2115, 10873, 2024, 2028, 1011, 2980, 19019, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 4937, 27203, 16458, 4765, 18981, 2100, 2065, 2115, 6845, 4244, 2024, 1015, 1010, 1016, 1010, 1017, 1012, 1012, 1012, 2030, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 12441, 16458, 4765, 18981, 2100, 2065, 2115, 10873, 2024, 2074, 1014, 1010, 1015, 1012, 102, 101, 9826, 1010, 2023, 2112, 1997, 1056, 2546, 1012, 17710, 8180, 2003, 1037, 2978, 24026, 1998, 2070, 10906, 2066, 2008, 2453, 2342, 17372, 1012, 102, 101, 2117, 1997, 2035, 1011, 2023, 2112, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10262, 1060, 3215, 2102, 2003, 2115, 3231, 2275, 2017, 2215, 2000, 4094, 2009, 2241, 2006, 2115, 2731, 2275, 1012, 102, 101, 2061, 1996, 6149, 25169, 2052, 2022, 2074, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3246, 2023, 7126, 999, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 6, 21, 43, 74, 126, 151, 160, 174, 195, 204, 218, 224], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59018552", "vertexSet": [[{"sent_id": 3, "name": "tf.compat.v1", "pos": [143, 152]}], [{"sent_id": 1, "name": "tf.compat.v2", "pos": [54, 63]}]], "sents": ["tf.compat is the compatibility module.", "v1 is the version 1.x of TensorFlow, while v2 is the version 2.x of TensorFlow (so although strange, you have tf2 inside tf2, through tf.compat.v2).", "For the aliases: yes, if a function is an alias of another, you can use the one you prefer.", "However, using the compatibility module is often not needed and I recommend to do not reply on it too much (usually it is better to find the tf2 equivalent way of doing something, instead of writing code in the tf1 style, through tf.compat.v1)"], "sent_idxs": [101, 1056, 2546, 1012, 4012, 4502, 2102, 2003, 1996, 21778, 11336, 1012, 102, 101, 1058, 2487, 2003, 1996, 2544, 1015, 1012, 1060, 1997, 23435, 12314, 1010, 2096, 1058, 2475, 2003, 1996, 2544, 1016, 1012, 1060, 1997, 23435, 12314, 1006, 2061, 2348, 4326, 1010, 2017, 2031, 1056, 2546, 2475, 2503, 1056, 2546, 2475, 1010, 2083, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1007, 1012, 102, 101, 2005, 1996, 14593, 2229, 1024, 2748, 1010, 2065, 1037, 3853, 2003, 2019, 14593, 1997, 2178, 1010, 2017, 2064, 2224, 1996, 2028, 2017, 9544, 1012, 102, 101, 2174, 1010, 2478, 1996, 21778, 11336, 2003, 2411, 2025, 2734, 1998, 1045, 16755, 2000, 2079, 2025, 7514, 2006, 2009, 2205, 2172, 1006, 2788, 2009, 2003, 2488, 2000, 2424, 1996, 1056, 2546, 2475, 5662, 2126, 1997, 2725, 2242, 1010, 2612, 1997, 3015, 3642, 1999, 1996, 1056, 2546, 2487, 2806, 1010, 2083, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1007, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 3]}], "na_triple": [], "sent_ends": [0, 13, 66, 92, 154], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "47777046", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [1, 11]}], [{"sent_id": 0, "name": "tf.nn.convolution", "pos": [32, 42]}]], "sents": ["tf.nn.conv2d computes a 2-D convolution given 4-D input and filter tensors, while tf.nn.convolution computes sums of N-D convolutions.", "Both return a Tensor with same type of input.", "See tt.nn.convolution and tt.nn.con2d for further understanding."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 24134, 2015, 1037, 1016, 1011, 1040, 9530, 6767, 7630, 3508, 2445, 1018, 1011, 1040, 7953, 1998, 11307, 23435, 2015, 1010, 2096, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 6767, 7630, 3508, 24134, 2015, 20571, 1997, 1050, 1011, 1040, 9530, 6767, 7630, 9285, 1012, 102, 101, 2119, 2709, 1037, 23435, 2007, 2168, 2828, 1997, 7953, 1012, 102, 101, 2156, 23746, 1012, 1050, 2078, 1012, 9530, 6767, 7630, 3508, 1998, 23746, 1012, 1050, 2078, 1012, 9530, 2475, 2094, 2005, 2582, 4824, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 55, 67, 92], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56562119", "vertexSet": [[{"sent_id": 12, "name": "tf.compat.v1.placeholder", "pos": [229, 241]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [1, 6]}, {"sent_id": 15, "name": "tf.placeholder", "pos": [290, 295]}]], "sents": ["tf.placeholder() is meant to be fed to the session that when run receive the values from feed dict and perform the required operation.", "Generally, you would create a Session() with 'with' keyword and run it.", "But this might not favour all situations due to which you would require immediate execution.", "This is called eager execution.", "Example:", "generally, this is the procedure to run a Session:", "<code>Code Snippet</code>.", "But when we run with eager execution we run it as:", "<code>Code Snippet</code>.", "Therefore we need not run it inside a session explicitly and can be more intuitive in most of the cases.", "This provides more of an interactive execution.", "For further details visit:\nhttps://www.tensorflow.org/guide/eager", "If you are converting the code from tensorflow v1 to tensorflow v2, You must implement tf.compat.v1 and  Placeholder is present at tf.compat.v1.placeholder but this can only be executed in eager mode off.", "<code>Code Snippet</code>.", "TensorFlow released the eager execution mode, for which each node is immediately executed after definition.", "Statements using tf.placeholder are thus no longer valid."], "sent_idxs": [101, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 2003, 3214, 2000, 2022, 7349, 2000, 1996, 5219, 2008, 2043, 2448, 4374, 1996, 5300, 2013, 5438, 4487, 6593, 1998, 4685, 1996, 3223, 3169, 1012, 102, 101, 3227, 1010, 2017, 2052, 3443, 1037, 5219, 1006, 1007, 2007, 1005, 2007, 1005, 3145, 18351, 1998, 2448, 2009, 1012, 102, 101, 2021, 2023, 2453, 2025, 7927, 2035, 8146, 2349, 2000, 2029, 2017, 2052, 5478, 6234, 7781, 1012, 102, 101, 2023, 2003, 2170, 9461, 7781, 1012, 102, 101, 2742, 1024, 102, 101, 3227, 1010, 2023, 2003, 1996, 7709, 2000, 2448, 1037, 5219, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2021, 2043, 2057, 2448, 2007, 9461, 7781, 2057, 2448, 2009, 2004, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3568, 2057, 2342, 2025, 2448, 2009, 2503, 1037, 5219, 12045, 1998, 2064, 2022, 2062, 29202, 1999, 2087, 1997, 1996, 3572, 1012, 102, 101, 2023, 3640, 2062, 1997, 2019, 9123, 7781, 1012, 102, 101, 2005, 2582, 4751, 3942, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 9461, 102, 101, 2065, 2017, 2024, 16401, 1996, 3642, 2013, 23435, 12314, 1058, 2487, 2000, 23435, 12314, 1058, 2475, 1010, 2017, 2442, 10408, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1998, 2173, 14528, 2003, 2556, 2012, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 2173, 14528, 2021, 2023, 2064, 2069, 2022, 6472, 1999, 9461, 5549, 2125, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 23435, 12314, 2207, 1996, 9461, 7781, 5549, 1010, 2005, 2029, 2169, 13045, 2003, 3202, 6472, 2044, 6210, 1012, 102, 101, 8635, 2478, 1056, 2546, 1012, 2173, 14528, 2024, 2947, 2053, 2936, 9398, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [12, 14, 15]}, {"r": "S1", "h": 1, "t": 0, "evidence": [12, 14, 15]}], "na_triple": [], "sent_ends": [0, 33, 54, 72, 80, 84, 97, 111, 125, 139, 162, 172, 193, 253, 267, 287, 302], "sent_pos": [0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47021208", "vertexSet": [[{"sent_id": 2, "name": "tf.metrics.mean_squared_error", "pos": [50, 61]}], [{"sent_id": 3, "name": "tf.losses.mean_squared_error", "pos": [104, 114]}]], "sents": ["I don't really know why it works that way, but you actually need to run update before the inner state of the mse takes your data into account:", "<code>Code Snippet</code>.", "tf.metrics.mean_squared_error() is meant to compute the MSE on a whole dataset for instance, so you should not be using it if you want the result for batches independantly.", "For that, use tf.losses.mean_squared_error(a, b, loss_collection=None) for instance."], "sent_idxs": [101, 1045, 2123, 1005, 1056, 2428, 2113, 2339, 2009, 2573, 2008, 2126, 1010, 2021, 2017, 2941, 2342, 2000, 2448, 10651, 2077, 1996, 5110, 2110, 1997, 1996, 5796, 2063, 3138, 2115, 2951, 2046, 4070, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 12046, 2015, 1012, 2812, 1035, 19942, 1035, 7561, 1006, 1007, 2003, 3214, 2000, 24134, 1996, 5796, 2063, 2006, 1037, 2878, 2951, 13462, 2005, 6013, 1010, 2061, 2017, 2323, 2025, 2022, 2478, 2009, 2065, 2017, 2215, 1996, 2765, 2005, 14108, 2229, 27427, 13699, 10497, 15706, 1012, 102, 101, 2005, 2008, 1010, 2224, 1056, 2546, 1012, 6409, 1012, 2812, 1035, 19942, 1035, 7561, 1006, 1037, 1010, 1038, 1010, 3279, 1035, 3074, 1027, 3904, 1007, 2005, 6013, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3]}], "na_triple": [], "sent_ends": [0, 35, 49, 99, 129], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49225225", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [17, 27]}, {"sent_id": 1, "name": "tf.nn.conv2d", "pos": [53, 63]}], [{"sent_id": 0, "name": "tf.layers.conv2d", "pos": [28, 37]}, {"sent_id": 2, "name": "tf.layers.conv2d", "pos": [95, 104]}]], "sents": ["It's confusing, but TensorFlow has two conv2d methods: tf.nn.conv2d and tf.layers.conv2d.", "If you want to filter an image with a known kernel, call tf.nn.conv2d.", "If you want to create a layer in a convolutional neural network (CNN) that will determine its filters programmatically, call tf.layers.conv2d."], "sent_idxs": [101, 2009, 1005, 1055, 16801, 1010, 2021, 23435, 12314, 2038, 2048, 9530, 2615, 2475, 2094, 4725, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1998, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1012, 102, 101, 2065, 2017, 2215, 2000, 11307, 2019, 3746, 2007, 1037, 2124, 16293, 1010, 2655, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1012, 102, 101, 2065, 2017, 2215, 2000, 3443, 1037, 6741, 1999, 1037, 9530, 6767, 7630, 3508, 2389, 15756, 2897, 1006, 13229, 1007, 2008, 2097, 5646, 2049, 17736, 2565, 12644, 3973, 1010, 2655, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 39, 65, 106], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "48085077", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.dropout", "pos": [42, 50]}], [{"sent_id": 2, "name": "tf.layers.dropout", "pos": [56, 63]}, {"sent_id": 3, "name": "tf.layers.dropout", "pos": [83, 90]}]], "sents": ["On the training phase they are identical (as long as \"drop rate\" and \"keep rate\" are consistent).", "However, for evaluation (test) phase they are completely different.", "tf.nn.dropout will still do random dropping while tf.layers.dropout won't drop anything (transparent layer).", "In most cases it make sense to use tf.layers.dropout."], "sent_idxs": [101, 2006, 1996, 2731, 4403, 2027, 2024, 7235, 1006, 2004, 2146, 2004, 1000, 4530, 3446, 1000, 1998, 1000, 2562, 3446, 1000, 2024, 8335, 1007, 1012, 102, 101, 2174, 1010, 2005, 9312, 1006, 3231, 1007, 4403, 2027, 2024, 3294, 2367, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2097, 2145, 2079, 6721, 7510, 2096, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 2180, 1005, 1056, 4530, 2505, 1006, 13338, 6741, 1007, 1012, 102, 101, 1999, 2087, 3572, 2009, 2191, 3168, 2000, 2224, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}], "na_triple": [], "sent_ends": [0, 26, 41, 74, 92], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "54384163", "vertexSet": [[{"sent_id": 1, "name": "tf.data.experimental", "pos": [32, 38]}], [{"sent_id": 1, "name": "tf.contrib.data", "pos": [41, 49]}]], "sents": ["I just went through the source code of both TFRecordDataset and parallel_interleave.", "Note that I am looking at tf.data.experimental, as the tf.contrib.data one is deprecated.", "Funnily enough, they both call on the same class, ParallelInterleaveDataset to make use of parallel reading.", "I guess it then becomes the option of how better you can optimize your pipeline because you can use parameters like block_length, sloppy, buffer_output_elements and prefetch_input_elements when using parallel_interleave to potentially speed up your pipeline, while also imparting some randomness in ordering."], "sent_idxs": [101, 1045, 2074, 2253, 2083, 1996, 3120, 3642, 1997, 2119, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1998, 5903, 1035, 6970, 19738, 3726, 1012, 102, 101, 3602, 2008, 1045, 2572, 2559, 2012, 1056, 2546, 1012, 2951, 1012, 6388, 1010, 2004, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 2028, 2003, 2139, 28139, 12921, 1012, 102, 101, 4569, 3490, 2135, 2438, 1010, 2027, 2119, 2655, 2006, 1996, 2168, 2465, 1010, 5903, 18447, 2121, 19738, 28614, 18260, 2102, 2000, 2191, 2224, 1997, 5903, 3752, 1012, 102, 101, 1045, 3984, 2009, 2059, 4150, 1996, 5724, 1997, 2129, 2488, 2017, 2064, 23569, 27605, 4371, 2115, 13117, 2138, 2017, 2064, 2224, 11709, 2066, 3796, 1035, 3091, 1010, 28810, 1010, 17698, 1035, 6434, 1035, 3787, 1998, 3653, 7959, 10649, 1035, 7953, 1035, 3787, 2043, 2478, 5903, 1035, 6970, 19738, 3726, 2000, 9280, 3177, 2039, 2115, 13117, 1010, 2096, 2036, 17727, 8445, 2075, 2070, 6721, 2791, 1999, 13063, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 25, 56, 85, 154], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48791133", "vertexSet": [[{"sent_id": 5, "name": "tf.gather_nd", "pos": [105, 112]}], [{"sent_id": 5, "name": "tf.gather", "pos": [90, 94]}, {"sent_id": 5, "name": "tf.gather", "pos": [105, 109]}], [{"sent_id": 4, "name": "tf.slice", "pos": [66, 70]}], [{"sent_id": 6, "name": "tf.stack", "pos": [165, 169]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [6, 10]}], [{"sent_id": 4, "name": "tf.strided_slice", "pos": [71, 78]}]], "sents": ["Tensorflow indexing uses tf.Tensor.getitem:", "This operation extracts the specified region from the tensor.", "The notation is similar to NumPy with the restriction that currently only support basic indexing.", "That means that using a tensor as input is not currently allowed", "So using tf.slice and tf.strided_slice is out of the question as well.", "Whereas in tf.gather indices defines slices into the first dimension of Tensor, in tf.gather_nd, indices defines slices into the first N dimensions of the Tensor, where N = indices.shape[-1]", "Since you wanted the 3 values around the max, I manually extract the first, second and third element using a list comprehension, followed be a tf.stack", "<code>Code Snippet</code>.", "This will fail for the corner case where argm is the first or last element in the row, but it should be easy to resolve."], "sent_idxs": [101, 23435, 12314, 5950, 2075, 3594, 1056, 2546, 1012, 23435, 1012, 2131, 4221, 2213, 1024, 102, 101, 2023, 3169, 27059, 1996, 9675, 2555, 2013, 1996, 23435, 1012, 102, 101, 1996, 14869, 2003, 2714, 2000, 16371, 8737, 2100, 2007, 1996, 16840, 2008, 2747, 2069, 2490, 3937, 5950, 2075, 1012, 102, 101, 2008, 2965, 2008, 2478, 1037, 23435, 2004, 7953, 2003, 2025, 2747, 3039, 102, 101, 2061, 2478, 1056, 2546, 1012, 14704, 1998, 1056, 2546, 1012, 18045, 2094, 1035, 14704, 2003, 2041, 1997, 1996, 3160, 2004, 2092, 1012, 102, 101, 6168, 1999, 1056, 2546, 1012, 8587, 29299, 11859, 25609, 2046, 1996, 2034, 9812, 1997, 23435, 1010, 1999, 1056, 2546, 1012, 8587, 1035, 1050, 2094, 1010, 29299, 11859, 25609, 2046, 1996, 2034, 1050, 9646, 1997, 1996, 23435, 1010, 2073, 1050, 1027, 29299, 1012, 4338, 1031, 1011, 1015, 1033, 102, 101, 2144, 2017, 2359, 1996, 1017, 5300, 2105, 1996, 4098, 1010, 1045, 21118, 14817, 1996, 2034, 1010, 2117, 1998, 2353, 5783, 2478, 1037, 2862, 26683, 1010, 2628, 2022, 1037, 1056, 2546, 1012, 9991, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2097, 8246, 2005, 1996, 3420, 2553, 2073, 12098, 21693, 2003, 1996, 2034, 2030, 2197, 5783, 1999, 1996, 5216, 1010, 2021, 2009, 2323, 2022, 3733, 2000, 10663, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [5]}, {"r": "S1", "h": 0, "t": 1, "evidence": [5]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 16, 28, 49, 63, 87, 136, 170, 184, 214], "sent_pos": [0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53534547", "vertexSet": [[{"sent_id": 3, "name": "tf.keras", "pos": [107, 112]}, {"sent_id": 4, "name": "tf.keras", "pos": [129, 134]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [6, 12]}, {"sent_id": 3, "name": "tf.contrib", "pos": [67, 73]}], [{"sent_id": 0, "name": "tf.contrib.layers", "pos": [6, 14]}], [{"sent_id": 4, "name": "tf.keras.layers", "pos": [129, 136]}], [{"sent_id": 0, "name": "tf.layers", "pos": [16, 20]}, {"sent_id": 3, "name": "tf.layers", "pos": [90, 94]}]], "sents": ["The core functionality corresponding to tf.contrib.layers is in tf.layers.", "Some of the differences are discussed in this question.", "However this will not prepare you for TF 2.0.", "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras.", "So to best prepare for TF 2.0 you should start using tf.keras.layers instead.", "Here is a blog post about some of the practical differences to expect with TF 2.0."], "sent_idxs": [101, 1996, 4563, 15380, 7978, 2000, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 2003, 1999, 1056, 2546, 1012, 9014, 1012, 102, 101, 2070, 1997, 1996, 5966, 2024, 6936, 1999, 2023, 3160, 1012, 102, 101, 2174, 2023, 2097, 2025, 7374, 2017, 2005, 1056, 2546, 1016, 1012, 1014, 1012, 102, 101, 2065, 2115, 3125, 2003, 2000, 7374, 2115, 3642, 2005, 1056, 2546, 1016, 1012, 1014, 1010, 5136, 2008, 1056, 2546, 1012, 9530, 18886, 2497, 2097, 2022, 3718, 4498, 1006, 2593, 3975, 2013, 1056, 2546, 2030, 6377, 2046, 2009, 1007, 1998, 2008, 1056, 2546, 1012, 9014, 2205, 2097, 2022, 3718, 1998, 1996, 2152, 1011, 2504, 17928, 2097, 13960, 2104, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 2061, 2000, 2190, 7374, 2005, 1056, 2546, 1016, 1012, 1014, 2017, 2323, 2707, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 2612, 1012, 102, 101, 2182, 2003, 1037, 9927, 2695, 2055, 2070, 1997, 1996, 6742, 5966, 2000, 5987, 2007, 1056, 2546, 1016, 1012, 1014, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [3]}, {"r": "S1", "h": 2, "t": 4, "evidence": [0, 3]}, {"r": "S1", "h": 4, "t": 2, "evidence": [0, 3]}, {"r": "S1", "h": 4, "t": 3, "evidence": [3, 4]}, {"r": "S1", "h": 3, "t": 4, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2], [4, 0], [4, 1]], "sent_ends": [0, 22, 34, 49, 114, 139, 161], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44289344", "vertexSet": [[{"sent_id": 1, "name": "tf.image.decode_jpeg", "pos": [7, 17]}], [{"sent_id": 4, "name": "tf.image.resize_images", "pos": [101, 110]}], [{"sent_id": 2, "name": "tf.image.resize_image_with_crop_or_pad", "pos": [29, 46]}]], "sents": ["this happens because:", "tf.image.decode_jpeg returns a tf.uint8 tensor.", "tf.image.resize_image_with_crop_or_pad doesn't do any numerical operations on the pixel values, it just crops or pads to change the shape of the input tensor but returns a tensor of the same type.", "In this case, it's a tf.uint8 tensor..", "tf.image.resize_images returns a tf.float32 tensor.", "plt.imshow expects either a uint8 array or a float array with values between 0 and 1.", "Since the resized tensor is a float tensor but pixel values are not between 0 and 1, it's probably confusing plt.imgshow.", "Something like the following should fix things up:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 6433, 2138, 1024, 102, 101, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 5651, 1037, 1056, 2546, 1012, 21318, 3372, 2620, 23435, 1012, 102, 101, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 3746, 1035, 2007, 1035, 10416, 1035, 2030, 1035, 11687, 2987, 1005, 1056, 2079, 2151, 15973, 3136, 2006, 1996, 22138, 5300, 1010, 2009, 2074, 8765, 2030, 19586, 2000, 2689, 1996, 4338, 1997, 1996, 7953, 23435, 2021, 5651, 1037, 23435, 1997, 1996, 2168, 2828, 1012, 102, 101, 1999, 2023, 2553, 1010, 2009, 1005, 1055, 1037, 1056, 2546, 1012, 21318, 3372, 2620, 23435, 1012, 1012, 102, 101, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 5651, 1037, 1056, 2546, 1012, 14257, 16703, 23435, 1012, 102, 101, 20228, 2102, 1012, 10047, 22231, 2860, 24273, 2593, 1037, 21318, 3372, 2620, 9140, 2030, 1037, 14257, 9140, 2007, 5300, 2090, 1014, 1998, 1015, 1012, 102, 101, 2144, 1996, 24501, 3550, 23435, 2003, 1037, 14257, 23435, 2021, 22138, 5300, 2024, 2025, 2090, 1014, 1998, 1015, 1010, 2009, 1005, 1055, 2763, 16801, 20228, 2102, 1012, 10047, 5620, 14406, 1012, 102, 101, 2242, 2066, 1996, 2206, 2323, 8081, 2477, 2039, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 4]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 6, 28, 81, 100, 120, 146, 179, 190, 204], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40115530", "vertexSet": [[{"sent_id": 0, "name": "tf.compute_gradients", "pos": [1, 8]}], [{"sent_id": 2, "name": "tf.gradients", "pos": [62, 67]}, {"sent_id": 3, "name": "tf.gradients", "pos": [91, 96]}]], "sents": ["tf.compute_gradients computes derivative of a scalar quantity.", "If the quantity provided isn't scalar, it turns it into scalar by summing up the components which is what's happening in your example", "To compute full Hessian you need n calls to tf.gradients, The example is here.", "If you want just the diagonal part, then modify arguments to ith call to tf.gradients to differentiate with respect to ith variable, rather than all variables."], "sent_idxs": [101, 1056, 2546, 1012, 24134, 1035, 17978, 2015, 24134, 2015, 13819, 1997, 1037, 26743, 2099, 11712, 1012, 102, 101, 2065, 1996, 11712, 3024, 3475, 1005, 1056, 26743, 2099, 1010, 2009, 4332, 2009, 2046, 26743, 2099, 2011, 7680, 6562, 2039, 1996, 6177, 2029, 2003, 2054, 1005, 1055, 6230, 1999, 2115, 2742, 102, 101, 2000, 24134, 2440, 23484, 2937, 2017, 2342, 1050, 4455, 2000, 1056, 2546, 1012, 17978, 2015, 1010, 1996, 2742, 2003, 2182, 1012, 102, 101, 2065, 2017, 2215, 2074, 1996, 19754, 2112, 1010, 2059, 19933, 9918, 2000, 2009, 2232, 2655, 2000, 1056, 2546, 1012, 17978, 2015, 2000, 21032, 2007, 4847, 2000, 2009, 2232, 8023, 1010, 2738, 2084, 2035, 10857, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 18, 51, 74, 111], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38683387", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [28, 33]}], [{"sent_id": 1, "name": "tf.nn.conv2d", "pos": [28, 38]}]], "sents": ["subsample in Keras is the same as strides in tensorflow.", "You can use the strides argument in the tensorflow tf.nn.conv2d() function to implement this.", "Subsample / strides tells you how much to move the filter in each dimension as you perform the convolution.", "For instance with a stride of 1 in each direction you would shift the filter by one for each convolution and produce an output of the same size as the input (except for border padding effects).", "If strides was set to 2 the dimensions of the result would be half that of the original image."], "sent_idxs": [101, 4942, 21559, 10814, 1999, 17710, 8180, 2003, 1996, 2168, 2004, 22215, 1999, 23435, 12314, 1012, 102, 101, 2017, 2064, 2224, 1996, 22215, 6685, 1999, 1996, 23435, 12314, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1006, 1007, 3853, 2000, 10408, 2023, 1012, 102, 101, 4942, 21559, 10814, 1013, 22215, 4136, 2017, 2129, 2172, 2000, 2693, 1996, 11307, 1999, 2169, 9812, 2004, 2017, 4685, 1996, 9530, 6767, 7630, 3508, 1012, 102, 101, 2005, 6013, 2007, 1037, 18045, 1997, 1015, 1999, 2169, 3257, 2017, 2052, 5670, 1996, 11307, 2011, 2028, 2005, 2169, 9530, 6767, 7630, 3508, 1998, 3965, 2019, 6434, 1997, 1996, 2168, 2946, 2004, 1996, 7953, 1006, 3272, 2005, 3675, 11687, 4667, 3896, 1007, 1012, 102, 101, 2065, 22215, 2001, 2275, 2000, 1016, 1996, 9646, 1997, 1996, 2765, 2052, 2022, 2431, 2008, 1997, 1996, 2434, 3746, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 17, 46, 73, 118, 140], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41471616", "vertexSet": [[{"sent_id": 0, "name": "tf.map_fn", "pos": [8, 15]}, {"sent_id": 2, "name": "tf.map_fn", "pos": [81, 88]}, {"sent_id": 5, "name": "tf.map_fn", "pos": [159, 166]}], [{"sent_id": 4, "name": "tf.reduce_sum", "pos": [145, 151]}], [{"sent_id": 3, "name": "tf.multiply", "pos": [120, 125]}]], "sents": ["The TensorFlow Python API includes the tf.map_fn(fn, elems) higher-order operator, which allows you to specify a (Python) function fn that will be applied to each slice of elems in the 0th dimension (i.e.", "to each row if elems is a matrix).", "Note that, while tf.map_fn() is very general, it may be more efficient to use specialized ops that either broadcast their arguments on one or more dimensions (e.g.", "tf.multiply()), or reduce in parallel across one or more dimensions (e.g.", "tf.reduce_sum()).", "However, tf.map_fn() is useful when there is no built-in operator to do what you want."], "sent_idxs": [101, 1996, 23435, 12314, 18750, 17928, 2950, 1996, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1042, 2078, 1010, 3449, 6633, 2015, 1007, 3020, 1011, 2344, 6872, 1010, 2029, 4473, 2017, 2000, 20648, 1037, 1006, 18750, 1007, 3853, 1042, 2078, 2008, 2097, 2022, 4162, 2000, 2169, 14704, 1997, 3449, 6633, 2015, 1999, 1996, 1014, 2705, 9812, 1006, 1045, 1012, 1041, 1012, 102, 101, 2000, 2169, 5216, 2065, 3449, 6633, 2015, 2003, 1037, 8185, 1007, 1012, 102, 101, 3602, 2008, 1010, 2096, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2003, 2200, 2236, 1010, 2009, 2089, 2022, 2062, 8114, 2000, 2224, 7772, 23092, 2008, 2593, 3743, 2037, 9918, 2006, 2028, 2030, 2062, 9646, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 4800, 22086, 1006, 1007, 1007, 1010, 2030, 5547, 1999, 5903, 2408, 2028, 2030, 2062, 9646, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1007, 1007, 1012, 102, 101, 2174, 1010, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2003, 6179, 2043, 2045, 2003, 2053, 2328, 1011, 1999, 6872, 2000, 2079, 2054, 2017, 2215, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 5]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 62, 76, 119, 144, 156, 185], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49716693", "vertexSet": [[{"sent_id": 1, "name": "tf.train.exponentialmovingaverage", "pos": [9, 19]}], [{"sent_id": 3, "name": "tf.train.momentumoptimizer", "pos": [77, 87]}]], "sents": ["You're totally wrong.", "tf.train.ExponentialMovingAverage implements just the exponential moving average: the shadow_variable is the moving average at the current time step and it's updated using the formula you posted.", "Every time you execute the node that holds the moving average, what happens is just the execution of that formula.", "The tf.train.MomentumOptimizer, instead, is a way more complex object.", "In short, it implements a parameter update algorithm called Gradient Descent with Momentum that computes the gradient of the model parameters and executes the update step of every single network parameter using the computed gradient + the momentum term that's accumulated over the training steps.", "The momentum term is, of course, the moving average of the gradients.", "But the two functions execute different operations and have different aims."], "sent_idxs": [101, 2017, 1005, 2128, 6135, 3308, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 27258, 5302, 6455, 22208, 4270, 22164, 2074, 1996, 27258, 3048, 2779, 1024, 1996, 5192, 1035, 8023, 2003, 1996, 3048, 2779, 2012, 1996, 2783, 2051, 3357, 1998, 2009, 1005, 1055, 7172, 2478, 1996, 5675, 2017, 6866, 1012, 102, 101, 2296, 2051, 2017, 15389, 1996, 13045, 2008, 4324, 1996, 3048, 2779, 1010, 2054, 6433, 2003, 2074, 1996, 7781, 1997, 2008, 5675, 1012, 102, 101, 1996, 1056, 2546, 1012, 3345, 1012, 11071, 7361, 3775, 4328, 6290, 1010, 2612, 1010, 2003, 1037, 2126, 2062, 3375, 4874, 1012, 102, 101, 1999, 2460, 1010, 2009, 22164, 1037, 16381, 10651, 9896, 2170, 17978, 6934, 2007, 11071, 2008, 24134, 2015, 1996, 17978, 1997, 1996, 2944, 11709, 1998, 15389, 2015, 1996, 10651, 3357, 1997, 2296, 2309, 2897, 16381, 2478, 1996, 24806, 17978, 1009, 1996, 11071, 2744, 2008, 1005, 1055, 14830, 2058, 1996, 2731, 4084, 1012, 102, 101, 1996, 11071, 2744, 2003, 1010, 1997, 2607, 1010, 1996, 3048, 2779, 1997, 1996, 17978, 2015, 1012, 102, 101, 2021, 1996, 2048, 4972, 15389, 2367, 3136, 1998, 2031, 2367, 8704, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4, 5, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4, 5, 6]}], "na_triple": [], "sent_ends": [0, 8, 51, 75, 98, 151, 169, 183], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36267491", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.rnn", "pos": [11, 19]}, {"sent_id": 3, "name": "tf.nn.rnn", "pos": [89, 97]}], [{"sent_id": 0, "name": "tf.nn.dynamic_rnn", "pos": [21, 31]}], [{"sent_id": 4, "name": "tf.cond", "pos": [137, 142]}]], "sents": ["For RNNs specifically, there are two options: tf.nn.rnn, and tf.nn.dynamic_rnn.", "Neither one creates or destroys graphs temporarily.", "The first function creates T subgraphs, where T is the length of the python list of inputs you provide (that is, inputs is a len T python list of shape [batch, depth] tensors).", "tf.nn.rnn always expects a fixed number of time steps.", "Note, you can control which subgraphs are run for a given step by passing the sequence_length parameter; the function then uses conditional evaluation (tf.cond) to determine what operations get run.", "In contrast, dynamic_rnn uses a special TensorFlow while loop and other trickery that introduces a limited type of loop in the graph structure.", "In this case, there is exactly one subgraph for the \"time step\", and it gets run over and over until your input has been processed.", "In this case your input is a 3D tensor with dimensions [batch, time, depth] (it can be [time, batch, depth] if you set time_major=True); and the first two dimensions can vary from step to step."], "sent_idxs": [101, 2005, 29300, 3619, 4919, 1010, 2045, 2024, 2048, 7047, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1010, 1998, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1012, 102, 101, 4445, 2028, 9005, 2030, 20735, 19287, 8184, 1012, 102, 101, 1996, 2034, 3853, 9005, 1056, 4942, 27341, 1010, 2073, 1056, 2003, 1996, 3091, 1997, 1996, 18750, 2862, 1997, 20407, 2017, 3073, 1006, 2008, 2003, 1010, 20407, 2003, 1037, 18798, 1056, 18750, 2862, 1997, 4338, 1031, 14108, 1010, 5995, 1033, 23435, 2015, 1007, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 2467, 24273, 1037, 4964, 2193, 1997, 2051, 4084, 1012, 102, 101, 3602, 1010, 2017, 2064, 2491, 2029, 4942, 27341, 2024, 2448, 2005, 1037, 2445, 3357, 2011, 4458, 1996, 5537, 1035, 3091, 16381, 1025, 1996, 3853, 2059, 3594, 18462, 9312, 1006, 1056, 2546, 1012, 9530, 2094, 1007, 2000, 5646, 2054, 3136, 2131, 2448, 1012, 102, 101, 1999, 5688, 1010, 8790, 1035, 29300, 2078, 3594, 1037, 2569, 23435, 12314, 2096, 7077, 1998, 2060, 7577, 7301, 2008, 13999, 1037, 3132, 2828, 1997, 7077, 1999, 1996, 10629, 3252, 1012, 102, 101, 1999, 2023, 2553, 1010, 2045, 2003, 3599, 2028, 4942, 14413, 2005, 1996, 1000, 2051, 3357, 1000, 1010, 1998, 2009, 4152, 2448, 2058, 1998, 2058, 2127, 2115, 7953, 2038, 2042, 13995, 1012, 102, 101, 1999, 2023, 2553, 2115, 7953, 2003, 1037, 7605, 23435, 2007, 9646, 1031, 14108, 1010, 2051, 1010, 5995, 1033, 1006, 2009, 2064, 2022, 1031, 2051, 1010, 14108, 1010, 5995, 1033, 2065, 2017, 2275, 2051, 1035, 2350, 1027, 2995, 1007, 1025, 1998, 1996, 2034, 2048, 9646, 2064, 8137, 2013, 3357, 2000, 3357, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4, 5, 6, 7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4, 5, 6, 7]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 43, 88, 107, 151, 183, 216, 269], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48223162", "vertexSet": [[{"sent_id": 0, "name": "tf.layers.conv1d", "pos": [1, 10]}, {"sent_id": 4, "name": "tf.layers.conv1d", "pos": [125, 134]}], [{"sent_id": 1, "name": "tf.layers.conv2d", "pos": [47, 56]}, {"sent_id": 3, "name": "tf.layers.conv2d", "pos": [100, 109]}]], "sents": ["tf.layers.conv1d is used when you slide your convolution kernels along 1 dimensions (i.e.", "you reuse the same weights, sliding them along 1 dimensions), whereas tf.layers.conv2d is used when you slide your convolution kernels along 2 dimensions (i.e.", "you reuse the same weights, sliding them along 2 dimensions).", "So the typical use case for tf.layers.conv2d is if you have a 2D image.", "And possible use-cases for tf.layers.conv1d are, for example:", "Convolutions in Time.", "Convolutions on Piano notes."], "sent_idxs": [101, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 2003, 2109, 2043, 2017, 7358, 2115, 9530, 6767, 7630, 3508, 16293, 2015, 2247, 1015, 9646, 1006, 1045, 1012, 1041, 1012, 102, 101, 2017, 2128, 8557, 1996, 2168, 15871, 1010, 8058, 2068, 2247, 1015, 9646, 1007, 1010, 6168, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2003, 2109, 2043, 2017, 7358, 2115, 9530, 6767, 7630, 3508, 16293, 2015, 2247, 1016, 9646, 1006, 1045, 1012, 1041, 1012, 102, 101, 2017, 2128, 8557, 1996, 2168, 15871, 1010, 8058, 2068, 2247, 1016, 9646, 1007, 1012, 102, 101, 2061, 1996, 5171, 2224, 2553, 2005, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2003, 2065, 2017, 2031, 1037, 14134, 3746, 1012, 102, 101, 1998, 2825, 2224, 1011, 3572, 2005, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 2024, 1010, 2005, 2742, 1024, 102, 101, 9530, 6767, 7630, 9285, 1999, 2051, 1012, 102, 101, 9530, 6767, 7630, 9285, 2006, 3682, 3964, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4, 5, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4, 5, 6]}], "na_triple": [], "sent_ends": [0, 31, 77, 93, 118, 140, 149, 159], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48910328", "vertexSet": [[{"sent_id": 9, "name": "tf.contrib.metrics.accuracy", "pos": [149, 160]}], [{"sent_id": 9, "name": "tf.metrics.accuracy", "pos": [162, 169]}, {"sent_id": 15, "name": "tf.metrics.accuracy", "pos": [275, 282]}], [{"sent_id": 3, "name": "tf.local_variables_initializer", "pos": [36, 45]}]], "sents": ["<code>Code Snippet</code>.", "output:", "<code>Code Snippet</code>.", "Simply moving the tf.local_variables_initializer() inside the loop makes sure that the values in accuracy metric tensor get's re-initialised.", "Why does it work?", "As per the documentation", "The accuracy function creates two local variables, total and count\n  that are used to compute the frequency with which predictions matches\n  labels.", "If we don't reinitialise the local variables then value from previous iteration remains in it, leading to wrong results as you were experiencing.", "The other approach is to use:", "tf.contrib.metrics.accuracy instead of tf.metrics.accuracy.", "But this gives some residual value at the end like 0.800000011920929 instead of 0.8.", "It's also deprecated as pointed out by OP in comments.", "Source:", "https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy", "https://github.com/tensorflow/tensorflow/issues/3971", "How to properly use tf.metrics.accuracy?"], "sent_idxs": [101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3432, 3048, 1996, 1056, 2546, 1012, 2334, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 2503, 1996, 7077, 3084, 2469, 2008, 1996, 5300, 1999, 10640, 12046, 23435, 2131, 1005, 1055, 2128, 1011, 3988, 5084, 1012, 102, 101, 2339, 2515, 2009, 2147, 1029, 102, 101, 2004, 2566, 1996, 12653, 102, 101, 1996, 10640, 3853, 9005, 2048, 2334, 10857, 1010, 2561, 1998, 4175, 2008, 2024, 2109, 2000, 24134, 1996, 6075, 2007, 2029, 20932, 3503, 10873, 1012, 102, 101, 2065, 2057, 2123, 1005, 1056, 27788, 29050, 6856, 2063, 1996, 2334, 10857, 2059, 3643, 2013, 3025, 27758, 3464, 1999, 2009, 1010, 2877, 2000, 3308, 3463, 2004, 2017, 2020, 13417, 1012, 102, 101, 1996, 2060, 3921, 2003, 2000, 2224, 1024, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 12046, 2015, 1012, 10640, 2612, 1997, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 1012, 102, 101, 2021, 2023, 3957, 2070, 21961, 3643, 2012, 1996, 2203, 2066, 1014, 1012, 5385, 8889, 8889, 14526, 2683, 11387, 2683, 24594, 2612, 1997, 1014, 1012, 1022, 1012, 102, 101, 2009, 1005, 1055, 2036, 2139, 28139, 12921, 2004, 4197, 2041, 2011, 6728, 1999, 7928, 1012, 102, 101, 3120, 1024, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 12046, 2015, 1013, 10640, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3314, 1013, 4464, 2581, 2487, 102, 101, 2129, 2000, 7919, 2224, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 1029, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [9, 10, 11]}, {"r": "S1", "h": 1, "t": 0, "evidence": [9, 10, 11]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 18, 32, 68, 75, 81, 107, 139, 148, 171, 199, 216, 220, 247, 270, 284], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "36744555", "vertexSet": [[{"sent_id": 0, "name": "tf.diag_part", "pos": [15, 22]}], [{"sent_id": 2, "name": "tf.linalg.tensor_diag_part", "pos": [43, 56]}]], "sents": ["with tensorflow 0.8 its possible to extract the diagonal elements with tf.diag_part() (see documentation)", "UPDATE", "for tensorflow >= r1.12 its tf.linalg.tensor_diag_part (see documentation)"], "sent_idxs": [101, 2007, 23435, 12314, 1014, 1012, 1022, 2049, 2825, 2000, 14817, 1996, 19754, 3787, 2007, 1056, 2546, 1012, 22939, 2290, 1035, 2112, 1006, 1007, 1006, 2156, 12653, 1007, 102, 101, 10651, 102, 101, 2005, 23435, 12314, 1028, 1027, 1054, 2487, 1012, 2260, 2049, 1056, 2546, 1012, 27022, 2140, 2290, 1012, 23435, 1035, 22939, 2290, 1035, 2112, 1006, 2156, 12653, 1007, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2]}], "na_triple": [], "sent_ends": [0, 29, 32, 61], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0]}, {"title": "39981684", "vertexSet": [[{"sent_id": 0, "name": "tf.name_scope", "pos": [11, 17]}, {"sent_id": 1, "name": "tf.name_scope", "pos": [20, 26]}], [{"sent_id": 0, "name": "tf.variable_scope", "pos": [3, 9]}], [{"sent_id": 1, "name": "tf.get_variable", "pos": [37, 43]}]], "sents": ["Just use tf.variable_scope instead of tf.name_scope.", "tf.name_scope doesn't add prefixes to the variables created with tf.get_variable()."], "sent_idxs": [101, 2074, 2224, 1056, 2546, 1012, 8023, 1035, 9531, 2612, 1997, 1056, 2546, 1012, 2171, 1035, 9531, 1012, 102, 101, 1056, 2546, 1012, 2171, 1035, 9531, 2987, 1005, 1056, 5587, 17576, 2229, 2000, 1996, 10857, 2580, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 47], "sent_pos": [0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0]}, {"title": "47987212", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib.layers.xavier_initializer", "pos": [3, 16]}, {"sent_id": 2, "name": "tf.contrib.layers.xavier_initializer", "pos": [76, 89]}], [{"sent_id": 0, "name": "tf.glorot_uniform_initializer", "pos": [17, 28]}, {"sent_id": 1, "name": "tf.glorot_uniform_initializer", "pos": [63, 74]}]], "sents": ["Yes, tf.contrib.layers.xavier_initializer and tf.glorot_uniform_initializer both implement the same concept described in this JMLR paper: Understanding the difficulty of training deep feedforward neural networks, which can be seen in the code:", "tf.glorot_uniform_initializer", "tf.contrib.layers.xavier_initializer", "With typical values for fan_in, fan_out, mode = FAN_AVG , and uniform = True, both implementations sample values from the standard uniform distribution over the limit [-sqrt(3), sqrt(3))", "Because tf.initializer has support for a wide variety of initialization strategies, it's highly likely that it will stay whereas the initialization from contrib which just has xavier_initialization will most probably be deprecated in future versions.", "So, yes it's highly likely that in future versions the tf.contrib.layers.xavier_initialier way of initialization might go away."], "sent_idxs": [101, 2748, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 10062, 1035, 3988, 17629, 1998, 1056, 2546, 1012, 1043, 10626, 4140, 1035, 6375, 1035, 3988, 17629, 2119, 10408, 1996, 2168, 4145, 2649, 1999, 2023, 1046, 19968, 2099, 3259, 1024, 4824, 1996, 7669, 1997, 2731, 2784, 5438, 29278, 7652, 15756, 6125, 1010, 2029, 2064, 2022, 2464, 1999, 1996, 3642, 1024, 102, 101, 1056, 2546, 1012, 1043, 10626, 4140, 1035, 6375, 1035, 3988, 17629, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 10062, 1035, 3988, 17629, 102, 101, 2007, 5171, 5300, 2005, 5470, 1035, 1999, 1010, 5470, 1035, 2041, 1010, 5549, 1027, 5470, 1035, 20704, 2290, 1010, 1998, 6375, 1027, 2995, 1010, 2119, 24977, 7099, 5300, 2013, 1996, 3115, 6375, 4353, 2058, 1996, 5787, 1031, 1011, 5490, 5339, 1006, 1017, 1007, 1010, 5490, 5339, 1006, 1017, 1007, 1007, 102, 101, 2138, 1056, 2546, 1012, 3988, 17629, 2038, 2490, 2005, 1037, 2898, 3528, 1997, 3988, 3989, 9942, 1010, 2009, 1005, 1055, 3811, 3497, 2008, 2009, 2097, 2994, 6168, 1996, 3988, 3989, 2013, 9530, 18886, 2497, 2029, 2074, 2038, 10062, 1035, 3988, 3989, 2097, 2087, 2763, 2022, 2139, 28139, 12921, 1999, 2925, 4617, 1012, 102, 101, 2061, 1010, 2748, 2009, 1005, 1055, 3811, 3497, 2008, 1999, 2925, 4617, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 10062, 1035, 3988, 3771, 2126, 1997, 3988, 3989, 2453, 2175, 2185, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 3, 4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 3, 4, 5]}], "na_triple": [], "sent_ends": [0, 62, 75, 90, 142, 196, 232], "sent_pos": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42144422", "vertexSet": [[{"sent_id": 3, "name": "tf.nn.batch_norm", "pos": [86, 95]}], [{"sent_id": 1, "name": "tf.contrib.layers.batch_norm", "pos": [22, 34]}, {"sent_id": 3, "name": "tf.contrib.layers.batch_norm", "pos": [113, 125]}], [{"sent_id": 1, "name": "tf.nn.batch_normalization", "pos": [41, 51]}]], "sents": ["tf.contrib refers to a high level machine learning API over Tensorflow.", "So tf.contrib.layers.batch_norm basically acts as a wrapper over tf.nn.batch_normalization making it easier to use but less flexible than the latter.", "So, depending upon the application you can use whatever fits your needs.", "For example in case of tf.nn.batch_norm you will have to give a symbolic link to mean and variance tensors by yourself, but tf.contrib.layers.batch_norm will take care of that for you."], "sent_idxs": [101, 1056, 2546, 1012, 9530, 18886, 2497, 5218, 2000, 1037, 2152, 2504, 3698, 4083, 17928, 2058, 23435, 12314, 1012, 102, 101, 2061, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 14108, 1035, 13373, 10468, 4490, 2004, 1037, 10236, 4842, 2058, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 3671, 3989, 2437, 2009, 6082, 2000, 2224, 2021, 2625, 12379, 2084, 1996, 3732, 1012, 102, 101, 2061, 1010, 5834, 2588, 1996, 4646, 2017, 2064, 2224, 3649, 16142, 2115, 3791, 1012, 102, 101, 2005, 2742, 1999, 2553, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 14108, 1035, 13373, 2017, 2097, 2031, 2000, 2507, 1037, 12613, 4957, 2000, 2812, 1998, 23284, 23435, 2015, 2011, 4426, 1010, 2021, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 14108, 1035, 13373, 2097, 2202, 2729, 1997, 2008, 2005, 2017, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 64, 80, 134], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38101834", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [28, 45]}], [{"sent_id": 7, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [104, 123]}]], "sents": ["I would just like to add 2 things to accepted answer that you can also find in TF documentation.", "First:", "tf.nn.softmax_cross_entropy_with_logits", "NOTE: While the classes are mutually exclusive, their probabilities\nneed not be.", "All that is required is that each row of labels is a\nvalid probability distribution.", "If they are not, the computation of\nthe gradient will be incorrect.", "Second:", "tf.nn.sparse_softmax_cross_entropy_with_logits", "NOTE: For this operation, the probability of a given label is\nconsidered exclusive.", "That is, soft classes are not allowed, and the\nlabels vector must provide a single specific index for the true class\nfor each row of logits (each minibatch entry)."], "sent_idxs": [101, 1045, 2052, 2074, 2066, 2000, 5587, 1016, 2477, 2000, 3970, 3437, 2008, 2017, 2064, 2036, 2424, 1999, 1056, 2546, 12653, 1012, 102, 101, 2034, 1024, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 102, 101, 3602, 1024, 2096, 1996, 4280, 2024, 20271, 7262, 1010, 2037, 4013, 3676, 14680, 2342, 2025, 2022, 1012, 102, 101, 2035, 2008, 2003, 3223, 2003, 2008, 2169, 5216, 1997, 10873, 2003, 1037, 9398, 9723, 4353, 1012, 102, 101, 2065, 2027, 2024, 2025, 1010, 1996, 22334, 1997, 1996, 17978, 2097, 2022, 16542, 1012, 102, 101, 2117, 1024, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 102, 101, 3602, 1024, 2005, 2023, 3169, 1010, 1996, 9723, 1997, 1037, 2445, 3830, 2003, 2641, 7262, 1012, 102, 101, 2008, 2003, 1010, 3730, 4280, 2024, 2025, 3039, 1010, 1998, 1996, 10873, 9207, 2442, 3073, 1037, 2309, 3563, 5950, 2005, 1996, 2995, 2465, 2005, 2169, 5216, 1997, 8833, 12762, 1006, 2169, 7163, 14479, 2818, 4443, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3, 4, 5, 7, 8, 9]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3, 4, 5, 7, 8, 9]}], "na_triple": [], "sent_ends": [0, 23, 27, 46, 65, 83, 99, 103, 124, 142, 181], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50090357", "vertexSet": [[{"sent_id": 1, "name": "tf.sigmoid", "pos": [51, 57]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [1, 9]}], [{"sent_id": 1, "name": "tf.greater", "pos": [62, 66]}], [{"sent_id": 7, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [158, 176]}]], "sents": ["tf.nn.softmax() computes probability distribution over classes (output neurons), if you have just 1 output neuron then probability distribution over 1 neuron will always be 1.0.", "I would suggest to use tf.sigmoid() in combination with tf.greater(), e.g:", "<code>Code Snippet</code>.", "prediction will return True if A3 is greater than 0.5, False otherwise.", "You can convert boolean prediction to other types such as integers:", "<code>Code Snippet</code>.", "This way True will be converted to 1 and False to 0.", "As for cost function, use tf.nn.sigmoid_cross_entropy_with_logits():", "<code>Code Snippet</code>.", "This post has more details about single neuron binary classification, I would further suggest you look into it."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1007, 24134, 2015, 9723, 4353, 2058, 4280, 1006, 6434, 15698, 1007, 1010, 2065, 2017, 2031, 2074, 1015, 6434, 11265, 21017, 2059, 9723, 4353, 2058, 1015, 11265, 21017, 2097, 2467, 2022, 1015, 1012, 1014, 1012, 102, 101, 1045, 2052, 6592, 2000, 2224, 1056, 2546, 1012, 9033, 21693, 9314, 1006, 1007, 1999, 5257, 2007, 1056, 2546, 1012, 3618, 1006, 1007, 1010, 1041, 1012, 1043, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 17547, 2097, 2709, 2995, 2065, 1037, 2509, 2003, 3618, 2084, 1014, 1012, 1019, 1010, 6270, 4728, 1012, 102, 101, 2017, 2064, 10463, 22017, 20898, 17547, 2000, 2060, 4127, 2107, 2004, 24028, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2126, 2995, 2097, 2022, 4991, 2000, 1015, 1998, 6270, 2000, 1014, 1012, 102, 101, 2004, 2005, 3465, 3853, 1010, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2695, 2038, 2062, 4751, 2055, 2309, 11265, 21017, 12441, 5579, 1010, 1045, 2052, 2582, 6592, 2017, 2298, 2046, 2009, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 45, 74, 88, 107, 122, 136, 151, 180, 194, 217], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48063821", "vertexSet": [[{"sent_id": 2, "name": "tf.metrics.accuracy", "pos": [148, 155]}], [{"sent_id": 1, "name": "tf.metrics.precision_at_thresholds", "pos": [70, 82]}], [{"sent_id": 0, "name": "tf.metrics.precision", "pos": [1, 8]}, {"sent_id": 1, "name": "tf.metrics.precision", "pos": [70, 77]}], [{"sent_id": 2, "name": "tf.argmax", "pos": [108, 114]}, {"sent_id": 2, "name": "tf.argmax", "pos": [158, 164]}]], "sents": ["tf.metrics.precision is meant to be used in binary classification problems only, and its arguments must be all 0 or 1 since, as the docs say, they will be converted to bool.", "If you are indeed working in a binary classification problem but want to use the logits as parameter, you can look at tf.metrics.precision_at_thresholds, which allows you to specify the threshold at which a prediction will be considered true.", "However, since your manual calculation uses tf.argmax, it looks more like a multiclass classification problem, in which case you don't usually talk about precision/recall, but just accuracy, so you can look at tf.metrics.accuracy, and pass tf.argmax(self.input_y, 1) and self.predictions as parameters."], "sent_idxs": [101, 1056, 2546, 1012, 12046, 2015, 1012, 11718, 2003, 3214, 2000, 2022, 2109, 1999, 12441, 5579, 3471, 2069, 1010, 1998, 2049, 9918, 2442, 2022, 2035, 1014, 2030, 1015, 2144, 1010, 2004, 1996, 9986, 2015, 2360, 1010, 2027, 2097, 2022, 4991, 2000, 22017, 2140, 1012, 102, 101, 2065, 2017, 2024, 5262, 2551, 1999, 1037, 12441, 5579, 3291, 2021, 2215, 2000, 2224, 1996, 8833, 12762, 2004, 16381, 1010, 2017, 2064, 2298, 2012, 1056, 2546, 1012, 12046, 2015, 1012, 11718, 1035, 2012, 1035, 11207, 2015, 1010, 2029, 4473, 2017, 2000, 20648, 1996, 11207, 2012, 2029, 1037, 17547, 2097, 2022, 2641, 2995, 1012, 102, 101, 2174, 1010, 2144, 2115, 6410, 17208, 3594, 1056, 2546, 1012, 12098, 21693, 8528, 1010, 2009, 3504, 2062, 2066, 1037, 4800, 26266, 5579, 3291, 1010, 1999, 2029, 2553, 2017, 2123, 1005, 1056, 2788, 2831, 2055, 11718, 1013, 9131, 1010, 2021, 2074, 10640, 1010, 2061, 2017, 2064, 2298, 2012, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 1010, 1998, 3413, 1056, 2546, 1012, 12098, 21693, 8528, 1006, 2969, 1012, 7953, 1035, 1061, 1010, 1015, 1007, 1998, 2969, 1012, 20932, 2004, 11709, 1012, 102], "labels": [{"r": "S1", "h": 2, "t": 0, "evidence": [0, 2]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 2]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1]}], "na_triple": [[0, 1], [0, 3], [1, 0], [1, 3], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 45, 100, 181], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57427659", "vertexSet": [[{"sent_id": 2, "name": "tf.image.resize_images", "pos": [82, 91]}], [{"sent_id": 4, "name": "tf.image.pad_to_bounding_box", "pos": [118, 131]}]], "sents": ["The issue comes from the fact that the dataset contains variable-sized images (see the dataset description here).", "Tensorflow can only batch together things with the same shape, so you first need to either reshape the images to a common shape (e.g., the input shape of your network) or pad them accordingly.", "If you want to resize, use tf.image.resize_images:", "<code>Code Snippet</code>.", "If, instead, you want to pad, use tf.image.pad_to_bounding_box (just replace it in the above preprocess function and adapt the parameters as needed).", "Normally, for most of the networks I'm aware of, resizing is used.", "Finally, map the function on your dataset:", "<code>Code Snippet</code>.", "Note: The variable shapes in the error codes come from the shuffle call."], "sent_idxs": [101, 1996, 3277, 3310, 2013, 1996, 2755, 2008, 1996, 2951, 13462, 3397, 8023, 1011, 7451, 4871, 1006, 2156, 1996, 2951, 13462, 6412, 2182, 1007, 1012, 102, 101, 23435, 12314, 2064, 2069, 14108, 2362, 2477, 2007, 1996, 2168, 4338, 1010, 2061, 2017, 2034, 2342, 2000, 2593, 24501, 3270, 5051, 1996, 4871, 2000, 1037, 2691, 4338, 1006, 1041, 1012, 1043, 1012, 1010, 1996, 7953, 4338, 1997, 2115, 2897, 1007, 2030, 11687, 2068, 11914, 1012, 102, 101, 2065, 2017, 2215, 2000, 24501, 4697, 1010, 2224, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 1010, 2612, 1010, 2017, 2215, 2000, 11687, 1010, 2224, 1056, 2546, 1012, 3746, 1012, 11687, 1035, 2000, 1035, 5391, 2075, 1035, 3482, 1006, 2074, 5672, 2009, 1999, 1996, 2682, 17463, 3217, 9623, 2015, 3853, 1998, 15581, 1996, 11709, 2004, 2734, 1007, 1012, 102, 101, 5373, 1010, 2005, 2087, 1997, 1996, 6125, 1045, 1005, 1049, 5204, 1997, 1010, 24501, 6026, 2003, 2109, 1012, 102, 101, 2633, 1010, 4949, 1996, 3853, 2006, 2115, 2951, 13462, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 1024, 1996, 8023, 10466, 1999, 1996, 7561, 9537, 2272, 2013, 1996, 23046, 2655, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 4]}], "na_triple": [], "sent_ends": [0, 26, 73, 93, 107, 152, 172, 184, 198, 215], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43304425", "vertexSet": [[{"sent_id": 0, "name": "tf.summary.filewriter", "pos": [12, 19]}], [{"sent_id": 0, "name": "tf.train.summarywriter", "pos": [22, 29]}]], "sents": ["For future reference of anyone in the same situation, changing tf.summary.FileWriter() to tf.train.SummaryWriter() solved this issue and allowed for graph visualisation in Tensorboard.", "As I thought, it seems like FileWriter may be deprecated (although it does oddly still appear when searching through tf methods in the IDE)"], "sent_idxs": [101, 2005, 2925, 4431, 1997, 3087, 1999, 1996, 2168, 3663, 1010, 5278, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1006, 1007, 2000, 1056, 2546, 1012, 3345, 1012, 12654, 15994, 1006, 1007, 13332, 2023, 3277, 1998, 3039, 2005, 10629, 5107, 6648, 1999, 23435, 6277, 1012, 102, 101, 2004, 1045, 2245, 1010, 2009, 3849, 2066, 5371, 15994, 2089, 2022, 2139, 28139, 12921, 1006, 2348, 2009, 2515, 15056, 2145, 3711, 2043, 6575, 2083, 1056, 2546, 4725, 1999, 1996, 8909, 2063, 1007, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 45, 79], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55601831", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.sigmoid", "pos": [11, 20]}, {"sent_id": 3, "name": "tf.nn.sigmoid", "pos": [76, 85]}], [{"sent_id": 1, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [11, 29]}]], "sents": ["Solved it.", "The problem was that the tf.nn.sigmoid_cross_entropy_with_logits runs the logits through a sigmoid which is of course not used at validation time since the loss operation is only called during train time.", "The solution therefore is:", "make sure to run the network outputs through a tf.nn.sigmoid at validation/test time like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 13332, 2009, 1012, 102, 101, 1996, 3291, 2001, 2008, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 3216, 1996, 8833, 12762, 2083, 1037, 9033, 21693, 9314, 2029, 2003, 1997, 2607, 2025, 2109, 2012, 27354, 2051, 2144, 1996, 3279, 3169, 2003, 2069, 2170, 2076, 3345, 2051, 1012, 102, 101, 1996, 5576, 3568, 2003, 1024, 102, 101, 2191, 2469, 2000, 2448, 1996, 2897, 27852, 2083, 1037, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 2012, 27354, 1013, 3231, 2051, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 3]}], "na_triple": [], "sent_ends": [0, 5, 59, 66, 94, 108], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34696282", "vertexSet": [[{"sent_id": 1, "name": "tf.add", "pos": [39, 43]}, {"sent_id": 6, "name": "tf.add", "pos": [132, 136]}], [{"sent_id": 6, "name": "tf.add_n", "pos": [132, 138]}], [{"sent_id": 6, "name": "tf.reduce_sum", "pos": [145, 151]}]], "sents": ["Thanks to your question, we prioritized adding support for string concatenation in TensorFlow, and added it in this commit.", "String concatenation is implemented using the existing tf.add() operator, to match the behavior of NumPy's add operator (including broadcasting).", "To implement your example, you can write:", "<code>Code Snippet</code>.", "\u2026or, equivalently, but if you want to name the resulting tensor:", "<code>Code Snippet</code>.", "We have not yet added support for strings in tf.add_n() (or related ops like tf.reduce_sum()) but will consider this if there are use cases for it.", "NOTE: To use this functionality immediately, you will need to build TensorFlow from source.", "The new op will be available in the next release of TensorFlow (0.7.0)."], "sent_idxs": [101, 4283, 2000, 2115, 3160, 1010, 2057, 3188, 25090, 5422, 5815, 2490, 2005, 5164, 9530, 16280, 9323, 1999, 23435, 12314, 1010, 1998, 2794, 2009, 1999, 2023, 10797, 1012, 102, 101, 5164, 9530, 16280, 9323, 2003, 7528, 2478, 1996, 4493, 1056, 2546, 1012, 5587, 1006, 1007, 6872, 1010, 2000, 2674, 1996, 5248, 1997, 16371, 8737, 2100, 1005, 1055, 5587, 6872, 1006, 2164, 5062, 1007, 1012, 102, 101, 2000, 10408, 2115, 2742, 1010, 2017, 2064, 4339, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1529, 2030, 1010, 5662, 2135, 1010, 2021, 2065, 2017, 2215, 2000, 2171, 1996, 4525, 23435, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2057, 2031, 2025, 2664, 2794, 2490, 2005, 7817, 1999, 1056, 2546, 1012, 5587, 1035, 1050, 1006, 1007, 1006, 2030, 3141, 23092, 2066, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1007, 1007, 2021, 2097, 5136, 2023, 2065, 2045, 2024, 2224, 3572, 2005, 2009, 1012, 102, 101, 3602, 1024, 2000, 2224, 2023, 15380, 3202, 1010, 2017, 2097, 2342, 2000, 3857, 23435, 12314, 2013, 3120, 1012, 102, 101, 1996, 2047, 6728, 2097, 2022, 2800, 1999, 1996, 2279, 2713, 1997, 23435, 12314, 1006, 1014, 1012, 1021, 1012, 1014, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 6]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 65, 76, 90, 108, 122, 167, 187, 210], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38820162", "vertexSet": [[{"sent_id": 8, "name": "tf.fill", "pos": [188, 192]}], [{"sent_id": 0, "name": "tf.get_variable", "pos": [25, 31]}], [{"sent_id": 0, "name": "tf.constant_initializer", "pos": [2, 9]}, {"sent_id": 4, "name": "tf.constant_initializer", "pos": [83, 90]}, {"sent_id": 8, "name": "tf.constant_initializer", "pos": [162, 169]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [16, 20]}, {"sent_id": 0, "name": "tf.tensor", "pos": [36, 40]}, {"sent_id": 8, "name": "tf.tensor", "pos": [175, 179]}]], "sents": ["The tf.constant_initializer() function might not accept a tf.Tensor as an argument, but tf.get_variable() does accept a tf.Tensor as its initializer argument.", "This means you can write:", "<code>Code Snippet</code>.", "...which requires even fewer characters!", "The reason tf.constant_initializer() doesn't take an arbitrary tensor is that it is designed to initialize variables of many different shapes with the same constant value for each element.", "For example, a statement like:", "<code>Code Snippet</code>.", "...wouldn't make much sense.", "Arguably we could make tf.constant_initializer() accept a scalar tf.Tensor, and then it would have semantics similar to tf.fill(), but we haven't had any demand for that yet.", "Feel free to raise a GitHub issue if it would be useful though!"], "sent_idxs": [101, 1996, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1006, 1007, 3853, 2453, 2025, 5138, 1037, 1056, 2546, 1012, 23435, 2004, 2019, 6685, 1010, 2021, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 2515, 5138, 1037, 1056, 2546, 1012, 23435, 2004, 2049, 3988, 17629, 6685, 1012, 102, 101, 2023, 2965, 2017, 2064, 4339, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2029, 5942, 2130, 8491, 3494, 999, 102, 101, 1996, 3114, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1006, 1007, 2987, 1005, 1056, 2202, 2019, 15275, 23435, 2003, 2008, 2009, 2003, 2881, 2000, 3988, 4697, 10857, 1997, 2116, 2367, 10466, 2007, 1996, 2168, 5377, 3643, 2005, 2169, 5783, 1012, 102, 101, 2005, 2742, 1010, 1037, 4861, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2876, 1005, 1056, 2191, 2172, 3168, 1012, 102, 101, 15835, 2057, 2071, 2191, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1006, 1007, 5138, 1037, 26743, 2099, 1056, 2546, 1012, 23435, 1010, 1998, 2059, 2009, 2052, 2031, 28081, 2714, 2000, 1056, 2546, 1012, 6039, 1006, 1007, 1010, 2021, 2057, 4033, 1005, 1056, 2018, 2151, 5157, 2005, 2008, 2664, 1012, 102, 101, 2514, 2489, 2000, 5333, 1037, 21025, 2705, 12083, 3277, 2065, 2009, 2052, 2022, 6179, 2295, 999, 102], "labels": [{"r": "S1", "h": 2, "t": 0, "evidence": [8]}, {"r": "S1", "h": 0, "t": 2, "evidence": [8]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 4]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 4]}], "na_triple": [[0, 1], [0, 3], [1, 0], [1, 3], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 47, 55, 69, 80, 122, 131, 145, 157, 208, 226], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44522505", "vertexSet": [[{"sent_id": 8, "name": "tf.nn.l2_loss", "pos": [154, 164]}], [{"sent_id": 7, "name": "tf.losses.mean_squared_error", "pos": [118, 128]}, {"sent_id": 11, "name": "tf.losses.mean_squared_error", "pos": [222, 232]}]], "sents": ["To get your training step back, the documentation suggests you add it to a collection before saving it as a way to be able to point at it to after restoring your graph.", "Saving:", "<code>Code Snippet</code>.", "Restore:", "<code>Code Snippet</code>.", "Why did your attempt at recovering the tensor by name fail?", "You can indeed get the tensor by its name -- the catch is that you need the correct name.", "And notice that your error argument to tf.losses.mean_squared_error is a scope name, not the name of the returned operation.", "This can be confusing, as other operations, such as tf.nn.l2_loss, accept a name argument.", "In the end, the name of your error operation is MSE/error/value:0, which you can use to get it by name.", "That is, until it breaks again in the future when you update tensorflow.", "tf.losses.mean_squared_error does not give you any guarantee on the name of its output, so it very well may change for some reason.", "I think this is what motivates the use of collections: the lack of guarantee on the names of the operators you don't control yourself.", "Alternatively, if for some reason you really want to use names, you could rename your operator like this:", "<code>Code Snippet</code>.", "Then you can rely on graph.get_tensor_by_name('MSE/my_error:0') safely."], "sent_idxs": [101, 2000, 2131, 2115, 2731, 3357, 2067, 1010, 1996, 12653, 6083, 2017, 5587, 2009, 2000, 1037, 3074, 2077, 7494, 2009, 2004, 1037, 2126, 2000, 2022, 2583, 2000, 2391, 2012, 2009, 2000, 2044, 16487, 2115, 10629, 1012, 102, 101, 7494, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 9239, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2339, 2106, 2115, 3535, 2012, 13400, 1996, 23435, 2011, 2171, 8246, 1029, 102, 101, 2017, 2064, 5262, 2131, 1996, 23435, 2011, 2049, 2171, 1011, 1011, 1996, 4608, 2003, 2008, 2017, 2342, 1996, 6149, 2171, 1012, 102, 101, 1998, 5060, 2008, 2115, 7561, 6685, 2000, 1056, 2546, 1012, 6409, 1012, 2812, 1035, 19942, 1035, 7561, 2003, 1037, 9531, 2171, 1010, 2025, 1996, 2171, 1997, 1996, 2513, 3169, 1012, 102, 101, 2023, 2064, 2022, 16801, 1010, 2004, 2060, 3136, 1010, 2107, 2004, 1056, 2546, 1012, 1050, 2078, 1012, 1048, 2475, 1035, 3279, 1010, 5138, 1037, 2171, 6685, 1012, 102, 101, 1999, 1996, 2203, 1010, 1996, 2171, 1997, 2115, 7561, 3169, 2003, 5796, 2063, 1013, 7561, 1013, 3643, 1024, 1014, 1010, 2029, 2017, 2064, 2224, 2000, 2131, 2009, 2011, 2171, 1012, 102, 101, 2008, 2003, 1010, 2127, 2009, 7807, 2153, 1999, 1996, 2925, 2043, 2017, 10651, 23435, 12314, 1012, 102, 101, 1056, 2546, 1012, 6409, 1012, 2812, 1035, 19942, 1035, 7561, 2515, 2025, 2507, 2017, 2151, 11302, 2006, 1996, 2171, 1997, 2049, 6434, 1010, 2061, 2009, 2200, 2092, 2089, 2689, 2005, 2070, 3114, 1012, 102, 101, 1045, 2228, 2023, 2003, 2054, 9587, 29068, 8520, 1996, 2224, 1997, 6407, 1024, 1996, 3768, 1997, 11302, 2006, 1996, 3415, 1997, 1996, 9224, 2017, 2123, 1005, 1056, 2491, 4426, 1012, 102, 101, 14084, 1010, 2065, 2005, 2070, 3114, 2017, 2428, 2215, 2000, 2224, 3415, 1010, 2017, 2071, 14916, 14074, 2115, 6872, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 2017, 2064, 11160, 2006, 10629, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 1005, 5796, 2063, 1013, 2026, 1035, 7561, 1024, 1014, 1005, 1007, 9689, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [7, 8]}], "na_triple": [], "sent_ends": [0, 37, 41, 55, 59, 73, 87, 110, 142, 171, 203, 221, 256, 288, 312, 326, 356], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55431562", "vertexSet": [[{"sent_id": 1, "name": " tf.keras.layers.dense", "pos": [21, 30]}], [{"sent_id": 0, "name": "tf.layers.dense", "pos": [9, 15]}, {"sent_id": 1, "name": "tf.layers.dense", "pos": [46, 52]}]], "sents": ["Yes, it is the same as using tf.layers.dense().", "Using tf.keras.layers.Dense() is actually a preferred way in newest tensorflow version 1.13 (tf.layers.dense() is deprectated).", "For example", "<code>Code Snippet</code>."], "sent_idxs": [101, 2748, 1010, 2009, 2003, 1996, 2168, 2004, 2478, 1056, 2546, 1012, 9014, 1012, 9742, 1006, 1007, 1012, 102, 101, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 1007, 2003, 2941, 1037, 6871, 2126, 1999, 14751, 23435, 12314, 2544, 1015, 1012, 2410, 1006, 1056, 2546, 1012, 9014, 1012, 9742, 1006, 1007, 2003, 2139, 28139, 25572, 3064, 1007, 1012, 102, 101, 2005, 2742, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 19, 62, 66, 80], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62889525", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator.inputs.pandas_input_fn", "pos": [27, 43]}], [{"sent_id": 1, "name": "tf.compat.v1.estimator.inputs.pandas_input_fn", "pos": [64, 87]}]], "sents": ["In Tensorflow V1 While reading input from Pandas DataFrames for tf.estimator we used this command: tf.estimator.inputs.pandas_input_fn.", "But now due to API changes, we will have to replace that command with this one: tf.compat.v1.estimator.inputs.pandas_input_fn.", "Here we are getting the same command from the Version1 into Version2 of Tensorflow."], "sent_idxs": [101, 1999, 23435, 12314, 1058, 2487, 2096, 3752, 7953, 2013, 25462, 2015, 2951, 15643, 2015, 2005, 1056, 2546, 1012, 9765, 9581, 4263, 2057, 2109, 2023, 3094, 1024, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 20407, 1012, 25462, 2015, 1035, 7953, 1035, 1042, 2078, 1012, 102, 101, 2021, 2085, 2349, 2000, 17928, 3431, 1010, 2057, 2097, 2031, 2000, 5672, 2008, 3094, 2007, 2023, 2028, 1024, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 9765, 9581, 4263, 1012, 20407, 1012, 25462, 2015, 1035, 7953, 1035, 1042, 2078, 1012, 102, 101, 2182, 2057, 2024, 2893, 1996, 2168, 3094, 2013, 1996, 2544, 2487, 2046, 2544, 2475, 1997, 23435, 12314, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 45, 89, 109], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54223597", "vertexSet": [[{"sent_id": 5, "name": "tf.nn.relu", "pos": [56, 64]}], [{"sent_id": 7, "name": "tf.maximum", "pos": [119, 123]}]], "sents": ["Once relu is defined as:", "<code>Code Snippet</code>.", "they are basically the same.", "Both take a tensor as input and return a tensor.", "The only difference is the supported types.", "The tf.nn.relu supports the following types:", "float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8.", "while  tf.maximum supports a subset of the above types:", "half, float32, float64, int32, int64."], "sent_idxs": [101, 2320, 2128, 7630, 2003, 4225, 2004, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2027, 2024, 10468, 1996, 2168, 1012, 102, 101, 2119, 2202, 1037, 23435, 2004, 7953, 1998, 2709, 1037, 23435, 1012, 102, 101, 1996, 2069, 4489, 2003, 1996, 3569, 4127, 1012, 102, 101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 2128, 7630, 6753, 1996, 2206, 4127, 1024, 102, 101, 14257, 16703, 1010, 14257, 21084, 1010, 20014, 16703, 1010, 21318, 3372, 2620, 1010, 20014, 16048, 1010, 20014, 2620, 1010, 20014, 21084, 1010, 28939, 4135, 4017, 16048, 1010, 21318, 3372, 16048, 1010, 2431, 1010, 21318, 3372, 16703, 1010, 21318, 3372, 21084, 1010, 19781, 2102, 2620, 1012, 102, 101, 2096, 1056, 2546, 1012, 4555, 6753, 1037, 16745, 1997, 1996, 2682, 4127, 1024, 102, 101, 2431, 1010, 14257, 16703, 1010, 14257, 21084, 1010, 20014, 16703, 1010, 20014, 21084, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3, 4, 5, 6, 7, 8]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3, 4, 5, 6, 7, 8]}], "na_triple": [], "sent_ends": [0, 9, 23, 31, 44, 54, 70, 117, 132, 148], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53894805", "vertexSet": [[{"sent_id": 4, "name": "tf.image.resize_image_with_pad", "pos": [73, 86]}], [{"sent_id": 0, "name": "tf.image.resize_images", "pos": [4, 13]}, {"sent_id": 1, "name": "tf.image.resize_images", "pos": [24, 33]}]], "sents": ["You can use tf.image.resize_images method to achieve this.", "According to docs tf.image.resize_images:", "Resize images to size using the specified method.", "Resized images will be distorted if their original aspect ratio is not\n  the same as size.", "To avoid distortions see\n  tf.image.resize_image_with_pad.", "How to use it?", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 4118, 2000, 6162, 2023, 1012, 102, 101, 2429, 2000, 9986, 2015, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 1024, 102, 101, 24501, 4697, 4871, 2000, 2946, 2478, 1996, 9675, 4118, 1012, 102, 101, 24501, 3550, 4871, 2097, 2022, 19112, 2065, 2037, 2434, 7814, 6463, 2003, 2025, 1996, 2168, 2004, 2946, 1012, 102, 101, 2000, 4468, 20870, 2015, 2156, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 3746, 1035, 2007, 1035, 11687, 1012, 102, 101, 2129, 2000, 2224, 2009, 1029, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 19, 35, 47, 67, 88, 95, 109], "sent_pos": [0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61724959", "vertexSet": [[{"sent_id": 12, "name": "tf.config.list_physical_devices", "pos": [185, 197]}], [{"sent_id": 8, "name": "tf.test.is_gpu_available", "pos": [134, 145]}], [{"sent_id": 7, "name": "tf.test.is_built_with_cuda", "pos": [104, 117]}]], "sents": ["The recommended way is to check if TensorFlow is using GPU is the following:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "The following will also return the name of your GPU devices.", "<code>Code Snippet</code>.", "If a non-GPU version of the package is installed, the function would also return False.", "Use tf.test.is_built_with_cuda to validate if TensorFlow was build with CUDA support.", "Note: tf.test.is_gpu_available is deprecated.", "Please refer here", "Warning: THIS FUNCTION IS DEPRECATED.", "It will be removed in a future version.", "Instructions for updating: Use tf.config.list_physical_devices('GPU') instead.", "Best way to test is to run code and check that GPU is using with nvidia-smi as mentioned by Matias Valdenegro or run simple code as below", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 6749, 2126, 2003, 2000, 4638, 2065, 23435, 12314, 2003, 2478, 14246, 2226, 2003, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2206, 2097, 2036, 2709, 1996, 2171, 1997, 2115, 14246, 2226, 5733, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 1037, 2512, 1011, 14246, 2226, 2544, 1997, 1996, 7427, 2003, 5361, 1010, 1996, 3853, 2052, 2036, 2709, 6270, 1012, 102, 101, 2224, 1056, 2546, 1012, 3231, 1012, 2003, 1035, 2328, 1035, 2007, 1035, 12731, 2850, 2000, 9398, 3686, 2065, 23435, 12314, 2001, 3857, 2007, 12731, 2850, 2490, 1012, 102, 101, 3602, 1024, 1056, 2546, 1012, 3231, 1012, 2003, 1035, 14246, 2226, 1035, 2800, 2003, 2139, 28139, 12921, 1012, 102, 101, 3531, 6523, 2182, 102, 101, 5432, 1024, 2023, 3853, 2003, 2139, 28139, 12921, 1012, 102, 101, 2009, 2097, 2022, 3718, 1999, 1037, 2925, 2544, 1012, 102, 101, 8128, 2005, 2039, 16616, 1024, 2224, 1056, 2546, 1012, 9530, 8873, 2290, 1012, 2862, 1035, 3558, 1035, 5733, 1006, 1005, 14246, 2226, 1005, 1007, 2612, 1012, 102, 101, 2190, 2126, 2000, 3231, 2003, 2000, 2448, 3642, 1998, 4638, 2008, 14246, 2226, 2003, 2478, 2007, 1050, 17258, 2401, 1011, 15488, 2072, 2004, 3855, 2011, 13523, 7951, 11748, 4181, 13910, 3217, 2030, 2448, 3722, 3642, 2004, 2917, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [8, 11, 12]}, {"r": "S1", "h": 0, "t": 1, "evidence": [8, 11, 12]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 33, 37, 51, 66, 80, 102, 131, 151, 156, 167, 178, 206, 245, 259, 263, 277], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43104527", "vertexSet": [[{"sent_id": 0, "name": "tf.tensordot", "pos": [2, 7]}], [{"sent_id": 0, "name": "tf.matmul", "pos": [31, 37]}, {"sent_id": 2, "name": "tf.matmul", "pos": [104, 110]}], [{"sent_id": 0, "name": "tf.einsum", "pos": [10, 15]}, {"sent_id": 0, "name": "tf.einsum", "pos": [45, 50]}], [{"sent_id": 0, "name": "tf.multiply", "pos": [59, 64]}]], "sents": ["Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).", "In the limit, I'd expect all three functions to have equivalent performance for the same computation.", "However, for smaller matrices it may be more efficient to use tf.matmul() directly, because it would yield a simpler TensorFlow graph with fewer operations, and hence the per-operation invocation costs will be lower."], "sent_idxs": [101, 2119, 1056, 2546, 1012, 23435, 27364, 1006, 1007, 1998, 1056, 2546, 1012, 16417, 17421, 1006, 1007, 2024, 19962, 2696, 13306, 5699, 2008, 10236, 2028, 2030, 2062, 1999, 19152, 2015, 1997, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1007, 1006, 2348, 1999, 2070, 2569, 3572, 1056, 2546, 1012, 16417, 17421, 1006, 1007, 2064, 5547, 2000, 1996, 16325, 5783, 14244, 1056, 2546, 1012, 4800, 22086, 1006, 1007, 1007, 1012, 102, 101, 1999, 1996, 5787, 1010, 1045, 1005, 1040, 5987, 2035, 2093, 4972, 2000, 2031, 5662, 2836, 2005, 1996, 2168, 22334, 1012, 102, 101, 2174, 1010, 2005, 3760, 21520, 2009, 2089, 2022, 2062, 8114, 2000, 2224, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1007, 3495, 1010, 2138, 2009, 2052, 10750, 1037, 16325, 23435, 12314, 10629, 2007, 8491, 3136, 1010, 1998, 6516, 1996, 2566, 1011, 3169, 1999, 19152, 5366, 2097, 2022, 2896, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1, 2]}], "na_triple": [[0, 3], [1, 3], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 69, 91, 141], "sent_pos": [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61071698", "vertexSet": [[{"sent_id": 0, "name": "tf.io.fixedlenfeature", "pos": [28, 38]}], [{"sent_id": 0, "name": "tf.fixedlenfeature", "pos": [19, 27]}, {"sent_id": 1, "name": "tf.fixedlenfeature", "pos": [44, 52]}]], "sents": ["If you are using TensorFlow 2.x, FixedLenFeature has moved from tf.FixedLenFeature into tf.io.FixedLenFeature.", "The use of tf.FixedLenFeature was marked as deprecated in previous versions."], "sent_idxs": [101, 2065, 2017, 2024, 2478, 23435, 12314, 1016, 1012, 1060, 1010, 4964, 7770, 7959, 4017, 5397, 2038, 2333, 2013, 1056, 2546, 1012, 4964, 7770, 7959, 4017, 5397, 2046, 1056, 2546, 1012, 22834, 1012, 4964, 7770, 7959, 4017, 5397, 1012, 102, 101, 1996, 2224, 1997, 1056, 2546, 1012, 4964, 7770, 7959, 4017, 5397, 2001, 4417, 2004, 2139, 28139, 12921, 1999, 3025, 4617, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 40, 63], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50592353", "vertexSet": [[{"sent_id": 8, "name": "tf.floordiv", "pos": [196, 202]}], [{"sent_id": 0, "name": "tf.div", "pos": [1, 6]}, {"sent_id": 8, "name": "tf.div", "pos": [215, 220]}, {"sent_id": 8, "name": "tf.div", "pos": [231, 236]}], [{"sent_id": 7, "name": "tf.divide", "pos": [158, 162]}], [{"sent_id": 8, "name": "tf.floor_div", "pos": [203, 210]}], [{"sent_id": 10, "name": "tf.realdiv", "pos": [266, 272]}], [{"sent_id": 11, "name": "tf.truncatediv", "pos": [298, 303]}], [{"sent_id": 4, "name": "tf.truediv", "pos": [81, 87]}, {"sent_id": 10, "name": "tf.truediv", "pos": [283, 289]}]], "sents": ["tf.div - Enforces python v2 division semantics e.g.", "uses integer division (also known as \"floor division\") if both arguments are integers and normal floating point division if arguments are float or complex numbers.", "The result is integer if both arguments are integers, float otherwise.", "<code>Code Snippet</code>.", "tf.truediv - enforces python v3 division semantics, e.g.", "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:", "<code>Code Snippet</code>.", "tf.divide(x,y) essentially calls x/y, therefore the result depends on the behavior of the / operator in the environment in which the division is performed.", "tf.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:", "<code>Code Snippet</code>.", "tf.realdiv - normal floating point division, this is the operation that tf.truediv calls after it casts its arguments.", "tf.truncatediv - rounds the result of division towards zero:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 4487, 2615, 1011, 16306, 2015, 18750, 1058, 2475, 2407, 28081, 1041, 1012, 1043, 1012, 102, 101, 3594, 16109, 2407, 1006, 2036, 2124, 2004, 1000, 2723, 2407, 1000, 1007, 2065, 2119, 9918, 2024, 24028, 1998, 3671, 8274, 2391, 2407, 2065, 9918, 2024, 14257, 2030, 3375, 3616, 1012, 102, 101, 1996, 2765, 2003, 16109, 2065, 2119, 9918, 2024, 24028, 1010, 14257, 4728, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 2995, 4305, 2615, 1011, 16306, 2015, 18750, 1058, 2509, 2407, 28081, 1010, 1041, 1012, 1043, 1012, 102, 101, 2065, 2119, 9918, 2024, 24028, 2027, 2024, 2034, 3459, 2046, 14257, 2828, 1006, 1996, 12653, 4773, 3931, 27171, 2029, 2828, 1997, 16109, 2003, 4991, 2000, 2029, 2828, 1997, 14257, 1007, 1998, 2059, 1996, 3671, 8274, 2391, 2407, 2003, 4162, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 11443, 1006, 1060, 1010, 1061, 1007, 7687, 4455, 1060, 1013, 1061, 1010, 3568, 1996, 2765, 9041, 2006, 1996, 5248, 1997, 1996, 1013, 6872, 1999, 1996, 4044, 1999, 2029, 1996, 2407, 2003, 2864, 1012, 102, 101, 1056, 2546, 1012, 2723, 4305, 2615, 1998, 1056, 2546, 1012, 2723, 1035, 4487, 2615, 2709, 1996, 2168, 2765, 2004, 1056, 2546, 1012, 4487, 2615, 2065, 2119, 9918, 2024, 24028, 1998, 1056, 2546, 1012, 2723, 1006, 1056, 2546, 1012, 4487, 2615, 1006, 1060, 1010, 1061, 1007, 1007, 2065, 2119, 9918, 2024, 8274, 2391, 3616, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 2613, 4305, 2615, 1011, 3671, 8274, 2391, 2407, 1010, 2023, 2003, 1996, 3169, 2008, 1056, 2546, 1012, 2995, 4305, 2615, 4455, 2044, 2009, 23942, 2049, 9918, 1012, 102, 101, 1056, 2546, 1012, 25449, 12848, 1011, 6241, 1996, 2765, 1997, 2407, 2875, 5717, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 4, "evidence": [8, 10]}, {"r": "S1", "h": 4, "t": 0, "evidence": [8, 10]}, {"r": "S1", "h": 0, "t": 5, "evidence": [8, 11]}, {"r": "S1", "h": 5, "t": 0, "evidence": [8, 11]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 8]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1, 2, 7]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1, 2, 7]}, {"r": "S1", "h": 1, "t": 3, "evidence": [0, 1, 2, 8]}, {"r": "S1", "h": 3, "t": 1, "evidence": [0, 1, 2, 8]}, {"r": "S1", "h": 1, "t": 4, "evidence": [0, 1, 2, 10]}, {"r": "S1", "h": 4, "t": 1, "evidence": [0, 1, 2, 10]}, {"r": "S1", "h": 1, "t": 5, "evidence": [0, 1, 2, 11]}, {"r": "S1", "h": 5, "t": 1, "evidence": [0, 1, 2, 11]}, {"r": "S1", "h": 1, "t": 6, "evidence": [0, 1, 2, 4, 5]}, {"r": "S1", "h": 6, "t": 1, "evidence": [0, 1, 2, 4, 5]}, {"r": "S1", "h": 2, "t": 0, "evidence": [7, 8]}, {"r": "S1", "h": 0, "t": 2, "evidence": [7, 8]}, {"r": "S1", "h": 2, "t": 3, "evidence": [7, 8]}, {"r": "S1", "h": 3, "t": 2, "evidence": [7, 8]}, {"r": "S1", "h": 3, "t": 4, "evidence": [8, 10]}, {"r": "S1", "h": 4, "t": 3, "evidence": [8, 10]}, {"r": "S1", "h": 3, "t": 5, "evidence": [8, 11]}, {"r": "S1", "h": 5, "t": 3, "evidence": [8, 11]}, {"r": "S1", "h": 4, "t": 5, "evidence": [10, 11]}, {"r": "S1", "h": 5, "t": 4, "evidence": [10, 11]}, {"r": "S1", "h": 6, "t": 0, "evidence": [4, 5, 8]}, {"r": "S1", "h": 0, "t": 6, "evidence": [4, 5, 8]}, {"r": "S1", "h": 6, "t": 2, "evidence": [4, 5, 7]}, {"r": "S1", "h": 2, "t": 6, "evidence": [4, 5, 7]}, {"r": "S1", "h": 6, "t": 3, "evidence": [4, 5, 8]}, {"r": "S1", "h": 3, "t": 6, "evidence": [4, 5, 8]}, {"r": "S1", "h": 6, "t": 4, "evidence": [4, 5, 10]}, {"r": "S1", "h": 4, "t": 6, "evidence": [4, 5, 10]}, {"r": "S1", "h": 6, "t": 5, "evidence": [4, 5, 11]}, {"r": "S1", "h": 5, "t": 6, "evidence": [4, 5, 11]}], "na_triple": [[0, 3], [2, 4], [2, 5], [3, 0], [4, 2], [5, 2]], "sent_ends": [0, 19, 51, 66, 80, 101, 143, 157, 195, 251, 265, 297, 313, 327], "sent_pos": [0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49837908", "vertexSet": [[{"sent_id": 12, "name": "tf.image.random_flip_left_right", "pos": [327, 339]}], [{"sent_id": 11, "name": "tf.image.flip_left_right", "pos": [284, 294]}, {"sent_id": 12, "name": "tf.image.flip_left_right", "pos": [341, 351]}], [{"sent_id": 5, "name": "tf.data", "pos": [81, 85]}, {"sent_id": 10, "name": "tf.data", "pos": [223, 227]}], [{"sent_id": 7, "name": "tf.py_func", "pos": [143, 151]}], [{"sent_id": 13, "name": "tf.estimator", "pos": [361, 367]}, {"sent_id": 13, "name": "tf.estimator", "pos": [388, 394]}], [{"sent_id": 5, "name": "tf.data.dataset", "pos": [81, 88]}, {"sent_id": 10, "name": "tf.data.dataset", "pos": [223, 230]}], [{"sent_id": 13, "name": "tf.estimator.estimator", "pos": [361, 371]}], [{"sent_id": 13, "name": "tf.estimator.train_and_evaluate", "pos": [388, 400]}]], "sents": ["There's a really good article and talk released recently that go over the API in a lot more detail than my response here.", "Here's a brief example:", "<code>Code Snippet</code>.", "A few notes:", "This approach relies on being able to load your entire dataset into memory.", "If you cannot, consider using tf.data.Dataset.from_generator.", "This can lead to slow shuffle times if your shuffle buffer is large.", "My preferred method is to load some keys tensor entirely into memory - it might just be the indices of each example - then map that key value to data values using tf.py_func.", "This is slightly less efficient than converting to tfrecords, but with prefetching it likely won't affect performance.", "Since the shuffling is done before the mapping, you only have to load shuffle_buffer keys into memory, rather than shuffle_buffer examples..", "To augment your dataset, use tf.data.Dataset.map either before or after the batch operation, depending on whether or not you want to apply a batch-wise operation (something working on a 4D image tensor) or element-wise operation (3D image tensor).", "Note it looks like the documentation for tf.image.flip_left_right is out of date, since I get an error when I try and use a 4D tensor.", "If you want to augment you data randomly, use tf.image.random_flip_left_right rather than tf.image.flip_left_right..", "If you're using a tf.estimator.Estimator (or wouldn't mind converting your code to using it), then check out tf.estimator.train_and_evaluate for an in-built way of switching between datasets..", "Consider shuffling/repeating your dataset with the shuffle/repeat methods.", "See the article for notes on efficiencies.", "In particular, repeat -> shuffle -> map -> batch -> batch-wise map -> prefetch seems to be the best ordering of operations for most applications.."], "sent_idxs": [101, 2045, 1005, 1055, 1037, 2428, 2204, 3720, 1998, 2831, 2207, 3728, 2008, 2175, 2058, 1996, 17928, 1999, 1037, 2843, 2062, 6987, 2084, 2026, 3433, 2182, 1012, 102, 101, 2182, 1005, 1055, 1037, 4766, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1037, 2261, 3964, 1024, 102, 101, 2023, 3921, 16803, 2006, 2108, 2583, 2000, 7170, 2115, 2972, 2951, 13462, 2046, 3638, 1012, 102, 101, 2065, 2017, 3685, 1010, 5136, 2478, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 13103, 1012, 102, 101, 2023, 2064, 2599, 2000, 4030, 23046, 2335, 2065, 2115, 23046, 17698, 2003, 2312, 1012, 102, 101, 2026, 6871, 4118, 2003, 2000, 7170, 2070, 6309, 23435, 4498, 2046, 3638, 1011, 2009, 2453, 2074, 2022, 1996, 29299, 1997, 2169, 2742, 1011, 2059, 4949, 2008, 3145, 3643, 2000, 2951, 5300, 2478, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 1012, 102, 101, 2023, 2003, 3621, 2625, 8114, 2084, 16401, 2000, 1056, 19699, 8586, 8551, 2015, 1010, 2021, 2007, 3653, 7959, 10649, 2075, 2009, 3497, 2180, 1005, 1056, 7461, 2836, 1012, 102, 101, 2144, 1996, 24770, 2003, 2589, 2077, 1996, 12375, 1010, 2017, 2069, 2031, 2000, 7170, 23046, 1035, 17698, 6309, 2046, 3638, 1010, 2738, 2084, 23046, 1035, 17698, 4973, 1012, 1012, 102, 101, 2000, 15476, 3672, 2115, 2951, 13462, 1010, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 4949, 2593, 2077, 2030, 2044, 1996, 14108, 3169, 1010, 5834, 2006, 3251, 2030, 2025, 2017, 2215, 2000, 6611, 1037, 14108, 1011, 7968, 3169, 1006, 2242, 2551, 2006, 1037, 1018, 2094, 3746, 23435, 1007, 2030, 5783, 1011, 7968, 3169, 1006, 7605, 3746, 23435, 1007, 1012, 102, 101, 3602, 2009, 3504, 2066, 1996, 12653, 2005, 1056, 2546, 1012, 3746, 1012, 11238, 1035, 2187, 1035, 2157, 2003, 2041, 1997, 3058, 1010, 2144, 1045, 2131, 2019, 7561, 2043, 1045, 3046, 1998, 2224, 1037, 1018, 2094, 23435, 1012, 102, 101, 2065, 2017, 2215, 2000, 15476, 3672, 2017, 2951, 18154, 1010, 2224, 1056, 2546, 1012, 3746, 1012, 6721, 1035, 11238, 1035, 2187, 1035, 2157, 2738, 2084, 1056, 2546, 1012, 3746, 1012, 11238, 1035, 2187, 1035, 2157, 1012, 1012, 102, 101, 2065, 2017, 1005, 2128, 2478, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1006, 2030, 2876, 1005, 1056, 2568, 16401, 2115, 3642, 2000, 2478, 2009, 1007, 1010, 2059, 4638, 2041, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 3345, 1035, 1998, 1035, 16157, 2005, 2019, 1999, 1011, 2328, 2126, 1997, 11991, 2090, 2951, 13462, 2015, 1012, 1012, 102, 101, 5136, 24770, 1013, 15192, 2115, 2951, 13462, 2007, 1996, 23046, 1013, 9377, 4725, 1012, 102, 101, 2156, 1996, 3720, 2005, 3964, 2006, 1041, 26989, 23402, 14767, 1012, 102, 101, 1999, 3327, 1010, 9377, 1011, 1028, 23046, 1011, 1028, 4949, 1011, 1028, 14108, 1011, 1028, 14108, 1011, 7968, 4949, 1011, 1028, 3653, 7959, 10649, 3849, 2000, 2022, 1996, 2190, 13063, 1997, 3136, 2005, 2087, 5097, 1012, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [12]}, {"r": "S1", "h": 1, "t": 0, "evidence": [12]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 28, 37, 51, 57, 74, 94, 110, 153, 183, 214, 276, 315, 354, 415, 431, 444, 483], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43722074", "vertexSet": [[{"sent_id": 1, "name": "tf.session", "pos": [54, 58]}], [{"sent_id": 0, "name": "tf.train.supervisor", "pos": [4, 10]}], [{"sent_id": 0, "name": "tf.train.start_queue_runners", "pos": [16, 26]}]], "sents": ["When you use tf.train.Supervisor, the framework code automatically calls tf.train.start_queue_runners(sess) (along with initializing variables) at the beginning of the session.", "If you switch back to using a raw tf.Session, you must call this manually to start the input pipeline.", "A change like the following should work:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2043, 2017, 2224, 1056, 2546, 1012, 3345, 1012, 12366, 1010, 1996, 7705, 3642, 8073, 4455, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 7367, 4757, 1007, 1006, 2247, 2007, 3988, 6026, 10857, 1007, 2012, 1996, 2927, 1997, 1996, 5219, 1012, 102, 101, 2065, 2017, 6942, 2067, 2000, 2478, 1037, 6315, 1056, 2546, 1012, 5219, 1010, 2017, 2442, 2655, 2023, 21118, 2000, 2707, 1996, 7953, 13117, 1012, 102, 101, 1037, 2689, 2066, 1996, 2206, 2323, 2147, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 45, 71, 81, 95], "sent_pos": [0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45642294", "vertexSet": [[{"sent_id": 2, "name": "tf.sigmoid", "pos": [130, 136]}, {"sent_id": 6, "name": "tf.sigmoid", "pos": [321, 327]}], [{"sent_id": 2, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [87, 105]}], [{"sent_id": 4, "name": "tf.nn.softmax", "pos": [179, 187]}, {"sent_id": 4, "name": "tf.nn.softmax", "pos": [202, 210]}], [{"sent_id": 6, "name": "tf.reduce_mean", "pos": [314, 320]}], [{"sent_id": 4, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [179, 196]}]], "sents": ["Since you map your values to a target with one element, you should not use softmax cross entropy, since the softmax operation transforms the input into a probability distribution, with the sum of all probabilities equal to 1.", "Since your target has only one element, it will simply output 1 everytime, since this is the only possible way to transform the input into a probability distribution.", "You should instead use tf.nn.sigmoid_cross_entropy_with_logits() (which is used for binary classification) and also remove the softmax from Y_obt and convert it into tf.sigmoid() for Y_obt_test.", "Another way is to one-hot encode your targets and use a network with a two-element output.", "In this case, you should use tf.nn.softmax_cross_entropy_with_logits(), but remove the tf.nn.softmax() from Y_obt, since the softmax cross entropy expects unscaled logits (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits).", "For the Y_obt_test, you should of course not remove it in this case.", "Another thing: It might also help to take the mean of the cross entropies with cross_entropy = tf.reduce_mean(tf.sigmoid_cross_entropy_...)."], "sent_idxs": [101, 2144, 2017, 4949, 2115, 5300, 2000, 1037, 4539, 2007, 2028, 5783, 1010, 2017, 2323, 2025, 2224, 3730, 17848, 2892, 23077, 1010, 2144, 1996, 3730, 17848, 3169, 21743, 1996, 7953, 2046, 1037, 9723, 4353, 1010, 2007, 1996, 7680, 1997, 2035, 4013, 3676, 14680, 5020, 2000, 1015, 1012, 102, 101, 2144, 2115, 4539, 2038, 2069, 2028, 5783, 1010, 2009, 2097, 3432, 6434, 1015, 2296, 7292, 1010, 2144, 2023, 2003, 1996, 2069, 2825, 2126, 2000, 10938, 1996, 7953, 2046, 1037, 9723, 4353, 1012, 102, 101, 2017, 2323, 2612, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 1006, 2029, 2003, 2109, 2005, 12441, 5579, 1007, 1998, 2036, 6366, 1996, 3730, 17848, 2013, 1061, 1035, 27885, 2102, 1998, 10463, 2009, 2046, 1056, 2546, 1012, 9033, 21693, 9314, 1006, 1007, 2005, 1061, 1035, 27885, 2102, 1035, 3231, 1012, 102, 101, 2178, 2126, 2003, 2000, 2028, 1011, 2980, 4372, 16044, 2115, 7889, 1998, 2224, 1037, 2897, 2007, 1037, 2048, 1011, 5783, 6434, 1012, 102, 101, 1999, 2023, 2553, 1010, 2017, 2323, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 1010, 2021, 6366, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1007, 2013, 1061, 1035, 27885, 2102, 1010, 2144, 1996, 3730, 17848, 2892, 23077, 24273, 4895, 15782, 3709, 8833, 12762, 1006, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 1050, 2078, 1013, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1007, 1012, 102, 101, 2005, 1996, 1061, 1035, 27885, 2102, 1035, 3231, 1010, 2017, 2323, 1997, 2607, 2025, 6366, 2009, 1999, 2023, 2553, 1012, 102, 101, 2178, 2518, 1024, 2009, 2453, 2036, 2393, 2000, 2202, 1996, 2812, 1997, 1996, 2892, 4372, 13181, 13046, 2007, 2892, 1035, 23077, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1056, 2546, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 1012, 1012, 1012, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 48, 82, 147, 171, 269, 291, 338], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46699540", "vertexSet": [[{"sent_id": 0, "name": "tf.import_graph_def", "pos": [2, 10]}], [{"sent_id": 1, "name": "tf.constant", "pos": [77, 81]}], [{"sent_id": 0, "name": "tf.graphkeys", "pos": [24, 30]}]], "sents": ["The tf.import_graph_def() function doesn't have enough information to reconstruct the tf.GraphKeys.TRAINABLE_VARIABLES collection (for that, you would need a MetaGraphDef).", "However, if output.pb contains a \"frozen\" GraphDef, then all of the weights will be stored in tf.constant() nodes in the graph.", "To extract them, you can do something like the following:", "<code>Code Snippet</code>.", "Note that constant_values will probably contain more values than just the weights, so you may need to filter further by op.name or some other criterion."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 3853, 2987, 1005, 1056, 2031, 2438, 2592, 2000, 28667, 5644, 18300, 1996, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3345, 3085, 1035, 10857, 3074, 1006, 2005, 2008, 1010, 2017, 2052, 2342, 1037, 18804, 14413, 3207, 2546, 1007, 1012, 102, 101, 2174, 1010, 2065, 6434, 1012, 1052, 2497, 3397, 1037, 1000, 7708, 1000, 10629, 3207, 2546, 1010, 2059, 2035, 1997, 1996, 15871, 2097, 2022, 8250, 1999, 1056, 2546, 1012, 5377, 1006, 1007, 14164, 1999, 1996, 10629, 1012, 102, 101, 2000, 14817, 2068, 1010, 2017, 2064, 2079, 2242, 2066, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 5377, 1035, 5300, 2097, 2763, 5383, 2062, 5300, 2084, 2074, 1996, 15871, 1010, 2061, 2017, 2089, 2342, 2000, 11307, 2582, 2011, 6728, 1012, 2171, 2030, 2070, 2060, 19229, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 51, 89, 103, 117, 150], "sent_pos": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50229276", "vertexSet": [[{"sent_id": 1, "name": "tf.train.import_meta_graph", "pos": [50, 60]}], [{"sent_id": 0, "name": "tf.import_graph_def", "pos": [16, 24]}]], "sents": ["The tool import_pb_to_tensorboard.py uses tf.import_graph_def to import the graph and uses default name argument, which is \"import\" as documented.", "Your code imports the graph through tf.train.import_meta_graph and uses default import_scope argument, which will not prefix imported tensor or operation name.", "It is obvious then you have two options to correct this error:", "Do the following in place of your import_meta_graph line:", "<code>Code Snippet</code>.", "Remove import/ prefix when trying to get tensor or operation by name like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 6994, 12324, 1035, 1052, 2497, 1035, 2000, 1035, 23435, 6277, 1012, 1052, 2100, 3594, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 2000, 12324, 1996, 10629, 1998, 3594, 12398, 2171, 6685, 1010, 2029, 2003, 1000, 12324, 1000, 2004, 8832, 1012, 102, 101, 2115, 3642, 17589, 1996, 10629, 2083, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1998, 3594, 12398, 12324, 1035, 9531, 6685, 1010, 2029, 2097, 2025, 17576, 10964, 23435, 2030, 3169, 2171, 1012, 102, 101, 2009, 2003, 5793, 2059, 2017, 2031, 2048, 7047, 2000, 6149, 2023, 7561, 1024, 102, 101, 2079, 1996, 2206, 1999, 2173, 1997, 2115, 12324, 1035, 18804, 1035, 10629, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6366, 12324, 1013, 17576, 2043, 2667, 2000, 2131, 23435, 2030, 3169, 2011, 2171, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 43, 79, 94, 110, 124, 142, 156], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43290702", "vertexSet": [[{"sent_id": 4, "name": "tf.while_loop", "pos": [162, 168]}], [{"sent_id": 0, "name": "tf.cond", "pos": [11, 16]}]], "sents": ["TensorFlow does not support recursive calls to tf.cond(), because its implementation eagerly calls each of the two lambda functions that you pass for the two branches (to build the symbolic graphs), and the termination condition is not known at graph building time, so it will recurse until you run out of stack space.", "There is experimental support for defining (possibly recursive) functions in TensorFlow, using the tensorflow.python.framework.function library which is not currently part of the public API.", "The support for recursive functions is rudimentary, but you can define one using an explicit forward declaration, as follows:", "<code>Code Snippet</code>.", "Of course, an iterative solution using tf.while_loop() would be much more efficient!"], "sent_idxs": [101, 23435, 12314, 2515, 2025, 2490, 28667, 9236, 3512, 4455, 2000, 1056, 2546, 1012, 9530, 2094, 1006, 1007, 1010, 2138, 2049, 7375, 17858, 4455, 2169, 1997, 1996, 2048, 23375, 4972, 2008, 2017, 3413, 2005, 1996, 2048, 5628, 1006, 2000, 3857, 1996, 12613, 19287, 1007, 1010, 1998, 1996, 18287, 4650, 2003, 2025, 2124, 2012, 10629, 2311, 2051, 1010, 2061, 2009, 2097, 28667, 28393, 2127, 2017, 2448, 2041, 1997, 9991, 2686, 1012, 102, 101, 2045, 2003, 6388, 2490, 2005, 12854, 1006, 4298, 28667, 9236, 3512, 1007, 4972, 1999, 23435, 12314, 1010, 2478, 1996, 23435, 12314, 1012, 18750, 1012, 7705, 1012, 3853, 3075, 2029, 2003, 2025, 2747, 2112, 1997, 1996, 2270, 17928, 1012, 102, 101, 1996, 2490, 2005, 28667, 9236, 3512, 4972, 2003, 21766, 21341, 5649, 1010, 2021, 2017, 2064, 9375, 2028, 2478, 2019, 13216, 2830, 8170, 1010, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1997, 2607, 1010, 2019, 2009, 25284, 5576, 2478, 1056, 2546, 1012, 2096, 1035, 7077, 1006, 1007, 2052, 2022, 2172, 2062, 8114, 999, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 71, 111, 139, 153, 177], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43095070", "vertexSet": [[{"sent_id": 0, "name": "tf.where", "pos": [22, 26]}], [{"sent_id": 6, "name": "tf.case", "pos": [124, 128]}, {"sent_id": 7, "name": "tf.case", "pos": [135, 139]}], [{"sent_id": 6, "name": "tf.map_fn", "pos": [115, 122]}]], "sents": ["The only operation I'm aware of that evaluates a condition separately on each element of a vector is tf.where.", "You would leave x=None, y=None:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "However, this only evaluates the truth of a single condition.", "If you want to evaluate the truth of multiple conditions, over each element of a vector, I think you'll have to use tf.map_fn combined with tf.case.", "AFAIK, tf.case is the only operation that evaluates the truth of many conditions on a given value:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2069, 3169, 1045, 1005, 1049, 5204, 1997, 2008, 16157, 2015, 1037, 4650, 10329, 2006, 2169, 5783, 1997, 1037, 9207, 2003, 1056, 2546, 1012, 2073, 1012, 102, 101, 2017, 2052, 2681, 1060, 1027, 3904, 1010, 1061, 1027, 3904, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 1010, 2023, 2069, 16157, 2015, 1996, 3606, 1997, 1037, 2309, 4650, 1012, 102, 101, 2065, 2017, 2215, 2000, 16157, 1996, 3606, 1997, 3674, 3785, 1010, 2058, 2169, 5783, 1997, 1037, 9207, 1010, 1045, 2228, 2017, 1005, 2222, 2031, 2000, 2224, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 4117, 2007, 1056, 2546, 1012, 2553, 1012, 102, 101, 21358, 4886, 2243, 1010, 1056, 2546, 1012, 2553, 2003, 1996, 2069, 3169, 2008, 16157, 2015, 1996, 3606, 1997, 2116, 3785, 2006, 1037, 2445, 3643, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 5, 6, 7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 5, 6, 7]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 41, 55, 59, 73, 88, 130, 157, 171, 175, 189], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44486260", "vertexSet": [[{"sent_id": 3, "name": "tf.tensor", "pos": [35, 39]}], [{"sent_id": 2, "name": "tf.variable", "pos": [28, 32]}, {"sent_id": 3, "name": "tf.variable", "pos": [44, 48]}], [{"sent_id": 5, "name": "tf.sparse_to_dense", "pos": [66, 74]}]], "sents": ["Currently, Tensorflow does not support numpy-like assignment.", "Here is a couple of workarounds:", "tf.Variable.", "tf.Tensor cannot be changed, but tf.Variable can.", "<code>Code Snippet</code>.", "tf.sparse_to_dense().", "<code>Code Snippet</code>."], "sent_idxs": [101, 2747, 1010, 23435, 12314, 2515, 2025, 2490, 16371, 8737, 2100, 1011, 2066, 8775, 1012, 102, 101, 2182, 2003, 1037, 3232, 1997, 2147, 24490, 2015, 1024, 102, 101, 1056, 2546, 1012, 8023, 1012, 102, 101, 1056, 2546, 1012, 23435, 3685, 2022, 2904, 1010, 2021, 1056, 2546, 1012, 8023, 2064, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 20288, 1035, 2000, 1035, 9742, 1006, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 16, 27, 34, 51, 65, 78, 92], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57732427", "vertexSet": [[{"sent_id": 6, "name": "tf.keras.backend.clear_session", "pos": [173, 185]}], [{"sent_id": 9, "name": "tf.reset_default_graph", "pos": [236, 244]}, {"sent_id": 11, "name": "tf.reset_default_graph", "pos": [297, 305]}]], "sents": ["There are two main concepts in TensorFlow 1.0 Graphs and Sessions.", "Graph - It is a set of connected operations and placeholders which doesn't hold any tensor(numpy array) or values without a session.", "As an analogy, you can consider a food processing assembly line without any ingredients, but process and recipes are defined.", "Session - It takes the graph and initializes the variable with initial values and ready to take some to feed in the placeholder to start implementing the operations defined the graph to the feed values in placeholders, at last, it will deliver you the final output from you desired operation node (in neural network nodes of the last layer.", ")(like feeding tomatoes and getting ketchup as output.)", "coming back to your real question.", "If you use the tf.keras.backend.clear_session it will discard the values resides in the variable defined in the graph, leaving an empty vessel.", "(It will free up your RAM space.", "), now you can load weights from some other files.", "If you use the tf.reset_default_graph() it will reset the graph and it will remove all the defined operations and their inter-connection with corresponding weights.", "Now you have to load both models architecture and weights for the execution.", "practically it seems it is doing same stuff cause it is tf.reset_default_graph() will be called internally while calling k.clear_session() but clear_session will also intiate the fresh graph for the new operation you can check the source code here"], "sent_idxs": [101, 2045, 2024, 2048, 2364, 8474, 1999, 23435, 12314, 1015, 1012, 1014, 19287, 1998, 6521, 1012, 102, 101, 10629, 1011, 2009, 2003, 1037, 2275, 1997, 4198, 3136, 1998, 2173, 17794, 2029, 2987, 1005, 1056, 2907, 2151, 23435, 1006, 16371, 8737, 2100, 9140, 1007, 2030, 5300, 2302, 1037, 5219, 1012, 102, 101, 2004, 2019, 23323, 1010, 2017, 2064, 5136, 1037, 2833, 6364, 3320, 2240, 2302, 2151, 12760, 1010, 2021, 2832, 1998, 19328, 2024, 4225, 1012, 102, 101, 5219, 1011, 2009, 3138, 1996, 10629, 1998, 3988, 10057, 1996, 8023, 2007, 3988, 5300, 1998, 3201, 2000, 2202, 2070, 2000, 5438, 1999, 1996, 2173, 14528, 2000, 2707, 14972, 1996, 3136, 4225, 1996, 10629, 2000, 1996, 5438, 5300, 1999, 2173, 17794, 1010, 2012, 2197, 1010, 2009, 2097, 8116, 2017, 1996, 2345, 6434, 2013, 2017, 9059, 3169, 13045, 1006, 1999, 15756, 2897, 14164, 1997, 1996, 2197, 6741, 1012, 102, 101, 1007, 1006, 2066, 8521, 12851, 1998, 2893, 17710, 10649, 6279, 2004, 6434, 1012, 1007, 102, 101, 2746, 2067, 2000, 2115, 2613, 3160, 1012, 102, 101, 2065, 2017, 2224, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 3154, 1035, 5219, 2009, 2097, 5860, 4232, 1996, 5300, 11665, 1999, 1996, 8023, 4225, 1999, 1996, 10629, 1010, 2975, 2019, 4064, 6258, 1012, 102, 101, 1006, 2009, 2097, 2489, 2039, 2115, 8223, 2686, 1012, 102, 101, 1007, 1010, 2085, 2017, 2064, 7170, 15871, 2013, 2070, 2060, 6764, 1012, 102, 101, 2065, 2017, 2224, 1996, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2009, 2097, 25141, 1996, 10629, 1998, 2009, 2097, 6366, 2035, 1996, 4225, 3136, 1998, 2037, 6970, 1011, 4434, 2007, 7978, 15871, 1012, 102, 101, 2085, 2017, 2031, 2000, 7170, 2119, 4275, 4294, 1998, 15871, 2005, 1996, 7781, 1012, 102, 101, 8134, 2009, 3849, 2009, 2003, 2725, 2168, 4933, 3426, 2009, 2003, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2097, 2022, 2170, 16058, 2096, 4214, 1047, 1012, 3154, 1035, 5219, 1006, 1007, 2021, 3154, 1035, 5219, 2097, 2036, 20014, 13143, 1996, 4840, 10629, 2005, 1996, 2047, 3169, 2017, 2064, 4638, 1996, 3120, 3642, 2182, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [6, 7, 8, 9, 10, 11]}, {"r": "S1", "h": 1, "t": 0, "evidence": [6, 7, 8, 9, 10, 11]}], "na_triple": [], "sent_ends": [0, 17, 50, 75, 143, 159, 168, 206, 217, 231, 269, 285, 343], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44551409", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib.data", "pos": [69, 77]}, {"sent_id": 2, "name": "tf.contrib.data", "pos": [111, 119]}], [{"sent_id": 0, "name": "tf.train.string_input_producer", "pos": [6, 16]}]], "sents": ["As Nicolas observes, the tf.train.string_input_producer() API does not give you the ability to detect when the end of an epoch is reached; instead it concatenates together all epochs into one long batch.", "For this reason, we recently added (in TensorFlow 1.2) the tf.contrib.data API, which makes it possible to express more sophisticated pipelines, including your use case.", "The following code snippet shows how you would write your program using tf.contrib.data:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 9473, 24451, 1010, 1996, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 17928, 2515, 2025, 2507, 2017, 1996, 3754, 2000, 11487, 2043, 1996, 2203, 1997, 2019, 25492, 2003, 2584, 1025, 2612, 2009, 9530, 16280, 12556, 2015, 2362, 2035, 25492, 2015, 2046, 2028, 2146, 14108, 1012, 102, 101, 2005, 2023, 3114, 1010, 2057, 3728, 2794, 1006, 1999, 23435, 12314, 1015, 1012, 1016, 1007, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 17928, 1010, 2029, 3084, 2009, 2825, 2000, 4671, 2062, 12138, 13117, 2015, 1010, 2164, 2115, 2224, 2553, 1012, 102, 101, 1996, 2206, 3642, 1055, 3490, 29519, 3065, 2129, 2017, 2052, 4339, 2115, 2565, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 52, 96, 121, 135], "sent_pos": [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60088810", "vertexSet": [[{"sent_id": 3, "name": "tf.random.uniform", "pos": [123, 129]}], [{"sent_id": 0, "name": "tf.random.set_seed", "pos": [33, 41]}], [{"sent_id": 1, "name": "tf.compat.v2.random.set_seed", "pos": [66, 81]}], [{"sent_id": 2, "name": "tf.function", "pos": [86, 90]}]], "sents": ["Tensorflow 2.0 Compatible Answer: For Tensorflow version greater than 2.0, if we want to set the Global Random Seed, the Command used is tf.random.set_seed.", "If we are migrating from Tensorflow Version 1.x to 2.x, we can use the command, \ntf.compat.v2.random.set_seed.", "Note that tf.function acts like a re-run of a program in this case.", "To set the Operation Level Seed (as answered above), we can use the command, tf.random.uniform([1], seed=1).", "For more details, refer this Tensorflow Page."], "sent_idxs": [101, 23435, 12314, 1016, 1012, 1014, 11892, 3437, 1024, 2005, 23435, 12314, 2544, 3618, 2084, 1016, 1012, 1014, 1010, 2065, 2057, 2215, 2000, 2275, 1996, 3795, 6721, 6534, 1010, 1996, 3094, 2109, 2003, 1056, 2546, 1012, 6721, 1012, 2275, 1035, 6534, 1012, 102, 101, 2065, 2057, 2024, 28636, 2013, 23435, 12314, 2544, 1015, 1012, 1060, 2000, 1016, 1012, 1060, 1010, 2057, 2064, 2224, 1996, 3094, 1010, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 6721, 1012, 2275, 1035, 6534, 1012, 102, 101, 3602, 2008, 1056, 2546, 1012, 3853, 4490, 2066, 1037, 2128, 1011, 2448, 1997, 1037, 2565, 1999, 2023, 2553, 1012, 102, 101, 2000, 2275, 1996, 3169, 2504, 6534, 1006, 2004, 4660, 2682, 1007, 1010, 2057, 2064, 2224, 1996, 3094, 1010, 1056, 2546, 1012, 6721, 1012, 6375, 1006, 1031, 1015, 1033, 1010, 6534, 1027, 1015, 1007, 1012, 102, 101, 2005, 2062, 4751, 1010, 6523, 2023, 23435, 12314, 3931, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 3]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [0, 3], [1, 3], [2, 0], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 43, 83, 104, 140, 152], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48589833", "vertexSet": [[{"sent_id": 8, "name": "tf.contrib.seq2seq.greedyembeddinghelper", "pos": [237, 256]}], [{"sent_id": 10, "name": "tf.contrib.seq2seq.sampleembeddinghelper", "pos": [297, 316]}], [{"sent_id": 6, "name": "tf.contrib.seq2seq.traininghelper", "pos": [187, 203]}], [{"sent_id": 3, "name": "tf.contrib.seq2seq.helper", "pos": [90, 105]}], [{"sent_id": 4, "name": "tf.contrib.seq2seq.basicdecoder", "pos": [113, 130]}]], "sents": ["The exact answer depends on which building blocks you take from Neural Machine Translation model (NMT) and which ones you would replace with your own.", "I assume the graph structure exactly as in NMT.", "If so, at inference time, you can feed just a vector of zeros to the decoder.", "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).", "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().", "There are different implementations of Helper interface, e.g.,", "tf.contrib.seq2seq.TrainingHelper is returning the next decoder input (which is usually ground truth).", "This helper is used in training as indicated in the tutorial..", "tf.contrib.seq2seq.GreedyEmbeddingHelper discards the inputs, and returns the argmax sampled token from the previous output.", "NMT uses this helper in inference when sampling_temperature hyper-parameter is 0..", "tf.contrib.seq2seq.SampleEmbeddingHelper does the same, but samples the token according to categorical (a.k.a.", "generalized Bernoulli) distribution.", "NMT uses this helper in inference when sampling_temperature > 0..", "....", "The code is in BaseModel._build_decoder method.", "Note that both GreedyEmbeddingHelper and SampleEmbeddingHelper don't care what the decoder input is.", "So in fact you can feed anything, but the zero tensor is the standard choice."], "sent_idxs": [101, 1996, 6635, 3437, 9041, 2006, 2029, 2311, 5991, 2017, 2202, 2013, 15756, 3698, 5449, 2944, 1006, 13221, 2102, 1007, 1998, 2029, 3924, 2017, 2052, 5672, 2007, 2115, 2219, 1012, 102, 101, 1045, 7868, 1996, 10629, 3252, 3599, 2004, 1999, 13221, 2102, 1012, 102, 101, 2065, 2061, 1010, 2012, 28937, 2051, 1010, 2017, 2064, 5438, 2074, 1037, 9207, 1997, 5717, 2015, 2000, 1996, 21933, 4063, 1012, 102, 101, 4722, 4751, 1024, 13221, 2102, 3594, 1996, 9178, 2170, 2393, 2121, 2000, 5646, 1996, 2279, 7953, 1999, 1996, 21933, 4063, 1006, 2156, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 2393, 2121, 12653, 1007, 1012, 102, 101, 1999, 3327, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 3937, 3207, 16044, 2099, 16803, 9578, 2006, 2393, 2121, 2043, 2009, 10438, 1037, 3357, 1024, 1996, 2279, 1035, 20407, 2008, 1996, 2024, 7349, 1999, 2000, 1996, 4745, 3526, 2003, 3599, 1996, 2709, 3643, 1997, 2393, 2121, 1012, 2279, 1035, 20407, 1006, 1007, 1012, 102, 101, 2045, 2024, 2367, 24977, 1997, 2393, 2121, 8278, 1010, 1041, 1012, 1043, 1012, 1010, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 2731, 16001, 4842, 2003, 4192, 1996, 2279, 21933, 4063, 7953, 1006, 2029, 2003, 2788, 2598, 3606, 1007, 1012, 102, 101, 2023, 2393, 2121, 2003, 2109, 1999, 2731, 2004, 5393, 1999, 1996, 14924, 4818, 1012, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 20505, 6633, 8270, 4667, 16001, 4842, 5860, 18117, 1996, 20407, 1010, 1998, 5651, 1996, 12098, 21693, 8528, 18925, 19204, 2013, 1996, 3025, 6434, 1012, 102, 101, 13221, 2102, 3594, 2023, 2393, 2121, 1999, 28937, 2043, 16227, 1035, 4860, 23760, 1011, 16381, 2003, 1014, 1012, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 7099, 6633, 8270, 4667, 16001, 4842, 2515, 1996, 2168, 1010, 2021, 8168, 1996, 19204, 2429, 2000, 4937, 27203, 1006, 1037, 1012, 1047, 1012, 1037, 1012, 102, 101, 18960, 16595, 7140, 6894, 1007, 4353, 1012, 102, 101, 13221, 2102, 3594, 2023, 2393, 2121, 1999, 28937, 2043, 16227, 1035, 4860, 1028, 1014, 1012, 1012, 102, 101, 1012, 1012, 1012, 1012, 102, 101, 1996, 3642, 2003, 1999, 2918, 5302, 9247, 1012, 1035, 3857, 1035, 21933, 4063, 4118, 1012, 102, 101, 3602, 2008, 2119, 20505, 6633, 8270, 4667, 16001, 4842, 1998, 7099, 6633, 8270, 4667, 16001, 4842, 2123, 1005, 1056, 2729, 2054, 1996, 21933, 4063, 7953, 2003, 1012, 102, 101, 2061, 1999, 2755, 2017, 2064, 5438, 2505, 1010, 2021, 1996, 5717, 23435, 2003, 1996, 3115, 3601, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 4, 5, 8, 9, 10, 11, 12, 15, 16]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 4, 5, 8, 9, 10, 11, 12, 15, 16]}, {"r": "S1", "h": 2, "t": 0, "evidence": [3, 4, 5, 6, 7, 8, 9]}, {"r": "S1", "h": 0, "t": 2, "evidence": [3, 4, 5, 6, 7, 8, 9]}, {"r": "S1", "h": 2, "t": 1, "evidence": [3, 4, 5, 6, 7, 10, 11, 12]}, {"r": "S1", "h": 1, "t": 2, "evidence": [3, 4, 5, 6, 7, 10, 11, 12]}], "na_triple": [[0, 3], [0, 4], [1, 3], [1, 4], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 31, 44, 67, 109, 170, 186, 219, 236, 275, 296, 336, 345, 363, 369, 386, 415, 434], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53761947", "vertexSet": [[{"sent_id": 1, "name": "tf.layers.dense", "pos": [35, 41]}], [{"sent_id": 0, "name": "tf.nn.xw_plus_b", "pos": [1, 13]}, {"sent_id": 10, "name": "tf.nn.xw_plus_b", "pos": [179, 191]}, {"sent_id": 11, "name": "tf.nn.xw_plus_b", "pos": [218, 230]}]], "sents": ["tf.nn.xw_plus_b is a low-level operation that only computes x*W+b and requires existing variables.", "tf.layers.dense is a high-level \"layer\" that creates variables, apply activation can set constrains and apply regularization.", "According to the documentation default activation is linear (no activation).", "activation: Activation function (callable).", "Set it to None to maintain\n  a linear activation.", "Update", "In Tensorflow 1.12 Dense layer inherits  keras.layers.Dense (code):", "<code>Code Snippet</code>.", "Keras implementation of this layer does the following (code):", "<code>Code Snippet</code>.", "So it is not implemented using tf.nn.xw_plus_b but uses two separate operations.", "To answer your question: Dense layer without activation, constraints and regularization should do the same as tf.nn.xw_plus_b."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 1060, 2860, 1035, 4606, 1035, 1038, 2003, 1037, 2659, 1011, 2504, 3169, 2008, 2069, 24134, 2015, 1060, 1008, 1059, 1009, 1038, 1998, 5942, 4493, 10857, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9742, 2003, 1037, 2152, 1011, 2504, 1000, 6741, 1000, 2008, 9005, 10857, 1010, 6611, 13791, 2064, 2275, 9530, 20528, 7076, 1998, 6611, 3180, 3989, 1012, 102, 101, 2429, 2000, 1996, 12653, 12398, 13791, 2003, 7399, 1006, 2053, 13791, 1007, 1012, 102, 101, 13791, 1024, 13791, 3853, 1006, 2655, 3085, 1007, 1012, 102, 101, 2275, 2009, 2000, 3904, 2000, 5441, 1037, 7399, 13791, 1012, 102, 101, 10651, 102, 101, 1999, 23435, 12314, 1015, 1012, 2260, 9742, 6741, 22490, 2015, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 3642, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 17710, 8180, 7375, 1997, 2023, 6741, 2515, 1996, 2206, 1006, 3642, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2061, 2009, 2003, 2025, 7528, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 1060, 2860, 1035, 4606, 1035, 1038, 2021, 3594, 2048, 3584, 3136, 1012, 102, 101, 2000, 3437, 2115, 3160, 1024, 9742, 6741, 2302, 13791, 1010, 14679, 1998, 3180, 3989, 2323, 2079, 1996, 2168, 2004, 1056, 2546, 1012, 1050, 2078, 1012, 1060, 2860, 1035, 4606, 1035, 1038, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 34, 66, 81, 92, 104, 107, 129, 143, 158, 172, 198, 232], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "50307810", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.conv2d_transpose", "pos": [7, 20]}], [{"sent_id": 3, "name": "tf.layers.conv2d_transpose", "pos": [71, 83]}]], "sents": ["The input args filter to tf.nn.conv2d_transpose is the weights matrix itself and not just the size of the filter.", "The modified code that fixes the above problem is shown below:", "<code>Code Snippet</code>.", "Note: Its better to use tf.layers.conv2d_transpose API, where the filters arg is the filter size and the weights initialization happens within."], "sent_idxs": [101, 1996, 7953, 12098, 5620, 11307, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2003, 1996, 15871, 8185, 2993, 1998, 2025, 2074, 1996, 2946, 1997, 1996, 11307, 1012, 102, 101, 1996, 6310, 3642, 2008, 8081, 2229, 1996, 2682, 3291, 2003, 3491, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 1024, 2049, 2488, 2000, 2224, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 17928, 1010, 2073, 1996, 17736, 12098, 2290, 2003, 1996, 11307, 2946, 1998, 1996, 15871, 3988, 3989, 6433, 2306, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 3]}], "na_triple": [], "sent_ends": [0, 35, 50, 64, 103], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58264744", "vertexSet": [[{"sent_id": 1, "name": "tf.app.flags", "pos": [25, 31]}], [{"sent_id": 1, "name": "tf.compat.v1.flags", "pos": [32, 43]}]], "sents": ["Which Tensorflow version, are you using?", "If it is TF2.0 then you need to replace tf.app.flags with tf.compat.v1.flags defined here since it is no longer supported."], "sent_idxs": [101, 2029, 23435, 12314, 2544, 1010, 2024, 2017, 2478, 1029, 102, 101, 2065, 2009, 2003, 1056, 2546, 2475, 1012, 1014, 2059, 2017, 2342, 2000, 5672, 1056, 2546, 1012, 10439, 1012, 9245, 2007, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 9245, 4225, 2182, 2144, 2009, 2003, 2053, 2936, 3569, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 11, 53], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54718798", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.layers", "pos": [17, 24]}, {"sent_id": 2, "name": "tf.keras.layers", "pos": [51, 58]}, {"sent_id": 8, "name": "tf.keras.layers", "pos": [169, 176]}], [{"sent_id": 0, "name": "tf.layers", "pos": [8, 12]}, {"sent_id": 2, "name": "tf.layers", "pos": [38, 42]}, {"sent_id": 4, "name": "tf.layers", "pos": [87, 91]}, {"sent_id": 8, "name": "tf.layers", "pos": [159, 163]}]], "sents": ["Since TensorFlow 1.12, tf.layers are merely wrappers around tf.keras.layers.", "A few examples:", "Convolutional tf.layers just inherit from the convolutional tf.keras.layers, see source code here:", "<code>Code Snippet</code>.", "The same is true for all core tf.layers, e.g.", ":", "<code>Code Snippet</code>.", "With the integration of Keras into TensorFlow, it would make little sense to maintain several different layer implementations.", "tf.keras is becoming the de-facto high-level API for TensorFlow, therefore tf.layers are now just wrappers around tf.keras.layers."], "sent_idxs": [101, 2144, 23435, 12314, 1015, 1012, 2260, 1010, 1056, 2546, 1012, 9014, 2024, 6414, 10236, 7347, 2105, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 102, 101, 1037, 2261, 4973, 1024, 102, 101, 9530, 6767, 7630, 3508, 2389, 1056, 2546, 1012, 9014, 2074, 22490, 2013, 1996, 9530, 6767, 7630, 3508, 2389, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1010, 2156, 3120, 3642, 2182, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2168, 2003, 2995, 2005, 2035, 4563, 1056, 2546, 1012, 9014, 1010, 1041, 1012, 1043, 1012, 102, 101, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2007, 1996, 8346, 1997, 17710, 8180, 2046, 23435, 12314, 1010, 2009, 2052, 2191, 2210, 3168, 2000, 5441, 2195, 2367, 6741, 24977, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 2003, 3352, 1996, 2139, 1011, 13743, 2152, 1011, 2504, 17928, 2005, 23435, 12314, 1010, 3568, 1056, 2546, 1012, 9014, 2024, 2085, 2074, 10236, 7347, 2105, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 7, 8]}], "na_triple": [], "sent_ends": [0, 26, 32, 65, 79, 97, 100, 114, 138, 178], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "60316058", "vertexSet": [[{"sent_id": 2, "name": "tf.compat.v1.placeholder", "pos": [84, 96]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [15, 20]}], [{"sent_id": 3, "name": "tf.variable", "pos": [111, 115]}], [{"sent_id": 4, "name": "tf.compat.v2", "pos": [138, 147]}], [{"sent_id": 4, "name": "tf.compat.v2.variable", "pos": [138, 149]}]], "sents": ["Tensorflow 2.0 Compatible Answer: The concept of Placeholders, tf.placeholder will not be available in Tensorflow 2.x (>= 2.0) by default, as the Default Execution Mode is Eager Execution.", "However, we can use them if used in Graph Mode (Disable Eager Execution).", "Equivalent command for TF Placeholder in version 2.x is tf.compat.v1.placeholder.", "Equivalent Command for TF Variable in version 2.x is tf.Variable and if you want to migrate the code from 1.x to 2.x, the equivalent command is", "tf.compat.v2.Variable.", "Please refer this Tensorflow Page for more information about Tensorflow Version 2.0.", "Please refer the Migration Guide for more information about migration from versions 1.x to 2.x."], "sent_idxs": [101, 23435, 12314, 1016, 1012, 1014, 11892, 3437, 1024, 1996, 4145, 1997, 2173, 17794, 1010, 1056, 2546, 1012, 2173, 14528, 2097, 2025, 2022, 2800, 1999, 23435, 12314, 1016, 1012, 1060, 1006, 1028, 1027, 1016, 1012, 1014, 1007, 2011, 12398, 1010, 2004, 1996, 12398, 7781, 5549, 2003, 9461, 7781, 1012, 102, 101, 2174, 1010, 2057, 2064, 2224, 2068, 2065, 2109, 1999, 10629, 5549, 1006, 4487, 19150, 9461, 7781, 1007, 1012, 102, 101, 5662, 3094, 2005, 1056, 2546, 2173, 14528, 1999, 2544, 1016, 1012, 1060, 2003, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 2173, 14528, 1012, 102, 101, 5662, 3094, 2005, 1056, 2546, 8023, 1999, 2544, 1016, 1012, 1060, 2003, 1056, 2546, 1012, 8023, 1998, 2065, 2017, 2215, 2000, 22806, 1996, 3642, 2013, 1015, 1012, 1060, 2000, 1016, 1012, 1060, 1010, 1996, 5662, 3094, 2003, 102, 101, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 8023, 1012, 102, 101, 3531, 6523, 2023, 23435, 12314, 3931, 2005, 2062, 2592, 2055, 23435, 12314, 2544, 1016, 1012, 1014, 1012, 102, 101, 3531, 6523, 1996, 9230, 5009, 2005, 2062, 2592, 2055, 9230, 2013, 4617, 1015, 1012, 1060, 2000, 1016, 1012, 1060, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 2]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 50, 70, 98, 137, 151, 170, 192], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43587561", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [1, 11]}, {"sent_id": 4, "name": "tf.nn.conv2d", "pos": [113, 123]}], [{"sent_id": 1, "name": "tf.contrib.layers.conv2d", "pos": [35, 48]}], [{"sent_id": 3, "name": "tf.layers.conv2d", "pos": [95, 104]}, {"sent_id": 5, "name": "tf.layers.conv2d", "pos": [139, 148]}, {"sent_id": 13, "name": "tf.layers.conv2d", "pos": [290, 299]}]], "sents": ["tf.nn.conv2d(...) is the core, low-level convolution functionality provided by TensorFlow.", "tf.contrib.layers.conv2d(...) is part of a higher-level API build around core-TensorFlow.", "Note, that in current TensorFlow versions, parts of layers are now in core, too, e.g.", "tf.layers.conv2d.", "The difference is simply, that tf.nn.conv2d is an op, that does convolution, nothing else.", "tf.layers.conv2d does more, e.g.", "it also creates variables for the kernel and the biases amongst other things.", "Check out the Tensorflow Tutorial on CNNs which uses Tensorflow core (here).", "With the low-level API the convolutional layers are created like this:", "<code>Code Snippet</code>.", "Compare that to the TF Layers Tutorial for CNNs (here).", "With TF Layers convolutional layers are create like this:", "<code>Code Snippet</code>.", "Without knowing your use case: Most likely you want to use tf.layers.conv2d."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1006, 1012, 1012, 1012, 1007, 2003, 1996, 4563, 1010, 2659, 1011, 2504, 9530, 6767, 7630, 3508, 15380, 3024, 2011, 23435, 12314, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1006, 1012, 1012, 1012, 1007, 2003, 2112, 1997, 1037, 3020, 1011, 2504, 17928, 3857, 2105, 4563, 1011, 23435, 12314, 1012, 102, 101, 3602, 1010, 2008, 1999, 2783, 23435, 12314, 4617, 1010, 3033, 1997, 9014, 2024, 2085, 1999, 4563, 1010, 2205, 1010, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1012, 102, 101, 1996, 4489, 2003, 3432, 1010, 2008, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 2003, 2019, 6728, 1010, 2008, 2515, 9530, 6767, 7630, 3508, 1010, 2498, 2842, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2515, 2062, 1010, 1041, 1012, 1043, 1012, 102, 101, 2009, 2036, 9005, 10857, 2005, 1996, 16293, 1998, 1996, 13827, 2229, 5921, 2060, 2477, 1012, 102, 101, 4638, 2041, 1996, 23435, 12314, 14924, 4818, 2006, 13229, 2015, 2029, 3594, 23435, 12314, 4563, 1006, 2182, 1007, 1012, 102, 101, 2007, 1996, 2659, 1011, 2504, 17928, 1996, 9530, 6767, 7630, 3508, 2389, 9014, 2024, 2580, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 12826, 2008, 2000, 1996, 1056, 2546, 9014, 14924, 4818, 2005, 13229, 2015, 1006, 2182, 1007, 1012, 102, 101, 2007, 1056, 2546, 9014, 9530, 6767, 7630, 3508, 2389, 9014, 2024, 3443, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2302, 4209, 2115, 2224, 2553, 1024, 2087, 3497, 2017, 2215, 2000, 2224, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 2, 3, 4, 5, 6]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 2, 3, 4, 5, 6]}, {"r": "S1", "h": 1, "t": 2, "evidence": [1, 2, 3]}, {"r": "S1", "h": 2, "t": 1, "evidence": [1, 2, 3]}], "na_triple": [], "sent_ends": [0, 34, 69, 94, 106, 138, 156, 173, 194, 214, 228, 246, 263, 277, 301], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0]}, {"title": "34345827", "vertexSet": [[{"sent_id": 2, "name": "tf.read_file", "pos": [54, 60]}, {"sent_id": 3, "name": "tf.read_file", "pos": [84, 90]}], [{"sent_id": 2, "name": "tf.wholefilereader", "pos": [66, 74]}], [{"sent_id": 1, "name": "tf.train", "pos": [16, 20]}], [{"sent_id": 1, "name": "tf.train.string_input_producer", "pos": [16, 26]}]], "sents": ["There are three main steps to solving this problem:", "Populate the tf.train.string_input_producer() with a list of strings containing the original, space-delimited string containing the filename and the label.", "Use tf.read_file(filename) rather than tf.WholeFileReader() to read your image files.", "tf.read_file() is a stateless op that consumes a single filename and produces a single string containing the contents of the file.", "It has the advantage that it's a pure function, so it's easy to associate data with the input and the output.", "For example, your read_my_file_format function would become:", "<code>Code Snippet</code>.", "Invoke the new version of read_my_file_format by passing a single dequeued element from the input_queue:", "<code>Code Snippet</code>.", "You can then use the image and label tensors in the remainder of your model."], "sent_idxs": [101, 2045, 2024, 2093, 2364, 4084, 2000, 13729, 2023, 3291, 1024, 102, 101, 3769, 9869, 1996, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 2007, 1037, 2862, 1997, 7817, 4820, 1996, 2434, 1010, 2686, 1011, 3972, 27605, 3064, 5164, 4820, 1996, 5371, 18442, 1998, 1996, 3830, 1012, 102, 101, 2224, 1056, 2546, 1012, 3191, 1035, 5371, 1006, 5371, 18442, 1007, 2738, 2084, 1056, 2546, 1012, 2878, 8873, 3917, 13775, 2121, 1006, 1007, 2000, 3191, 2115, 3746, 6764, 1012, 102, 101, 1056, 2546, 1012, 3191, 1035, 5371, 1006, 1007, 2003, 1037, 2110, 3238, 6728, 2008, 16678, 2015, 1037, 2309, 5371, 18442, 1998, 7137, 1037, 2309, 5164, 4820, 1996, 8417, 1997, 1996, 5371, 1012, 102, 101, 2009, 2038, 1996, 5056, 2008, 2009, 1005, 1055, 1037, 5760, 3853, 1010, 2061, 2009, 1005, 1055, 3733, 2000, 5482, 2951, 2007, 1996, 7953, 1998, 1996, 6434, 1012, 102, 101, 2005, 2742, 1010, 2115, 3191, 1035, 2026, 1035, 5371, 1035, 4289, 3853, 2052, 2468, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 6767, 3489, 1996, 2047, 2544, 1997, 3191, 1035, 2026, 1035, 5371, 1035, 4289, 2011, 4458, 1037, 2309, 2139, 4226, 5657, 2094, 5783, 2013, 1996, 7953, 1035, 24240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2059, 2224, 1996, 3746, 1998, 3830, 23435, 2015, 1999, 1996, 6893, 1997, 2115, 2944, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3, 4]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 12, 52, 83, 117, 146, 163, 177, 208, 222, 241], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63435824", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [9, 13]}], [{"sent_id": 0, "name": "tf.constant", "pos": [27, 31]}], [{"sent_id": 9, "name": "tf.py_func", "pos": [188, 196]}], [{"sent_id": 1, "name": "tf.make_tensor_proto", "pos": [37, 45]}]], "sents": ["What was wrong: You cannot convert a tf.variable to a numpy array using graph execution and need to use a tf.constant instead.", "Also using tf.make_tensor_proto is pointless since sess.run converts the tensor into a numpy array for you.", "Explanation: Convert your actions to TF constants instead of Variables or use Constants instead (not possible to convert variables).", "Then apply the same logic to your bound.", "Next, create the mask as you did previously.", "Finally, create a session and evaluate the mask in the session.", "Here is the code to convert your mask into a numpy array using graph execution:", "<code>Code Snippet</code>.", "Based on your situation I think this is the best way of doing it.", "But you could also use tf.py_func to create a function that would convert the mask into a numpy array."], "sent_idxs": [101, 2054, 2001, 3308, 1024, 2017, 3685, 10463, 1037, 1056, 2546, 1012, 8023, 2000, 1037, 16371, 8737, 2100, 9140, 2478, 10629, 7781, 1998, 2342, 2000, 2224, 1037, 1056, 2546, 1012, 5377, 2612, 1012, 102, 101, 2036, 2478, 1056, 2546, 1012, 2191, 1035, 23435, 1035, 15053, 2003, 23100, 2144, 7367, 4757, 1012, 2448, 19884, 1996, 23435, 2046, 1037, 16371, 8737, 2100, 9140, 2005, 2017, 1012, 102, 101, 7526, 1024, 10463, 2115, 4506, 2000, 1056, 2546, 5377, 2015, 2612, 1997, 10857, 2030, 2224, 5377, 2015, 2612, 1006, 2025, 2825, 2000, 10463, 10857, 1007, 1012, 102, 101, 2059, 6611, 1996, 2168, 7961, 2000, 2115, 5391, 1012, 102, 101, 2279, 1010, 3443, 1996, 7308, 2004, 2017, 2106, 3130, 1012, 102, 101, 2633, 1010, 3443, 1037, 5219, 1998, 16157, 1996, 7308, 1999, 1996, 5219, 1012, 102, 101, 2182, 2003, 1996, 3642, 2000, 10463, 2115, 7308, 2046, 1037, 16371, 8737, 2100, 9140, 2478, 10629, 7781, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2241, 2006, 2115, 3663, 1045, 2228, 2023, 2003, 1996, 2190, 2126, 1997, 2725, 2009, 1012, 102, 101, 2021, 2017, 2071, 2036, 2224, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 2000, 3443, 1037, 3853, 2008, 2052, 10463, 1996, 7308, 2046, 1037, 16371, 8737, 2100, 9140, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 34, 65, 93, 104, 116, 131, 151, 165, 182, 213], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46210187", "vertexSet": [[{"sent_id": 4, "name": "tf.contrib", "pos": [159, 165]}], [{"sent_id": 4, "name": "tf.contrib.keras", "pos": [159, 168]}]], "sents": ["Currently, there is no direct in-built support in Tensorflow or Keras to convert the frozen model or the checkpoint file to hdf5 format.", "But since you have mentioned that you have the code of Tensorflow model, you will have to rewrite that model's code in Keras.", "Then, you will have to read the values of your variables from the checkpoint file and assign it to Keras model using layer.load_weights(weights) method.", "More than this methodology, I would suggest to you to do the training directly in Keras as it claimed that Keras' optimizers are 5-10% times faster than Tensorflow's optimizers.", "Other way is to write your code in Tensorflow with tf.contrib.keras module and save the file directly in hdf5 format."], "sent_idxs": [101, 2747, 1010, 2045, 2003, 2053, 3622, 1999, 1011, 2328, 2490, 1999, 23435, 12314, 2030, 17710, 8180, 2000, 10463, 1996, 7708, 2944, 2030, 1996, 26520, 5371, 2000, 10751, 2546, 2629, 4289, 1012, 102, 101, 2021, 2144, 2017, 2031, 3855, 2008, 2017, 2031, 1996, 3642, 1997, 23435, 12314, 2944, 1010, 2017, 2097, 2031, 2000, 2128, 26373, 2008, 2944, 1005, 1055, 3642, 1999, 17710, 8180, 1012, 102, 101, 2059, 1010, 2017, 2097, 2031, 2000, 3191, 1996, 5300, 1997, 2115, 10857, 2013, 1996, 26520, 5371, 1998, 23911, 2009, 2000, 17710, 8180, 2944, 2478, 6741, 1012, 7170, 1035, 15871, 1006, 15871, 1007, 4118, 1012, 102, 101, 2062, 2084, 2023, 16134, 1010, 1045, 2052, 6592, 2000, 2017, 2000, 2079, 1996, 2731, 3495, 1999, 17710, 8180, 2004, 2009, 3555, 2008, 17710, 8180, 1005, 23569, 27605, 16750, 2024, 1019, 1011, 2184, 1003, 2335, 5514, 2084, 23435, 12314, 1005, 1055, 23569, 27605, 16750, 1012, 102, 101, 2060, 2126, 2003, 2000, 4339, 2115, 3642, 1999, 23435, 12314, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 17710, 8180, 11336, 1998, 3828, 1996, 5371, 3495, 1999, 10751, 2546, 2629, 4289, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 33, 65, 101, 147, 181], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38753139", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.rnn", "pos": [2, 10]}, {"sent_id": 0, "name": "tf.nn.rnn", "pos": [32, 40]}, {"sent_id": 1, "name": "tf.nn.rnn", "pos": [60, 68]}], [{"sent_id": 0, "name": "tf.nn.dynamic_rnn", "pos": [13, 23]}], [{"sent_id": 0, "name": "tf.nn.rnn_cell", "pos": [32, 42]}, {"sent_id": 1, "name": "tf.nn.rnn_cell", "pos": [60, 70]}], [{"sent_id": 0, "name": "tf.nn.rnn_cell.rnncell", "pos": [32, 46]}], [{"sent_id": 1, "name": "tf.nn.rnn_cell.basiclstmcell", "pos": [60, 76]}]], "sents": ["The tf.nn.rnn() and tf.nn.dynamic_rnn() functions accept an argument cell of type tf.nn.rnn_cell.RNNCell.", "For example you can take a look at the implementation of tf.nn.rnn_cell.BasicLSTMCell (in particular the BasicLSTMCell.__call__() method), which might be a good starting point for your customized LSTM."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1006, 1007, 1998, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1006, 1007, 4972, 5138, 2019, 6685, 3526, 1997, 2828, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 29300, 5897, 3363, 1012, 102, 101, 2005, 2742, 2017, 2064, 2202, 1037, 2298, 2012, 1996, 7375, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 3937, 4877, 21246, 29109, 2140, 1006, 1999, 3327, 1996, 3937, 4877, 21246, 29109, 2140, 1012, 1035, 1035, 2655, 1035, 1035, 1006, 1007, 4118, 1007, 1010, 2029, 2453, 2022, 1037, 2204, 3225, 2391, 2005, 2115, 28749, 1048, 3367, 2213, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 48, 111], "sent_pos": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47288830", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib.learn.experiment", "pos": [1, 11]}, {"sent_id": 4, "name": "tf.contrib.learn.experiment", "pos": [128, 138]}], [{"sent_id": 4, "name": "tf.estimator.estimator", "pos": [93, 103]}, {"sent_id": 5, "name": "tf.estimator.estimator", "pos": [185, 195]}], [{"sent_id": 4, "name": "tf.train", "pos": [155, 159]}, {"sent_id": 4, "name": "tf.train", "pos": [164, 168]}], [{"sent_id": 4, "name": "tf.train.server", "pos": [164, 170]}], [{"sent_id": 4, "name": "tf.train.clusterspec", "pos": [155, 163]}]], "sents": ["tf.contrib.learn.Experiment is a high-level API for distributed training.", "Here's from its doc:", "Experiment is a class containing all information needed to train a\n  model.", "After an experiment is created (by passing an Estimator and inputs for\n  training and evaluation), an Experiment instance knows how to invoke\n  training and eval loops in a sensible fashion for distributed\n  training.", "Just like tf.estimator.Estimator (and the derived classes) is a high-level API that hides matrix multiplications, saving checkpoints and so on, tf.contrib.learn.Experiment tries to hide the boilerplate you'd need to do for distributed computation, namely tf.train.ClusterSpec, tf.train.Server, jobs, tasks, etc.", "You can train and evaluate the tf.estimator.Estimator on a single machine without an Experiment.", "See the examples in this tutorial."], "sent_idxs": [101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7551, 2003, 1037, 2152, 1011, 2504, 17928, 2005, 5500, 2731, 1012, 102, 101, 2182, 1005, 1055, 2013, 2049, 9986, 1024, 102, 101, 7551, 2003, 1037, 2465, 4820, 2035, 2592, 2734, 2000, 3345, 1037, 2944, 1012, 102, 101, 2044, 2019, 7551, 2003, 2580, 1006, 2011, 4458, 2019, 9765, 9581, 4263, 1998, 20407, 2005, 2731, 1998, 9312, 1007, 1010, 2019, 7551, 6013, 4282, 2129, 2000, 1999, 6767, 3489, 2731, 1998, 9345, 2140, 15932, 1999, 1037, 21082, 4827, 2005, 5500, 2731, 1012, 102, 101, 2074, 2066, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1006, 1998, 1996, 5173, 4280, 1007, 2003, 1037, 2152, 1011, 2504, 17928, 2008, 17382, 8185, 24856, 2015, 1010, 7494, 26520, 2015, 1998, 2061, 2006, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7551, 5363, 2000, 5342, 1996, 15635, 15725, 2017, 1005, 1040, 2342, 2000, 2079, 2005, 5500, 22334, 1010, 8419, 1056, 2546, 1012, 3345, 1012, 12906, 5051, 2278, 1010, 1056, 2546, 1012, 3345, 1012, 8241, 1010, 5841, 1010, 8518, 1010, 4385, 1012, 102, 101, 2017, 2064, 3345, 1998, 16157, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 2006, 1037, 2309, 3698, 2302, 2019, 7551, 1012, 102, 101, 2156, 1996, 4973, 1999, 2023, 14924, 4818, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 22, 31, 46, 90, 178, 204, 214], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59579870", "vertexSet": [[{"sent_id": 8, "name": "tf.math.floordiv", "pos": [335, 343]}], [{"sent_id": 6, "name": "tf.math.truediv", "pos": [243, 251]}, {"sent_id": 8, "name": "tf.math.truediv", "pos": [317, 325]}], [{"sent_id": 9, "name": "tf.pow", "pos": [351, 355]}, {"sent_id": 10, "name": "tf.pow", "pos": [390, 394]}], [{"sent_id": 0, "name": "tf.sparse", "pos": [17, 21]}, {"sent_id": 5, "name": "tf.sparse", "pos": [204, 208]}], [{"sent_id": 0, "name": "tf.multiply", "pos": [33, 38]}]], "sents": ["Assuming the two operands of * are both tf.Tensors and not tf.sparse.SparseTensors , the * operator is the same as tf.multiply, i.e., elementwise multiplication with broadcasting support.", "If you are interested in studying the source code that performs the operator overloading, the key parts are:", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L891.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1225.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1201.", "For tf.sparse.SparseTensors, * is overloaded with sparse tensor-specific multiplication ops.", "Assuming you're using Python3, the / operator is overloaded to the tf.math.truediv (i.e., floating-point division, which corresponds to the RealDiv op of TensorFlow).", "In Python2, the / operator may be doing integer division, in which case it's overloaded in a dtype-dependent way.", "For floating dtypes, it's tf.math.truediv, for integer dtypes, it's tf.math.floordiv (integer floor division).", "tf.pow() uses a different operator (i.e., the Pow) operator.", "But assuming all your dtypes are floating-point, 1 / x and tf.pow(x, -1.0) should be equivalent."], "sent_idxs": [101, 10262, 1996, 2048, 3850, 18376, 1997, 1008, 2024, 2119, 1056, 2546, 1012, 23435, 2015, 1998, 2025, 1056, 2546, 1012, 20288, 1012, 20288, 25808, 5668, 1010, 1996, 1008, 6872, 2003, 1996, 2168, 2004, 1056, 2546, 1012, 4800, 22086, 1010, 1045, 1012, 1041, 1012, 1010, 5783, 14244, 24856, 2007, 5062, 2490, 1012, 102, 101, 2065, 2017, 2024, 4699, 1999, 5702, 1996, 3120, 3642, 2008, 10438, 1996, 6872, 2058, 18570, 1010, 1996, 3145, 3033, 2024, 1024, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 8785, 1035, 23092, 1012, 1052, 2100, 1001, 1048, 2620, 2683, 2487, 1012, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 8785, 1035, 23092, 1012, 1052, 2100, 1001, 1048, 12521, 17788, 1012, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 8785, 1035, 23092, 1012, 1052, 2100, 1001, 1048, 12521, 24096, 1012, 102, 101, 2005, 1056, 2546, 1012, 20288, 1012, 20288, 25808, 5668, 1010, 1008, 2003, 2058, 17468, 2007, 20288, 23435, 1011, 3563, 24856, 23092, 1012, 102, 101, 10262, 2017, 1005, 2128, 2478, 18750, 2509, 1010, 1996, 1013, 6872, 2003, 2058, 17468, 2000, 1996, 1056, 2546, 1012, 8785, 1012, 2995, 4305, 2615, 1006, 1045, 1012, 1041, 1012, 1010, 8274, 1011, 2391, 2407, 1010, 2029, 14788, 2000, 1996, 2613, 4305, 2615, 6728, 1997, 23435, 12314, 1007, 1012, 102, 101, 1999, 18750, 2475, 1010, 1996, 1013, 6872, 2089, 2022, 2725, 16109, 2407, 1010, 1999, 2029, 2553, 2009, 1005, 1055, 2058, 17468, 1999, 1037, 26718, 18863, 1011, 7790, 2126, 1012, 102, 101, 2005, 8274, 26718, 18863, 2015, 1010, 2009, 1005, 1055, 1056, 2546, 1012, 8785, 1012, 2995, 4305, 2615, 1010, 2005, 16109, 26718, 18863, 2015, 1010, 2009, 1005, 1055, 1056, 2546, 1012, 8785, 1012, 2723, 4305, 2615, 1006, 16109, 2723, 2407, 1007, 1012, 102, 101, 1056, 2546, 1012, 23776, 1006, 1007, 3594, 1037, 2367, 6872, 1006, 1045, 1012, 1041, 1012, 1010, 1996, 23776, 1007, 6872, 1012, 102, 101, 2021, 10262, 2035, 2115, 26718, 18863, 2015, 2024, 8274, 1011, 2391, 1010, 1015, 1013, 1060, 1998, 1056, 2546, 1012, 23776, 1006, 1060, 1010, 1011, 1015, 1012, 1014, 1007, 2323, 2022, 5662, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [6, 7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [6, 7, 8]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 52, 75, 118, 160, 202, 226, 276, 307, 350, 373, 407], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53855704", "vertexSet": [[{"sent_id": 4, "name": "tf.contrib.image.angles_to_projective_transforms", "pos": [82, 98]}], [{"sent_id": 3, "name": "tf.contrib.image.transform", "pos": [62, 72]}], [{"sent_id": 0, "name": "tf.contrib.image.rotate", "pos": [6, 16]}]], "sents": ["You can't feed tf.contrib.image.rotate with an angles tensor.", "But if you inspect the source code you can see it just makes a bunch of argument validations, and then:", "<code>Code Snippet</code>.", "tf.contrib.image.transform() receives a projective transform matrix.", "tf.contrib.image.angles_to_projective_transforms() generates projective transforms from the rotation angles.", "Both accept tensors as arguments, so you can just call the underlying functions.", "Here is an example using MNIST", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 1005, 1056, 5438, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 24357, 2007, 2019, 12113, 23435, 1012, 102, 101, 2021, 2065, 2017, 22459, 1996, 3120, 3642, 2017, 2064, 2156, 2009, 2074, 3084, 1037, 9129, 1997, 6685, 27354, 2015, 1010, 1998, 2059, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 10938, 1006, 1007, 8267, 1037, 27473, 10938, 8185, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 12113, 1035, 2000, 1035, 27473, 1035, 21743, 1006, 1007, 19421, 27473, 21743, 2013, 1996, 9963, 12113, 1012, 102, 101, 2119, 5138, 23435, 2015, 2004, 9918, 1010, 2061, 2017, 2064, 2074, 2655, 1996, 10318, 4972, 1012, 102, 101, 2182, 2003, 2019, 2742, 2478, 24098, 2923, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [3, 4, 5]}, {"r": "S1", "h": 0, "t": 1, "evidence": [3, 4, 5]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 47, 61, 81, 109, 127, 136, 150], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51306350", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.dropout", "pos": [22, 30]}, {"sent_id": 2, "name": "tf.nn.dropout", "pos": [66, 74]}], [{"sent_id": 0, "name": "tf.contrib.nn.alpha_dropout", "pos": [1, 15]}], [{"sent_id": 2, "name": "tf.layers.dropout", "pos": [56, 63]}]], "sents": ["tf.contrib.nn.alpha_dropout should be seen as an analogue to tf.nn.dropout.", "The latter function also does not have an argument for a training switch.", "It is not to be confused with tf.layers.dropout, which wraps tf.nn.dropout and has a training argument.", "As we can see in the implementation, the layers version returns either the result of nn.dropout or the identity depending on the training switch.", "It should be relatively easy to define your own wrapper around alpha_dropout in a similar manner.", "To avoid any confusion: layers.dropout eventually calls the \"keras layers\" version of dropout which is the implementation linked above."], "sent_idxs": [101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1050, 2078, 1012, 6541, 1035, 4530, 5833, 2323, 2022, 2464, 2004, 2019, 21800, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1012, 102, 101, 1996, 3732, 3853, 2036, 2515, 2025, 2031, 2019, 6685, 2005, 1037, 2731, 6942, 1012, 102, 101, 2009, 2003, 2025, 2000, 2022, 5457, 2007, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 1010, 2029, 19735, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1998, 2038, 1037, 2731, 6685, 1012, 102, 101, 2004, 2057, 2064, 2156, 1999, 1996, 7375, 1010, 1996, 9014, 2544, 5651, 2593, 1996, 2765, 1997, 1050, 2078, 1012, 4530, 5833, 2030, 1996, 4767, 5834, 2006, 1996, 2731, 6942, 1012, 102, 101, 2009, 2323, 2022, 4659, 3733, 2000, 9375, 2115, 2219, 10236, 4842, 2105, 6541, 1035, 4530, 5833, 1999, 1037, 2714, 5450, 1012, 102, 101, 2000, 4468, 2151, 6724, 1024, 9014, 1012, 4530, 5833, 2776, 4455, 1996, 1000, 17710, 8180, 9014, 1000, 2544, 1997, 4530, 5833, 2029, 2003, 1996, 7375, 5799, 2682, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 2, "t": 0, "evidence": [1, 2, 3, 5]}, {"r": "S1", "h": 0, "t": 2, "evidence": [1, 2, 3, 5]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 32, 48, 81, 113, 136, 166], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34869168", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [46, 51]}, {"sent_id": 5, "name": "tf.nn", "pos": [158, 163]}], [{"sent_id": 1, "name": "tf.nn.conv2d", "pos": [46, 56]}], [{"sent_id": 5, "name": "tf.nn.relu_layer", "pos": [158, 168]}]], "sents": ["The CIFAR-10 model from the tutorial uses \"parameter sharing\" in the first two layers ('conv1' and 'conv2').", "The sharing is implied by the use of the tf.nn.conv2d() operator, which effectively extracts patches from the input image and applies the same filter (i.e.", "a shared parameter) to each patch.", "It's not trivial to \"turn off\" parameter sharing when you have a set of convolutional layers: instead you have to replace them with a different type of layer.", "The simplest change might be to replace the convolutional layers with a fully connected layer, e.g.", "by using tf.nn.relu_layer() (as in the 'local3' and 'local4' layers), which internally performs a matrix multiplication and maintains separate parameters for each input neuron.", "N.B.", "Fully connected layers are often over-parameterized for vision tasks, and a more appropriate middle ground would be to use a \"local receptive field\", which (informally) maintains separate parameters for each input (as in a fully connected layer), but only combines values from \"nearby\" inputs to produce an output (as in a convolution).", "Unfortunately, TensorFlow doesn't yet contain an implementation of local receptive fields, but adding support for them would be a useful project."], "sent_idxs": [101, 1996, 25022, 14971, 1011, 2184, 2944, 2013, 1996, 14924, 4818, 3594, 1000, 16381, 6631, 1000, 1999, 1996, 2034, 2048, 9014, 1006, 1005, 9530, 2615, 2487, 1005, 1998, 1005, 9530, 2615, 2475, 1005, 1007, 1012, 102, 101, 1996, 6631, 2003, 13339, 2011, 1996, 2224, 1997, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1006, 1007, 6872, 1010, 2029, 6464, 27059, 13864, 2013, 1996, 7953, 3746, 1998, 12033, 1996, 2168, 11307, 1006, 1045, 1012, 1041, 1012, 102, 101, 1037, 4207, 16381, 1007, 2000, 2169, 8983, 1012, 102, 101, 2009, 1005, 1055, 2025, 20610, 2000, 1000, 2735, 2125, 1000, 16381, 6631, 2043, 2017, 2031, 1037, 2275, 1997, 9530, 6767, 7630, 3508, 2389, 9014, 1024, 2612, 2017, 2031, 2000, 5672, 2068, 2007, 1037, 2367, 2828, 1997, 6741, 1012, 102, 101, 1996, 21304, 2689, 2453, 2022, 2000, 5672, 1996, 9530, 6767, 7630, 3508, 2389, 9014, 2007, 1037, 3929, 4198, 6741, 1010, 1041, 1012, 1043, 1012, 102, 101, 2011, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 2128, 7630, 1035, 6741, 1006, 1007, 1006, 2004, 1999, 1996, 1005, 2334, 2509, 1005, 1998, 1005, 2334, 2549, 1005, 9014, 1007, 1010, 2029, 16058, 10438, 1037, 8185, 24856, 1998, 9319, 3584, 11709, 2005, 2169, 7953, 11265, 21017, 1012, 102, 101, 1050, 1012, 1038, 1012, 102, 101, 3929, 4198, 9014, 2024, 2411, 2058, 1011, 16381, 3550, 2005, 4432, 8518, 1010, 1998, 1037, 2062, 6413, 2690, 2598, 2052, 2022, 2000, 2224, 1037, 1000, 2334, 28667, 22048, 2492, 1000, 1010, 2029, 1006, 21858, 1007, 9319, 3584, 11709, 2005, 2169, 7953, 1006, 2004, 1999, 1037, 3929, 4198, 6741, 1007, 1010, 2021, 2069, 13585, 5300, 2013, 1000, 3518, 1000, 20407, 2000, 3965, 2019, 6434, 1006, 2004, 1999, 1037, 9530, 6767, 7630, 3508, 1007, 1012, 102, 101, 6854, 1010, 23435, 12314, 2987, 1005, 1056, 2664, 5383, 2019, 7375, 1997, 2334, 28667, 22048, 4249, 1010, 2021, 5815, 2490, 2005, 2068, 2052, 2022, 1037, 6179, 2622, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 36, 79, 89, 129, 155, 203, 209, 284, 314], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41352964", "vertexSet": [[{"sent_id": 6, "name": "tf.variable", "pos": [168, 172]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [5, 10]}], [{"sent_id": 8, "name": "tf.add", "pos": [274, 278]}], [{"sent_id": 0, "name": "tf.session", "pos": [38, 42]}, {"sent_id": 5, "name": "tf.session", "pos": [132, 136]}, {"sent_id": 10, "name": "tf.session", "pos": [363, 367]}]], "sents": ["TensorFlow's tf.placeholder() tensors do not require you to specify a shape, in order to allow you to feed tensors of different shapes in a later tf.Session.run() call.", "By default, a placeholder has a completely unconstrained shape, but you can constrain it by passing the optional shape argument.", "For example:", "<code>Code Snippet</code>.", "When you create a placeholder, TensorFlow does not allocate any memory.", "Instead, when you feed the placeholder, in the call to tf.Session.run(), TensorFlow will allocate appropriately sized memory for the input (and subsequently for any necessary intermediate) tensors.", "Note that tf.Variable objects typically do require a shape when you create them, and this shape is inferred from the first argument to the initializer.", "In your program, rng.randn() (an alias for numpy.random.randn()) returns a scalar value, and so the variables W and b will have scalar shape.", "Although the placeholders in your code (X and Y) have unconstrained shape, some of the operators, such as tf.add() and tf.mul(), have additional requirements about the shape of their arguments (viz.", "that they are compatible with the NumPy broadcasting rules).", "Since TensorFlow doesn't know when you build the graph what the actual shapes of those tensors will be, it trusts that the user knows what they are doing, and performs the check dynamically (during the call to tf.Session.run()).", "If instead you constrain the shapes of the placeholders, you enable TensorFlow to perform some checks earlier, and doing this can help to reduce bugs."], "sent_idxs": [101, 23435, 12314, 1005, 1055, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23435, 2015, 2079, 2025, 5478, 2017, 2000, 20648, 1037, 4338, 1010, 1999, 2344, 2000, 3499, 2017, 2000, 5438, 23435, 2015, 1997, 2367, 10466, 1999, 1037, 2101, 1056, 2546, 1012, 5219, 1012, 2448, 1006, 1007, 2655, 1012, 102, 101, 2011, 12398, 1010, 1037, 2173, 14528, 2038, 1037, 3294, 4895, 8663, 20528, 21280, 4338, 1010, 2021, 2017, 2064, 9530, 20528, 2378, 2009, 2011, 4458, 1996, 11887, 4338, 6685, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2043, 2017, 3443, 1037, 2173, 14528, 1010, 23435, 12314, 2515, 2025, 2035, 24755, 2618, 2151, 3638, 1012, 102, 101, 2612, 1010, 2043, 2017, 5438, 1996, 2173, 14528, 1010, 1999, 1996, 2655, 2000, 1056, 2546, 1012, 5219, 1012, 2448, 1006, 1007, 1010, 23435, 12314, 2097, 2035, 24755, 2618, 23263, 7451, 3638, 2005, 1996, 7953, 1006, 1998, 3525, 2005, 2151, 4072, 7783, 1007, 23435, 2015, 1012, 102, 101, 3602, 2008, 1056, 2546, 1012, 8023, 5200, 4050, 2079, 5478, 1037, 4338, 2043, 2017, 3443, 2068, 1010, 1998, 2023, 4338, 2003, 1999, 7512, 5596, 2013, 1996, 2034, 6685, 2000, 1996, 3988, 17629, 1012, 102, 101, 1999, 2115, 2565, 1010, 29300, 2290, 1012, 14566, 2078, 1006, 1007, 1006, 2019, 14593, 2005, 16371, 8737, 2100, 1012, 6721, 1012, 14566, 2078, 1006, 1007, 1007, 5651, 1037, 26743, 2099, 3643, 1010, 1998, 2061, 1996, 10857, 1059, 1998, 1038, 2097, 2031, 26743, 2099, 4338, 1012, 102, 101, 2348, 1996, 2173, 17794, 1999, 2115, 3642, 1006, 1060, 1998, 1061, 1007, 2031, 4895, 8663, 20528, 21280, 4338, 1010, 2070, 1997, 1996, 9224, 1010, 2107, 2004, 1056, 2546, 1012, 5587, 1006, 1007, 1998, 1056, 2546, 1012, 14163, 2140, 1006, 1007, 1010, 2031, 3176, 5918, 2055, 1996, 4338, 1997, 2037, 9918, 1006, 26619, 1012, 102, 101, 2008, 2027, 2024, 11892, 2007, 1996, 16371, 8737, 2100, 5062, 3513, 1007, 1012, 102, 101, 2144, 23435, 12314, 2987, 1005, 1056, 2113, 2043, 2017, 3857, 1996, 10629, 2054, 1996, 5025, 10466, 1997, 2216, 23435, 2015, 2097, 2022, 1010, 2009, 20278, 2008, 1996, 5310, 4282, 2054, 2027, 2024, 2725, 1010, 1998, 10438, 1996, 4638, 8790, 3973, 1006, 2076, 1996, 2655, 2000, 1056, 2546, 1012, 5219, 1012, 2448, 1006, 1007, 1007, 1012, 102, 101, 2065, 2612, 2017, 9530, 20528, 2378, 1996, 10466, 1997, 1996, 2173, 17794, 1010, 2017, 9585, 23435, 12314, 2000, 4685, 2070, 14148, 3041, 1010, 1998, 2725, 2023, 2064, 2393, 2000, 5547, 12883, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 6]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 6]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 49, 80, 85, 99, 118, 165, 200, 247, 302, 317, 374, 408], "sent_pos": [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36853403", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.max_pool", "pos": [1, 10]}, {"sent_id": 4, "name": "tf.nn.max_pool", "pos": [109, 118]}], [{"sent_id": 2, "name": "tf.reduce_max", "pos": [47, 53]}]], "sents": ["tf.nn.max_pool does not support pooling over the depth dimension which is why you get an error.", "You can use a max reduction instead to achieve what you're looking for:", "tf.reduce_max(input_tensor, reduction_indices=[3], keep_dims=True)", "The keep_dims parameter above ensures that the rank of the tensor is preserved.", "This ensures that the behavior of the max reduction will be consistent with what the tf.nn.max_pool operation would do if it supported pooling over the depth dimension."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 4098, 1035, 4770, 2515, 2025, 2490, 4770, 2075, 2058, 1996, 5995, 9812, 2029, 2003, 2339, 2017, 2131, 2019, 7561, 1012, 102, 101, 2017, 2064, 2224, 1037, 4098, 7312, 2612, 2000, 6162, 2054, 2017, 1005, 2128, 2559, 2005, 1024, 102, 101, 1056, 2546, 1012, 5547, 1035, 4098, 1006, 7953, 1035, 23435, 1010, 7312, 1035, 29299, 1027, 1031, 1017, 1033, 1010, 2562, 1035, 11737, 2015, 1027, 2995, 1007, 102, 101, 1996, 2562, 1035, 11737, 2015, 16381, 2682, 21312, 2008, 1996, 4635, 1997, 1996, 23435, 2003, 6560, 1012, 102, 101, 2023, 21312, 2008, 1996, 5248, 1997, 1996, 4098, 7312, 2097, 2022, 8335, 2007, 2054, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 4098, 1035, 4770, 3169, 2052, 2079, 2065, 2009, 3569, 4770, 2075, 2058, 1996, 5995, 9812, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 28, 46, 74, 93, 132], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50359422", "vertexSet": [[{"sent_id": 10, "name": "tf.map_fn", "pos": [242, 249]}, {"sent_id": 12, "name": "tf.map_fn", "pos": [316, 323]}], [{"sent_id": 9, "name": "tf.py_func", "pos": [208, 216]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [21, 27]}], [{"sent_id": 0, "name": "tf.contrib.integrate", "pos": [21, 29]}], [{"sent_id": 0, "name": "tf.contrib.integrate.odeint_fixed", "pos": [21, 34]}]], "sents": ["If you have at least TensorFlow 1.8.0, you're probably best off using tf.contrib.integrate.odeint_fixed() like this code (tested):", "<code>Code Snippet</code>.", "will output:", "[ 0.", "0.33333334  2.6666667   9.", "21.333334  ]", "properly integrating x2 over the intervals of [\u00a00,\u00a00\u00a0], [\u00a00,\u00a01\u00a0], [\u00a00,\u00a02\u00a0], [\u00a00,\u00a03\u00a0], and [\u00a00,\u00a04\u00a0] as per x = [ 0, 1, 2, 3, 4 ] above.", "(The primitive function of x2 is \u2153\u00a0x3, so for example 43 / 3 = 64/3 = 21 \u2153.)", "Otherwise, for earlier TensorFlow versions, here's how to fix your code.", "So the main issue is that you have to use tf.py_func() to map a Python function (scipy.integrate.quad() in this case) on a tensor.", "tf.map_fn() will map other TensorFlow operations and passes and expects tensors as operands.", "Therefore x[ 0 ] will never be a simple float, it will be a scalar tensor and scipy.integrate.quad() will not know what to do with that.", "You can't completely get rid of tf.map_fn() either, unless you want to manually loop over numpy arrays.", "Furthermore, scipy.integrate.quad() returns a double (float64), whereas your tensors are float32.", "I've simplified your code a lot, because I don't have access to the rest of it and it looks too complicated compared to the core of this question.", "The following code (tested):", "<code>Code Snippet</code>.", "will also output:", "[ 0.33333333  2.66666667  9.", "21.33333333]"], "sent_idxs": [101, 2065, 2017, 2031, 2012, 2560, 23435, 12314, 1015, 1012, 1022, 1012, 1014, 1010, 2017, 1005, 2128, 2763, 2190, 2125, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 17409, 1012, 24040, 18447, 1035, 4964, 1006, 1007, 2066, 2023, 3642, 1006, 7718, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2097, 6434, 1024, 102, 101, 1031, 1014, 1012, 102, 101, 1014, 1012, 21211, 22394, 22394, 2549, 1016, 1012, 5764, 28756, 28756, 2581, 1023, 1012, 102, 101, 2538, 1012, 21211, 22394, 2549, 1033, 102, 101, 7919, 22380, 1060, 2475, 2058, 1996, 14025, 1997, 1031, 1014, 1010, 1014, 1033, 1010, 1031, 1014, 1010, 1015, 1033, 1010, 1031, 1014, 1010, 1016, 1033, 1010, 1031, 1014, 1010, 1017, 1033, 1010, 1998, 1031, 1014, 1010, 1018, 1033, 2004, 2566, 1060, 1027, 1031, 1014, 1010, 1015, 1010, 1016, 1010, 1017, 1010, 1018, 1033, 2682, 1012, 102, 101, 1006, 1996, 10968, 3853, 1997, 1060, 2475, 2003, 1581, 1060, 2509, 1010, 2061, 2005, 2742, 4724, 1013, 1017, 1027, 4185, 1013, 1017, 1027, 2538, 1581, 1012, 1007, 102, 101, 4728, 1010, 2005, 3041, 23435, 12314, 4617, 1010, 2182, 1005, 1055, 2129, 2000, 8081, 2115, 3642, 1012, 102, 101, 2061, 1996, 2364, 3277, 2003, 2008, 2017, 2031, 2000, 2224, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 1006, 1007, 2000, 4949, 1037, 18750, 3853, 1006, 16596, 7685, 1012, 17409, 1012, 17718, 1006, 1007, 1999, 2023, 2553, 1007, 2006, 1037, 23435, 1012, 102, 101, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2097, 4949, 2060, 23435, 12314, 3136, 1998, 5235, 1998, 24273, 23435, 2015, 2004, 3850, 18376, 1012, 102, 101, 3568, 1060, 1031, 1014, 1033, 2097, 2196, 2022, 1037, 3722, 14257, 1010, 2009, 2097, 2022, 1037, 26743, 2099, 23435, 1998, 16596, 7685, 1012, 17409, 1012, 17718, 1006, 1007, 2097, 2025, 2113, 2054, 2000, 2079, 2007, 2008, 1012, 102, 101, 2017, 2064, 1005, 1056, 3294, 2131, 9436, 1997, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2593, 1010, 4983, 2017, 2215, 2000, 21118, 7077, 2058, 16371, 8737, 2100, 27448, 1012, 102, 101, 7297, 1010, 16596, 7685, 1012, 17409, 1012, 17718, 1006, 1007, 5651, 1037, 3313, 1006, 14257, 21084, 1007, 1010, 6168, 2115, 23435, 2015, 2024, 14257, 16703, 1012, 102, 101, 1045, 1005, 2310, 11038, 2115, 3642, 1037, 2843, 1010, 2138, 1045, 2123, 1005, 1056, 2031, 3229, 2000, 1996, 2717, 1997, 2009, 1998, 2009, 3504, 2205, 8552, 4102, 2000, 1996, 4563, 1997, 2023, 3160, 1012, 102, 101, 1996, 2206, 3642, 1006, 7718, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2097, 2036, 6434, 1024, 102, 101, 1031, 1014, 1012, 21211, 22394, 22394, 2509, 1016, 1012, 5764, 28756, 28756, 2575, 2581, 1023, 1012, 102, 101, 2538, 1012, 21211, 22394, 22394, 2509, 1033, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [9, 10]}, {"r": "S1", "h": 0, "t": 1, "evidence": [9, 10]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 44, 58, 63, 68, 84, 92, 149, 178, 197, 241, 268, 307, 340, 368, 404, 413, 427, 433, 451, 460], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37380546", "vertexSet": [[{"sent_id": 1, "name": "tf.fill", "pos": [80, 84]}], [{"sent_id": 1, "name": "tf.constant", "pos": [52, 56]}], [{"sent_id": 5, "name": "tf.less", "pos": [127, 131]}], [{"sent_id": 0, "name": "tf.shape", "pos": [20, 24]}]], "sents": ["The easiest way to refer to the shape of a (dynamically sized) tensor is using the tf.shape(x) op, which produces at runtime a vector of integers that contain the true shape of a tensor x.", "Note that tf.constant() does not accept a dynamic shape as an argument\u2014for then it would not be constant!\u2014but the similar tf.fill() op does.", "Therefore you can write:", "<code>Code Snippet</code>.", "PS.", "Note that, if k is a scalar, the tf.less() op should broadcast the shape of k to match y, and the following should work:", "<code>Code Snippet</code>.", "...but it's not clear why that's not working for you."], "sent_idxs": [101, 1996, 25551, 2126, 2000, 6523, 2000, 1996, 4338, 1997, 1037, 1006, 8790, 3973, 7451, 1007, 23435, 2003, 2478, 1996, 1056, 2546, 1012, 4338, 1006, 1060, 1007, 6728, 1010, 2029, 7137, 2012, 2448, 7292, 1037, 9207, 1997, 24028, 2008, 5383, 1996, 2995, 4338, 1997, 1037, 23435, 1060, 1012, 102, 101, 3602, 2008, 1056, 2546, 1012, 5377, 1006, 1007, 2515, 2025, 5138, 1037, 8790, 4338, 2004, 2019, 6685, 1517, 2005, 2059, 2009, 2052, 2025, 2022, 5377, 999, 1517, 2021, 1996, 2714, 1056, 2546, 1012, 6039, 1006, 1007, 6728, 2515, 1012, 102, 101, 3568, 2017, 2064, 4339, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 8827, 1012, 102, 101, 3602, 2008, 1010, 2065, 1047, 2003, 1037, 26743, 2099, 1010, 1996, 1056, 2546, 1012, 2625, 1006, 1007, 6728, 2323, 3743, 1996, 4338, 1997, 1047, 2000, 2674, 1061, 1010, 1998, 1996, 2206, 2323, 2147, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2021, 2009, 1005, 1055, 2025, 3154, 2339, 2008, 1005, 1055, 2025, 2551, 2005, 2017, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 49, 90, 97, 111, 115, 151, 165, 185], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53749704", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.optimizers.adam", "pos": [5, 16]}], [{"sent_id": 0, "name": "tf.train.adamoptimizer", "pos": [23, 33]}]], "sents": ["Optimizer like tf.keras.optimizers.Adam() will be saved, and tf.train.AdamOptimizer() will not be saved on model.save().", "As the time as writing this some official tutorials on TensorFlow use tf.train.", "* optimizer while I strongly believe selecting the tf.keras.optimizers.", "* is the best way to go."], "sent_idxs": [101, 23569, 27605, 6290, 2066, 1056, 2546, 1012, 17710, 8180, 1012, 23569, 27605, 16750, 1012, 4205, 1006, 1007, 2097, 2022, 5552, 1010, 1998, 1056, 2546, 1012, 3345, 1012, 4205, 7361, 3775, 4328, 6290, 1006, 1007, 2097, 2025, 2022, 5552, 2006, 2944, 1012, 3828, 1006, 1007, 1012, 102, 101, 2004, 1996, 2051, 2004, 3015, 2023, 2070, 2880, 14924, 26340, 2006, 23435, 12314, 2224, 1056, 2546, 1012, 3345, 1012, 102, 101, 1008, 23569, 27605, 6290, 2096, 1045, 6118, 2903, 17739, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 23569, 27605, 16750, 1012, 102, 101, 1008, 2003, 1996, 2190, 2126, 2000, 2175, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 47, 68, 90, 100], "sent_pos": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61823969", "vertexSet": [[{"sent_id": 8, "name": "tf.compat.v1.nn.rnn_cell.dropoutwrapper", "pos": [195, 218]}], [{"sent_id": 8, "name": "tf.contrib.rnn.dropoutwrapper", "pos": [175, 190]}]], "sents": ["tf.contrib is basically a contribution made by the TensorFlow community, it works like below.", "Members of the community can submit code which is then distributed with the standard TensorFlow package.", "Their code is reviewed by the\nTensorFlow team and tested as part of TensorFlow's tests.", ".", "Now in tensorflow 2, Tensorflow removed contrib and now each project in contrib has one of three options for its future: move to core; move to a separate repository; or delete.", "You can check all the lists of projects falling into which category from this link.", "Coming to the alternative solution,migarting code from Tensorflow 1 to Tensorflow 2 will not happen automatically, which you have to change manually.", "You can follow the below alternatives instead.", "tf.contrib.rnn.DropoutWrapper you can change it to tf.compat.v1.nn.rnn_cell.DropoutWrapper", "For sequence to sequence, you can use TensorFlow Addons.", "The TensorFlow Addons project includes many sequence-to-sequence\ntools to let you easily build production-ready Encoder\u2013Decoders.", "For example, you can use something like below.", "<code>Code Snippet</code>.", "Same way you need to change all the methods using tf.contrib to the compatible one's.", "I hope this answeres your question."], "sent_idxs": [101, 1056, 2546, 1012, 9530, 18886, 2497, 2003, 10468, 1037, 6691, 2081, 2011, 1996, 23435, 12314, 2451, 1010, 2009, 2573, 2066, 2917, 1012, 102, 101, 2372, 1997, 1996, 2451, 2064, 12040, 3642, 2029, 2003, 2059, 5500, 2007, 1996, 3115, 23435, 12314, 7427, 1012, 102, 101, 2037, 3642, 2003, 8182, 2011, 1996, 23435, 12314, 2136, 1998, 7718, 2004, 2112, 1997, 23435, 12314, 1005, 1055, 5852, 1012, 102, 101, 1012, 102, 101, 2085, 1999, 23435, 12314, 1016, 1010, 23435, 12314, 3718, 9530, 18886, 2497, 1998, 2085, 2169, 2622, 1999, 9530, 18886, 2497, 2038, 2028, 1997, 2093, 7047, 2005, 2049, 2925, 1024, 2693, 2000, 4563, 1025, 2693, 2000, 1037, 3584, 22409, 1025, 2030, 3972, 12870, 1012, 102, 101, 2017, 2064, 4638, 2035, 1996, 7201, 1997, 3934, 4634, 2046, 2029, 4696, 2013, 2023, 4957, 1012, 102, 101, 2746, 2000, 1996, 4522, 5576, 1010, 19117, 8445, 2075, 3642, 2013, 23435, 12314, 1015, 2000, 23435, 12314, 1016, 2097, 2025, 4148, 8073, 1010, 2029, 2017, 2031, 2000, 2689, 21118, 1012, 102, 101, 2017, 2064, 3582, 1996, 2917, 15955, 2612, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 4530, 5833, 13088, 29098, 2121, 2017, 2064, 2689, 2009, 2000, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 4530, 5833, 13088, 29098, 2121, 102, 101, 2005, 5537, 2000, 5537, 1010, 2017, 2064, 2224, 23435, 12314, 5587, 5644, 1012, 102, 101, 1996, 23435, 12314, 5587, 5644, 2622, 2950, 2116, 5537, 1011, 2000, 1011, 5537, 5906, 2000, 2292, 2017, 4089, 3857, 2537, 1011, 3201, 4372, 16044, 2099, 1516, 21933, 13375, 1012, 102, 101, 2005, 2742, 1010, 2017, 2064, 2224, 2242, 2066, 2917, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2168, 2126, 2017, 2342, 2000, 2689, 2035, 1996, 4725, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 2000, 1996, 11892, 2028, 1005, 1055, 1012, 102, 101, 1045, 3246, 2023, 3437, 2229, 2115, 3160, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [6, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [6, 8]}], "na_triple": [], "sent_ends": [0, 24, 44, 66, 69, 114, 132, 164, 174, 219, 234, 265, 277, 291, 316, 326], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43536220", "vertexSet": [[{"sent_id": 1, "name": "tf.variable", "pos": [32, 36]}], [{"sent_id": 0, "name": "tf.constant", "pos": [1, 5]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [8, 13]}]], "sents": ["tf.constant() and tf.placeholder() are nodes in the graph (ops or operations).", "On the other hand tf.Variable() is a class.", "And in PEP8 python style guide:", "Class names should normally use the CapWords convention."], "sent_idxs": [101, 1056, 2546, 1012, 5377, 1006, 1007, 1998, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 2024, 14164, 1999, 1996, 10629, 1006, 23092, 2030, 3136, 1007, 1012, 102, 101, 2006, 1996, 2060, 2192, 1056, 2546, 1012, 8023, 1006, 1007, 2003, 1037, 2465, 1012, 102, 101, 1998, 1999, 27233, 2620, 18750, 2806, 5009, 1024, 102, 101, 2465, 3415, 2323, 5373, 2224, 1996, 6178, 22104, 4680, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 27, 43, 53, 65], "sent_pos": [0, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36088396", "vertexSet": [[{"sent_id": 3, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [107, 124]}], [{"sent_id": 6, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [182, 201]}], [{"sent_id": 8, "name": "tf.argmax", "pos": [241, 247]}], [{"sent_id": 0, "name": "tf.nn.in_top_k", "pos": [5, 16]}, {"sent_id": 5, "name": "tf.nn.in_top_k", "pos": [154, 165]}]], "sents": ["The targets argument to tf.nn.in_top_k(predictions, targets, k) must be a vector of class IDs (i.e.", "indices of columns in the predictions matrix).", "This means that it only works for single-class classification problems.", "If your problem is a single-class problem, then I assume that your y_ tensor is a one-hot encoding of the true labels for your examples (for example because you also pass them to an op like tf.nn.softmax_cross_entropy_with_logits().", "In that case, you have two options:", "If the labels were originally stored as integer labels, pass them directly to tf.nn.in_top_k() without converting them to one-hot.", "(Also, consider using tf.nn.sparse_softmax_cross_entropy_with_logits() as your loss function, because it may be more efficient.", ").", "If the labels were originally stored in the one-hot format, you can convert them to integers using tf.argmax():", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 7889, 6685, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 1999, 1035, 2327, 1035, 1047, 1006, 20932, 1010, 7889, 1010, 1047, 1007, 2442, 2022, 1037, 9207, 1997, 2465, 8909, 2015, 1006, 1045, 1012, 1041, 1012, 102, 101, 29299, 1997, 7753, 1999, 1996, 20932, 8185, 1007, 1012, 102, 101, 2023, 2965, 2008, 2009, 2069, 2573, 2005, 2309, 1011, 2465, 5579, 3471, 1012, 102, 101, 2065, 2115, 3291, 2003, 1037, 2309, 1011, 2465, 3291, 1010, 2059, 1045, 7868, 2008, 2115, 1061, 1035, 23435, 2003, 1037, 2028, 1011, 2980, 17181, 1997, 1996, 2995, 10873, 2005, 2115, 4973, 1006, 2005, 2742, 2138, 2017, 2036, 3413, 2068, 2000, 2019, 6728, 2066, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 1012, 102, 101, 1999, 2008, 2553, 1010, 2017, 2031, 2048, 7047, 1024, 102, 101, 2065, 1996, 10873, 2020, 2761, 8250, 2004, 16109, 10873, 1010, 3413, 2068, 3495, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 1999, 1035, 2327, 1035, 1047, 1006, 1007, 2302, 16401, 2068, 2000, 2028, 1011, 2980, 1012, 102, 101, 1006, 2036, 1010, 5136, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 2004, 2115, 3279, 3853, 1010, 2138, 2009, 2089, 2022, 2062, 8114, 1012, 102, 101, 1007, 1012, 102, 101, 2065, 1996, 10873, 2020, 2761, 8250, 1999, 1996, 2028, 1011, 2980, 4289, 1010, 2017, 2064, 10463, 2068, 2000, 24028, 2478, 1056, 2546, 1012, 12098, 21693, 8528, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 6]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 37, 48, 63, 128, 139, 176, 216, 220, 251, 265], "sent_pos": [0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46686881", "vertexSet": [[{"sent_id": 6, "name": "tf.contrib.layers.softmax", "pos": [119, 130]}], [{"sent_id": 1, "name": "tf.nn.softmax", "pos": [39, 47]}, {"sent_id": 7, "name": "tf.nn.softmax", "pos": [166, 174]}]], "sents": ["You aren't defining your logits for the size 10 softmax layer in your code, and you would have to do that explicitly.", "Once that was done, you could use tf.nn.softmax, applying it separately to both of your logit tensors.", "For example, for your 20-class softmax tensor:", "<code>Code Snippet</code>.", "For the other layer, you could do:", "<code>Code Snippet</code>.", "There is also a tf.contrib.layers.softmax which allows you to apply the softmax on the final axis of a tensor with greater than 2 dimensions, but it doesn't look like you need anything like that.", "tf.nn.softmax should work here.", "Side note: output_layer is not the greatest name for that list - should be something involving weights.", "These weights and biases (output_layer, output_bias) also do not represent the output layer of your network (as that will come from whatever you do to your softmax outputs, right?).", "[Sorry, couldn't help myself.]"], "sent_idxs": [101, 2017, 4995, 1005, 1056, 12854, 2115, 8833, 12762, 2005, 1996, 2946, 2184, 3730, 17848, 6741, 1999, 2115, 3642, 1010, 1998, 2017, 2052, 2031, 2000, 2079, 2008, 12045, 1012, 102, 101, 2320, 2008, 2001, 2589, 1010, 2017, 2071, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1010, 11243, 2009, 10329, 2000, 2119, 1997, 2115, 8833, 4183, 23435, 2015, 1012, 102, 101, 2005, 2742, 1010, 2005, 2115, 2322, 1011, 2465, 3730, 17848, 23435, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 1996, 2060, 6741, 1010, 2017, 2071, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2045, 2003, 2036, 1037, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 3730, 17848, 2029, 4473, 2017, 2000, 6611, 1996, 3730, 17848, 2006, 1996, 2345, 8123, 1997, 1037, 23435, 2007, 3618, 2084, 1016, 9646, 1010, 2021, 2009, 2987, 1005, 1056, 2298, 2066, 2017, 2342, 2505, 2066, 2008, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 2323, 2147, 2182, 1012, 102, 101, 2217, 3602, 1024, 6434, 1035, 6741, 2003, 2025, 1996, 4602, 2171, 2005, 2008, 2862, 1011, 2323, 2022, 2242, 5994, 15871, 1012, 102, 101, 2122, 15871, 1998, 13827, 2229, 1006, 6434, 1035, 6741, 1010, 6434, 1035, 13827, 1007, 2036, 2079, 2025, 5050, 1996, 6434, 6741, 1997, 2115, 2897, 1006, 2004, 2008, 2097, 2272, 2013, 3649, 2017, 2079, 2000, 2115, 3730, 17848, 27852, 1010, 2157, 1029, 1007, 1012, 102, 101, 1031, 3374, 1010, 2481, 1005, 1056, 2393, 2870, 1012, 1033, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 6, 7]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 6, 7]}], "na_triple": [], "sent_ends": [0, 30, 61, 75, 89, 100, 114, 165, 179, 202, 247, 259], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49813762", "vertexSet": [[{"sent_id": 7, "name": "tf.nn.weighted_cross_entropy_with_logits", "pos": [267, 283]}], [{"sent_id": 2, "name": "tf.losses.sigmoid_cross_entropy", "pos": [112, 124]}]], "sents": ["If you're scaling the loss with a scalar, like 2.0, then basically you're multiplying the loss and therefore the gradient for backpropagation.", "It's similar to increasing the learning rate, but not exactly the same, because you're also changing the ratio to regularization losses such as weight decay.", "If your classes are heavily skewed, and you want to balance it at the calculation of loss, then you have to specify a tensor as weight, as described in the manual for tf.losses.sigmoid_cross_entropy():", "weights: Optional Tensor whose rank is either 0, or the same rank as labels, and must be broadcastable to labels (i.e., all dimensions must be either 1, or the same as the corresponding losses dimension).", "That is make the weights tensor 1.0 for class 0, and maybe 10 for class 1, and now \"false negative\" losses will be much more heavily counted.", "It is an art how much you should over-weigh the underrepresented class.", "If you overdo it, the model will collapse and will predict the over-weighted class all the time.", "An alternative to achieve the same thing is using tf.nn.weighted_cross_entropy_with_logits(), which has a pos_weight argument for the exact same purpose.", "But it's in tf.nn not tf.losses so you have to manually add it to the losses collection.", "Generally another method to handle this is to arbitrarily increase the proportion of the underrepresented class at sampling.", "That should not be overdone either, however.", "You can do both of these things too."], "sent_idxs": [101, 2065, 2017, 1005, 2128, 25169, 1996, 3279, 2007, 1037, 26743, 2099, 1010, 2066, 1016, 1012, 1014, 1010, 2059, 10468, 2017, 1005, 2128, 4800, 22086, 2075, 1996, 3279, 1998, 3568, 1996, 17978, 2005, 2067, 21572, 4502, 12540, 1012, 102, 101, 2009, 1005, 1055, 2714, 2000, 4852, 1996, 4083, 3446, 1010, 2021, 2025, 3599, 1996, 2168, 1010, 2138, 2017, 1005, 2128, 2036, 5278, 1996, 6463, 2000, 3180, 3989, 6409, 2107, 2004, 3635, 13121, 1012, 102, 101, 2065, 2115, 4280, 2024, 4600, 15315, 7974, 2098, 1010, 1998, 2017, 2215, 2000, 5703, 2009, 2012, 1996, 17208, 1997, 3279, 1010, 2059, 2017, 2031, 2000, 20648, 1037, 23435, 2004, 3635, 1010, 2004, 2649, 1999, 1996, 6410, 2005, 1056, 2546, 1012, 6409, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1006, 1007, 1024, 102, 101, 15871, 1024, 11887, 23435, 3005, 4635, 2003, 2593, 1014, 1010, 2030, 1996, 2168, 4635, 2004, 10873, 1010, 1998, 2442, 2022, 3743, 3085, 2000, 10873, 1006, 1045, 1012, 1041, 1012, 1010, 2035, 9646, 2442, 2022, 2593, 1015, 1010, 2030, 1996, 2168, 2004, 1996, 7978, 6409, 9812, 1007, 1012, 102, 101, 2008, 2003, 2191, 1996, 15871, 23435, 1015, 1012, 1014, 2005, 2465, 1014, 1010, 1998, 2672, 2184, 2005, 2465, 1015, 1010, 1998, 2085, 1000, 6270, 4997, 1000, 6409, 2097, 2022, 2172, 2062, 4600, 8897, 1012, 102, 101, 2009, 2003, 2019, 2396, 2129, 2172, 2017, 2323, 2058, 1011, 17042, 1996, 2104, 2890, 28994, 14088, 2465, 1012, 102, 101, 2065, 2017, 2058, 3527, 2009, 1010, 1996, 2944, 2097, 7859, 1998, 2097, 16014, 1996, 2058, 1011, 18215, 2465, 2035, 1996, 2051, 1012, 102, 101, 2019, 4522, 2000, 6162, 1996, 2168, 2518, 2003, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 18215, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 1010, 2029, 2038, 1037, 13433, 2015, 1035, 3635, 6685, 2005, 1996, 6635, 2168, 3800, 1012, 102, 101, 2021, 2009, 1005, 1055, 1999, 1056, 2546, 1012, 1050, 2078, 2025, 1056, 2546, 1012, 6409, 2061, 2017, 2031, 2000, 21118, 5587, 2009, 2000, 1996, 6409, 3074, 1012, 102, 101, 3227, 2178, 4118, 2000, 5047, 2023, 2003, 2000, 12098, 16313, 19848, 6588, 3623, 1996, 10817, 1997, 1996, 2104, 2890, 28994, 14088, 2465, 2012, 16227, 1012, 102, 101, 2008, 2323, 2025, 2022, 2058, 5280, 2063, 2593, 1010, 2174, 1012, 102, 101, 2017, 2064, 2079, 2119, 1997, 2122, 2477, 2205, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2, 4, 7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2, 4, 7, 8]}], "na_triple": [], "sent_ends": [0, 39, 74, 128, 177, 213, 233, 257, 301, 330, 357, 370, 381], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50161581", "vertexSet": [[{"sent_id": 4, "name": "tf.contrib.eager.py_func", "pos": [131, 145]}], [{"sent_id": 3, "name": "tf.contrib.eager.defun", "pos": [85, 96]}], [{"sent_id": 5, "name": "tf.enable_eager_execution", "pos": [183, 191]}], [{"sent_id": 0, "name": "tf.contrib.eager.run_test_in_graph_and_eager_modes", "pos": [31, 53]}]], "sents": ["With the caveat that anything in the tf.contrib namespace is subject to change between releases, you can decorate your test with @tf.contrib.eager.run_test_in_graph_and_eager_modes.", "Some other projects, like TensorFlow Probability seem to use this.", "For non-tests, some things to look into are:", "tf.contrib.eager.defun: Is useful when you have eager execution enabled but want to \"compile\" some computation into a graph to benefit from memory and/or performance optimizations..", "tf.contrib.eager.py_func: Is useful when do not have eager execution enabled but want to execute some computation in the graph as a Python function..", "One may question the reasoning behind not allowing a call to tf.enable_eager_execution() to be undone.", "The idea is that library authors should not invoke it, only the end-user should invoke it in main().", "The reduces the chances that libraries are written incompatible ways (where say functions in one library disable eager execution and return symbolic tensors while functions in another library enable eager execution and expects concrete valued tensors.", "This would make mixing the libraries problematic).", "Hope that helps"], "sent_idxs": [101, 2007, 1996, 5430, 4017, 2008, 2505, 1999, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 3415, 15327, 2003, 3395, 2000, 2689, 2090, 7085, 1010, 2017, 2064, 29460, 2115, 3231, 2007, 1030, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 2448, 1035, 3231, 1035, 1999, 1035, 10629, 1035, 1998, 1035, 9461, 1035, 11583, 1012, 102, 101, 2070, 2060, 3934, 1010, 2066, 23435, 12314, 9723, 4025, 2000, 2224, 2023, 1012, 102, 101, 2005, 2512, 1011, 5852, 1010, 2070, 2477, 2000, 2298, 2046, 2024, 1024, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 13366, 4609, 1024, 2003, 6179, 2043, 2017, 2031, 9461, 7781, 9124, 2021, 2215, 2000, 1000, 4012, 22090, 1000, 2070, 22334, 2046, 1037, 10629, 2000, 5770, 2013, 3638, 1998, 1013, 2030, 2836, 20600, 2015, 1012, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 1052, 2100, 1035, 4569, 2278, 1024, 2003, 6179, 2043, 2079, 2025, 2031, 9461, 7781, 9124, 2021, 2215, 2000, 15389, 2070, 22334, 1999, 1996, 10629, 2004, 1037, 18750, 3853, 1012, 1012, 102, 101, 2028, 2089, 3160, 1996, 13384, 2369, 2025, 4352, 1037, 2655, 2000, 1056, 2546, 1012, 9585, 1035, 9461, 1035, 7781, 1006, 1007, 2000, 2022, 25757, 1012, 102, 101, 1996, 2801, 2003, 2008, 3075, 6048, 2323, 2025, 1999, 6767, 3489, 2009, 1010, 2069, 1996, 2203, 1011, 5310, 2323, 1999, 6767, 3489, 2009, 1999, 2364, 1006, 1007, 1012, 102, 101, 1996, 13416, 1996, 9592, 2008, 8860, 2024, 2517, 25876, 3971, 1006, 2073, 2360, 4972, 1999, 2028, 3075, 4487, 19150, 9461, 7781, 1998, 2709, 12613, 23435, 2015, 2096, 4972, 1999, 2178, 3075, 9585, 9461, 7781, 1998, 24273, 5509, 11126, 23435, 2015, 1012, 102, 101, 2023, 2052, 2191, 6809, 1996, 8860, 18636, 1007, 1012, 102, 101, 3246, 2008, 7126, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 55, 70, 84, 130, 171, 198, 228, 271, 282, 287], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38726143", "vertexSet": [[{"sent_id": 7, "name": "tf.train.string_input_producer", "pos": [145, 155]}], [{"sent_id": 3, "name": "tf.py_func", "pos": [69, 77]}, {"sent_id": 6, "name": "tf.py_func", "pos": [103, 111]}], [{"sent_id": 8, "name": "tf.train.batch", "pos": [167, 173]}]], "sents": ["According to this there are several ways to read data in TensorFlow.", "If you want to delegate shuffling and batching to the framework then you need to create an input pipeline.", "The problem is this - how do you inject lmdb data into the symbolic input pipeline.", "A possible solution is to use the tf.py_func operation.", "Here is an example:", "<code>Code Snippet</code>.", "The tf.py_func op inserts a call to regular python code inside of the TensorFlow graph, we need to specify the inputs and the number and types of the outputs.", "The tf.train.string_input_producer creates a shuffled queue with the given keys.", "The tf.train.batch op create another queue that contains batches of data.", "When training, each evaluation of batch_examples or batch_labels will dequeue another batch from that queue.", "Because we created queues we need to take care and run the QueueRunner objects before we start training.", "This is done like this (from the TensorFlow doc):\n<code>Code Snippet</code>", "Because we created queues we need to take care and run the QueueRunner objects before we start training.", "This is done like this (from the TensorFlow doc):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2429, 2000, 2023, 2045, 2024, 2195, 3971, 2000, 3191, 2951, 1999, 23435, 12314, 1012, 102, 101, 2065, 2017, 2215, 2000, 11849, 24770, 1998, 14108, 2075, 2000, 1996, 7705, 2059, 2017, 2342, 2000, 3443, 2019, 7953, 13117, 1012, 102, 101, 1996, 3291, 2003, 2023, 1011, 2129, 2079, 2017, 1999, 20614, 1048, 26876, 2497, 2951, 2046, 1996, 12613, 7953, 13117, 1012, 102, 101, 1037, 2825, 5576, 2003, 2000, 2224, 1996, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 3169, 1012, 102, 101, 2182, 2003, 2019, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 6728, 19274, 2015, 1037, 2655, 2000, 3180, 18750, 3642, 2503, 1997, 1996, 23435, 12314, 10629, 1010, 2057, 2342, 2000, 20648, 1996, 20407, 1998, 1996, 2193, 1998, 4127, 1997, 1996, 27852, 1012, 102, 101, 1996, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 9005, 1037, 18764, 24240, 2007, 1996, 2445, 6309, 1012, 102, 101, 1996, 1056, 2546, 1012, 3345, 1012, 14108, 6728, 3443, 2178, 24240, 2008, 3397, 14108, 2229, 1997, 2951, 1012, 102, 101, 2043, 2731, 1010, 2169, 9312, 1997, 14108, 1035, 4973, 2030, 14108, 1035, 10873, 2097, 2139, 4226, 5657, 2178, 14108, 2013, 2008, 24240, 1012, 102, 101, 2138, 2057, 2580, 24240, 2015, 2057, 2342, 2000, 2202, 2729, 1998, 2448, 1996, 24240, 23195, 5200, 2077, 2057, 2707, 2731, 1012, 102, 101, 2023, 2003, 2589, 2066, 2023, 1006, 2013, 1996, 23435, 12314, 9986, 1007, 1024, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 102, 101, 2138, 2057, 2580, 24240, 2015, 2057, 2342, 2000, 2202, 2729, 1998, 2448, 1996, 24240, 23195, 5200, 2077, 2057, 2707, 2731, 1012, 102, 101, 2023, 2003, 2589, 2066, 2023, 1006, 2013, 1996, 23435, 12314, 9986, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 2, "evidence": [7, 8]}, {"r": "S1", "h": 2, "t": 0, "evidence": [7, 8]}, {"r": "S1", "h": 1, "t": 0, "evidence": [6, 7]}, {"r": "S1", "h": 0, "t": 1, "evidence": [6, 7]}, {"r": "S1", "h": 1, "t": 2, "evidence": [6, 8]}, {"r": "S1", "h": 2, "t": 1, "evidence": [6, 8]}], "na_triple": [], "sent_ends": [0, 16, 39, 61, 80, 87, 101, 143, 165, 185, 210, 233, 259, 282, 297, 311], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58538279", "vertexSet": [[{"sent_id": 1, "name": "tf.keras.layers", "pos": [56, 63]}], [{"sent_id": 0, "name": "tf.layers", "pos": [30, 34]}, {"sent_id": 1, "name": "tf.layers", "pos": [51, 55]}]], "sents": ["So I faced the same error but discovered that my version of tensorflow (which \n is 2.0) moved layers from the tf package (tf.layers) to tf.keras.", "An easy fix would be to replace tf.layers with tf.keras.layers"], "sent_idxs": [101, 2061, 1045, 4320, 1996, 2168, 7561, 2021, 3603, 2008, 2026, 2544, 1997, 23435, 12314, 1006, 2029, 2003, 1016, 1012, 1014, 1007, 2333, 9014, 2013, 1996, 1056, 2546, 7427, 1006, 1056, 2546, 1012, 9014, 1007, 2000, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 2019, 3733, 8081, 2052, 2022, 2000, 5672, 1056, 2546, 1012, 9014, 2007, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 43, 64], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 0]}, {"title": "56994817", "vertexSet": [[{"sent_id": 3, "name": "tf.configproto", "pos": [42, 50]}], [{"sent_id": 3, "name": "tf.compat.v1.configproto", "pos": [51, 66]}]], "sents": ["ConfigProto disappeared in tf 2.0, so an elegant solution is:", "<code>Code Snippet</code>.", "and then replace:", "tf.ConfigProto by tf.compat.v1.ConfigProto", "In fact, the compatibility built in 2.0 to get tf 1.XX: tf.compat.v1 is really helpful.", "Useful link: \nMigrate your tensorflow 1. code to tensorflow 2.:\nhttps://www.tensorflow.org/guide/migrate"], "sent_idxs": [101, 9530, 8873, 21600, 21709, 2080, 5419, 1999, 1056, 2546, 1016, 1012, 1014, 1010, 2061, 2019, 11552, 5576, 2003, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2059, 5672, 1024, 102, 101, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 2011, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 9530, 8873, 21600, 21709, 2080, 102, 101, 1999, 2755, 1010, 1996, 21778, 2328, 1999, 1016, 1012, 1014, 2000, 2131, 1056, 2546, 1015, 1012, 22038, 1024, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 2003, 2428, 14044, 1012, 102, 101, 6179, 4957, 1024, 22806, 2115, 23435, 12314, 1015, 1012, 3642, 2000, 23435, 12314, 1016, 1012, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 22806, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 3]}], "na_triple": [], "sent_ends": [0, 21, 35, 41, 67, 100, 132], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59390988", "vertexSet": [[{"sent_id": 4, "name": "tf.compat.v1.session", "pos": [64, 75]}], [{"sent_id": 4, "name": "tf.session", "pos": [78, 82]}], [{"sent_id": 5, "name": "tf.contrib", "pos": [92, 98]}], [{"sent_id": 5, "name": "tf.distribute", "pos": [105, 109]}], [{"sent_id": 5, "name": "tf.contrib.cluster_resolver", "pos": [92, 103]}], [{"sent_id": 5, "name": "tf.distribute.cluster_resolver", "pos": [105, 114]}]], "sents": ["Firstly the code given in the tutorial is not 2.x compatible", "You need to choose runtime as TPU in colab to execute code in TPU.", "For the error", "AttributeError: module 'tensorflow' has no attribute 'Session'", "you need to use tf.compat.v1.Session() as tf.session is deprecated.", "In place of tf.contrib.cluster_resolver please use tf.distribute.cluster_resolver.", "Please refer Tensorflow Addon-repo to convert code from 1.x to 2.x compatible."], "sent_idxs": [101, 15847, 1996, 3642, 2445, 1999, 1996, 14924, 4818, 2003, 2025, 1016, 1012, 1060, 11892, 102, 101, 2017, 2342, 2000, 5454, 2448, 7292, 2004, 1056, 14289, 1999, 15270, 2497, 2000, 15389, 3642, 1999, 1056, 14289, 1012, 102, 101, 2005, 1996, 7561, 102, 101, 17961, 2121, 29165, 1024, 11336, 1005, 23435, 12314, 1005, 2038, 2053, 17961, 1005, 5219, 1005, 102, 101, 2017, 2342, 2000, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 5219, 1006, 1007, 2004, 1056, 2546, 1012, 5219, 2003, 2139, 28139, 12921, 1012, 102, 101, 1999, 2173, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9324, 1035, 10663, 2099, 3531, 2224, 1056, 2546, 1012, 16062, 1012, 9324, 1035, 10663, 2099, 1012, 102, 101, 3531, 6523, 23435, 12314, 5587, 2239, 1011, 16360, 2080, 2000, 10463, 3642, 2013, 1015, 1012, 1060, 2000, 1016, 1012, 1060, 11892, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 16, 37, 42, 59, 88, 116, 140], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41489484", "vertexSet": [[{"sent_id": 3, "name": "tf.global_variables_initializer", "pos": [39, 48]}], [{"sent_id": 1, "name": "tf.initialize_all_variables", "pos": [11, 20]}], [{"sent_id": 5, "name": "tf.train", "pos": [70, 74]}], [{"sent_id": 5, "name": "tf.train.string_input_producer", "pos": [70, 80]}], [{"sent_id": 8, "name": "tf.local_variables_initializer", "pos": [134, 143]}]], "sents": ["In the newer version of TensorFlow:", "tf.initialize_all_variables() is deprecated.", "They mention that you have to use:", "tf.global_variables_initializer()", "This doesn't solve the problem.", "If we look at the newer API of tf.train.string_input_producer(), it mentions that num_epochs will be created as a local variable.", "What is happening here is that there is nothing in the queue for it to read, hence it says requested 1 current 0.", "Just add this:", "tf.local_variables_initializer()", "I have pushed the update."], "sent_idxs": [101, 1999, 1996, 10947, 2544, 1997, 23435, 12314, 1024, 102, 101, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 2003, 2139, 28139, 12921, 1012, 102, 101, 2027, 5254, 2008, 2017, 2031, 2000, 2224, 1024, 102, 101, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 102, 101, 2023, 2987, 1005, 1056, 9611, 1996, 3291, 1012, 102, 101, 2065, 2057, 2298, 2012, 1996, 10947, 17928, 1997, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 1010, 2009, 9704, 2008, 16371, 2213, 1035, 25492, 2015, 2097, 2022, 2580, 2004, 1037, 2334, 8023, 1012, 102, 101, 2054, 2003, 6230, 2182, 2003, 2008, 2045, 2003, 2498, 1999, 1996, 24240, 2005, 2009, 2000, 3191, 1010, 6516, 2009, 2758, 7303, 1015, 2783, 1014, 1012, 102, 101, 2074, 5587, 2023, 1024, 102, 101, 1056, 2546, 1012, 2334, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 102, 101, 1045, 2031, 3724, 1996, 10651, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 10, 28, 38, 51, 61, 100, 127, 133, 146, 154], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36745793", "vertexSet": [[{"sent_id": 1, "name": "tf.gradients", "pos": [49, 54]}, {"sent_id": 2, "name": "tf.gradients", "pos": [113, 118]}], [{"sent_id": 3, "name": "tf.while_loop", "pos": [148, 154]}]], "sents": ["All backpropagation in TensorFlow is implemented by automatically differentiating the operations in the forward pass of the network, and adding explicit operations for computing the gradient at each point in the network.", "The general implementation can be found in tf.gradients(), but the particular version used depends on how your LSTM is implemented:", "If the LSTM is implemented as an unrolled loop for a finite number of timesteps, the usual approach is truncated backpropagation through time, which uses the algorithm in tf.gradients() to build an unrolled backpropagation loop in the opposite direction..", "If the LSTM is implemented as a tf.while_loop(), it uses additional support for differentiating loops in control_flow_grad.py.."], "sent_idxs": [101, 2035, 2067, 21572, 4502, 12540, 1999, 23435, 12314, 2003, 7528, 2011, 8073, 2367, 15370, 1996, 3136, 1999, 1996, 2830, 3413, 1997, 1996, 2897, 1010, 1998, 5815, 13216, 3136, 2005, 9798, 1996, 17978, 2012, 2169, 2391, 1999, 1996, 2897, 1012, 102, 101, 1996, 2236, 7375, 2064, 2022, 2179, 1999, 1056, 2546, 1012, 17978, 2015, 1006, 1007, 1010, 2021, 1996, 3327, 2544, 2109, 9041, 2006, 2129, 2115, 1048, 3367, 2213, 2003, 7528, 1024, 102, 101, 2065, 1996, 1048, 3367, 2213, 2003, 7528, 2004, 2019, 4895, 28402, 2098, 7077, 2005, 1037, 10713, 2193, 1997, 2335, 2618, 4523, 1010, 1996, 5156, 3921, 2003, 25449, 2067, 21572, 4502, 12540, 2083, 2051, 1010, 2029, 3594, 1996, 9896, 1999, 1056, 2546, 1012, 17978, 2015, 1006, 1007, 2000, 3857, 2019, 4895, 28402, 2098, 2067, 21572, 4502, 12540, 7077, 1999, 1996, 4500, 3257, 1012, 1012, 102, 101, 2065, 1996, 1048, 3367, 2213, 2003, 7528, 2004, 1037, 1056, 2546, 1012, 2096, 1035, 7077, 1006, 1007, 1010, 2009, 3594, 3176, 2490, 2005, 2367, 15370, 15932, 1999, 2491, 1035, 4834, 1035, 24665, 4215, 1012, 1052, 2100, 1012, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 41, 73, 138, 178], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48537309", "vertexSet": [[{"sent_id": 3, "name": "tf.nn.conv2d", "pos": [82, 92]}], [{"sent_id": 4, "name": "tf.layers.conv2d", "pos": [125, 134]}]], "sents": ["Stride is the amount you want to skip in a particular direction.", "Each of your batch is 4 dimensional (batch_size, height, width, channels).", "But, you know that the computation should not skip any batch and nor should it skip any channel, but what a GPU sees is just a 4D tensor, and hence asks for the stride along each dimension.", "tf.nn.conv2d is a low-level implementation in Tensorflow, which exposes the GPU API as it is.", "There is another high-level implementation as well, tf.layers.Conv2d which only allows you to pass a two element tuple, with height stride and width stride.", "But, if you want to use the low-level API (maybe due to more control over the parameters), you should always keep batch and column stride to 1."], "sent_idxs": [101, 18045, 2003, 1996, 3815, 2017, 2215, 2000, 13558, 1999, 1037, 3327, 3257, 1012, 102, 101, 2169, 1997, 2115, 14108, 2003, 1018, 8789, 1006, 14108, 1035, 2946, 1010, 4578, 1010, 9381, 1010, 6833, 1007, 1012, 102, 101, 2021, 1010, 2017, 2113, 2008, 1996, 22334, 2323, 2025, 13558, 2151, 14108, 1998, 4496, 2323, 2009, 13558, 2151, 3149, 1010, 2021, 2054, 1037, 14246, 2226, 5927, 2003, 2074, 1037, 1018, 2094, 23435, 1010, 1998, 6516, 5176, 2005, 1996, 18045, 2247, 2169, 9812, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 2003, 1037, 2659, 1011, 2504, 7375, 1999, 23435, 12314, 1010, 2029, 14451, 2015, 1996, 14246, 2226, 17928, 2004, 2009, 2003, 1012, 102, 101, 2045, 2003, 2178, 2152, 1011, 2504, 7375, 2004, 2092, 1010, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2029, 2069, 4473, 2017, 2000, 3413, 1037, 2048, 5783, 10722, 10814, 1010, 2007, 4578, 18045, 1998, 9381, 18045, 1012, 102, 101, 2021, 1010, 2065, 2017, 2215, 2000, 2224, 1996, 2659, 1011, 2504, 17928, 1006, 2672, 2349, 2000, 2062, 2491, 2058, 1996, 11709, 1007, 1010, 2017, 2323, 2467, 2562, 14108, 1998, 5930, 18045, 2000, 1015, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 4, 5]}], "na_triple": [], "sent_ends": [0, 15, 36, 81, 114, 154, 190], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57227547", "vertexSet": [[{"sent_id": 4, "name": "tf.contrib.distribute", "pos": [113, 121]}], [{"sent_id": 6, "name": "tf.distribute", "pos": [148, 152]}]], "sents": ["There are some binaries for NCCL on Windows, but they can be quite annoying to deal with.", "As an alternative, Tensorflow gives you three other options in MirroredStrategy that are compatible with Windows natively.", "They are Hierarchical Copy, Reduce to First GPU, and Reduce to CPU.", "What you are most likely looking for is Hierarchical Copy, but you can test each of them to see what gives you the best result.", "If you are using tensorflow versions older than 2.0, you will use tf.contrib.distribute:", "<code>Code Snippet</code>.", "After 2.0, you only need to use tf.distribute!", "Here is an example setting up an Xception model with 2 GPUs:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2024, 2070, 8026, 12086, 2005, 13316, 20464, 2006, 3645, 1010, 2021, 2027, 2064, 2022, 3243, 15703, 2000, 3066, 2007, 1012, 102, 101, 2004, 2019, 4522, 1010, 23435, 12314, 3957, 2017, 2093, 2060, 7047, 1999, 22243, 20528, 2618, 6292, 2008, 2024, 11892, 2007, 3645, 3128, 2135, 1012, 102, 101, 2027, 2024, 25835, 6100, 1010, 5547, 2000, 2034, 14246, 2226, 1010, 1998, 5547, 2000, 17368, 1012, 102, 101, 2054, 2017, 2024, 2087, 3497, 2559, 2005, 2003, 25835, 6100, 1010, 2021, 2017, 2064, 3231, 2169, 1997, 2068, 2000, 2156, 2054, 3957, 2017, 1996, 2190, 2765, 1012, 102, 101, 2065, 2017, 2024, 2478, 23435, 12314, 4617, 3080, 2084, 1016, 1012, 1014, 1010, 2017, 2097, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 16062, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2044, 1016, 1012, 1014, 1010, 2017, 2069, 2342, 2000, 2224, 1056, 2546, 1012, 16062, 999, 102, 101, 2182, 2003, 2019, 2742, 4292, 2039, 2019, 1060, 24422, 2944, 2007, 1016, 14246, 2271, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [4, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [4, 6]}], "na_triple": [], "sent_ends": [0, 23, 49, 67, 96, 123, 137, 154, 171, 185], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58428014", "vertexSet": [[{"sent_id": 4, "name": "tf.compat.v1.reset_default_graph", "pos": [142, 157]}], [{"sent_id": 3, "name": "tf.reset_default_graph", "pos": [111, 119]}]], "sents": ["When using Tensorflow (1.X) as a backend, whenever you add a new layer to any model, the name of the layer -unless manually set- will be set to the default name for that layer, plus an incremental index at the end.", "Defining a new model is not enough to reset the incrementing index, because all models end up on the same underlying graph.", "To reset the index, you must reset the underlying graph.", "In TF 1.X, this is done via tf.reset_default_graph().", "In TF 2.0, you can do this via the v1 compatibility API: tf.compat.v1.reset_default_graph() (the latter will also solve some deprecation warnings you might get with the latest versions of TF 1.X)"], "sent_idxs": [101, 2043, 2478, 23435, 12314, 1006, 1015, 1012, 1060, 1007, 2004, 1037, 2067, 10497, 1010, 7188, 2017, 5587, 1037, 2047, 6741, 2000, 2151, 2944, 1010, 1996, 2171, 1997, 1996, 6741, 1011, 4983, 21118, 2275, 1011, 2097, 2022, 2275, 2000, 1996, 12398, 2171, 2005, 2008, 6741, 1010, 4606, 2019, 4297, 28578, 21050, 5950, 2012, 1996, 2203, 1012, 102, 101, 12854, 1037, 2047, 2944, 2003, 2025, 2438, 2000, 25141, 1996, 4297, 28578, 26951, 5950, 1010, 2138, 2035, 4275, 2203, 2039, 2006, 1996, 2168, 10318, 10629, 1012, 102, 101, 2000, 25141, 1996, 5950, 1010, 2017, 2442, 25141, 1996, 10318, 10629, 1012, 102, 101, 1999, 1056, 2546, 1015, 1012, 1060, 1010, 2023, 2003, 2589, 3081, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 102, 101, 1999, 1056, 2546, 1016, 1012, 1014, 1010, 2017, 2064, 2079, 2023, 3081, 1996, 1058, 2487, 21778, 17928, 1024, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 1006, 1996, 3732, 2097, 2036, 9611, 2070, 2139, 28139, 10719, 16234, 2017, 2453, 2131, 2007, 1996, 6745, 4617, 1997, 1056, 2546, 1015, 1012, 1060, 1007, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 57, 85, 99, 123, 185], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52021442", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [10, 15]}, {"sent_id": 3, "name": "tf.nn", "pos": [71, 76]}, {"sent_id": 5, "name": "tf.nn", "pos": [120, 125]}, {"sent_id": 7, "name": "tf.nn", "pos": [171, 176]}, {"sent_id": 9, "name": "tf.nn", "pos": [236, 241]}, {"sent_id": 10, "name": "tf.nn", "pos": [256, 261]}], [{"sent_id": 2, "name": "tf.layers", "pos": [51, 55]}], [{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [10, 20]}, {"sent_id": 3, "name": "tf.nn.conv2d", "pos": [71, 81]}, {"sent_id": 5, "name": "tf.nn.conv2d", "pos": [120, 130]}, {"sent_id": 7, "name": "tf.nn.conv2d", "pos": [171, 181]}, {"sent_id": 9, "name": "tf.nn.conv2d", "pos": [236, 246]}, {"sent_id": 10, "name": "tf.nn.conv2d", "pos": [256, 266]}], [{"sent_id": 2, "name": "tf.layers.conv2d", "pos": [51, 60]}], [{"sent_id": 7, "name": "tf.nn.conv2d_transpose", "pos": [171, 184]}, {"sent_id": 9, "name": "tf.nn.conv2d_transpose", "pos": [236, 249]}]], "sents": ["I suppose you mean you want to know how tf.nn.conv2d and sorts are implemented.", "If you're new to TensorFlow, you'll notice that there are layers (e.g.", "tf.layers.conv2d) and nn (e.g.", "tf.nn.conv2d).", "layers are all wrappers for nn, so if you just want to get to the implementation right away, ignore the layers.", "Now if you read the documentation for tf.nn.conv2d, it says:", "Defined in generated file: tensorflow/python/ops/gen_nn_ops.py.", "Just for comparison, take a look at the documentation for tf.nn.conv2d_transpose, which says:", "Defined in tensorflow/python/ops/nn_ops.py.", "Now if you click tensorflow/python/ops/nn_ops.py, it actually takes you to the file where tf.nn.conv2d_transpose is defined.", "But for tf.nn.conv2d, which you are interested in, this link does not exist.", "This is because you can write the layer in C++ and let TensorFlow generate the Python part, hence the text defined in generated file.", "The actual implementations are spread across three files here:", "tensorflow/core/kernels/conv_2d.h.", "tensorflow/core/kernels/conv_ops.cc.", "tensorflow/core/kernels/conv_ops_gpu.h."], "sent_idxs": [101, 1045, 6814, 2017, 2812, 2017, 2215, 2000, 2113, 2129, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1998, 11901, 2024, 7528, 1012, 102, 101, 2065, 2017, 1005, 2128, 2047, 2000, 23435, 12314, 1010, 2017, 1005, 2222, 5060, 2008, 2045, 2024, 9014, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1007, 1998, 1050, 2078, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1007, 1012, 102, 101, 9014, 2024, 2035, 10236, 7347, 2005, 1050, 2078, 1010, 2061, 2065, 2017, 2074, 2215, 2000, 2131, 2000, 1996, 7375, 2157, 2185, 1010, 8568, 1996, 9014, 1012, 102, 101, 2085, 2065, 2017, 3191, 1996, 12653, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1010, 2009, 2758, 1024, 102, 101, 4225, 1999, 7013, 5371, 1024, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 8991, 1035, 1050, 2078, 1035, 23092, 1012, 1052, 2100, 1012, 102, 101, 2074, 2005, 7831, 1010, 2202, 1037, 2298, 2012, 1996, 12653, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 1010, 2029, 2758, 1024, 102, 101, 4225, 1999, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 1050, 2078, 1035, 23092, 1012, 1052, 2100, 1012, 102, 101, 2085, 2065, 2017, 11562, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 1050, 2078, 1035, 23092, 1012, 1052, 2100, 1010, 2009, 2941, 3138, 2017, 2000, 1996, 5371, 2073, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2003, 4225, 1012, 102, 101, 2021, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1010, 2029, 2017, 2024, 4699, 1999, 1010, 2023, 4957, 2515, 2025, 4839, 1012, 102, 101, 2023, 2003, 2138, 2017, 2064, 4339, 1996, 6741, 1999, 1039, 1009, 1009, 1998, 2292, 23435, 12314, 9699, 1996, 18750, 2112, 1010, 6516, 1996, 3793, 4225, 1999, 7013, 5371, 1012, 102, 101, 1996, 5025, 24977, 2024, 3659, 2408, 2093, 6764, 2182, 1024, 102, 101, 23435, 12314, 1013, 4563, 1013, 16293, 2015, 1013, 9530, 2615, 1035, 14134, 1012, 1044, 1012, 102, 101, 23435, 12314, 1013, 4563, 1013, 16293, 2015, 1013, 9530, 2615, 1035, 23092, 1012, 10507, 1012, 102, 101, 23435, 12314, 1013, 4563, 1013, 16293, 2015, 1013, 9530, 2615, 1035, 23092, 1035, 14246, 2226, 1012, 1044, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 26, 50, 70, 84, 112, 135, 159, 189, 208, 253, 280, 311, 323, 340, 357, 377], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55199043", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.conv2d_transpose", "pos": [45, 58]}], [{"sent_id": 0, "name": "tf.keras.layers.conv2dtranspose", "pos": [8, 23]}]], "sents": ["TensoFlow 2.0 has tf.keras.layers.Conv2DTranspose.", "Keras is the default high level API for TF 2.0, but it still have tf.nn.conv2d_transpose for low level applications", "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2DTranspose"], "sent_idxs": [101, 15295, 11253, 8261, 1016, 1012, 1014, 2038, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2475, 11927, 5521, 13102, 9232, 1012, 102, 101, 17710, 8180, 2003, 1996, 12398, 2152, 2504, 17928, 2005, 1056, 2546, 1016, 1012, 1014, 1010, 2021, 2009, 2145, 2031, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2005, 2659, 2504, 5097, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 4617, 1013, 1054, 2475, 1012, 1014, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 17710, 8180, 1013, 9014, 1013, 9530, 2615, 2475, 11927, 5521, 13102, 9232, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 25, 63, 105], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46219309", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib.layers.l2_regularizer", "pos": [50, 64]}], [{"sent_id": 1, "name": "tf.contrib.layers.apply_regularization", "pos": [14, 27]}]], "sents": ["It really depends on what you want to achieve exactly:", "tf.contrib.layers.apply_regularization allows you to combine a regularizer and a set of tensors on which it should be applied..", "tf.contrib.layers.l2_regularizer allows you to defines the scope on which the l2 should be applied..", "But in essence a regularizer is just something to be added to the cost function, so any function (tensor) which you add to the cost function can be considered as a regularizer and will be taken into account.."], "sent_idxs": [101, 2009, 2428, 9041, 2006, 2054, 2017, 2215, 2000, 6162, 3599, 1024, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 6611, 1035, 3180, 3989, 4473, 2017, 2000, 11506, 1037, 3180, 17629, 1998, 1037, 2275, 1997, 23435, 2015, 2006, 2029, 2009, 2323, 2022, 4162, 1012, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 1048, 2475, 1035, 3180, 17629, 4473, 2017, 2000, 11859, 1996, 9531, 2006, 2029, 1996, 1048, 2475, 2323, 2022, 4162, 1012, 1012, 102, 101, 2021, 1999, 11305, 1037, 3180, 17629, 2003, 2074, 2242, 2000, 2022, 2794, 2000, 1996, 3465, 3853, 1010, 2061, 2151, 3853, 1006, 23435, 1007, 2029, 2017, 5587, 2000, 1996, 3465, 3853, 2064, 2022, 2641, 2004, 1037, 3180, 17629, 1998, 2097, 2022, 2579, 2046, 4070, 1012, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3]}], "na_triple": [], "sent_ends": [0, 13, 49, 81, 128], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57441807", "vertexSet": [[{"sent_id": 9, "name": "tf.global_variables_initializer", "pos": [182, 191]}, {"sent_id": 10, "name": "tf.global_variables_initializer", "pos": [208, 217]}, {"sent_id": 14, "name": "tf.global_variables_initializer", "pos": [345, 354]}], [{"sent_id": 9, "name": "tf.constant_initializer", "pos": [192, 199]}, {"sent_id": 11, "name": "tf.constant_initializer", "pos": [247, 254]}, {"sent_id": 13, "name": "tf.constant_initializer", "pos": [302, 309]}, {"sent_id": 14, "name": "tf.constant_initializer", "pos": [362, 369]}], [{"sent_id": 12, "name": "tf.session", "pos": [286, 290]}, {"sent_id": 14, "name": "tf.session", "pos": [338, 342]}], [{"sent_id": 1, "name": "tf.variable", "pos": [27, 31]}, {"sent_id": 2, "name": "tf.variable", "pos": [54, 58]}, {"sent_id": 5, "name": "tf.variable", "pos": [132, 136]}, {"sent_id": 8, "name": "tf.variable", "pos": [164, 168]}, {"sent_id": 13, "name": "tf.variable", "pos": [324, 328]}, {"sent_id": 14, "name": "tf.variable", "pos": [374, 378]}], [{"sent_id": 11, "name": "tf.random_normal_initializer", "pos": [255, 264]}], [{"sent_id": 11, "name": "tf.glorot_uniform_initializer", "pos": [265, 276]}]], "sents": ["Let's assume that you want to create a Tensorflow graph and train it from scratch.", "Therefore, all of the tf.Variable in your TF Graph will be initialized randomly.", "For example, in the code bellow, the tf.Variable will be initialized with values from a normal distribution.", "<code>Code Snippet</code>.", "Now, let us assume that you again want to create a TF Graph and train it from scratch.", "However, right now, for some weird reason, you know the exact values that you want to use for some of the tf.Variable in your graph.", "Therefore:", "<code>Code Snippet</code>.", "you initialized those tf.Variable with a the values you wish.", "As for the distinction between tf.global_variables_initializer and tf.constant_initializer, they are something completely different:", "tf.global_variables_initializer is an operation that you execute to initialize all variables in your graph.", "It doesn't matter whether the variables will get initialized with tf.constant_initializer, tf.random_normal_initializer or tf.glorot_uniform_initializer.", "You just pass that operation to a tf.Session so that the graph variables will get initialized.", "tf.constant_initializer on the other hand, is just an initializer that you pass to the tf.Variable of your graph.", "Then, when a tf.Session runs the operation tf.global_variables_initializer, the TF Graph will use the tf.constant_initializer to initialize the corresponding tf.Variable with the constant values provided."], "sent_idxs": [101, 2292, 1005, 1055, 7868, 2008, 2017, 2215, 2000, 3443, 1037, 23435, 12314, 10629, 1998, 3345, 2009, 2013, 11969, 1012, 102, 101, 3568, 1010, 2035, 1997, 1996, 1056, 2546, 1012, 8023, 1999, 2115, 1056, 2546, 10629, 2097, 2022, 3988, 3550, 18154, 1012, 102, 101, 2005, 2742, 1010, 1999, 1996, 3642, 4330, 5004, 1010, 1996, 1056, 2546, 1012, 8023, 2097, 2022, 3988, 3550, 2007, 5300, 2013, 1037, 3671, 4353, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 1010, 2292, 2149, 7868, 2008, 2017, 2153, 2215, 2000, 3443, 1037, 1056, 2546, 10629, 1998, 3345, 2009, 2013, 11969, 1012, 102, 101, 2174, 1010, 2157, 2085, 1010, 2005, 2070, 6881, 3114, 1010, 2017, 2113, 1996, 6635, 5300, 2008, 2017, 2215, 2000, 2224, 2005, 2070, 1997, 1996, 1056, 2546, 1012, 8023, 1999, 2115, 10629, 1012, 102, 101, 3568, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 3988, 3550, 2216, 1056, 2546, 1012, 8023, 2007, 1037, 1996, 5300, 2017, 4299, 1012, 102, 101, 2004, 2005, 1996, 7835, 2090, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1998, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1010, 2027, 2024, 2242, 3294, 2367, 1024, 102, 101, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 2003, 2019, 3169, 2008, 2017, 15389, 2000, 3988, 4697, 2035, 10857, 1999, 2115, 10629, 1012, 102, 101, 2009, 2987, 1005, 1056, 3043, 3251, 1996, 10857, 2097, 2131, 3988, 3550, 2007, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1010, 1056, 2546, 1012, 6721, 1035, 3671, 1035, 3988, 17629, 2030, 1056, 2546, 1012, 1043, 10626, 4140, 1035, 6375, 1035, 3988, 17629, 1012, 102, 101, 2017, 2074, 3413, 2008, 3169, 2000, 1037, 1056, 2546, 1012, 5219, 2061, 2008, 1996, 10629, 10857, 2097, 2131, 3988, 3550, 1012, 102, 101, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 2006, 1996, 2060, 2192, 1010, 2003, 2074, 2019, 3988, 17629, 2008, 2017, 3413, 2000, 1996, 1056, 2546, 1012, 8023, 1997, 2115, 10629, 1012, 102, 101, 2059, 1010, 2043, 1037, 1056, 2546, 1012, 5219, 3216, 1996, 3169, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1010, 1996, 1056, 2546, 10629, 2097, 2224, 1996, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 2000, 3988, 4697, 1996, 7978, 1056, 2546, 1012, 8023, 2007, 1996, 5377, 5300, 3024, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [9, 10, 11, 12, 13, 14]}, {"r": "S1", "h": 1, "t": 0, "evidence": [9, 10, 11, 12, 13, 14]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 21, 43, 70, 84, 107, 141, 145, 159, 176, 207, 233, 278, 301, 333, 385], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39133562", "vertexSet": [[{"sent_id": 0, "name": "tf.int32", "pos": [22, 27]}, {"sent_id": 2, "name": "tf.int32", "pos": [97, 102]}], [{"sent_id": 1, "name": "tf.float32", "pos": [79, 84]}, {"sent_id": 7, "name": "tf.float32", "pos": [269, 274]}], [{"sent_id": 1, "name": "tf.zeros", "pos": [58, 63]}], [{"sent_id": 0, "name": "tf.variable", "pos": [5, 9]}, {"sent_id": 1, "name": "tf.variable", "pos": [53, 57]}]], "sents": ["This error arises because tf.Variable(0, ...) defines a variable of element type tf.int32, and there is no kernel that implements int32 variables on GPU in the standard TensorFlow distribution.", "When you use tf.Variable(tf.zeros([1])), you're defining a variable of element type tf.float32, which is supported on GPU.", "The story of tf.int32 on GPUs in TensorFlow is a long one.", "While it's technically easy to support integer operations running on a GPU, our experience has been that most integer operations actually take place on the metadata of tensors, and this metadata lives on the CPU, so it's more efficient to operate on it there.", "As a short-term workaround, several kernel registrations for int32 on GPUs were removed.", "However, if these would be useful for your models, it would be possible to add them as custom ops.", "Source: In TensorFlow 0.10, the Variable-related kernels are registered using the TF_CALL_GPU_NUMBER_TYPES() macro.", "The current \"GPU number types\" are tf.float16, tf.float32, and tf.float64."], "sent_idxs": [101, 2023, 7561, 18653, 2138, 1056, 2546, 1012, 8023, 1006, 1014, 1010, 1012, 1012, 1012, 1007, 11859, 1037, 8023, 1997, 5783, 2828, 1056, 2546, 1012, 20014, 16703, 1010, 1998, 2045, 2003, 2053, 16293, 2008, 22164, 20014, 16703, 10857, 2006, 14246, 2226, 1999, 1996, 3115, 23435, 12314, 4353, 1012, 102, 101, 2043, 2017, 2224, 1056, 2546, 1012, 8023, 1006, 1056, 2546, 1012, 5717, 2015, 1006, 1031, 1015, 1033, 1007, 1007, 1010, 2017, 1005, 2128, 12854, 1037, 8023, 1997, 5783, 2828, 1056, 2546, 1012, 14257, 16703, 1010, 2029, 2003, 3569, 2006, 14246, 2226, 1012, 102, 101, 1996, 2466, 1997, 1056, 2546, 1012, 20014, 16703, 2006, 14246, 2271, 1999, 23435, 12314, 2003, 1037, 2146, 2028, 1012, 102, 101, 2096, 2009, 1005, 1055, 10892, 3733, 2000, 2490, 16109, 3136, 2770, 2006, 1037, 14246, 2226, 1010, 2256, 3325, 2038, 2042, 2008, 2087, 16109, 3136, 2941, 2202, 2173, 2006, 1996, 27425, 1997, 23435, 2015, 1010, 1998, 2023, 27425, 3268, 2006, 1996, 17368, 1010, 2061, 2009, 1005, 1055, 2062, 8114, 2000, 5452, 2006, 2009, 2045, 1012, 102, 101, 2004, 1037, 2460, 1011, 2744, 2147, 24490, 1010, 2195, 16293, 8819, 2015, 2005, 20014, 16703, 2006, 14246, 2271, 2020, 3718, 1012, 102, 101, 2174, 1010, 2065, 2122, 2052, 2022, 6179, 2005, 2115, 4275, 1010, 2009, 2052, 2022, 2825, 2000, 5587, 2068, 2004, 7661, 23092, 1012, 102, 101, 3120, 1024, 1999, 23435, 12314, 1014, 1012, 2184, 1010, 1996, 8023, 1011, 3141, 16293, 2015, 2024, 5068, 2478, 1996, 1056, 2546, 1035, 2655, 1035, 14246, 2226, 1035, 2193, 1035, 4127, 1006, 1007, 26632, 1012, 102, 101, 1996, 2783, 1000, 14246, 2226, 2193, 4127, 1000, 2024, 1056, 2546, 1012, 14257, 16048, 1010, 1056, 2546, 1012, 14257, 16703, 1010, 1998, 1056, 2546, 1012, 14257, 21084, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 49, 93, 114, 170, 193, 217, 253, 283], "sent_pos": [0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35727708", "vertexSet": [[{"sent_id": 0, "name": "tf.float64", "pos": [12, 17]}, {"sent_id": 3, "name": "tf.float64", "pos": [76, 81]}], [{"sent_id": 0, "name": "tf.float32", "pos": [18, 23]}, {"sent_id": 4, "name": "tf.float32", "pos": [115, 120]}], [{"sent_id": 0, "name": "tf.cast", "pos": [25, 29]}], [{"sent_id": 4, "name": "tf.variable", "pos": [98, 102]}]], "sents": ["The short answer is that you can convert a tensor from tf.float64 to tf.float32 using the tf.cast() op:", "<code>Code Snippet</code>.", "The longer answer is that this will not solve all of your problems with the optimizers.", "(The lack of support for tf.float64 is a known issue.)", "The optimizers require that all of the tf.Variable objects that you are trying to optimize must also have type tf.float32."], "sent_idxs": [101, 1996, 2460, 3437, 2003, 2008, 2017, 2064, 10463, 1037, 23435, 2013, 1056, 2546, 1012, 14257, 21084, 2000, 1056, 2546, 1012, 14257, 16703, 2478, 1996, 1056, 2546, 1012, 3459, 1006, 1007, 6728, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2936, 3437, 2003, 2008, 2023, 2097, 2025, 9611, 2035, 1997, 2115, 3471, 2007, 1996, 23569, 27605, 16750, 1012, 102, 101, 1006, 1996, 3768, 1997, 2490, 2005, 1056, 2546, 1012, 14257, 21084, 2003, 1037, 2124, 3277, 1012, 1007, 102, 101, 1996, 23569, 27605, 16750, 5478, 2008, 2035, 1997, 1996, 1056, 2546, 1012, 8023, 5200, 2008, 2017, 2024, 2667, 2000, 23569, 27605, 4371, 2442, 2036, 2031, 2828, 1056, 2546, 1012, 14257, 16703, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 34, 48, 69, 88, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0]}, {"title": "54372304", "vertexSet": [[{"sent_id": 4, "name": "tf.sparse_placeholder", "pos": [83, 90]}], [{"sent_id": 4, "name": "tf.constant", "pos": [63, 67]}], [{"sent_id": 4, "name": "tf.placeholder", "pos": [69, 74]}]], "sents": ["The problem is with the snippet", "<code>Code Snippet</code>.", "In this code you construct a dictionary where the values are tensors.", "Like you said, this won't work for a VarLenFeature.", "Instead of using tf.constant try using tf.placeholder for a a FixedLenFeature and tf.sparse_placeholder for a VarLenFeature."], "sent_idxs": [101, 1996, 3291, 2003, 2007, 1996, 1055, 3490, 29519, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 2023, 3642, 2017, 9570, 1037, 9206, 2073, 1996, 5300, 2024, 23435, 2015, 1012, 102, 101, 2066, 2017, 2056, 1010, 2023, 2180, 1005, 1056, 2147, 2005, 1037, 13075, 7770, 7959, 4017, 5397, 1012, 102, 101, 2612, 1997, 2478, 1056, 2546, 1012, 5377, 3046, 2478, 1056, 2546, 1012, 2173, 14528, 2005, 1037, 1037, 4964, 7770, 7959, 4017, 5397, 1998, 1056, 2546, 1012, 20288, 1035, 2173, 14528, 2005, 1037, 13075, 7770, 7959, 4017, 5397, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2, 3, 4]}, {"r": "S1", "h": 1, "t": 2, "evidence": [2, 3, 4]}, {"r": "S1", "h": 2, "t": 1, "evidence": [2, 3, 4]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 10, 24, 40, 59, 99], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35855219", "vertexSet": [[{"sent_id": 1, "name": "tf.fill", "pos": [59, 63]}], [{"sent_id": 1, "name": "tf.shape", "pos": [66, 70]}], [{"sent_id": 0, "name": "tf.constant", "pos": [2, 6]}]], "sents": ["A tf.constant() has fixed size and value at graph construction time, so it probably isn't the right op for your application.", "If you are trying to create a tensor with a dynamic size and the same (constant) value for every element, you can use tf.fill() and tf.shape() to create an appropriately-shaped tensor.", "For example, to create a tensor t that has the same shape as input and the value 0.5 everywhere:", "<code>Code Snippet</code>.", "As Yaroslav mentions in his comment, you may also be able to use (NumPy-style) broadcasting to avoid materializing a tensor with dynamic shape.", "For example, if input has shape (None, 32) and t has shape (1, 32) then computing tf.mul(input, t) will broadcast t on the first dimension to match the shape of input."], "sent_idxs": [101, 1037, 1056, 2546, 1012, 5377, 1006, 1007, 2038, 4964, 2946, 1998, 3643, 2012, 10629, 2810, 2051, 1010, 2061, 2009, 2763, 3475, 1005, 1056, 1996, 2157, 6728, 2005, 2115, 4646, 1012, 102, 101, 2065, 2017, 2024, 2667, 2000, 3443, 1037, 23435, 2007, 1037, 8790, 2946, 1998, 1996, 2168, 1006, 5377, 1007, 3643, 2005, 2296, 5783, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 6039, 1006, 1007, 1998, 1056, 2546, 1012, 4338, 1006, 1007, 2000, 3443, 2019, 23263, 1011, 5044, 23435, 1012, 102, 101, 2005, 2742, 1010, 2000, 3443, 1037, 23435, 1056, 2008, 2038, 1996, 2168, 4338, 2004, 7953, 1998, 1996, 3643, 1014, 1012, 1019, 7249, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2004, 8038, 7352, 14973, 9704, 1999, 2010, 7615, 1010, 2017, 2089, 2036, 2022, 2583, 2000, 2224, 1006, 16371, 8737, 2100, 1011, 2806, 1007, 5062, 2000, 4468, 3430, 6026, 1037, 23435, 2007, 8790, 4338, 1012, 102, 101, 2005, 2742, 1010, 2065, 7953, 2038, 4338, 1006, 3904, 1010, 3590, 1007, 1998, 1056, 2038, 4338, 1006, 1015, 1010, 3590, 1007, 2059, 9798, 1056, 2546, 1012, 14163, 2140, 1006, 7953, 1010, 1056, 1007, 2097, 3743, 1056, 2006, 1996, 2034, 9812, 2000, 2674, 1996, 4338, 1997, 7953, 1012, 102], "labels": [{"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1]}], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 32, 81, 106, 120, 156, 205], "sent_pos": [0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42562650", "vertexSet": [[{"sent_id": 0, "name": "tf.tensor", "pos": [31, 35]}, {"sent_id": 2, "name": "tf.tensor", "pos": [81, 85]}], [{"sent_id": 0, "name": "tf.sparsetensor", "pos": [18, 24]}, {"sent_id": 2, "name": "tf.sparsetensor", "pos": [57, 63]}, {"sent_id": 7, "name": "tf.sparsetensor", "pos": [236, 242]}, {"sent_id": 7, "name": "tf.sparsetensor", "pos": [249, 255]}, {"sent_id": 9, "name": "tf.sparsetensor", "pos": [292, 298]}, {"sent_id": 9, "name": "tf.sparsetensor", "pos": [309, 315]}], [{"sent_id": 7, "name": "tf.sparsetensorvalue", "pos": [249, 257]}, {"sent_id": 9, "name": "tf.sparsetensorvalue", "pos": [292, 300]}, {"sent_id": 9, "name": "tf.sparsetensorvalue", "pos": [309, 317]}], [{"sent_id": 7, "name": "tf.sparse_placeholder", "pos": [224, 231]}]], "sents": ["The closest thing TensorFlow has to scipy.sparse.coo_matrix is tf.SparseTensor, which is the sparse equivalent of tf.Tensor.", "It will probably be easiest to feed a coo_matrix into your program.", "A tf.SparseTensor is a slight generalization of COO matrices, where the tensor is represented as three dense tf.Tensor objects:", "indices: An N x D matrix of tf.int64 values in which each row represents the coordinates of a non-zero value.", "N is the number of non-zeroes, and D is the rank of the equivalent dense tensor (2 in the case of a matrix)..", "values: A length-N vector of values, where element i is the value of the element whose coordinates are given on row i of indices..", "dense_shape: A length-D vector of tf.int64, representing the shape of the equivalent dense tensor..", "For example, you could use the following code, which uses tf.sparse_placeholder() to define a tf.SparseTensor that you can feed, and a tf.SparseTensorValue that represents the actual value being fed :", "<code>Code Snippet</code>.", "Once you have converted your coo_matrix to a tf.SparseTensorValue, you can feed sparse_input with the tf.SparseTensorValue directly:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 7541, 2518, 23435, 12314, 2038, 2000, 16596, 7685, 1012, 20288, 1012, 2522, 2080, 1035, 8185, 2003, 1056, 2546, 1012, 20288, 25808, 2953, 1010, 2029, 2003, 1996, 20288, 5662, 1997, 1056, 2546, 1012, 23435, 1012, 102, 101, 2009, 2097, 2763, 2022, 25551, 2000, 5438, 1037, 2522, 2080, 1035, 8185, 2046, 2115, 2565, 1012, 102, 101, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 2003, 1037, 7263, 2236, 3989, 1997, 2522, 2080, 21520, 1010, 2073, 1996, 23435, 2003, 3421, 2004, 2093, 9742, 1056, 2546, 1012, 23435, 5200, 1024, 102, 101, 29299, 1024, 2019, 1050, 1060, 1040, 8185, 1997, 1056, 2546, 1012, 20014, 21084, 5300, 1999, 2029, 2169, 5216, 5836, 1996, 12093, 1997, 1037, 2512, 1011, 5717, 3643, 1012, 102, 101, 1050, 2003, 1996, 2193, 1997, 2512, 1011, 5717, 2229, 1010, 1998, 1040, 2003, 1996, 4635, 1997, 1996, 5662, 9742, 23435, 1006, 1016, 1999, 1996, 2553, 1997, 1037, 8185, 1007, 1012, 1012, 102, 101, 5300, 1024, 1037, 3091, 1011, 1050, 9207, 1997, 5300, 1010, 2073, 5783, 1045, 2003, 1996, 3643, 1997, 1996, 5783, 3005, 12093, 2024, 2445, 2006, 5216, 1045, 1997, 29299, 1012, 1012, 102, 101, 9742, 1035, 4338, 1024, 1037, 3091, 1011, 1040, 9207, 1997, 1056, 2546, 1012, 20014, 21084, 1010, 5052, 1996, 4338, 1997, 1996, 5662, 9742, 23435, 1012, 1012, 102, 101, 2005, 2742, 1010, 2017, 2071, 2224, 1996, 2206, 3642, 1010, 2029, 3594, 1056, 2546, 1012, 20288, 1035, 2173, 14528, 1006, 1007, 2000, 9375, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 2008, 2017, 2064, 5438, 1010, 1998, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 10175, 5657, 2008, 5836, 1996, 5025, 3643, 2108, 7349, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2320, 2017, 2031, 4991, 2115, 2522, 2080, 1035, 8185, 2000, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 10175, 5657, 1010, 2017, 2064, 5438, 20288, 1035, 7953, 2007, 1996, 1056, 2546, 1012, 20288, 25808, 2953, 10175, 5657, 3495, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 3, 4, 5, 6]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 3, 4, 5, 6]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 37, 55, 88, 118, 151, 183, 211, 266, 280, 320, 334], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54742449", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib.tpu.tpuestimator", "pos": [29, 44]}], [{"sent_id": 0, "name": "tf.estimator.estimator", "pos": [16, 26]}]], "sents": ["If you actually don't need TPU inference support you can create a tf.estimator.Estimator instead of a tf.contrib.tpu.TPUEstimator one, using the same model_fn and trained model.", "Then, you should be able to export the model."], "sent_idxs": [101, 2065, 2017, 2941, 2123, 1005, 1056, 2342, 1056, 14289, 28937, 2490, 2017, 2064, 3443, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 2612, 1997, 1037, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1056, 14289, 1012, 1056, 14289, 4355, 9581, 4263, 2028, 1010, 2478, 1996, 2168, 2944, 1035, 1042, 2078, 1998, 4738, 2944, 1012, 102, 101, 2059, 1010, 2017, 2323, 2022, 2583, 2000, 9167, 1996, 2944, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 58, 71], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34232533", "vertexSet": [[{"sent_id": 2, "name": "tf.name_scope", "pos": [83, 89]}, {"sent_id": 5, "name": "tf.name_scope", "pos": [163, 169]}], [{"sent_id": 3, "name": "tf.variable_scope", "pos": [108, 114]}, {"sent_id": 5, "name": "tf.variable_scope", "pos": [196, 202]}], [{"sent_id": 0, "name": "tf.get_variable", "pos": [7, 13]}, {"sent_id": 2, "name": "tf.get_variable", "pos": [63, 69]}, {"sent_id": 5, "name": "tf.get_variable", "pos": [187, 193]}, {"sent_id": 5, "name": "tf.get_variable", "pos": [210, 216]}]], "sents": ["When you create a variable with tf.get_variable instead of tf.Variable, Tensorflow will start checking the names of the vars created with the same method to see if they collide.", "If they do, an exception will be raised.", "If you created a var with tf.get_variable and you try to change the prefix of your variable names by using the tf.name_scope context manager, this won't prevent the Tensorflow of raising an exception.", "Only tf.variable_scope context manager will effectively change the name of your var in this case.", "Or if you want to reuse the variable you should call scope.reuse_variables() before creating the var the second time.", "In summary, tf.name_scope just add a prefix to all tensor created in that scope (except the vars created with tf.get_variable), and tf.variable_scope add a prefix to the variables created with tf.get_variable."], "sent_idxs": [101, 2043, 2017, 3443, 1037, 8023, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 2612, 1997, 1056, 2546, 1012, 8023, 1010, 23435, 12314, 2097, 2707, 9361, 1996, 3415, 1997, 1996, 13075, 2015, 2580, 2007, 1996, 2168, 4118, 2000, 2156, 2065, 2027, 8902, 24198, 1012, 102, 101, 2065, 2027, 2079, 1010, 2019, 6453, 2097, 2022, 2992, 1012, 102, 101, 2065, 2017, 2580, 1037, 13075, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 1998, 2017, 3046, 2000, 2689, 1996, 17576, 1997, 2115, 8023, 3415, 2011, 2478, 1996, 1056, 2546, 1012, 2171, 1035, 9531, 6123, 3208, 1010, 2023, 2180, 1005, 1056, 4652, 1996, 23435, 12314, 1997, 6274, 2019, 6453, 1012, 102, 101, 2069, 1056, 2546, 1012, 8023, 1035, 9531, 6123, 3208, 2097, 6464, 2689, 1996, 2171, 1997, 2115, 13075, 1999, 2023, 2553, 1012, 102, 101, 2030, 2065, 2017, 2215, 2000, 2128, 8557, 1996, 8023, 2017, 2323, 2655, 9531, 1012, 2128, 8557, 1035, 10857, 1006, 1007, 2077, 4526, 1996, 13075, 1996, 2117, 2051, 1012, 102, 101, 1999, 12654, 1010, 1056, 2546, 1012, 2171, 1035, 9531, 2074, 5587, 1037, 17576, 2000, 2035, 23435, 2580, 1999, 2008, 9531, 1006, 3272, 1996, 13075, 2015, 2580, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 1007, 1010, 1998, 1056, 2546, 1012, 8023, 1035, 9531, 5587, 1037, 17576, 2000, 1996, 10857, 2580, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3, 5]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 44, 56, 106, 129, 159, 218], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0]}, {"title": "48815730", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.max_pool ", "pos": [1, 10]}], [{"sent_id": 1, "name": "tf.layers.max_pooling1d", "pos": [49, 60]}]], "sents": ["tf.nn.max_pool is for 2D pooling, i.e., it expects input tensor of rank 4 (yours is rank 3).", "You should either expand the dimensions of the input or simply use tf.layers.max_pooling1d:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 4098, 1035, 4770, 2003, 2005, 14134, 4770, 2075, 1010, 1045, 1012, 1041, 1012, 1010, 2009, 24273, 7953, 23435, 1997, 4635, 1018, 1006, 6737, 2003, 4635, 1017, 1007, 1012, 102, 101, 2017, 2323, 2593, 7818, 1996, 9646, 1997, 1996, 7953, 2030, 3432, 2224, 1056, 2546, 1012, 9014, 1012, 4098, 1035, 4770, 2075, 2487, 2094, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 36, 62, 76], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47583685", "vertexSet": [[{"sent_id": 4, "name": "tf.reduce_sum", "pos": [63, 69]}], [{"sent_id": 0, "name": "tf.multiply", "pos": [2, 7]}, {"sent_id": 7, "name": "tf.multiply", "pos": [187, 192]}], [{"sent_id": 7, "name": "tf.matmul", "pos": [175, 181]}]], "sents": ["What tf.multiply(X, X) does is essentially multiplying each element of the matrix with itself, like", "<code>Code Snippet</code>.", "would turn into", "<code>Code Snippet</code>.", "whereas tf.reduce_sum(_, axis=1) takes a sum of each row, so the result for the previous example will be", "<code>Code Snippet</code>.", "which is exactly (by definition) equal to [X[0, :] @ X[0, :], X[1, :] @ X[1, :]].", "Just put it down with variable names [[a b] [c d]] instead of actual numbers and look at what does tf.matmul(X, X) and tf.multiply(X, X) do."], "sent_idxs": [101, 2054, 1056, 2546, 1012, 4800, 22086, 1006, 1060, 1010, 1060, 1007, 2515, 2003, 7687, 4800, 22086, 2075, 2169, 5783, 1997, 1996, 8185, 2007, 2993, 1010, 2066, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2052, 2735, 2046, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6168, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1035, 1010, 8123, 1027, 1015, 1007, 3138, 1037, 7680, 1997, 2169, 5216, 1010, 2061, 1996, 2765, 2005, 1996, 3025, 2742, 2097, 2022, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2029, 2003, 3599, 1006, 2011, 6210, 1007, 5020, 2000, 1031, 1060, 1031, 1014, 1010, 1024, 1033, 1030, 1060, 1031, 1014, 1010, 1024, 1033, 1010, 1060, 1031, 1015, 1010, 1024, 1033, 1030, 1060, 1031, 1015, 1010, 1024, 1033, 1033, 1012, 102, 101, 2074, 2404, 2009, 2091, 2007, 8023, 3415, 1031, 1031, 1037, 1038, 1033, 1031, 1039, 1040, 1033, 1033, 2612, 1997, 5025, 3616, 1998, 2298, 2012, 2054, 2515, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 1010, 1060, 1007, 1998, 1056, 2546, 1012, 4800, 22086, 1006, 1060, 1010, 1060, 1007, 2079, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 4]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 42, 47, 61, 93, 107, 148, 200], "sent_pos": [0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47235448", "vertexSet": [[{"sent_id": 1, "name": "tf.train.saver", "pos": [31, 38]}], [{"sent_id": 4, "name": "tf.saved_model", "pos": [106, 112]}]], "sents": ["My environment: Python 3.6, Tensorflow 1.3.0", "Though there have been many solutions, most of them is based on tf.train.Saver.", "When we load a .ckpt saved by Saver, we have to either redefine the tensorflow network or use some weird and hard-remembered name, e.g.", "'placehold_0:0','dense/Adam/Weight:0'.", "Here I recommend to use tf.saved_model, one simplest example given below, your can learn more from Serving a TensorFlow Model:", "Save the model:", "<code>Code Snippet</code>.", "Load the model:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2026, 4044, 1024, 18750, 1017, 1012, 1020, 1010, 23435, 12314, 1015, 1012, 1017, 1012, 1014, 102, 101, 2295, 2045, 2031, 2042, 2116, 7300, 1010, 2087, 1997, 2068, 2003, 2241, 2006, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 1012, 102, 101, 2043, 2057, 7170, 1037, 1012, 23616, 13876, 5552, 2011, 3828, 2099, 1010, 2057, 2031, 2000, 2593, 2417, 12879, 3170, 1996, 23435, 12314, 2897, 2030, 2224, 2070, 6881, 1998, 2524, 1011, 4622, 2171, 1010, 1041, 1012, 1043, 1012, 102, 101, 1005, 2173, 12640, 1035, 1014, 1024, 1014, 1005, 1010, 1005, 9742, 1013, 4205, 1013, 3635, 1024, 1014, 1005, 1012, 102, 101, 2182, 1045, 16755, 2000, 2224, 1056, 2546, 1012, 5552, 1035, 2944, 1010, 2028, 21304, 2742, 2445, 2917, 1010, 2115, 2064, 4553, 2062, 2013, 3529, 1037, 23435, 12314, 2944, 1024, 102, 101, 3828, 1996, 2944, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 7170, 1996, 2944, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 17, 40, 79, 100, 131, 137, 151, 157, 171], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39646025", "vertexSet": [[{"sent_id": 1, "name": "tf.clip_by_norm", "pos": [38, 46]}, {"sent_id": 7, "name": "tf.clip_by_norm", "pos": [101, 109]}], [{"sent_id": 0, "name": "tf.nn.dropout", "pos": [1, 9]}], [{"sent_id": 8, "name": "tf.nn.xw_plus_b", "pos": [121, 133]}]], "sents": ["tf.nn.dropout does not impose any norm constraint.", "I believe what you're looking for is to \"process the gradients before applying them\" using tf.clip_by_norm.", "For example, instead of simply:", "<code>Code Snippet</code>.", "You could:", "<code>Code Snippet</code>.", "I hope this helps.", "Final notes about tf.clip_by_norm's axes parameter:", "If you're calculating tf.nn.xw_plus_b(x, weights, biases), or equivalently matmul(x, weights) + biases, when the dimensions of x and weights are (batch, in_units) and (in_units, out_units) respectively, then you probably want to set axes == [0] (because in this usage each column details all incoming weights to a specific unit)..", "Pay attention to the shape/dimensions of your variables above and whether/how exactly you want to clip_by_norm each of them!", "E.g.", "if some of [weights1, weights2, ...] are matrices and some aren't, and you call clip_by_norm() on the grads_and_vars with the same axes value like in the List Comprehension above, this doesn't mean the same thing for all the variables!", "In fact, if you're lucky, this will result in a weird error like ValueError: Invalid reduction dimension 1 for input with 1 dimensions, but otherwise it's a very sneaky bug.."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2515, 2025, 17607, 2151, 13373, 27142, 1012, 102, 101, 1045, 2903, 2054, 2017, 1005, 2128, 2559, 2005, 2003, 2000, 1000, 2832, 1996, 17978, 2015, 2077, 11243, 2068, 1000, 2478, 1056, 2546, 1012, 12528, 1035, 2011, 1035, 13373, 1012, 102, 101, 2005, 2742, 1010, 2612, 1997, 3432, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2071, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 3246, 2023, 7126, 1012, 102, 101, 2345, 3964, 2055, 1056, 2546, 1012, 12528, 1035, 2011, 1035, 13373, 1005, 1055, 19589, 16381, 1024, 102, 101, 2065, 2017, 1005, 2128, 20177, 1056, 2546, 1012, 1050, 2078, 1012, 1060, 2860, 1035, 4606, 1035, 1038, 1006, 1060, 1010, 15871, 1010, 13827, 2229, 1007, 1010, 2030, 5662, 2135, 13523, 12274, 2140, 1006, 1060, 1010, 15871, 1007, 1009, 13827, 2229, 1010, 2043, 1996, 9646, 1997, 1060, 1998, 15871, 2024, 1006, 14108, 1010, 1999, 1035, 3197, 1007, 1998, 1006, 1999, 1035, 3197, 1010, 2041, 1035, 3197, 1007, 4414, 1010, 2059, 2017, 2763, 2215, 2000, 2275, 19589, 1027, 1027, 1031, 1014, 1033, 1006, 2138, 1999, 2023, 8192, 2169, 5930, 4751, 2035, 14932, 15871, 2000, 1037, 3563, 3131, 1007, 1012, 1012, 102, 101, 3477, 3086, 2000, 1996, 4338, 1013, 9646, 1997, 2115, 10857, 2682, 1998, 3251, 1013, 2129, 3599, 2017, 2215, 2000, 12528, 1035, 2011, 1035, 13373, 2169, 1997, 2068, 999, 102, 101, 1041, 1012, 1043, 1012, 102, 101, 2065, 2070, 1997, 1031, 15871, 2487, 1010, 15871, 2475, 1010, 1012, 1012, 1012, 1033, 2024, 21520, 1998, 2070, 4995, 1005, 1056, 1010, 1998, 2017, 2655, 12528, 1035, 2011, 1035, 13373, 1006, 1007, 2006, 1996, 24665, 19303, 1035, 1998, 1035, 13075, 2015, 2007, 1996, 2168, 19589, 3643, 2066, 1999, 1996, 2862, 26683, 2682, 1010, 2023, 2987, 1005, 1056, 2812, 1996, 2168, 2518, 2005, 2035, 1996, 10857, 999, 102, 101, 1999, 2755, 1010, 2065, 2017, 1005, 2128, 5341, 1010, 2023, 2097, 2765, 1999, 1037, 6881, 7561, 2066, 3643, 2121, 29165, 1024, 19528, 7312, 9812, 1015, 2005, 7953, 2007, 1015, 9646, 1010, 2021, 4728, 2009, 1005, 1055, 1037, 2200, 13583, 2100, 11829, 1012, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 17, 48, 57, 71, 76, 90, 97, 115, 215, 245, 251, 319, 364], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42811536", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib.rnn", "pos": [23, 32]}, {"sent_id": 3, "name": "tf.contrib.rnn", "pos": [72, 81]}], [{"sent_id": 3, "name": "tf.contrib.rnn.lstmcell", "pos": [72, 86]}]], "sents": ["You get an error because you are trying to call a module object as a function.", "In particular, tf.contrib.rnn is the TensorFlow module which contains all the RNN cells.", "You can find the related documentation here.", "If you want to use an LSTM cell you need to use an tf.contrib.rnn.LSTMCell."], "sent_idxs": [101, 2017, 2131, 2019, 7561, 2138, 2017, 2024, 2667, 2000, 2655, 1037, 11336, 4874, 2004, 1037, 3853, 1012, 102, 101, 1999, 3327, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 2003, 1996, 23435, 12314, 11336, 2029, 3397, 2035, 1996, 29300, 2078, 4442, 1012, 102, 101, 2017, 2064, 2424, 1996, 3141, 12653, 2182, 1012, 102, 101, 2065, 2017, 2215, 2000, 2224, 2019, 1048, 3367, 2213, 3526, 2017, 2342, 2000, 2224, 2019, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 1048, 3367, 12458, 5349, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 3]}], "na_triple": [], "sent_ends": [0, 19, 46, 56, 88], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "40032112", "vertexSet": [[{"sent_id": 3, "name": "tf.contrib.learn.datasets.base.load_csv_without_header", "pos": [93, 116]}], [{"sent_id": 0, "name": "tf.contrib.learn.datasets.base.load_csv", "pos": [3, 22]}, {"sent_id": 2, "name": "tf.contrib.learn.datasets.base.load_csv", "pos": [65, 84]}, {"sent_id": 3, "name": "tf.contrib.learn.datasets.base.load_csv", "pos": [93, 112]}], [{"sent_id": 2, "name": "tf.contrib.learn.datasets.base.load_csv_with_header", "pos": [65, 88]}]], "sents": ["The function tf.contrib.learn.datasets.base.load_csv() was removed in TensorFlow release 0.11.", "Depending on whether the file has a header or not (and the Iris dataset does have a header), the replacement functions are:", "tf.contrib.learn.datasets.base.load_csv_with_header().", "tf.contrib.learn.datasets.base.load_csv_without_header()."], "sent_idxs": [101, 1996, 3853, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 2951, 13462, 2015, 1012, 2918, 1012, 7170, 1035, 20116, 2615, 1006, 1007, 2001, 3718, 1999, 23435, 12314, 2713, 1014, 1012, 2340, 1012, 102, 101, 5834, 2006, 3251, 1996, 5371, 2038, 1037, 20346, 2030, 2025, 1006, 1998, 1996, 11173, 2951, 13462, 2515, 2031, 1037, 20346, 1007, 1010, 1996, 6110, 4972, 2024, 1024, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 2951, 13462, 2015, 1012, 2918, 1012, 7170, 1035, 20116, 2615, 1035, 2007, 1035, 20346, 1006, 1007, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 2951, 13462, 2015, 1012, 2918, 1012, 7170, 1035, 20116, 2615, 1035, 2302, 1035, 20346, 1006, 1007, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 3]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1, 2]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1, 2]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 35, 64, 92, 120], "sent_pos": [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0]}, {"title": "36412802", "vertexSet": [[{"sent_id": 3, "name": "tf.train.import_meta_graph", "pos": [122, 132]}], [{"sent_id": 4, "name": "tf.import_graph_def", "pos": [156, 164]}], [{"sent_id": 2, "name": "tf.variable", "pos": [89, 93]}], [{"sent_id": 0, "name": "tf.constant", "pos": [18, 22]}, {"sent_id": 1, "name": "tf.constant", "pos": [37, 41]}], [{"sent_id": 2, "name": "tf.trainable_variables", "pos": [59, 66]}]], "sents": ["This pretrained VGG-16 model encodes all of the model parameters as tf.constant() ops.", "(See, for example, the calls to tf.constant() here.)", "As a result, the model parameters would not appear in tf.trainable_variables(), and the model is not mutable without substantial surgery: you would need to replace the constant nodes with tf.Variable objects that start with the same value in order to continue training.", "In general, when importing a graph for retraining, the tf.train.import_meta_graph() function should be used, as this function loads additional metadata (including the collections of variables).", "The tf.import_graph_def() function is lower level, and does not populate these collections."], "sent_idxs": [101, 2023, 3653, 23654, 2098, 1058, 13871, 1011, 2385, 2944, 4372, 23237, 2035, 1997, 1996, 2944, 11709, 2004, 1056, 2546, 1012, 5377, 1006, 1007, 23092, 1012, 102, 101, 1006, 2156, 1010, 2005, 2742, 1010, 1996, 4455, 2000, 1056, 2546, 1012, 5377, 1006, 1007, 2182, 1012, 1007, 102, 101, 2004, 1037, 2765, 1010, 1996, 2944, 11709, 2052, 2025, 3711, 1999, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 1010, 1998, 1996, 2944, 2003, 2025, 14163, 10880, 2302, 6937, 5970, 1024, 2017, 2052, 2342, 2000, 5672, 1996, 5377, 14164, 2007, 1056, 2546, 1012, 8023, 5200, 2008, 2707, 2007, 1996, 2168, 3643, 1999, 2344, 2000, 3613, 2731, 1012, 102, 101, 1999, 2236, 1010, 2043, 12324, 2075, 1037, 10629, 2005, 2128, 23654, 2075, 1010, 1996, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 3853, 2323, 2022, 2109, 1010, 2004, 2023, 3853, 15665, 3176, 27425, 1006, 2164, 1996, 6407, 1997, 10857, 1007, 1012, 102, 101, 1996, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 3853, 2003, 2896, 2504, 1010, 1998, 2515, 2025, 3769, 9869, 2122, 6407, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [3, 4]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 27, 47, 107, 154, 180], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38042782", "vertexSet": [[{"sent_id": 4, "name": "tf.truncated_normal", "pos": [95, 101]}], [{"sent_id": 6, "name": "tf.nn", "pos": [154, 159]}], [{"sent_id": 6, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [154, 171]}]], "sents": ["The issue stems from this line in your code:", "<code>Code Snippet</code>.", "Initializing your weights to zero is a common mistake when defining a neural network.", "This article explains the reasoning behind it (very approximately, all the neurons will have the same value, so the network won't learn).", "Instead you should initialize your weights to small random numbers, and a typical scheme is to use tf.truncated_normal() with a standard deviation inversely proporational to the number of input units:", "<code>Code Snippet</code>.", "rrao's suggestions to add a bias term, and switch to the more numerically stable tf.nn.softmax_cross_entropy_with_logits() op for your loss function are good ideas as well, and these will probably be necessary steps to get reasonable accuracy."], "sent_idxs": [101, 1996, 3277, 12402, 2013, 2023, 2240, 1999, 2115, 3642, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3988, 6026, 2115, 15871, 2000, 5717, 2003, 1037, 2691, 6707, 2043, 12854, 1037, 15756, 2897, 1012, 102, 101, 2023, 3720, 7607, 1996, 13384, 2369, 2009, 1006, 2200, 3155, 1010, 2035, 1996, 15698, 2097, 2031, 1996, 2168, 3643, 1010, 2061, 1996, 2897, 2180, 1005, 1056, 4553, 1007, 1012, 102, 101, 2612, 2017, 2323, 3988, 4697, 2115, 15871, 2000, 2235, 6721, 3616, 1010, 1998, 1037, 5171, 5679, 2003, 2000, 2224, 1056, 2546, 1012, 25449, 1035, 3671, 1006, 1007, 2007, 1037, 3115, 24353, 19262, 2135, 17678, 21223, 2389, 2000, 1996, 2193, 1997, 7953, 3197, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 25269, 7113, 1005, 1055, 15690, 2000, 5587, 1037, 13827, 2744, 1010, 1998, 6942, 2000, 1996, 2062, 15973, 2135, 6540, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 6728, 2005, 2115, 3279, 3853, 2024, 2204, 4784, 2004, 2092, 1010, 1998, 2122, 2097, 2763, 2022, 4072, 4084, 2000, 2131, 9608, 10640, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 26, 44, 75, 120, 134, 197], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62127770", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [90, 95]}, {"sent_id": 3, "name": "tf.keras", "pos": [104, 109]}], [{"sent_id": 2, "name": "tf.keras.losses", "pos": [90, 97]}, {"sent_id": 3, "name": "tf.keras.losses", "pos": [104, 111]}], [{"sent_id": 2, "name": "tf.keras.losses.loss", "pos": [90, 99]}]], "sents": ["As I understood your question this might be useful.you can use from tensorflow.keras import backend as K to calculate anything using y_true, y_pred as the following example\nwhen you compile, you should add the functions  **to the loss **.", "then it will provide y_true, y_pred values.", "loss:String (name of objective function), objective function or tf.keras.losses.Loss instance.", "See tf.keras.losses.", "An objective function is any callable with the signature loss = fn(y_true, y_pred)", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 1045, 5319, 2115, 3160, 2023, 2453, 2022, 6179, 1012, 2017, 2064, 2224, 2013, 23435, 12314, 1012, 17710, 8180, 12324, 2067, 10497, 2004, 1047, 2000, 18422, 2505, 2478, 1061, 1035, 2995, 1010, 1061, 1035, 3653, 2094, 2004, 1996, 2206, 2742, 2043, 2017, 4012, 22090, 1010, 2017, 2323, 5587, 1996, 4972, 1008, 1008, 2000, 1996, 3279, 1008, 1008, 1012, 102, 101, 2059, 2009, 2097, 3073, 1061, 1035, 2995, 1010, 1061, 1035, 3653, 2094, 5300, 1012, 102, 101, 3279, 1024, 5164, 1006, 2171, 1997, 7863, 3853, 1007, 1010, 7863, 3853, 2030, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 3279, 6013, 1012, 102, 101, 2156, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 102, 101, 2019, 7863, 3853, 2003, 2151, 2655, 3085, 2007, 1996, 8085, 3279, 1027, 1042, 2078, 1006, 1061, 1035, 2995, 1010, 1061, 1035, 3653, 2094, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 60, 76, 102, 113, 139, 153], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64113757", "vertexSet": [[{"sent_id": 4, "name": "tf.keras", "pos": [73, 78]}, {"sent_id": 4, "name": "tf.keras", "pos": [91, 96]}], [{"sent_id": 4, "name": "tf.keras.layers", "pos": [73, 80]}, {"sent_id": 4, "name": "tf.keras.layers", "pos": [91, 98]}], [{"sent_id": 4, "name": "tf.keras.layers.dense", "pos": [73, 82]}, {"sent_id": 4, "name": "tf.keras.layers.dense", "pos": [91, 100]}]], "sents": ["You should pay attention to the model.", "After the RNN layer you used a Dense layer where the output dimension is 2!", "So the size of the output you get by running the model.predict() is fine.", "If you wish it to have other dimensions alter the output size of the Dense layer.", "From x = tf.keras.layers.Dense(K)(x) to x = tf.keras.layers.Dense(D)(x)", "The question if the model does/doesn't predict properly depend on multiple issues such as the training data, hyper-parameter etc."], "sent_idxs": [101, 2017, 2323, 3477, 3086, 2000, 1996, 2944, 1012, 102, 101, 2044, 1996, 29300, 2078, 6741, 2017, 2109, 1037, 9742, 6741, 2073, 1996, 6434, 9812, 2003, 1016, 999, 102, 101, 2061, 1996, 2946, 1997, 1996, 6434, 2017, 2131, 2011, 2770, 1996, 2944, 1012, 16014, 1006, 1007, 2003, 2986, 1012, 102, 101, 2065, 2017, 4299, 2009, 2000, 2031, 2060, 9646, 11477, 1996, 6434, 2946, 1997, 1996, 9742, 6741, 1012, 102, 101, 2013, 1060, 1027, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 1047, 1007, 1006, 1060, 1007, 2000, 1060, 1027, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 1040, 1007, 1006, 1060, 1007, 102, 101, 1996, 3160, 2065, 1996, 2944, 2515, 1013, 2987, 1005, 1056, 16014, 7919, 12530, 2006, 3674, 3314, 2107, 2004, 1996, 2731, 2951, 1010, 23760, 1011, 16381, 4385, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 29, 50, 69, 107, 136], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35675700", "vertexSet": [[{"sent_id": 3, "name": "tf.nn", "pos": [138, 143]}], [{"sent_id": 2, "name": "tf.train", "pos": [48, 52]}, {"sent_id": 3, "name": "tf.train", "pos": [107, 111]}], [{"sent_id": 3, "name": "tf.train.saver", "pos": [107, 114]}], [{"sent_id": 3, "name": "tf.all_variables", "pos": [119, 125]}], [{"sent_id": 2, "name": "tf.train.string_input_producer", "pos": [48, 58]}]], "sents": ["TL;DR: In cifar10_eval.py, change the saver constructor so that it is:", "<code>Code Snippet</code>.", "This problem arises because tf.train.string_input_producer() internally creates a variable (called \"input_producer/limit_epochs/epochs\") when its num_epochs argument is not None.", "When, in cifar10_eval.py a tf.train.Saver is created, it uses tf.all_variables(), which includes the implicitly-created variable from the tf.nn.string_input_producer().", "This list of variables determines the set of names that TensorFlow looks up in the checkpoint file.", "Currently there isn't a great way to refer to implicitly created variables, other than by their name.", "Therefore, the best fix is to exclude the variable from the Saver constructor by name."], "sent_idxs": [101, 1056, 2140, 1025, 2852, 1024, 1999, 25022, 14971, 10790, 1035, 9345, 2140, 1012, 1052, 2100, 1010, 2689, 1996, 3828, 2099, 9570, 2953, 2061, 2008, 2009, 2003, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 3291, 18653, 2138, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 16058, 9005, 1037, 8023, 1006, 2170, 1000, 7953, 1035, 3135, 1013, 5787, 1035, 25492, 2015, 1013, 25492, 2015, 1000, 1007, 2043, 2049, 16371, 2213, 1035, 25492, 2015, 6685, 2003, 2025, 3904, 1012, 102, 101, 2043, 1010, 1999, 25022, 14971, 10790, 1035, 9345, 2140, 1012, 1052, 2100, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2003, 2580, 1010, 2009, 3594, 1056, 2546, 1012, 2035, 1035, 10857, 1006, 1007, 1010, 2029, 2950, 1996, 24655, 2135, 1011, 2580, 8023, 2013, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 1012, 102, 101, 2023, 2862, 1997, 10857, 16463, 1996, 2275, 1997, 3415, 2008, 23435, 12314, 3504, 2039, 1999, 1996, 26520, 5371, 1012, 102, 101, 2747, 2045, 3475, 1005, 1056, 1037, 2307, 2126, 2000, 6523, 2000, 24655, 2135, 2580, 10857, 1010, 2060, 2084, 2011, 2037, 2171, 1012, 102, 101, 3568, 1010, 1996, 2190, 8081, 2003, 2000, 23329, 1996, 8023, 2013, 1996, 3828, 2099, 9570, 2953, 2011, 2171, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 29, 43, 93, 153, 174, 198, 219], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51138713", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [22, 28]}], [{"sent_id": 0, "name": "tf.contrib.estimator", "pos": [22, 32]}], [{"sent_id": 0, "name": "tf.contrib.estimator.clip_gradients_by_norm", "pos": [22, 41]}]], "sents": ["IMO the best solution is wrapping your optimizer with TF's estimator decorator tf.contrib.estimator.clip_gradients_by_norm:", "<code>Code Snippet</code>.", "This way you only have to define this once, and not run it after every gradients calculation.", "Documentation:\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/estimator/clip_gradients_by_norm"], "sent_idxs": [101, 10047, 2080, 1996, 2190, 5576, 2003, 12252, 2115, 23569, 27605, 6290, 2007, 1056, 2546, 1005, 1055, 9765, 9581, 4263, 25545, 8844, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9765, 9581, 4263, 1012, 12528, 1035, 17978, 2015, 1035, 2011, 1035, 13373, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2126, 2017, 2069, 2031, 2000, 9375, 2023, 2320, 1010, 1998, 2025, 2448, 2009, 2044, 2296, 17978, 2015, 17208, 1012, 102, 101, 12653, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 9530, 18886, 2497, 1013, 9765, 9581, 4263, 1013, 12528, 1035, 17978, 2015, 1035, 2011, 1035, 13373, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 43, 57, 79, 120], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48404009", "vertexSet": [[{"sent_id": 0, "name": "tf.tile", "pos": [17, 21]}, {"sent_id": 2, "name": "tf.tile", "pos": [46, 50]}], [{"sent_id": 0, "name": "tf.concat", "pos": [11, 16]}, {"sent_id": 4, "name": "tf.concat", "pos": [112, 117]}], [{"sent_id": 0, "name": "tf.expand_dims", "pos": [22, 29]}, {"sent_id": 3, "name": "tf.expand_dims", "pos": [92, 99]}]], "sents": ["You can obtain the desired output with a combination of tf.concat, tf.tile and tf.expand_dims:", "<code>Code Snippet</code>.", "tf.tile repeats the first element of _in creating a tensor of length len(_in)-1 (I compute separately the shape of the tile because we want to tile only on the first dimension).", "tf.expand_dims adds a dimension we can then concat on", "Finally, tf.concat stitches together the two tensors giving the desired result.", "EDIT: Rewrote to fit the OP's actual use-case with multidimensional tensors."], "sent_idxs": [101, 2017, 2064, 6855, 1996, 9059, 6434, 2007, 1037, 5257, 1997, 1056, 2546, 1012, 9530, 11266, 1010, 1056, 2546, 1012, 14090, 1998, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 14090, 17993, 1996, 2034, 5783, 1997, 1035, 1999, 4526, 1037, 23435, 1997, 3091, 18798, 1006, 1035, 1999, 1007, 1011, 1015, 1006, 1045, 24134, 10329, 1996, 4338, 1997, 1996, 14090, 2138, 2057, 2215, 2000, 14090, 2069, 2006, 1996, 2034, 9812, 1007, 1012, 102, 101, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 9909, 1037, 9812, 2057, 2064, 2059, 9530, 11266, 2006, 102, 101, 2633, 1010, 1056, 2546, 1012, 9530, 11266, 25343, 2362, 1996, 2048, 23435, 2015, 3228, 1996, 9059, 2765, 1012, 102, 101, 10086, 1024, 2128, 13088, 12184, 2000, 4906, 1996, 6728, 1005, 1055, 5025, 2224, 1011, 2553, 2007, 4800, 22172, 6132, 19301, 23435, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 45, 91, 109, 129, 154], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50933191", "vertexSet": [[{"sent_id": 6, "name": "tf.where", "pos": [128, 132]}], [{"sent_id": 2, "name": "tf.dynamic_stitch", "pos": [70, 76]}], [{"sent_id": 2, "name": "tf.dynamic_partition", "pos": [63, 69]}]], "sents": ["If you want softmax computed + applied only to elements T > 0:.", "An idea could be to create 2 partitions based on your condition (T > 0), apply the operation (softmax) to the target partition, then stitch them back together.", "Something like this, using tf.dynamic_partition and tf.dynamic_stitch:", "<code>Code Snippet</code>.", "Previous answer.", "This answer is valid only if you want softmax to be computed over all elements of T but applied only to those greater than 0.", "Using tf.where():", "<code>Code Snippet</code>."], "sent_idxs": [101, 2065, 2017, 2215, 3730, 17848, 24806, 1009, 4162, 2069, 2000, 3787, 1056, 1028, 1014, 1024, 1012, 102, 101, 2019, 2801, 2071, 2022, 2000, 3443, 1016, 13571, 2015, 2241, 2006, 2115, 4650, 1006, 1056, 1028, 1014, 1007, 1010, 6611, 1996, 3169, 1006, 3730, 17848, 1007, 2000, 1996, 4539, 13571, 1010, 2059, 26035, 2068, 2067, 2362, 1012, 102, 101, 2242, 2066, 2023, 1010, 2478, 1056, 2546, 1012, 8790, 1035, 13571, 1998, 1056, 2546, 1012, 8790, 1035, 26035, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3025, 3437, 1012, 102, 101, 2023, 3437, 2003, 9398, 2069, 2065, 2017, 2215, 3730, 17848, 2000, 2022, 24806, 2058, 2035, 3787, 1997, 1056, 2021, 4162, 2069, 2000, 2216, 3618, 2084, 1014, 1012, 102, 101, 2478, 1056, 2546, 1012, 2073, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 18, 57, 78, 92, 97, 126, 136, 150], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49826755", "vertexSet": [[{"sent_id": 2, "name": "tf.user_ops", "pos": [56, 62]}, {"sent_id": 7, "name": "tf.user_ops", "pos": [207, 213]}], [{"sent_id": 0, "name": "tf.load_op_library", "pos": [6, 14]}], [{"sent_id": 2, "name": "tf.user_ops.my_fact", "pos": [56, 66]}]], "sents": ["Loading custom op libraries via tf.load_op_library() is not supported on Windows (at least with TensorFlow 1.8).", "The workaround is to add your custom op into the TensorFlow library itself.", "Follow the example of tf.user_ops.my_fact implemented in tensorflow\\tensorflow\\core\\user_ops\\fact.cc:", "Put your C++ implementation in tensorflow\\tensorflow\\core\\user_ops.", "Add python binding in tensorflow\\tensorflow\\python\\user_ops.", "Compile TensorFlow (read  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake).", "Replace tensorflow directory in your Conda environment with \ntensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow.", "Your new op function will be imported in tf.user_ops."], "sent_idxs": [101, 10578, 7661, 6728, 8860, 3081, 1056, 2546, 1012, 7170, 1035, 6728, 1035, 3075, 1006, 1007, 2003, 2025, 3569, 2006, 3645, 1006, 2012, 2560, 2007, 23435, 12314, 1015, 1012, 1022, 1007, 1012, 102, 101, 1996, 2147, 24490, 2003, 2000, 5587, 2115, 7661, 6728, 2046, 1996, 23435, 12314, 3075, 2993, 1012, 102, 101, 3582, 1996, 2742, 1997, 1056, 2546, 1012, 5310, 1035, 23092, 1012, 2026, 1035, 2755, 7528, 1999, 23435, 12314, 1032, 23435, 12314, 1032, 4563, 1032, 5310, 1035, 23092, 1032, 2755, 1012, 10507, 1024, 102, 101, 2404, 2115, 1039, 1009, 1009, 7375, 1999, 23435, 12314, 1032, 23435, 12314, 1032, 4563, 1032, 5310, 1035, 23092, 1012, 102, 101, 5587, 18750, 8031, 1999, 23435, 12314, 1032, 23435, 12314, 1032, 18750, 1032, 5310, 1035, 23092, 1012, 102, 101, 4012, 22090, 23435, 12314, 1006, 3191, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3392, 1013, 3040, 1013, 23435, 12314, 1013, 9530, 18886, 2497, 1013, 4642, 13808, 1007, 1012, 102, 101, 5672, 23435, 12314, 14176, 1999, 2115, 9530, 2850, 4044, 2007, 23435, 12314, 1032, 23435, 12314, 1032, 9530, 18886, 2497, 1032, 4642, 13808, 1032, 3857, 1032, 1056, 2546, 1035, 18750, 1032, 23435, 12314, 1012, 102, 101, 2115, 2047, 6728, 3853, 2097, 2022, 10964, 1999, 1056, 2546, 1012, 5310, 1035, 23092, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 51, 85, 106, 124, 163, 198, 215], "sent_pos": [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "45173993", "vertexSet": [[{"sent_id": 4, "name": "tf.abs", "pos": [153, 157]}], [{"sent_id": 1, "name": "tf.reshape", "pos": [45, 51]}], [{"sent_id": 4, "name": "tf.transpose", "pos": [169, 174]}], [{"sent_id": 4, "name": "tf.reduce_mean", "pos": [160, 166]}]], "sents": ["Is there a proper way to reshape tensors...", "If you are using Keras you should use the K.reshape(x,shape) method, which is a wrapper for tf.reshape(x,shape) as we can see in the docs.", "I also notice you are using get_shape() to obtain your tensor shape, when on Keras you can do this with K.int_shape(x) as also mentioned in the docs, like this:", "<code>Code Snippet</code>.", "Besides that there are several other operations you do directly calling your Tensorflow import, instead of the Keras Backend (like tf.abs(), tf.reduce_mean(), tf.transpose(), etc.).", "You should consider using its corresponding wrappers in the keras backend to have uniform notation and guarantee a more regular behaviour.", "Also, by using the Keras backend you are giving your program compatibility with both Theano and Tensorflow, so it is a big plus you should consider.", "Additionally, some TypeError may appear when working with tensors with undefined dimension(s).", "Please take a look at this question where they explain about reshaping tensors with undefined dimensions.", "Also, for its equivalent in Keras, check this other question, where in an answer I explain how to achieve that using Keras with Tensorflow as backend.", "...Now regarding your code.", "Basically, as you have some undefined dimensions, you can pass the value -1 to have it infer the shape no matter what size it could be (it is explained in the first linked question, but can also be seen in the docs).", "Something like:", "<code>Code Snippet</code>.", "Or using Keras backend:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2003, 2045, 1037, 5372, 2126, 2000, 24501, 3270, 5051, 23435, 2015, 1012, 1012, 1012, 102, 101, 2065, 2017, 2024, 2478, 17710, 8180, 2017, 2323, 2224, 1996, 1047, 1012, 24501, 3270, 5051, 1006, 1060, 1010, 4338, 1007, 4118, 1010, 2029, 2003, 1037, 10236, 4842, 2005, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1060, 1010, 4338, 1007, 2004, 2057, 2064, 2156, 1999, 1996, 9986, 2015, 1012, 102, 101, 1045, 2036, 5060, 2017, 2024, 2478, 2131, 1035, 4338, 1006, 1007, 2000, 6855, 2115, 23435, 4338, 1010, 2043, 2006, 17710, 8180, 2017, 2064, 2079, 2023, 2007, 1047, 1012, 20014, 1035, 4338, 1006, 1060, 1007, 2004, 2036, 3855, 1999, 1996, 9986, 2015, 1010, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 4661, 2008, 2045, 2024, 2195, 2060, 3136, 2017, 2079, 3495, 4214, 2115, 23435, 12314, 12324, 1010, 2612, 1997, 1996, 17710, 8180, 2067, 10497, 1006, 2066, 1056, 2546, 1012, 14689, 1006, 1007, 1010, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1007, 1010, 1056, 2546, 1012, 9099, 20688, 1006, 1007, 1010, 4385, 1012, 1007, 1012, 102, 101, 2017, 2323, 5136, 2478, 2049, 7978, 10236, 7347, 1999, 1996, 17710, 8180, 2067, 10497, 2000, 2031, 6375, 14869, 1998, 11302, 1037, 2062, 3180, 9164, 1012, 102, 101, 2036, 1010, 2011, 2478, 1996, 17710, 8180, 2067, 10497, 2017, 2024, 3228, 2115, 2565, 21778, 2007, 2119, 1996, 6761, 1998, 23435, 12314, 1010, 2061, 2009, 2003, 1037, 2502, 4606, 2017, 2323, 5136, 1012, 102, 101, 5678, 1010, 2070, 2828, 2121, 29165, 2089, 3711, 2043, 2551, 2007, 23435, 2015, 2007, 6151, 28344, 9812, 1006, 1055, 1007, 1012, 102, 101, 3531, 2202, 1037, 2298, 2012, 2023, 3160, 2073, 2027, 4863, 2055, 24501, 3270, 4691, 23435, 2015, 2007, 6151, 28344, 9646, 1012, 102, 101, 2036, 1010, 2005, 2049, 5662, 1999, 17710, 8180, 1010, 4638, 2023, 2060, 3160, 1010, 2073, 1999, 2019, 3437, 1045, 4863, 2129, 2000, 6162, 2008, 2478, 17710, 8180, 2007, 23435, 12314, 2004, 2067, 10497, 1012, 102, 101, 1012, 1012, 1012, 2085, 4953, 2115, 3642, 1012, 102, 101, 10468, 1010, 2004, 2017, 2031, 2070, 6151, 28344, 9646, 1010, 2017, 2064, 3413, 1996, 3643, 1011, 1015, 2000, 2031, 2009, 1999, 7512, 1996, 4338, 2053, 3043, 2054, 2946, 2009, 2071, 2022, 1006, 2009, 2003, 4541, 1999, 1996, 2034, 5799, 3160, 1010, 2021, 2064, 2036, 2022, 2464, 1999, 1996, 9986, 2015, 1007, 1012, 102, 101, 2242, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 2478, 17710, 8180, 2067, 10497, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 66, 113, 127, 182, 209, 244, 267, 290, 326, 336, 390, 395, 409, 418, 432], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44903769", "vertexSet": [[{"sent_id": 6, "name": "tf.graph", "pos": [168, 172]}], [{"sent_id": 4, "name": "tf.train", "pos": [101, 105]}], [{"sent_id": 1, "name": "tf.get_default_graph", "pos": [33, 41]}], [{"sent_id": 4, "name": "tf.train.import_meta_graph", "pos": [101, 111]}]], "sents": ["In Tensorflow, you are constructing graphs.", "By default, Tensorflow creates a default (sorry for tautology) graph (which you could access using tf.get_default_graph()).", "By default, any new Session object uses this default graph.", "In your case, you already have a graph (which is a default one), and you also saved exactly this graph into meta file.", "Then, you are trying to recover this graph using tf.train.import_meta_graph().", "However, since your session uses a default graph, and you are trying to recover an identical one, you are encountering an error since this operation is trying to duplicate the nodes, which is forbidden.", "When you explicitly create a new graph object by calling tf.Graph() and create a Session object using this graph (but not the default one) everything is fine since the nodes are created in another graph."], "sent_idxs": [101, 1999, 23435, 12314, 1010, 2017, 2024, 15696, 19287, 1012, 102, 101, 2011, 12398, 1010, 23435, 12314, 9005, 1037, 12398, 1006, 3374, 2005, 21642, 6779, 1007, 10629, 1006, 2029, 2017, 2071, 3229, 2478, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1007, 1012, 102, 101, 2011, 12398, 1010, 2151, 2047, 5219, 4874, 3594, 2023, 12398, 10629, 1012, 102, 101, 1999, 2115, 2553, 1010, 2017, 2525, 2031, 1037, 10629, 1006, 2029, 2003, 1037, 12398, 2028, 1007, 1010, 1998, 2017, 2036, 5552, 3599, 2023, 10629, 2046, 18804, 5371, 1012, 102, 101, 2059, 1010, 2017, 2024, 2667, 2000, 8980, 2023, 10629, 2478, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 1012, 102, 101, 2174, 1010, 2144, 2115, 5219, 3594, 1037, 12398, 10629, 1010, 1998, 2017, 2024, 2667, 2000, 8980, 2019, 7235, 2028, 1010, 2017, 2024, 8087, 2075, 2019, 7561, 2144, 2023, 3169, 2003, 2667, 2000, 24473, 1996, 14164, 1010, 2029, 2003, 10386, 1012, 102, 101, 2043, 2017, 12045, 3443, 1037, 2047, 10629, 4874, 2011, 4214, 1056, 2546, 1012, 10629, 1006, 1007, 1998, 3443, 1037, 5219, 4874, 2478, 2023, 10629, 1006, 2021, 2025, 1996, 12398, 2028, 1007, 2673, 2003, 2986, 2144, 1996, 14164, 2024, 2580, 1999, 2178, 10629, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 11, 46, 60, 90, 115, 157, 202], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58335974", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [4, 8]}], [{"sent_id": 2, "name": "tf.keras", "pos": [102, 107]}], [{"sent_id": 0, "name": "tf.convert_to_tensor", "pos": [15, 23]}, {"sent_id": 0, "name": "tf.convert_to_tensor", "pos": [29, 37]}]], "sents": ["You can use tf.data.from_tensor_slices(tf.convert_to_tensor(np_images), tf.convert_to_tensor(np_labels)) to create a dataset consisting (image, label) pairs.", "Now you can apply .batch(BATCHSIZE) function on this which will create batches of size you want.", "Then you can just directly feed this to the model function like .fit() if using tf.keras.", "For more info., go through following links-", "from_tensor_slices", "convert_to_tensor", "batch"], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 2951, 1012, 2013, 1035, 23435, 1035, 25609, 1006, 1056, 2546, 1012, 10463, 1035, 2000, 1035, 23435, 1006, 27937, 1035, 4871, 1007, 1010, 1056, 2546, 1012, 10463, 1035, 2000, 1035, 23435, 1006, 27937, 1035, 10873, 1007, 1007, 2000, 3443, 1037, 2951, 13462, 5398, 1006, 3746, 1010, 3830, 1007, 7689, 1012, 102, 101, 2085, 2017, 2064, 6611, 1012, 14108, 1006, 14108, 5332, 4371, 1007, 3853, 2006, 2023, 2029, 2097, 3443, 14108, 2229, 1997, 2946, 2017, 2215, 1012, 102, 101, 2059, 2017, 2064, 2074, 3495, 5438, 2023, 2000, 1996, 2944, 3853, 2066, 1012, 4906, 1006, 1007, 2065, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 2005, 2062, 18558, 1012, 1010, 2175, 2083, 2206, 6971, 1011, 102, 101, 2013, 1035, 23435, 1035, 25609, 102, 101, 10463, 1035, 2000, 1035, 23435, 102, 101, 14108, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 57, 83, 109, 121, 128, 135, 138], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35749825", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [110, 114]}], [{"sent_id": 1, "name": "tf.variable", "pos": [51, 55]}, {"sent_id": 1, "name": "tf.variable", "pos": [60, 64]}], [{"sent_id": 1, "name": "tf.placeholder", "pos": [37, 42]}], [{"sent_id": 2, "name": "tf.initialize_all_variables", "pos": [93, 102]}], [{"sent_id": 2, "name": "tf.train.start_queue_runners", "pos": [110, 120]}]], "sents": ["A solution is mentionned here:\nhttp://andyljones.tumblr.com/.", "You have to change your placeholder is_train = tf.placeholder(tf.int32) for a tf.Variable:is_train =  tf.Variable(True,  name='training').", "So you will be able to initialize through your call at sess.run(tf.initialize_all_variables()) before making a call too tf.train.start_queue_runners(sess=sess)."], "sent_idxs": [101, 1037, 5576, 2003, 5254, 7228, 2182, 1024, 8299, 1024, 1013, 1013, 5557, 2140, 14339, 2229, 1012, 10722, 14905, 20974, 1012, 4012, 1013, 1012, 102, 101, 2017, 2031, 2000, 2689, 2115, 2173, 14528, 2003, 1035, 3345, 1027, 1056, 2546, 1012, 2173, 14528, 1006, 1056, 2546, 1012, 20014, 16703, 1007, 2005, 1037, 1056, 2546, 1012, 8023, 1024, 2003, 1035, 3345, 1027, 1056, 2546, 1012, 8023, 1006, 2995, 1010, 2171, 1027, 1005, 2731, 1005, 1007, 1012, 102, 101, 2061, 2017, 2097, 2022, 2583, 2000, 3988, 4697, 2083, 2115, 2655, 2012, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 1007, 2077, 2437, 1037, 2655, 2205, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 7367, 4757, 1027, 7367, 4757, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 25, 75, 129], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35296384", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [31, 36]}, {"sent_id": 2, "name": "tf.nn", "pos": [112, 117]}], [{"sent_id": 0, "name": "tf.gather", "pos": [50, 54]}], [{"sent_id": 5, "name": "tf.reduce_sum", "pos": [245, 251]}], [{"sent_id": 0, "name": "tf.nn.embedding_lookup", "pos": [31, 43]}, {"sent_id": 2, "name": "tf.nn.embedding_lookup", "pos": [112, 124]}]], "sents": ["The shape error arises because you are using a two-dimensional tensor, x to index into a two-dimensional embedding tensor W. Think of tf.nn.embedding_lookup() (and its close cousin tf.gather()) as taking each integer value i in x and replacing it with the row W[i, :].", "From the error message, one can infer that n_input = 300 and embedding_size = 128.", "In general, the result of tf.nn.embedding_lookup() number of dimensions equal to rank(x) + rank(W) - 1\u2026 in this case, 3.", "The error arises when you try to multiply this result by _weights['h1'], which is a (two-dimensional) matrix.", "To fix this code, it depends on what you're trying to do, and why you are passing in a matrix of inputs to the embedding.", "One common thing to do is to aggregate the embedding vectors for each input example into a single row per example using an operation like tf.reduce_sum().", "For example, you might do the following:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 4338, 7561, 18653, 2138, 2017, 2024, 2478, 1037, 2048, 1011, 8789, 23435, 1010, 1060, 2000, 5950, 2046, 1037, 2048, 1011, 8789, 7861, 8270, 4667, 23435, 1059, 1012, 2228, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 7861, 8270, 4667, 1035, 2298, 6279, 1006, 1007, 1006, 1998, 2049, 2485, 5542, 1056, 2546, 1012, 8587, 1006, 1007, 1007, 2004, 2635, 2169, 16109, 3643, 1045, 1999, 1060, 1998, 6419, 2009, 2007, 1996, 5216, 1059, 1031, 1045, 1010, 1024, 1033, 1012, 102, 101, 2013, 1996, 7561, 4471, 1010, 2028, 2064, 1999, 7512, 2008, 1050, 1035, 7953, 1027, 3998, 1998, 7861, 8270, 4667, 1035, 2946, 1027, 11899, 1012, 102, 101, 1999, 2236, 1010, 1996, 2765, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 7861, 8270, 4667, 1035, 2298, 6279, 1006, 1007, 2193, 1997, 9646, 5020, 2000, 4635, 1006, 1060, 1007, 1009, 4635, 1006, 1059, 1007, 1011, 1015, 1529, 1999, 2023, 2553, 1010, 1017, 1012, 102, 101, 1996, 7561, 18653, 2043, 2017, 3046, 2000, 4800, 22086, 2023, 2765, 2011, 1035, 15871, 1031, 1005, 1044, 2487, 1005, 1033, 1010, 2029, 2003, 1037, 1006, 2048, 1011, 8789, 1007, 8185, 1012, 102, 101, 2000, 8081, 2023, 3642, 1010, 2009, 9041, 2006, 2054, 2017, 1005, 2128, 2667, 2000, 2079, 1010, 1998, 2339, 2017, 2024, 4458, 1999, 1037, 8185, 1997, 20407, 2000, 1996, 7861, 8270, 4667, 1012, 102, 101, 2028, 2691, 2518, 2000, 2079, 2003, 2000, 9572, 1996, 7861, 8270, 4667, 19019, 2005, 2169, 7953, 2742, 2046, 1037, 2309, 5216, 2566, 2742, 2478, 2019, 3169, 2066, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1007, 1012, 102, 101, 2005, 2742, 1010, 2017, 2453, 2079, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 79, 105, 150, 183, 217, 255, 266, 280], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42218732", "vertexSet": [[{"sent_id": 1, "name": "tf.subtract", "pos": [45, 51]}], [{"sent_id": 1, "name": "tf.multiply", "pos": [39, 44]}, {"sent_id": 2, "name": "tf.multiply", "pos": [71, 76]}], [{"sent_id": 1, "name": "tf.negative", "pos": [52, 56]}]], "sents": ["According to the tensorflow 1.0.0 release notes,", "tf.mul, tf.sub and tf.neg are deprecated in favor of tf.multiply, tf.subtract and tf.negative.", "You'll need to replace tf.mul with tf.multiply."], "sent_idxs": [101, 2429, 2000, 1996, 23435, 12314, 1015, 1012, 1014, 1012, 1014, 2713, 3964, 1010, 102, 101, 1056, 2546, 1012, 14163, 2140, 1010, 1056, 2546, 1012, 4942, 1998, 1056, 2546, 1012, 11265, 2290, 2024, 2139, 28139, 12921, 1999, 5684, 1997, 1056, 2546, 1012, 4800, 22086, 1010, 1056, 2546, 1012, 4942, 6494, 6593, 1998, 1056, 2546, 1012, 4997, 1012, 102, 101, 2017, 1005, 2222, 2342, 2000, 5672, 1056, 2546, 1012, 14163, 2140, 2007, 1056, 2546, 1012, 4800, 22086, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 58, 78], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0]}, {"title": "49668187", "vertexSet": [[{"sent_id": 6, "name": "tf.where", "pos": [100, 104]}, {"sent_id": 10, "name": "tf.where", "pos": [174, 178]}], [{"sent_id": 6, "name": "tf.concat", "pos": [94, 99]}, {"sent_id": 8, "name": "tf.concat", "pos": [150, 155]}], [{"sent_id": 6, "name": "tf.dynamic_stitch", "pos": [112, 118]}], [{"sent_id": 6, "name": "tf.dynamic_partition", "pos": [105, 111]}]], "sents": ["Since v1.1, Tensorflow covers such Numpy-like indexing, see Tensor.getitem.", "<code>Code Snippet</code>.", "EDIT after comment:", "Now, the problem is the \"*=\" part i.e.", "item assignment.", "This isn't a straightforward operation in Tensorflow.", "In your case however, this can be easily solved using tf.concat or tf.where (tf.dynamic_partition + tf.dynamic_stitch could be used for more complex cases).", "Find below a quick implementation of the two first solutions.", "Solution using Tensor.getitem and tf.concat:", "<code>Code Snippet</code>.", "Solution using tf.where:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2144, 1058, 2487, 1012, 1015, 1010, 23435, 12314, 4472, 2107, 16371, 8737, 2100, 1011, 2066, 5950, 2075, 1010, 2156, 23435, 1012, 2131, 4221, 2213, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10086, 2044, 7615, 1024, 102, 101, 2085, 1010, 1996, 3291, 2003, 1996, 1000, 1008, 1027, 1000, 2112, 1045, 1012, 1041, 1012, 102, 101, 8875, 8775, 1012, 102, 101, 2023, 3475, 1005, 1056, 1037, 19647, 3169, 1999, 23435, 12314, 1012, 102, 101, 1999, 2115, 2553, 2174, 1010, 2023, 2064, 2022, 4089, 13332, 2478, 1056, 2546, 1012, 9530, 11266, 2030, 1056, 2546, 1012, 2073, 1006, 1056, 2546, 1012, 8790, 1035, 13571, 1009, 1056, 2546, 1012, 8790, 1035, 26035, 2071, 2022, 2109, 2005, 2062, 3375, 3572, 1007, 1012, 102, 101, 2424, 2917, 1037, 4248, 7375, 1997, 1996, 2048, 2034, 7300, 1012, 102, 101, 5576, 2478, 23435, 1012, 2131, 4221, 2213, 1998, 1056, 2546, 1012, 9530, 11266, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5576, 2478, 1056, 2546, 1012, 2073, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 27, 41, 47, 64, 69, 82, 128, 141, 157, 171, 180, 194], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53541695", "vertexSet": [[{"sent_id": 7, "name": "tf.keras", "pos": [159, 164]}], [{"sent_id": 7, "name": "tf.keras.applications", "pos": [159, 166]}], [{"sent_id": 7, "name": "tf.keras.applications.vgg16", "pos": [159, 170]}]], "sents": ["Getting the graph.", "You can get the graph from Keras with:", "<code>Code Snippet</code>.", "You can probably pass it to import_graph_def, but I suspect it's already Tensorflow's default graph, since in the link below, the creator of Keras says there is only one graph.", "More in: https://github.com/keras-team/keras/issues/3223", "Working suggestion.", "I don't know what you're trying to achieve, but if the idea is using Keras regularly, you'd probably never need to grab the graph.", "In Keras, once you created your model with net = tf.keras.applications.VGG16(), you'd start using Keras methods from this model, such as:", "<code>Code Snippet</code>.", "Accessing weights and layers would be done by:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2893, 1996, 10629, 1012, 102, 101, 2017, 2064, 2131, 1996, 10629, 2013, 17710, 8180, 2007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2763, 3413, 2009, 2000, 12324, 1035, 10629, 1035, 13366, 1010, 2021, 1045, 8343, 2009, 1005, 1055, 2525, 23435, 12314, 1005, 1055, 12398, 10629, 1010, 2144, 1999, 1996, 4957, 2917, 1010, 1996, 8543, 1997, 17710, 8180, 2758, 2045, 2003, 2069, 2028, 10629, 1012, 102, 101, 2062, 1999, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 17710, 8180, 1011, 2136, 1013, 17710, 8180, 1013, 3314, 1013, 23768, 2509, 102, 101, 2551, 10293, 1012, 102, 101, 1045, 2123, 1005, 1056, 2113, 2054, 2017, 1005, 2128, 2667, 2000, 6162, 1010, 2021, 2065, 1996, 2801, 2003, 2478, 17710, 8180, 5570, 1010, 2017, 1005, 1040, 2763, 2196, 2342, 2000, 6723, 1996, 10629, 1012, 102, 101, 1999, 17710, 8180, 1010, 2320, 2017, 2580, 2115, 2944, 2007, 5658, 1027, 1056, 2546, 1012, 17710, 8180, 1012, 5097, 1012, 1058, 13871, 16048, 1006, 1007, 1010, 2017, 1005, 1040, 2707, 2478, 17710, 8180, 4725, 2013, 2023, 2944, 1010, 2107, 2004, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3229, 2075, 15871, 1998, 9014, 2052, 2022, 2589, 2011, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 6, 18, 32, 78, 105, 110, 146, 189, 203, 215, 229], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39938524", "vertexSet": [[{"sent_id": 4, "name": "tf.add_n", "pos": [200, 206]}], [{"sent_id": 0, "name": "tf.session", "pos": [42, 46]}], [{"sent_id": 0, "name": "tf.configproto", "pos": [51, 59]}]], "sents": ["There's a good chance you could get deterministic results if you run your network on CPU (export CUDA_VISIBLE_DEVICES=), with single-thread in Eigen thread pool (tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)), one Python thread (no multi-threaded queue-runners that you get from ops like tf.batch), and a single well-defined operation order.", "Also using inter_op_parallelism_threads=1 may help in some scenarios.", "One issue is that floating point addition/multiplication is non-associative, so one fool-proof way to get deterministic results is to use integer arithmetic or quantized values.", "Barring that, you could isolate which operation is non-deterministic, and try to avoid using that op.", "For instance, there's tf.add_n op, which doesn't say anything about the order in which it sums the values, but different orders produce different results.", "Getting deterministic results is a bit of an uphill battle because determinism is in conflict with performance, and performance is usually the goal that gets more attention.", "An alternative to trying to have exact same numbers on reruns is to focus on numerical stability -- if your algorithm is stable, then you will get reproducible results (ie, same number of misclassifications) even though exact parameter values may be slightly different"], "sent_idxs": [101, 2045, 1005, 1055, 1037, 2204, 3382, 2017, 2071, 2131, 28283, 25300, 10074, 3463, 2065, 2017, 2448, 2115, 2897, 2006, 17368, 1006, 9167, 12731, 2850, 1035, 5710, 1035, 5733, 1027, 1007, 1010, 2007, 2309, 1011, 11689, 1999, 1041, 29206, 11689, 4770, 1006, 1056, 2546, 1012, 5219, 1006, 9530, 8873, 2290, 1027, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 1006, 26721, 1035, 6728, 1035, 5903, 2964, 1035, 16457, 1027, 1015, 1007, 1007, 1010, 2028, 18750, 11689, 1006, 2053, 4800, 1011, 26583, 24240, 1011, 7190, 2008, 2017, 2131, 2013, 23092, 2066, 1056, 2546, 1012, 14108, 1007, 1010, 1998, 1037, 2309, 2092, 1011, 4225, 3169, 2344, 1012, 102, 101, 2036, 2478, 6970, 1035, 6728, 1035, 5903, 2964, 1035, 16457, 1027, 1015, 2089, 2393, 1999, 2070, 16820, 1012, 102, 101, 2028, 3277, 2003, 2008, 8274, 2391, 2804, 1013, 24856, 2003, 2512, 1011, 4632, 10085, 2401, 6024, 1010, 2061, 2028, 7966, 1011, 6947, 2126, 2000, 2131, 28283, 25300, 10074, 3463, 2003, 2000, 2224, 16109, 20204, 2030, 24110, 23355, 5300, 1012, 102, 101, 19820, 2075, 2008, 1010, 2017, 2071, 27152, 2029, 3169, 2003, 2512, 1011, 28283, 25300, 10074, 1010, 1998, 3046, 2000, 4468, 2478, 2008, 6728, 1012, 102, 101, 2005, 6013, 1010, 2045, 1005, 1055, 1056, 2546, 1012, 5587, 1035, 1050, 6728, 1010, 2029, 2987, 1005, 1056, 2360, 2505, 2055, 1996, 2344, 1999, 2029, 2009, 20571, 1996, 5300, 1010, 2021, 2367, 4449, 3965, 2367, 3463, 1012, 102, 101, 2893, 28283, 25300, 10074, 3463, 2003, 1037, 2978, 1997, 2019, 2039, 7100, 2645, 2138, 28283, 25300, 6491, 2003, 1999, 4736, 2007, 2836, 1010, 1998, 2836, 2003, 2788, 1996, 3125, 2008, 4152, 2062, 3086, 1012, 102, 101, 2019, 4522, 2000, 2667, 2000, 2031, 6635, 2168, 3616, 2006, 2128, 15532, 2015, 2003, 2000, 3579, 2006, 15973, 9211, 1011, 1011, 2065, 2115, 9896, 2003, 6540, 1010, 2059, 2017, 2097, 2131, 16360, 14127, 21104, 3463, 1006, 29464, 1010, 2168, 2193, 1997, 28616, 26266, 9031, 2015, 1007, 2130, 2295, 6635, 16381, 5300, 2089, 2022, 3621, 2367, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 106, 126, 167, 193, 232, 268, 325], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53056640", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [17, 23]}, {"sent_id": 0, "name": "tf.contrib", "pos": [41, 47]}], [{"sent_id": 0, "name": "tf.contrib.tpu", "pos": [17, 26]}, {"sent_id": 0, "name": "tf.contrib.tpu", "pos": [41, 50]}], [{"sent_id": 0, "name": "tf.contrib.tpu.keras_to_tpu_model", "pos": [17, 36]}], [{"sent_id": 0, "name": "tf.contrib.tpu.tpudistributionstrategy", "pos": [41, 59]}]], "sents": ["Another way to do this is to rewrite the model using Keras and use tf.contrib.tpu.keras_to_tpu_model(..) with tf.contrib.tpu.TPUDistributionStrategy(...).", "Here is small code snippet for this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2178, 2126, 2000, 2079, 2023, 2003, 2000, 2128, 26373, 1996, 2944, 2478, 17710, 8180, 1998, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1056, 14289, 1012, 17710, 8180, 1035, 2000, 1035, 1056, 14289, 1035, 2944, 1006, 1012, 1012, 1007, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1056, 14289, 1012, 1056, 14289, 10521, 18886, 29446, 20528, 2618, 6292, 1006, 1012, 1012, 1012, 1007, 1012, 102, 101, 2182, 2003, 2235, 3642, 1055, 3490, 29519, 2005, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 66, 78, 92], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50332692", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [79, 84]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [19, 25]}], [{"sent_id": 1, "name": "tf.contrib.rnn", "pos": [19, 28]}], [{"sent_id": 1, "name": "tf.nn.dynamic_rnn", "pos": [79, 89]}], [{"sent_id": 1, "name": "tf.contrib.rnn.outputprojectionwrapper", "pos": [19, 36]}]], "sents": ["Update for anyone trying to figure this out:", "There is a tensorflow function, tf.contrib.rnn.OutputProjectionWrapper, that seems to be specifically for attaching a dense layer to the output of an RNN cell, but wraps it up as part of the RNN cell itself that you can then unroll with a call to tf.nn.dynamic_rnn:", "<code>Code Snippet</code>.", "But more generally, if you want to operate on the outputs of the RNN, the usual practice seems to be to reshape the rnn_outputs by unrolling across the batch and time dimensions, performing your operations on that tensor, and rolling them back up for the final output."], "sent_idxs": [101, 10651, 2005, 3087, 2667, 2000, 3275, 2023, 2041, 1024, 102, 101, 2045, 2003, 1037, 23435, 12314, 3853, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 6434, 21572, 20614, 3258, 13088, 29098, 2121, 1010, 2008, 3849, 2000, 2022, 4919, 2005, 22476, 2075, 1037, 9742, 6741, 2000, 1996, 6434, 1997, 2019, 29300, 2078, 3526, 1010, 2021, 19735, 2009, 2039, 2004, 2112, 1997, 1996, 29300, 2078, 3526, 2993, 2008, 2017, 2064, 2059, 4895, 28402, 2007, 1037, 2655, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2021, 2062, 3227, 1010, 2065, 2017, 2215, 2000, 5452, 2006, 1996, 27852, 1997, 1996, 29300, 2078, 1010, 1996, 5156, 3218, 3849, 2000, 2022, 2000, 24501, 3270, 5051, 1996, 29300, 2078, 1035, 27852, 2011, 4895, 28402, 2075, 2408, 1996, 14108, 1998, 2051, 9646, 1010, 4488, 2115, 3136, 2006, 2008, 23435, 1010, 1998, 5291, 2068, 2067, 2039, 2005, 1996, 2345, 6434, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 11, 91, 105, 167], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51817722", "vertexSet": [[{"sent_id": 7, "name": "tf.nn", "pos": [170, 175]}], [{"sent_id": 4, "name": "tf.device", "pos": [135, 139]}, {"sent_id": 9, "name": "tf.device", "pos": [230, 234]}], [{"sent_id": 7, "name": "tf.matmul", "pos": [181, 187]}], [{"sent_id": 7, "name": "tf.nn.conv2d", "pos": [170, 180]}]], "sents": ["Multiple problems here:", "For one, Tensorflow has \"its own\" GPU numbering independent from the IDs on your machine.", "So when you pass CUDA_VISIBLE_DEVICES=2,3, Tensorflow will see those two GPUs, but they will be '/gpu:0' and '/gpu:1' in the program.", "Since neither '/gpu:2' nor '/gpu:3' exist, I suspect that all ops are simply put on '/gpu:0' or the CPU.", "However, the main problem is that this is not how you use with tf.device at all.", "You need to wrap the model creation into the context manager.", "I.e.", "all the op calls such as tf.nn.conv2d, tf.matmul etc.", "need to be wrapped.", "At the point you call model.fit, the ops have already been created (and put on '/gpu:0' by default) and your with tf.device statement does nothing."], "sent_idxs": [101, 3674, 3471, 2182, 1024, 102, 101, 2005, 2028, 1010, 23435, 12314, 2038, 1000, 2049, 2219, 1000, 14246, 2226, 15200, 2981, 2013, 1996, 8909, 2015, 2006, 2115, 3698, 1012, 102, 101, 2061, 2043, 2017, 3413, 12731, 2850, 1035, 5710, 1035, 5733, 1027, 1016, 1010, 1017, 1010, 23435, 12314, 2097, 2156, 2216, 2048, 14246, 2271, 1010, 2021, 2027, 2097, 2022, 1005, 1013, 14246, 2226, 1024, 1014, 1005, 1998, 1005, 1013, 14246, 2226, 1024, 1015, 1005, 1999, 1996, 2565, 1012, 102, 101, 2144, 4445, 1005, 1013, 14246, 2226, 1024, 1016, 1005, 4496, 1005, 1013, 14246, 2226, 1024, 1017, 1005, 4839, 1010, 1045, 8343, 2008, 2035, 23092, 2024, 3432, 2404, 2006, 1005, 1013, 14246, 2226, 1024, 1014, 1005, 2030, 1996, 17368, 1012, 102, 101, 2174, 1010, 1996, 2364, 3291, 2003, 2008, 2023, 2003, 2025, 2129, 2017, 2224, 2007, 1056, 2546, 1012, 5080, 2012, 2035, 1012, 102, 101, 2017, 2342, 2000, 10236, 1996, 2944, 4325, 2046, 1996, 6123, 3208, 1012, 102, 101, 1045, 1012, 1041, 1012, 102, 101, 2035, 1996, 6728, 4455, 2107, 2004, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1010, 1056, 2546, 1012, 13523, 12274, 2140, 4385, 1012, 102, 101, 2342, 2000, 2022, 5058, 1012, 102, 101, 2012, 1996, 2391, 2017, 2655, 2944, 1012, 4906, 1010, 1996, 23092, 2031, 2525, 2042, 2580, 1006, 1998, 2404, 2006, 1005, 1013, 14246, 2226, 1024, 1014, 1005, 2011, 12398, 1007, 1998, 2115, 2007, 1056, 2546, 1012, 5080, 4861, 2515, 2498, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 6, 30, 79, 120, 143, 157, 163, 190, 197, 239], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0]}, {"title": "57560621", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.conv2d_transpose", "pos": [12, 25]}, {"sent_id": 2, "name": "tf.nn.conv2d_transpose", "pos": [55, 68]}], [{"sent_id": 1, "name": "tf.layers.conv2d_transpose", "pos": [33, 45]}, {"sent_id": 3, "name": "tf.layers.conv2d_transpose", "pos": [82, 94]}, {"sent_id": 5, "name": "tf.layers.conv2d_transpose", "pos": [165, 177]}]], "sents": ["There is a significant difference between them.", "While tf.nn.conv2d_transpose represents an operation in the computational graph, tf.layers.conv2d_transpose defines the entire layer.", "Being more precise tf.nn.conv2d_transpose applies a convolutional filter to the inputs.", "tf.layers.conv2d_transpose, on the other hand, first creates trainable variables that will serve as filter according to the arguments given, and then it internally calls some conv2d_transpose operation.", "Based on the arguments, it also applies some other operations as adding bias, applying non-linearity, or normalizing the weights or inputs.", "With tf.layers.conv2d_transpose you do not specify output shape as it is computed from filter size, input size, and stride.", "Here is the formula."], "sent_idxs": [101, 2045, 2003, 1037, 3278, 4489, 2090, 2068, 1012, 102, 101, 2096, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 5836, 2019, 3169, 1999, 1996, 15078, 10629, 1010, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 11859, 1996, 2972, 6741, 1012, 102, 101, 2108, 2062, 10480, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 12033, 1037, 9530, 6767, 7630, 3508, 2389, 11307, 2000, 1996, 20407, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 1010, 2006, 1996, 2060, 2192, 1010, 2034, 9005, 3345, 3085, 10857, 2008, 2097, 3710, 2004, 11307, 2429, 2000, 1996, 9918, 2445, 1010, 1998, 2059, 2009, 16058, 4455, 2070, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 3169, 1012, 102, 101, 2241, 2006, 1996, 9918, 1010, 2009, 2036, 12033, 2070, 2060, 3136, 2004, 5815, 13827, 1010, 11243, 2512, 1011, 7399, 3012, 1010, 2030, 3671, 6026, 1996, 15871, 2030, 20407, 1012, 102, 101, 2007, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2017, 2079, 2025, 20648, 6434, 4338, 2004, 2009, 2003, 24806, 2013, 11307, 2946, 1010, 7953, 2946, 1010, 1998, 18045, 1012, 102, 101, 2182, 2003, 1996, 5675, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3, 4, 5]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3, 4, 5]}], "na_triple": [], "sent_ends": [0, 10, 51, 81, 132, 163, 198, 205], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46158415", "vertexSet": [[{"sent_id": 4, "name": "tf.group", "pos": [130, 134]}], [{"sent_id": 3, "name": "tf.graphkeys", "pos": [72, 78]}, {"sent_id": 4, "name": "tf.graphkeys", "pos": [112, 118]}], [{"sent_id": 4, "name": "tf.get_collection", "pos": [105, 111]}]], "sents": ["So I found a solution.", "Mainly using this reference: http://ruishu.io/2016/12/27/batchnorm/", "From the link:", "Note: When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in tf.GraphKeys.UPDATE_OPS so they need to be added as a dependency to the train_op, example:", "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) if update_ops: updates = tf.group(*update_ops) total_loss = control_flow_ops.with_dependencies([updates], total_loss)", "And to be straight to the point,\ninstead of creating the optimizer like so:", "<code>Code Snippet</code>.", "Do it like this:", "<code>Code Snippet</code>.", "That will solve the issue."], "sent_idxs": [101, 2061, 1045, 2179, 1037, 5576, 1012, 102, 101, 3701, 2478, 2023, 4431, 1024, 8299, 1024, 1013, 1013, 21766, 4509, 2226, 1012, 22834, 1013, 2355, 1013, 2260, 1013, 2676, 1013, 14108, 12131, 2213, 1013, 102, 101, 2013, 1996, 4957, 1024, 102, 101, 3602, 1024, 2043, 2003, 1035, 2731, 2003, 2995, 1996, 3048, 1035, 2812, 1998, 3048, 1035, 23284, 2342, 2000, 2022, 7172, 1010, 2011, 12398, 1996, 10651, 1035, 23092, 2024, 2872, 1999, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 2061, 2027, 2342, 2000, 2022, 2794, 2004, 1037, 24394, 2000, 1996, 3345, 1035, 6728, 1010, 2742, 1024, 102, 101, 10651, 1035, 23092, 1027, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 1007, 2065, 10651, 1035, 23092, 1024, 14409, 1027, 1056, 2546, 1012, 2177, 1006, 1008, 10651, 1035, 23092, 1007, 2561, 1035, 3279, 1027, 2491, 1035, 4834, 1035, 23092, 1012, 2007, 1035, 12530, 15266, 1006, 1031, 14409, 1033, 1010, 2561, 1035, 3279, 1007, 102, 101, 1998, 2000, 2022, 3442, 2000, 1996, 2391, 1010, 2612, 1997, 4526, 1996, 23569, 27605, 6290, 2066, 2061, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2079, 2009, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2008, 2097, 9611, 1996, 3277, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 8, 35, 41, 100, 164, 184, 198, 205, 219, 227], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37709422", "vertexSet": [[{"sent_id": 4, "name": "tf.train", "pos": [98, 102]}, {"sent_id": 9, "name": "tf.train", "pos": [204, 208]}, {"sent_id": 11, "name": "tf.train", "pos": [235, 239]}], [{"sent_id": 9, "name": "tf.train.feature", "pos": [204, 210]}], [{"sent_id": 4, "name": "tf.train.int64list", "pos": [98, 106]}, {"sent_id": 11, "name": "tf.train.int64list", "pos": [235, 243]}]], "sents": ["After a while of messing around with it and looking further in the documentation I found my own answer.", "In the above function using the example code as a base:", "<code>Code Snippet</code>.", "labels[index] is being cast to a list as [value] so you have [np.array([1,2,3])] which causes the error.", "The above cast was necessary in the example because tf.train.Int64List() expects either a list or numpy array and the example was passing in a single integer so they typecasted it to a list as so.", "In the example it was like this", "<code>Code Snippet</code>.", "If you want to pass in a list do this", "<code>Code Snippet</code>.", "I'll probably do a pull request because I found the original documentation for tf.train.Feature to be almost non-existent.", "TL;DR", "Pass either a list or numpy array to tf.train.Int64List() but not a list of lists or list of numpy arrays."], "sent_idxs": [101, 2044, 1037, 2096, 1997, 22308, 2105, 2007, 2009, 1998, 2559, 2582, 1999, 1996, 12653, 1045, 2179, 2026, 2219, 3437, 1012, 102, 101, 1999, 1996, 2682, 3853, 2478, 1996, 2742, 3642, 2004, 1037, 2918, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10873, 1031, 5950, 1033, 2003, 2108, 3459, 2000, 1037, 2862, 2004, 1031, 3643, 1033, 2061, 2017, 2031, 1031, 27937, 1012, 9140, 1006, 1031, 1015, 1010, 1016, 1010, 1017, 1033, 1007, 1033, 2029, 5320, 1996, 7561, 1012, 102, 101, 1996, 2682, 3459, 2001, 4072, 1999, 1996, 2742, 2138, 1056, 2546, 1012, 3345, 1012, 20014, 21084, 9863, 1006, 1007, 24273, 2593, 1037, 2862, 2030, 16371, 8737, 2100, 9140, 1998, 1996, 2742, 2001, 4458, 1999, 1037, 2309, 16109, 2061, 2027, 2828, 10526, 2098, 2009, 2000, 1037, 2862, 2004, 2061, 1012, 102, 101, 1999, 1996, 2742, 2009, 2001, 2066, 2023, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 2000, 3413, 1999, 1037, 2862, 2079, 2023, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 1005, 2222, 2763, 2079, 1037, 4139, 5227, 2138, 1045, 2179, 1996, 2434, 12653, 2005, 1056, 2546, 1012, 3345, 1012, 3444, 2000, 2022, 2471, 2512, 1011, 25953, 1012, 102, 101, 1056, 2140, 1025, 2852, 102, 101, 3413, 2593, 1037, 2862, 2030, 16371, 8737, 2100, 9140, 2000, 1056, 2546, 1012, 3345, 1012, 20014, 21084, 9863, 1006, 1007, 2021, 2025, 1037, 2862, 1997, 7201, 2030, 2862, 1997, 16371, 8737, 2100, 27448, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 36, 50, 88, 139, 148, 162, 174, 188, 218, 224, 260], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38998456", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [10, 16]}], [{"sent_id": 0, "name": "tf.contrib.layers", "pos": [10, 18]}], [{"sent_id": 0, "name": "tf.contrib.layers.batch_norm", "pos": [10, 22]}]], "sents": ["The link you provided has specified the function as tf.contrib.layers.batch_norm(*args, **kwargs).", "Looks like you should be able to pass beta and gamma as keyword arguments or **kwargs like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 4957, 2017, 3024, 2038, 9675, 1996, 3853, 2004, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 14108, 1035, 13373, 1006, 1008, 12098, 5620, 1010, 1008, 1008, 6448, 2906, 5620, 1007, 1012, 102, 101, 3504, 2066, 2017, 2323, 2022, 2583, 2000, 3413, 8247, 1998, 13091, 2004, 3145, 18351, 9918, 2030, 1008, 1008, 6448, 2906, 5620, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 35, 61, 75], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "33922859", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [5, 9]}, {"sent_id": 1, "name": "tf.train", "pos": [51, 55]}, {"sent_id": 1, "name": "tf.train", "pos": [64, 68]}, {"sent_id": 2, "name": "tf.train", "pos": [120, 124]}], [{"sent_id": 5, "name": "tf.variable", "pos": [185, 189]}], [{"sent_id": 1, "name": "tf.train.adamoptimizer", "pos": [64, 74]}], [{"sent_id": 1, "name": "tf.train.adagradoptimizer", "pos": [51, 62]}], [{"sent_id": 0, "name": "tf.train.gradientdescentoptimizer", "pos": [5, 17]}, {"sent_id": 2, "name": "tf.train.gradientdescentoptimizer", "pos": [120, 132]}]], "sents": ["First of all, tf.train.GradientDescentOptimizer is designed to use a constant learning rate for all variables in all steps.", "TensorFlow also provides out-of-the-box adaptive optimizers including the tf.train.AdagradOptimizer and the tf.train.AdamOptimizer, and these can be used as drop-in replacements.", "However, if you want to control the learning rate with otherwise-vanilla gradient descent, you can take advantage of the fact that the learning_rate argument to the tf.train.GradientDescentOptimizer constructor can be a Tensor object.", "This allows you to compute a different value for the learning rate in each step, for example:", "<code>Code Snippet</code>.", "Alternatively, you could create a scalar tf.Variable that holds the learning rate, and assign it each time you want to change the learning rate."], "sent_idxs": [101, 2034, 1997, 2035, 1010, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 2003, 2881, 2000, 2224, 1037, 5377, 4083, 3446, 2005, 2035, 10857, 1999, 2035, 4084, 1012, 102, 101, 23435, 12314, 2036, 3640, 2041, 1011, 1997, 1011, 1996, 1011, 3482, 19293, 23569, 27605, 16750, 2164, 1996, 1056, 2546, 1012, 3345, 1012, 15262, 16307, 7361, 3775, 4328, 6290, 1998, 1996, 1056, 2546, 1012, 3345, 1012, 4205, 7361, 3775, 4328, 6290, 1010, 1998, 2122, 2064, 2022, 2109, 2004, 4530, 1011, 1999, 23936, 1012, 102, 101, 2174, 1010, 2065, 2017, 2215, 2000, 2491, 1996, 4083, 3446, 2007, 4728, 1011, 21161, 17978, 6934, 1010, 2017, 2064, 2202, 5056, 1997, 1996, 2755, 2008, 1996, 4083, 1035, 3446, 6685, 2000, 1996, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 9570, 2953, 2064, 2022, 1037, 23435, 4874, 1012, 102, 101, 2023, 4473, 2017, 2000, 24134, 1037, 2367, 3643, 2005, 1996, 4083, 3446, 1999, 2169, 3357, 1010, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 14084, 1010, 2017, 2071, 3443, 1037, 26743, 2099, 1056, 2546, 1012, 8023, 2008, 4324, 1996, 4083, 3446, 1010, 1998, 23911, 2009, 2169, 2051, 2017, 2215, 2000, 2689, 1996, 4083, 3446, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 33, 87, 141, 162, 176, 209], "sent_pos": [0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44111564", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [9, 15]}], [{"sent_id": 0, "name": "tf.contrib.image", "pos": [9, 17]}], [{"sent_id": 3, "name": "tf.random_uniform", "pos": [86, 92]}], [{"sent_id": 0, "name": "tf.contrib.image.transform", "pos": [9, 19]}]], "sents": ["As an alternative, you could also use tf.contrib.image.transform() and use the parameters a2 and b2 to translate the image:", "<code>Code Snippet</code>.", "The transforms projection matrix can also be a tensor of size N x 8, so it is possible to shift every image of a batch differently.", "This can be easily extended by tf.random_uniform() to include some randomness to the x/y shift of each image.", "Edit:\nTo use random shifts for every image of the batch:", "<code>Code Snippet</code>.", "Edit 2:\nFor the sake of completion, here just another helper function with applies random rotation and shift to each single image of a batch:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 2019, 4522, 1010, 2017, 2071, 2036, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 10938, 1006, 1007, 1998, 2224, 1996, 11709, 22441, 1998, 1038, 2475, 2000, 17637, 1996, 3746, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 21743, 13996, 8185, 2064, 2036, 2022, 1037, 23435, 1997, 2946, 1050, 1060, 1022, 1010, 2061, 2009, 2003, 2825, 2000, 5670, 2296, 3746, 1997, 1037, 14108, 11543, 1012, 102, 101, 2023, 2064, 2022, 4089, 3668, 2011, 1056, 2546, 1012, 6721, 1035, 6375, 1006, 1007, 2000, 2421, 2070, 6721, 2791, 2000, 1996, 1060, 1013, 1061, 5670, 1997, 2169, 3746, 1012, 102, 101, 10086, 1024, 2000, 2224, 6721, 12363, 2005, 2296, 3746, 1997, 1996, 14108, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10086, 1016, 1024, 2005, 1996, 8739, 1997, 6503, 1010, 2182, 2074, 2178, 2393, 2121, 3853, 2007, 12033, 6721, 9963, 1998, 5670, 2000, 2169, 2309, 3746, 1997, 1037, 14108, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 35, 49, 79, 110, 125, 139, 170, 184], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48549654", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.dropout", "pos": [16, 24]}], [{"sent_id": 1, "name": "tf.layers.dropout", "pos": [39, 46]}]], "sents": ["Apart from the answers from @nikpod and @Salvador Dali", "The tf.nn.dropout scaled the weights by 1./keep prob during training phase, while tf.layers.dropout scaled the weights by 1./(1-rate).", "During evaluation, You could set the keep prob to 1 which is equivalent to set training to false."], "sent_idxs": [101, 4237, 2013, 1996, 6998, 2013, 1030, 23205, 27633, 1998, 1030, 10582, 29095, 102, 101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 18953, 1996, 15871, 2011, 1015, 1012, 1013, 2562, 4013, 2497, 2076, 2731, 4403, 1010, 2096, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 18953, 1996, 15871, 2011, 1015, 1012, 1013, 1006, 1015, 1011, 3446, 1007, 1012, 102, 101, 2076, 9312, 1010, 2017, 2071, 2275, 1996, 2562, 4013, 2497, 2000, 1015, 2029, 2003, 5662, 2000, 2275, 2731, 2000, 6270, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 14, 60, 83], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56635868", "vertexSet": [[{"sent_id": 0, "name": "tf.fill", "pos": [58, 62]}], [{"sent_id": 0, "name": "tf.tile", "pos": [63, 67]}], [{"sent_id": 0, "name": "tf.where", "pos": [2, 6]}, {"sent_id": 0, "name": "tf.where", "pos": [17, 21]}]], "sents": ["Use tf.where for that, for example like this (broadcasting support for tf.where seems to be on its way, but not there yet as far as I can tell, so you have to make sure all arguments have the same size with a vector of ones, or tf.fill, tf.tile...).", "<code>Code Snippet</code>."], "sent_idxs": [101, 2224, 1056, 2546, 1012, 2073, 2005, 2008, 1010, 2005, 2742, 2066, 2023, 1006, 5062, 2490, 2005, 1056, 2546, 1012, 2073, 3849, 2000, 2022, 2006, 2049, 2126, 1010, 2021, 2025, 2045, 2664, 2004, 2521, 2004, 1045, 2064, 2425, 1010, 2061, 2017, 2031, 2000, 2191, 2469, 2035, 9918, 2031, 1996, 2168, 2946, 2007, 1037, 9207, 1997, 3924, 1010, 2030, 1056, 2546, 1012, 6039, 1010, 1056, 2546, 1012, 14090, 1012, 1012, 1012, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 73, 87], "sent_pos": [0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44735282", "vertexSet": [[{"sent_id": 8, "name": "tf.graph", "pos": [181, 185]}], [{"sent_id": 8, "name": "tf.matmul", "pos": [170, 176]}], [{"sent_id": 8, "name": "tf.placeholder", "pos": [164, 169]}, {"sent_id": 9, "name": "tf.placeholder", "pos": [211, 216]}], [{"sent_id": 12, "name": "tf.get_variable", "pos": [294, 300]}]], "sents": ["Q1: This is of course a legit way to build your model but it's more a matter of opinion.", "I would only suggest to store your tensors as attribute (see answer to Q2.)", "self.X=....", "You could have a look to this very nice post on how to structure your TensorFlow model in an Objected Oriented way.", "Q2: The reason is really simple and lives in the fact that the variable X is not in the scope of your transform method.", "If you do the following everything would work fine:", "<code>Code Snippet</code>.", "To be more detailed, in TensorFlow, all the Tensors or Operations you define (e.g.", "tf.placeholder or tf.matmul) are defined in the tf.Graph() youre working on.", "You might want to store them in Python variable, as you did by doingX = tf.placeholder` but that's not mandatory.", "If you want to access after on to one of the Tensor you defined, you can either", "use the Python variable (it was your attempt except, that the variable X was not in the scope of the method) or,.", "retrieve them directly from the graph (you need to know it's name), using the tf.get_variable method).."], "sent_idxs": [101, 1053, 2487, 1024, 2023, 2003, 1997, 2607, 1037, 4190, 4183, 2126, 2000, 3857, 2115, 2944, 2021, 2009, 1005, 1055, 2062, 1037, 3043, 1997, 5448, 1012, 102, 101, 1045, 2052, 2069, 6592, 2000, 3573, 2115, 23435, 2015, 2004, 17961, 1006, 2156, 3437, 2000, 1053, 2475, 1012, 1007, 102, 101, 2969, 1012, 1060, 1027, 1012, 1012, 1012, 1012, 102, 101, 2017, 2071, 2031, 1037, 2298, 2000, 2023, 2200, 3835, 2695, 2006, 2129, 2000, 3252, 2115, 23435, 12314, 2944, 1999, 2019, 15959, 8048, 2126, 1012, 102, 101, 1053, 2475, 1024, 1996, 3114, 2003, 2428, 3722, 1998, 3268, 1999, 1996, 2755, 2008, 1996, 8023, 1060, 2003, 2025, 1999, 1996, 9531, 1997, 2115, 10938, 4118, 1012, 102, 101, 2065, 2017, 2079, 1996, 2206, 2673, 2052, 2147, 2986, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 2022, 2062, 6851, 1010, 1999, 23435, 12314, 1010, 2035, 1996, 23435, 2015, 2030, 3136, 2017, 9375, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 2173, 14528, 2030, 1056, 2546, 1012, 13523, 12274, 2140, 1007, 2024, 4225, 1999, 1996, 1056, 2546, 1012, 10629, 1006, 1007, 2115, 2063, 2551, 2006, 1012, 102, 101, 2017, 2453, 2215, 2000, 3573, 2068, 1999, 18750, 8023, 1010, 2004, 2017, 2106, 2011, 2725, 2595, 1027, 1056, 2546, 1012, 2173, 14528, 1036, 2021, 2008, 1005, 1055, 2025, 10915, 1012, 102, 101, 2065, 2017, 2215, 2000, 3229, 2044, 2006, 2000, 2028, 1997, 1996, 23435, 2017, 4225, 1010, 2017, 2064, 2593, 102, 101, 2224, 1996, 18750, 8023, 1006, 2009, 2001, 2115, 3535, 3272, 1010, 2008, 1996, 8023, 1060, 2001, 2025, 1999, 1996, 9531, 1997, 1996, 4118, 1007, 2030, 1010, 1012, 102, 101, 12850, 2068, 3495, 2013, 1996, 10629, 1006, 2017, 2342, 2000, 2113, 2009, 1005, 1055, 2171, 1007, 1010, 2478, 1996, 1056, 2546, 1012, 2131, 1035, 8023, 4118, 1007, 1012, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 27, 48, 58, 84, 113, 125, 139, 163, 193, 225, 245, 274, 305], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0]}, {"title": "50161812", "vertexSet": [[{"sent_id": 8, "name": "tf.data", "pos": [229, 233]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [15, 21]}], [{"sent_id": 8, "name": "tf.estimator", "pos": [219, 225]}], [{"sent_id": 8, "name": "tf.data.dataset", "pos": [229, 236]}], [{"sent_id": 0, "name": "tf.contrib.lookup", "pos": [15, 24]}], [{"sent_id": 7, "name": "tf.tables_initializer", "pos": [181, 188]}]], "sents": ["The built-in TensorFlow mechanism for doing this is to use a tf.contrib.lookup table.", "For example, if you have a list of string keys that you want to map to dense integers, you can define the following outside your _parse_fn():", "<code>Code Snippet</code>.", "...and then use products_to_class.lookup() in your _parse_fn().", "<code>Code Snippet</code>.", "Note that this places two additional constraints on your program:", "You must use Dataset.make_initializable_iterator() instead of Dataset.make_one_shot_iterator()..", "You must call sess.run(tf.tables_initializer()) before starting to consume elements from the input pipeline..", "Both of these will be handled for you if you use the high-level tf.estimator API and return the tf.data.Dataset from your input_fn."], "sent_idxs": [101, 1996, 2328, 1011, 1999, 23435, 12314, 7337, 2005, 2725, 2023, 2003, 2000, 2224, 1037, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2298, 6279, 2795, 1012, 102, 101, 2005, 2742, 1010, 2065, 2017, 2031, 1037, 2862, 1997, 5164, 6309, 2008, 2017, 2215, 2000, 4949, 2000, 9742, 24028, 1010, 2017, 2064, 9375, 1996, 2206, 2648, 2115, 1035, 11968, 3366, 1035, 1042, 2078, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 1998, 2059, 2224, 3688, 1035, 2000, 1035, 2465, 1012, 2298, 6279, 1006, 1007, 1999, 2115, 1035, 11968, 3366, 1035, 1042, 2078, 1006, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 2023, 3182, 2048, 3176, 14679, 2006, 2115, 2565, 1024, 102, 101, 2017, 2442, 2224, 2951, 13462, 1012, 2191, 1035, 3988, 21335, 3468, 1035, 2009, 6906, 4263, 1006, 1007, 2612, 1997, 2951, 13462, 1012, 2191, 1035, 2028, 1035, 2915, 1035, 2009, 6906, 4263, 1006, 1007, 1012, 1012, 102, 101, 2017, 2442, 2655, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 7251, 1035, 3988, 17629, 1006, 1007, 1007, 2077, 3225, 2000, 16678, 3787, 2013, 1996, 7953, 13117, 1012, 1012, 102, 101, 2119, 1997, 2122, 2097, 2022, 8971, 2005, 2017, 2065, 2017, 2224, 1996, 2152, 1011, 2504, 1056, 2546, 1012, 9765, 9581, 4263, 17928, 1998, 2709, 1996, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 2013, 2115, 7953, 1035, 1042, 2078, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 27, 65, 79, 108, 122, 135, 172, 203, 244], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47321605", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.conv2d", "pos": [19, 29]}, {"sent_id": 6, "name": "tf.nn.conv2d", "pos": [167, 177]}], [{"sent_id": 4, "name": "tf.layers.conv2d", "pos": [108, 117]}, {"sent_id": 6, "name": "tf.layers.conv2d", "pos": [141, 150]}]], "sents": ["As others mentioned the parameters are different especially the \"filter(s)\".", "tf.nn.conv2d takes a tensor as a filter, which means you can specify the weight decay (or maybe other properties) like the following in cifar10 code.", "(Whether you want/need to have weight decay in conv layer is another question.)", "<code>Code Snippet</code>.", "I'm not quite sure how to set weight decay in tf.layers.conv2d since it only take an integer as filters.", "Maybe using kernel_constraint?", "On the other hand, tf.layers.conv2d handles activation and bias automatically while you have to write additional codes for these if you use tf.nn.conv2d."], "sent_idxs": [101, 2004, 2500, 3855, 1996, 11709, 2024, 2367, 2926, 1996, 1000, 11307, 1006, 1055, 1007, 1000, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 3138, 1037, 23435, 2004, 1037, 11307, 1010, 2029, 2965, 2017, 2064, 20648, 1996, 3635, 13121, 1006, 2030, 2672, 2060, 5144, 1007, 2066, 1996, 2206, 1999, 25022, 14971, 10790, 3642, 1012, 102, 101, 1006, 3251, 2017, 2215, 1013, 2342, 2000, 2031, 3635, 13121, 1999, 9530, 2615, 6741, 2003, 2178, 3160, 1012, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 1005, 1049, 2025, 3243, 2469, 2129, 2000, 2275, 3635, 13121, 1999, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2144, 2009, 2069, 2202, 2019, 16109, 2004, 17736, 1012, 102, 101, 2672, 2478, 16293, 1035, 27142, 1029, 102, 101, 2006, 1996, 2060, 2192, 1010, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 16024, 13791, 1998, 13827, 8073, 2096, 2017, 2031, 2000, 4339, 3176, 9537, 2005, 2122, 2065, 2017, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 4, 6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 4, 6]}], "na_triple": [], "sent_ends": [0, 18, 60, 81, 95, 127, 135, 179], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "45558159", "vertexSet": [[{"sent_id": 5, "name": "tf.logging", "pos": [173, 177]}, {"sent_id": 5, "name": "tf.logging", "pos": [183, 187]}], [{"sent_id": 5, "name": "tf.logging.error", "pos": [183, 189]}], [{"sent_id": 5, "name": "tf.logging.set_verbosity", "pos": [173, 182]}]], "sents": ["If you don't explicitly give an initializer function to embedding_columns, tensorflow will use the default initializer, which is a normal with 0 mean and 1/sqrt(vocab_size) standard deviation as the code below:", "<code>Code Snippet</code>.", "That is just a warning that the default initializer function is going to be slightly changed in near future (although it looks like they were a bit late on the deadline).", "Nothing important and shouldn't effect your work (can take more or less time to converge but difference should be unnoticeable).", "If the warning disturbs you, you can give an initializer function explicitly when creating embedding columns or you can ignore warnings by setting verbosity to errors only.", "tf.logging.set_verbosity(tf.logging.ERROR)"], "sent_idxs": [101, 2065, 2017, 2123, 1005, 1056, 12045, 2507, 2019, 3988, 17629, 3853, 2000, 7861, 8270, 4667, 1035, 7753, 1010, 23435, 12314, 2097, 2224, 1996, 12398, 3988, 17629, 1010, 2029, 2003, 1037, 3671, 2007, 1014, 2812, 1998, 1015, 1013, 5490, 5339, 1006, 29536, 3540, 2497, 1035, 2946, 1007, 3115, 24353, 2004, 1996, 3642, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2008, 2003, 2074, 1037, 5432, 2008, 1996, 12398, 3988, 17629, 3853, 2003, 2183, 2000, 2022, 3621, 2904, 1999, 2379, 2925, 1006, 2348, 2009, 3504, 2066, 2027, 2020, 1037, 2978, 2397, 2006, 1996, 15117, 1007, 1012, 102, 101, 2498, 2590, 1998, 5807, 1005, 1056, 3466, 2115, 2147, 1006, 2064, 2202, 2062, 2030, 2625, 2051, 2000, 28314, 2021, 4489, 2323, 2022, 4895, 17048, 6610, 3085, 1007, 1012, 102, 101, 2065, 1996, 5432, 22995, 2015, 2017, 1010, 2017, 2064, 2507, 2019, 3988, 17629, 3853, 12045, 2043, 4526, 7861, 8270, 4667, 7753, 2030, 2017, 2064, 8568, 16234, 2011, 4292, 12034, 25949, 2000, 10697, 2069, 1012, 102, 101, 1056, 2546, 1012, 15899, 1012, 2275, 1035, 12034, 25949, 1006, 1056, 2546, 1012, 15899, 1012, 7561, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 55, 69, 106, 136, 172, 191], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "57536764", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [50, 54]}, {"sent_id": 1, "name": "tf.data", "pos": [91, 95]}, {"sent_id": 1, "name": "tf.data", "pos": [108, 112]}, {"sent_id": 1, "name": "tf.data", "pos": [118, 122]}, {"sent_id": 11, "name": "tf.data", "pos": [337, 341]}], [{"sent_id": 0, "name": "tf.compat", "pos": [28, 34]}, {"sent_id": 0, "name": "tf.compat", "pos": [60, 66]}, {"sent_id": 4, "name": "tf.compat", "pos": [195, 201]}, {"sent_id": 10, "name": "tf.compat", "pos": [302, 308]}], [{"sent_id": 0, "name": "tf.compat.v2", "pos": [28, 37]}, {"sent_id": 0, "name": "tf.compat.v2", "pos": [60, 69]}, {"sent_id": 4, "name": "tf.compat.v2", "pos": [195, 204]}, {"sent_id": 10, "name": "tf.compat.v2", "pos": [302, 311]}], [{"sent_id": 0, "name": "tf.data.options", "pos": [50, 56]}, {"sent_id": 1, "name": "tf.data.options", "pos": [91, 97]}], [{"sent_id": 1, "name": "tf.data.dataset", "pos": [108, 115]}, {"sent_id": 1, "name": "tf.data.dataset", "pos": [118, 125]}], [{"sent_id": 0, "name": "tf.compat.v2.data", "pos": [28, 39]}, {"sent_id": 0, "name": "tf.compat.v2.data", "pos": [60, 71]}, {"sent_id": 4, "name": "tf.compat.v2.data", "pos": [195, 206]}, {"sent_id": 10, "name": "tf.compat.v2.data", "pos": [302, 313]}], [{"sent_id": 11, "name": "tf.data.experimental", "pos": [337, 343]}], [{"sent_id": 0, "name": "tf.compat.v2.data.dataset", "pos": [28, 42]}, {"sent_id": 0, "name": "tf.compat.v2.data.dataset", "pos": [60, 74]}, {"sent_id": 4, "name": "tf.compat.v2.data.dataset", "pos": [195, 209]}, {"sent_id": 10, "name": "tf.compat.v2.data.dataset", "pos": [302, 316]}], [{"sent_id": 11, "name": "tf.data.experimental.make_csv_dataset", "pos": [337, 351]}]], "sents": ["tensorflow.python.data.ops.dataset_ops._OptionsDataset is just another class extending the base class tf.compat.v2.data.Dataset (DatasetV2) which holds tf.data.Options along with the original tf.compat.v2.data.Dataset dataset (The Portuguese-English tuples in your case).", "(tf.data.Options operates when you are using streaming functions over your dataset  tf.data.Dataset.map or tf.data.Dataset.interleave)", "How to view the individual elements?", "I'm sure there are many ways, but one straight way would be to use the iterator in the base class:", "Since examples['train'] is a type of _OptionsDataset here is iterating by calling a method from \ntf.compat.v2.data.Dataset", "<code>Code Snippet</code>.", "Here is the output:", "<code>Code Snippet</code>.", "Substituting with your own data:", "Since you've not mentioned what you want to substitute the original dataset with, I'll assume you have a CSV/TSV file of your own specific translations.", "Then it should be useful to create a separate tf.compat.v2.data.Dataset object itself by calling the CSV API to read your CSV file into a dataset:", "tf.data.experimental.make_csv_dataset", "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/load_data/csv.ipynb"], "sent_idxs": [101, 23435, 12314, 1012, 18750, 1012, 2951, 1012, 23092, 1012, 2951, 13462, 1035, 23092, 1012, 1035, 7047, 2850, 18260, 2102, 2003, 2074, 2178, 2465, 8402, 1996, 2918, 2465, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 2951, 1012, 2951, 13462, 1006, 2951, 13462, 2615, 2475, 1007, 2029, 4324, 1056, 2546, 1012, 2951, 1012, 7047, 2247, 2007, 1996, 2434, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 2951, 1012, 2951, 13462, 2951, 13462, 1006, 1996, 5077, 1011, 2394, 10722, 21112, 1999, 2115, 2553, 1007, 1012, 102, 101, 1006, 1056, 2546, 1012, 2951, 1012, 7047, 5748, 2043, 2017, 2024, 2478, 11058, 4972, 2058, 2115, 2951, 13462, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 4949, 2030, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 6970, 19738, 3726, 1007, 102, 101, 2129, 2000, 3193, 1996, 3265, 3787, 1029, 102, 101, 1045, 1005, 1049, 2469, 2045, 2024, 2116, 3971, 1010, 2021, 2028, 3442, 2126, 2052, 2022, 2000, 2224, 1996, 2009, 6906, 4263, 1999, 1996, 2918, 2465, 1024, 102, 101, 2144, 4973, 1031, 1005, 3345, 1005, 1033, 2003, 1037, 2828, 1997, 1035, 7047, 2850, 18260, 2102, 2182, 2003, 2009, 6906, 3436, 2011, 4214, 1037, 4118, 2013, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 2951, 1012, 2951, 13462, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2182, 2003, 1996, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 4942, 21532, 2007, 2115, 2219, 2951, 1024, 102, 101, 2144, 2017, 1005, 2310, 2025, 3855, 2054, 2017, 2215, 2000, 7681, 1996, 2434, 2951, 13462, 2007, 1010, 1045, 1005, 2222, 7868, 2017, 2031, 1037, 20116, 2615, 1013, 24529, 2615, 5371, 1997, 2115, 2219, 3563, 11913, 1012, 102, 101, 2059, 2009, 2323, 2022, 6179, 2000, 3443, 1037, 3584, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 2951, 1012, 2951, 13462, 4874, 2993, 2011, 4214, 1996, 20116, 2615, 17928, 2000, 3191, 2115, 20116, 2615, 5371, 2046, 1037, 2951, 13462, 1024, 102, 101, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 2191, 1035, 20116, 2615, 1035, 2951, 13462, 102, 101, 16770, 1024, 1013, 1013, 15270, 2497, 1012, 2470, 1012, 8224, 1012, 4012, 1013, 21025, 2705, 12083, 1013, 23435, 12314, 1013, 9986, 2015, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 2609, 1013, 4372, 1013, 1054, 2475, 1013, 14924, 26340, 1013, 7170, 1035, 2951, 1013, 20116, 2615, 1012, 12997, 6038, 2497, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 8], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [5, 8], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [6, 8], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 8], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7]], "sent_ends": [0, 89, 131, 140, 168, 210, 224, 231, 245, 254, 292, 336, 352, 403], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34343517", "vertexSet": [[{"sent_id": 3, "name": "tf.graph", "pos": [107, 111]}, {"sent_id": 5, "name": "tf.graph", "pos": [151, 155]}], [{"sent_id": 5, "name": "tf.constant", "pos": [161, 165]}, {"sent_id": 6, "name": "tf.constant", "pos": [235, 239]}], [{"sent_id": 8, "name": "tf.placeholder", "pos": [278, 283]}], [{"sent_id": 6, "name": "tf.import_graph_def", "pos": [195, 203]}]], "sents": ["EDIT: The freeze_graph.py script, which is part of the TensorFlow repository, now serves as a tool that generates a protocol buffer representing a \"frozen\" trained model, from an existing TensorFlow GraphDef and a saved checkpoint.", "It uses the same steps as described below, but it much easier to use.", "Currently the process isn't very well documented (and subject to refinement), but the approximate steps are as follows:", "Build and train your model as a tf.Graph called g_1..", "Fetch the final values of each of the variables and store them as numpy arrays (using Session.run())..", "In a new tf.Graph called g_2, create tf.constant() tensors for each of the variables, using the value of the corresponding numpy array fetched in step 2..", "Use tf.import_graph_def() to copy nodes from g_1 into g_2, and use the input_map argument to replace each variable in g_1 with the corresponding tf.constant() tensors created in step 3.", "You may also want to use input_map to specify a new input tensor (e.g.", "replacing an input pipeline with a tf.placeholder()).", "Use the return_elements argument to specify the name of the predicted output tensor.", "Call g_2.as_graph_def() to get a protocol buffer representation of the graph.", "(NOTE: The generated graph will have extra nodes in the graph for training.", "Although it is not part of the public API, you may wish to use the internal graph_util.extract_sub_graph() function to strip these nodes from the graph.)"], "sent_idxs": [101, 10086, 1024, 1996, 13184, 1035, 10629, 1012, 1052, 2100, 5896, 1010, 2029, 2003, 2112, 1997, 1996, 23435, 12314, 22409, 1010, 2085, 4240, 2004, 1037, 6994, 2008, 19421, 1037, 8778, 17698, 5052, 1037, 1000, 7708, 1000, 4738, 2944, 1010, 2013, 2019, 4493, 23435, 12314, 10629, 3207, 2546, 1998, 1037, 5552, 26520, 1012, 102, 101, 2009, 3594, 1996, 2168, 4084, 2004, 2649, 2917, 1010, 2021, 2009, 2172, 6082, 2000, 2224, 1012, 102, 101, 2747, 1996, 2832, 3475, 1005, 1056, 2200, 2092, 8832, 1006, 1998, 3395, 2000, 25416, 3170, 3672, 1007, 1010, 2021, 1996, 15796, 4084, 2024, 2004, 4076, 1024, 102, 101, 3857, 1998, 3345, 2115, 2944, 2004, 1037, 1056, 2546, 1012, 10629, 2170, 1043, 1035, 1015, 1012, 1012, 102, 101, 18584, 1996, 2345, 5300, 1997, 2169, 1997, 1996, 10857, 1998, 3573, 2068, 2004, 16371, 8737, 2100, 27448, 1006, 2478, 5219, 1012, 2448, 1006, 1007, 1007, 1012, 1012, 102, 101, 1999, 1037, 2047, 1056, 2546, 1012, 10629, 2170, 1043, 1035, 1016, 1010, 3443, 1056, 2546, 1012, 5377, 1006, 1007, 23435, 2015, 2005, 2169, 1997, 1996, 10857, 1010, 2478, 1996, 3643, 1997, 1996, 7978, 16371, 8737, 2100, 9140, 18584, 2098, 1999, 3357, 1016, 1012, 1012, 102, 101, 2224, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 2000, 6100, 14164, 2013, 1043, 1035, 1015, 2046, 1043, 1035, 1016, 1010, 1998, 2224, 1996, 7953, 1035, 4949, 6685, 2000, 5672, 2169, 8023, 1999, 1043, 1035, 1015, 2007, 1996, 7978, 1056, 2546, 1012, 5377, 1006, 1007, 23435, 2015, 2580, 1999, 3357, 1017, 1012, 102, 101, 2017, 2089, 2036, 2215, 2000, 2224, 7953, 1035, 4949, 2000, 20648, 1037, 2047, 7953, 23435, 1006, 1041, 1012, 1043, 1012, 102, 101, 6419, 2019, 7953, 13117, 2007, 1037, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 1007, 1012, 102, 101, 2224, 1996, 2709, 1035, 3787, 6685, 2000, 20648, 1996, 2171, 1997, 1996, 10173, 6434, 23435, 1012, 102, 101, 2655, 1043, 1035, 1016, 1012, 2004, 1035, 10629, 1035, 13366, 1006, 1007, 2000, 2131, 1037, 8778, 17698, 6630, 1997, 1996, 10629, 1012, 102, 101, 1006, 3602, 1024, 1996, 7013, 10629, 2097, 2031, 4469, 14164, 1999, 1996, 10629, 2005, 2731, 1012, 102, 101, 2348, 2009, 2003, 2025, 2112, 1997, 1996, 2270, 17928, 1010, 2017, 2089, 4299, 2000, 2224, 1996, 4722, 10629, 1035, 21183, 4014, 1012, 14817, 1035, 4942, 1035, 10629, 1006, 1007, 3853, 2000, 6167, 2122, 14164, 2013, 1996, 10629, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 53, 71, 99, 118, 147, 193, 249, 271, 288, 306, 330, 348, 389], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58779768", "vertexSet": [[{"sent_id": 2, "name": "tf.image", "pos": [50, 54]}], [{"sent_id": 4, "name": "tf.session", "pos": [101, 105]}], [{"sent_id": 2, "name": "tf.image.grayscale_to_rgb", "pos": [50, 63]}]], "sents": ["You should store the reshaped 3D [28x28x1] images in an array:", "<code>Code Snippet</code>.", "When converting, set an other array to the return value of the tf.image.grayscale_to_rgb() function  :", "<code>Code Snippet</code>.", "Finally, to plot out one example from the resulting tensor images with matplotlib and tf.session():", "<code>Code Snippet</code>.", "The complete code:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2323, 3573, 1996, 24501, 3270, 5669, 7605, 1031, 2654, 2595, 22407, 2595, 2487, 1033, 4871, 1999, 2019, 9140, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2043, 16401, 1010, 2275, 2019, 2060, 9140, 2000, 1996, 2709, 3643, 1997, 1996, 1056, 2546, 1012, 3746, 1012, 3897, 15782, 2571, 1035, 2000, 1035, 1054, 18259, 1006, 1007, 3853, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2633, 1010, 2000, 5436, 2041, 2028, 2742, 2013, 1996, 4525, 23435, 4871, 2007, 13523, 24759, 4140, 29521, 1998, 1056, 2546, 1012, 5219, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3143, 3642, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 36, 68, 82, 109, 123, 129, 143], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40936677", "vertexSet": [[{"sent_id": 5, "name": "tf.nn", "pos": [178, 183]}], [{"sent_id": 8, "name": "tf.cond", "pos": [260, 265]}], [{"sent_id": 3, "name": "tf.shape", "pos": [115, 119]}, {"sent_id": 4, "name": "tf.shape", "pos": [155, 159]}], [{"sent_id": 3, "name": "tf.tensor", "pos": [128, 132]}, {"sent_id": 5, "name": "tf.tensor", "pos": [192, 196]}], [{"sent_id": 5, "name": "tf.nn.max_pool", "pos": [178, 187]}]], "sents": ["When you do print(a.get_shape()), you are getting the static shape of the tensor a.", "Assuming you mean imgs.get_shape() and not a.get_shape() in the code above, dimensions 1 and 2 of imgs vary dynamically with the value of epoch_num_tf.", "Therefore the static shape in those dimensions is unknown, which TensorFlow represents as None.", "If you want to use the dynamic shape of imgs in subsequent code, you should use the tf.shape() operator to get the shape as a tf.Tensor.", "For example, instead of imgs.get_shape()[2], you can use tf.shape(imgs)[2].", "Unfortunately, the ksize and strides arguments of tf.nn.max_pool() do not accept tf.Tensor values.", "(I think this is a historical limitation, because these were configured as \"attrs\" rather than \"inputs\" of the corresponding kernel.", "Please open a GitHub issue if you'd like to request this feature!)", "One possible workaround would be to use another tf.cond():", "<code>Code Snippet</code>."], "sent_idxs": [101, 2043, 2017, 2079, 6140, 1006, 1037, 1012, 2131, 1035, 4338, 1006, 1007, 1007, 1010, 2017, 2024, 2893, 1996, 10763, 4338, 1997, 1996, 23435, 1037, 1012, 102, 101, 10262, 2017, 2812, 10047, 5620, 1012, 2131, 1035, 4338, 1006, 1007, 1998, 2025, 1037, 1012, 2131, 1035, 4338, 1006, 1007, 1999, 1996, 3642, 2682, 1010, 9646, 1015, 1998, 1016, 1997, 10047, 5620, 8137, 8790, 3973, 2007, 1996, 3643, 1997, 25492, 1035, 16371, 2213, 1035, 1056, 2546, 1012, 102, 101, 3568, 1996, 10763, 4338, 1999, 2216, 9646, 2003, 4242, 1010, 2029, 23435, 12314, 5836, 2004, 3904, 1012, 102, 101, 2065, 2017, 2215, 2000, 2224, 1996, 8790, 4338, 1997, 10047, 5620, 1999, 4745, 3642, 1010, 2017, 2323, 2224, 1996, 1056, 2546, 1012, 4338, 1006, 1007, 6872, 2000, 2131, 1996, 4338, 2004, 1037, 1056, 2546, 1012, 23435, 1012, 102, 101, 2005, 2742, 1010, 2612, 1997, 10047, 5620, 1012, 2131, 1035, 4338, 1006, 1007, 1031, 1016, 1033, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 4338, 1006, 10047, 5620, 1007, 1031, 1016, 1033, 1012, 102, 101, 6854, 1010, 1996, 29535, 4697, 1998, 22215, 9918, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 4098, 1035, 4770, 1006, 1007, 2079, 2025, 5138, 1056, 2546, 1012, 23435, 5300, 1012, 102, 101, 1006, 1045, 2228, 2023, 2003, 1037, 3439, 22718, 1010, 2138, 2122, 2020, 26928, 2004, 1000, 2012, 16344, 2015, 1000, 2738, 2084, 1000, 20407, 1000, 1997, 1996, 7978, 16293, 1012, 102, 101, 3531, 2330, 1037, 21025, 2705, 12083, 3277, 2065, 2017, 1005, 1040, 2066, 2000, 5227, 2023, 3444, 999, 1007, 102, 101, 2028, 2825, 2147, 24490, 2052, 2022, 2000, 2224, 2178, 1056, 2546, 1012, 9530, 2094, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 27, 76, 95, 134, 168, 199, 230, 250, 269, 283], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47523324", "vertexSet": [[{"sent_id": 3, "name": "tf.train", "pos": [79, 83]}], [{"sent_id": 3, "name": "tf.train.supervisor", "pos": [79, 85]}], [{"sent_id": 0, "name": "tf.global_variables_initializer", "pos": [20, 29]}]], "sents": ["Normally there are two ways of initializing variables, 1) using the sess.run(tf.global_variables_initializer()) as the previous answers noted; 2) the load the graph from checkpoint.", "You can do like this:", "<code>Code Snippet</code>.", "And the third method is to use the tf.train.Supervisor.", "The session will be", "Create a session on 'master', recovering or initializing the model as needed, or wait for a session to be ready.", "<code>Code Snippet</code>."], "sent_idxs": [101, 5373, 2045, 2024, 2048, 3971, 1997, 3988, 6026, 10857, 1010, 1015, 1007, 2478, 1996, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1007, 2004, 1996, 3025, 6998, 3264, 1025, 1016, 1007, 1996, 7170, 1996, 10629, 2013, 26520, 1012, 102, 101, 2017, 2064, 2079, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 1996, 2353, 4118, 2003, 2000, 2224, 1996, 1056, 2546, 1012, 3345, 1012, 12366, 1012, 102, 101, 1996, 5219, 2097, 2022, 102, 101, 3443, 1037, 5219, 2006, 1005, 3040, 1005, 1010, 13400, 2030, 3988, 6026, 1996, 2944, 2004, 2734, 1010, 2030, 3524, 2005, 1037, 5219, 2000, 2022, 3201, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 48, 56, 70, 87, 93, 121, 135], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47707275", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [18, 22]}, {"sent_id": 4, "name": "tf.train", "pos": [52, 56]}], [{"sent_id": 4, "name": "tf.train.latest_checkpoint", "pos": [52, 60]}], [{"sent_id": 2, "name": "tf.train.import_meta_graph", "pos": [18, 28]}]], "sents": ["Three suggestions:", "Check the path when restoring the model", "saver = tf.train.import_meta_graph(model_path)", "Check the path when restoring the checkpoint", "saver.restore(sess, tf.train.latest_checkpoint(cur_dir))", "Check the parameters when saving the model", "<code>Code Snippet</code>."], "sent_idxs": [101, 2093, 15690, 1024, 102, 101, 4638, 1996, 4130, 2043, 16487, 1996, 2944, 102, 101, 3828, 2099, 1027, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 2944, 1035, 4130, 1007, 102, 101, 4638, 1996, 4130, 2043, 16487, 1996, 26520, 102, 101, 3828, 2099, 1012, 9239, 1006, 7367, 4757, 1010, 1056, 2546, 1012, 3345, 1012, 6745, 1035, 26520, 1006, 12731, 2099, 1035, 16101, 1007, 1007, 102, 101, 4638, 1996, 11709, 2043, 7494, 1996, 2944, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 5, 14, 34, 43, 68, 77, 91], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48032091", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator", "pos": [3, 9]}, {"sent_id": 0, "name": "tf.estimator", "pos": [26, 32]}], [{"sent_id": 0, "name": "tf.estimator.estimator", "pos": [3, 13]}], [{"sent_id": 0, "name": "tf.estimator.runconfig", "pos": [26, 37]}]], "sents": ["Tensorflow tf.estimator.Estimator takes config as an optional argument, which can be a tf.estimator.RunConfig object to configure runtime settings.You can achieve this as follows:", "<code>Code Snippet</code>.", "config parameter is available in all classes (DNNClassifier, DNNLinearCombinedClassifier, LinearClassifier, etc.)", "that extend estimator.Estimator."], "sent_idxs": [101, 23435, 12314, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 3138, 9530, 8873, 2290, 2004, 2019, 11887, 6685, 1010, 2029, 2064, 2022, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 2448, 8663, 8873, 2290, 4874, 2000, 9530, 8873, 27390, 2063, 2448, 7292, 10906, 1012, 2017, 2064, 6162, 2023, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 9530, 8873, 2290, 16381, 2003, 2800, 1999, 2035, 4280, 1006, 1040, 10695, 26266, 18095, 1010, 1040, 10695, 4179, 2906, 18274, 21280, 26266, 18095, 1010, 7399, 26266, 18095, 1010, 4385, 1012, 1007, 102, 101, 2008, 7949, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 55, 69, 102, 114], "sent_pos": [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64147546", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [5, 10]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [5, 12]}], [{"sent_id": 0, "name": "tf.keras.layers.layer", "pos": [5, 14]}]], "sents": ["In the case of tf.keras.layers.Layer subclasses, like GlobalMaxPooling1D, valid keyword arguments are", "<code>Code Snippet</code>.", "In the __init__ of Layer subclasses, you will see a call like super().__init__(**kwargs), which passes the keyword arguments you enter to the initializer of the base Layer class.", "For example:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1999, 1996, 2553, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 6741, 4942, 26266, 2229, 1010, 2066, 3795, 17848, 16869, 2075, 2487, 2094, 1010, 9398, 3145, 18351, 9918, 2024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 1996, 1035, 1035, 1999, 4183, 1035, 1035, 1997, 6741, 4942, 26266, 2229, 1010, 2017, 2097, 2156, 1037, 2655, 2066, 3565, 1006, 1007, 1012, 1035, 1035, 1999, 4183, 1035, 1035, 1006, 1008, 1008, 6448, 2906, 5620, 1007, 1010, 2029, 5235, 1996, 3145, 18351, 9918, 2017, 4607, 2000, 1996, 3988, 17629, 1997, 1996, 2918, 6741, 2465, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 32, 46, 104, 109, 123], "sent_pos": [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55863830", "vertexSet": [[{"sent_id": 0, "name": "tf.summary", "pos": [8, 12]}, {"sent_id": 1, "name": "tf.summary", "pos": [27, 31]}, {"sent_id": 1, "name": "tf.summary", "pos": [38, 42]}, {"sent_id": 2, "name": "tf.summary", "pos": [60, 64]}], [{"sent_id": 1, "name": "tf.summary.scalar", "pos": [27, 34]}], [{"sent_id": 0, "name": "tf.summary.filewriter", "pos": [8, 15]}, {"sent_id": 1, "name": "tf.summary.filewriter", "pos": [38, 45]}, {"sent_id": 2, "name": "tf.summary.filewriter", "pos": [60, 67]}]], "sents": ["Here is an example, creating two tf.summary.FileWriters which share the same root directory.", "Creating a tf.summary.scalar shared by the two tf.summary.FileWriters.", "At every time step, get the summary and update each tf.summary.FileWriter.", "<code>Code Snippet</code>.", "Here is the result:", "The orange line shows the result of the evaluation stage, and correspondingly, the blue line illustrates the data of the training stage.", "Also, there is a very useful post by TF team to which you can refer."], "sent_idxs": [101, 2182, 2003, 2019, 2742, 1010, 4526, 2048, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 2015, 2029, 3745, 1996, 2168, 7117, 14176, 1012, 102, 101, 4526, 1037, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 4207, 2011, 1996, 2048, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 2015, 1012, 102, 101, 2012, 2296, 2051, 3357, 1010, 2131, 1996, 12654, 1998, 10651, 2169, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2182, 2003, 1996, 2765, 1024, 102, 101, 1996, 4589, 2240, 3065, 1996, 2765, 1997, 1996, 9312, 2754, 1010, 1998, 7978, 2135, 1010, 1996, 2630, 2240, 24899, 1996, 2951, 1997, 1996, 2731, 2754, 1012, 102, 101, 2036, 1010, 2045, 2003, 1037, 2200, 6179, 2695, 2011, 1056, 2546, 2136, 2000, 2029, 2017, 2064, 6523, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 48, 69, 83, 90, 118, 138], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55020634", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [6, 12]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [6, 15]}], [{"sent_id": 0, "name": "tf.contrib.rnn.nascell", "pos": [6, 19]}]], "sents": ["You can see example of tf.contrib.rnn.NASCell in this repo", "nascell-automl"], "sent_idxs": [101, 2017, 2064, 2156, 2742, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 17235, 29109, 2140, 1999, 2023, 16360, 2080, 102, 101, 17235, 29109, 2140, 1011, 8285, 19968, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 32], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59571286", "vertexSet": [[{"sent_id": 13, "name": "tf.tensor", "pos": [180, 184]}], [{"sent_id": 3, "name": "tf.variable", "pos": [35, 39]}, {"sent_id": 13, "name": "tf.variable", "pos": [171, 175]}], [{"sent_id": 5, "name": "tf.add", "pos": [62, 66]}, {"sent_id": 12, "name": "tf.add", "pos": [140, 144]}, {"sent_id": 13, "name": "tf.add", "pos": [163, 167]}], [{"sent_id": 17, "name": "tf.cond", "pos": [234, 239]}], [{"sent_id": 9, "name": "tf.identity", "pos": [110, 114]}], [{"sent_id": 3, "name": "tf.constant", "pos": [40, 44]}], [{"sent_id": 5, "name": "tf.multiply", "pos": [67, 72]}]], "sents": ["This is a bug in TF (Related Github issue: Here).", "For example, the following scenarios work", "What works.", "Changing tf.Variable to tf.constant.", "<code>Code Snippet</code>.", "Changing tf.add to tf.multiply.", "<code>Code Snippet</code>.", "Adding a constant.", "<code>Code Snippet</code>.", "Using tf.identity.", "<code>Code Snippet</code>.", "What doesn't work.", "But tf.add(x,x) or x+x fails.", "The cause of this is that tf.add has trouble working with tf.Variable types but works fine for tf.Tensor types.", "I have a hunch that some insight can be found in the source code.", "Will update as I find anything.", "Solution (For TF 1.15).", "You need to enable version 2 of tf.cond which apparently has this issue fixed.", "You can do this as follows.", "Unfortunately this does not work for 1.14.", "Using magic on Jupyter.", "<code>Code Snippet</code>.", "Using Python.", "<code>Code Snippet</code>.", "And that sould give you the desired result."], "sent_idxs": [101, 2023, 2003, 1037, 11829, 1999, 1056, 2546, 1006, 3141, 21025, 2705, 12083, 3277, 1024, 2182, 1007, 1012, 102, 101, 2005, 2742, 1010, 1996, 2206, 16820, 2147, 102, 101, 2054, 2573, 1012, 102, 101, 5278, 1056, 2546, 1012, 8023, 2000, 1056, 2546, 1012, 5377, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5278, 1056, 2546, 1012, 5587, 2000, 1056, 2546, 1012, 4800, 22086, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5815, 1037, 5377, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2478, 1056, 2546, 1012, 4767, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2054, 2987, 1005, 1056, 2147, 1012, 102, 101, 2021, 1056, 2546, 1012, 5587, 1006, 1060, 1010, 1060, 1007, 2030, 1060, 1009, 1060, 11896, 1012, 102, 101, 1996, 3426, 1997, 2023, 2003, 2008, 1056, 2546, 1012, 5587, 2038, 4390, 2551, 2007, 1056, 2546, 1012, 8023, 4127, 2021, 2573, 2986, 2005, 1056, 2546, 1012, 23435, 4127, 1012, 102, 101, 1045, 2031, 1037, 15876, 12680, 2008, 2070, 12369, 2064, 2022, 2179, 1999, 1996, 3120, 3642, 1012, 102, 101, 2097, 10651, 2004, 1045, 2424, 2505, 1012, 102, 101, 5576, 1006, 2005, 1056, 2546, 1015, 1012, 2321, 1007, 1012, 102, 101, 2017, 2342, 2000, 9585, 2544, 1016, 1997, 1056, 2546, 1012, 9530, 2094, 2029, 4593, 2038, 2023, 3277, 4964, 1012, 102, 101, 2017, 2064, 2079, 2023, 2004, 4076, 1012, 102, 101, 6854, 2023, 2515, 2025, 2147, 2005, 1015, 1012, 2403, 1012, 102, 101, 2478, 3894, 2006, 18414, 7685, 3334, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2478, 18750, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2008, 3969, 2094, 2507, 2017, 1996, 9059, 2765, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [13]}, {"r": "S1", "h": 0, "t": 1, "evidence": [13]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 19, 28, 33, 46, 60, 74, 88, 94, 108, 116, 130, 138, 156, 187, 205, 214, 226, 247, 256, 268, 277, 291, 296, 310, 322], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52050884", "vertexSet": [[{"sent_id": 4, "name": "tf.train", "pos": [111, 115]}], [{"sent_id": 3, "name": "tf.tensor", "pos": [77, 81]}, {"sent_id": 3, "name": "tf.tensor", "pos": [96, 100]}], [{"sent_id": 3, "name": "tf.matmul", "pos": [55, 61]}], [{"sent_id": 2, "name": "tf.constant", "pos": [29, 33]}], [{"sent_id": 4, "name": "tf.train.optimizer", "pos": [111, 119]}]], "sents": ["Take a look at following picture from Tensorflow official website that explains about Graph and Session concepts:", "According to documentation:", "Calling tf.constant() creates a single Operation that produces a value, adds it to the default graph..", "Calling tf.matmul(x, y) creates a single Operation that multiplies the values of tf.Tensor objects x and y, adds it to the default graph, and returns a tf.Tensor that represents the result of the multiplication.", "Calling tf.train.Optimizer.minimize will add operations and tensors to the default graph that calculates gradients, and return a Operation that, when run, will apply those gradients to a set of variables..", "when running the \u201csession.run()\u201d the variables weights and the biases\n  will be updated.", "Actually their value calculated not updated.", "For example, take a look at following example:", "<code>Code Snippet</code>.", "In this example no update will happen.", "Look at the above picture again, as you can see in the picture when we go forward we understand what parameters needs to updated so in the backward, the parameters updated according to loss by SGD optimizer."], "sent_idxs": [101, 2202, 1037, 2298, 2012, 2206, 3861, 2013, 23435, 12314, 2880, 4037, 2008, 7607, 2055, 10629, 1998, 5219, 8474, 1024, 102, 101, 2429, 2000, 12653, 1024, 102, 101, 4214, 1056, 2546, 1012, 5377, 1006, 1007, 9005, 1037, 2309, 3169, 2008, 7137, 1037, 3643, 1010, 9909, 2009, 2000, 1996, 12398, 10629, 1012, 1012, 102, 101, 4214, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 1010, 1061, 1007, 9005, 1037, 2309, 3169, 2008, 4800, 24759, 3111, 1996, 5300, 1997, 1056, 2546, 1012, 23435, 5200, 1060, 1998, 1061, 1010, 9909, 2009, 2000, 1996, 12398, 10629, 1010, 1998, 5651, 1037, 1056, 2546, 1012, 23435, 2008, 5836, 1996, 2765, 1997, 1996, 24856, 1012, 102, 101, 4214, 1056, 2546, 1012, 3345, 1012, 23569, 27605, 6290, 1012, 18478, 2097, 5587, 3136, 1998, 23435, 2015, 2000, 1996, 12398, 10629, 2008, 18422, 2015, 17978, 2015, 1010, 1998, 2709, 1037, 3169, 2008, 1010, 2043, 2448, 1010, 2097, 6611, 2216, 17978, 2015, 2000, 1037, 2275, 1997, 10857, 1012, 1012, 102, 101, 2043, 2770, 1996, 1523, 5219, 1012, 2448, 1006, 1007, 1524, 1996, 10857, 15871, 1998, 1996, 13827, 2229, 2097, 2022, 7172, 1012, 102, 101, 2941, 2037, 3643, 10174, 2025, 7172, 1012, 102, 101, 2005, 2742, 1010, 2202, 1037, 2298, 2012, 2206, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 2023, 2742, 2053, 10651, 2097, 4148, 1012, 102, 101, 2298, 2012, 1996, 2682, 3861, 2153, 1010, 2004, 2017, 2064, 2156, 1999, 1996, 3861, 2043, 2057, 2175, 2830, 2057, 3305, 2054, 11709, 3791, 2000, 7172, 2061, 1999, 1996, 8848, 1010, 1996, 11709, 7172, 2429, 2000, 3279, 2011, 22214, 2094, 23569, 27605, 6290, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 21, 27, 53, 109, 159, 182, 191, 203, 217, 227, 272], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44371483", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [27, 31]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [16, 21]}, {"sent_id": 1, "name": "tf.placeholder", "pos": [59, 64]}, {"sent_id": 2, "name": "tf.placeholder", "pos": [79, 84]}], [{"sent_id": 2, "name": "tf.add_to_collection", "pos": [92, 100]}], [{"sent_id": 4, "name": "tf.get_default_graph", "pos": [137, 145]}]], "sents": ["The tensors v1:0 and v2:0 were created from tf.placeholder() ops, whereas only tf.Variable objects are added to the \"variables\" (or \"trainable_variables\") collections.", "There is no general collection to which tf.placeholder() ops are added, so your options are:", "Add the tf.placeholder() ops to a collection (using tf.add_to_collection() when constructing the original graph.", "You might need to add more metadata in order to suggest how the placeholders should be used.", "Use [x for x in tf.get_default_graph().get_operations() if x.type == \"PlaceholderV2\"] to get a list of placeholder ops after you import the metagraph."], "sent_idxs": [101, 1996, 23435, 2015, 1058, 2487, 1024, 1014, 1998, 1058, 2475, 1024, 1014, 2020, 2580, 2013, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23092, 1010, 6168, 2069, 1056, 2546, 1012, 8023, 5200, 2024, 2794, 2000, 1996, 1000, 10857, 1000, 1006, 2030, 1000, 3345, 3085, 1035, 10857, 1000, 1007, 6407, 1012, 102, 101, 2045, 2003, 2053, 2236, 3074, 2000, 2029, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23092, 2024, 2794, 1010, 2061, 2115, 7047, 2024, 1024, 102, 101, 5587, 1996, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23092, 2000, 1037, 3074, 1006, 2478, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1006, 1007, 2043, 15696, 1996, 2434, 10629, 1012, 102, 101, 2017, 2453, 2342, 2000, 5587, 2062, 27425, 1999, 2344, 2000, 6592, 2129, 1996, 2173, 17794, 2323, 2022, 2109, 1012, 102, 101, 2224, 1031, 1060, 2005, 1060, 1999, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 2131, 1035, 3136, 1006, 1007, 2065, 1060, 1012, 2828, 1027, 1027, 1000, 2173, 14528, 2615, 2475, 1000, 1033, 2000, 2131, 1037, 2862, 1997, 2173, 14528, 23092, 2044, 2017, 12324, 1996, 18804, 14413, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 51, 76, 109, 130, 182], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47143569", "vertexSet": [[{"sent_id": 0, "name": "tf.saved_model", "pos": [26, 32]}], [{"sent_id": 0, "name": "tf.saved_model.builder", "pos": [26, 34]}], [{"sent_id": 0, "name": "tf.saved_model.builder.savedmodelbuilder", "pos": [26, 41]}]], "sents": ["You can use tfgo to easily load into Go and use a trained tensorflow model: just export the trained model using tf.saved_model.builder.SavedModelBuilder as shown in the tfgo README.", "However, you just have to extract from the graph the input placeholder and then feed the network using it.", "Let's suppose you exported your model calling it my_model and tagged it with the tag tag.", "Also let's suppose that your input placeholder is named \"Placeholder\".", "Moreover, you have to know the name of your output node.", "Let's call it output/node/path/op.", "Then your code should look like:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 3995, 2000, 4089, 7170, 2046, 2175, 1998, 2224, 1037, 4738, 23435, 12314, 2944, 1024, 2074, 9167, 1996, 4738, 2944, 2478, 1056, 2546, 1012, 5552, 1035, 2944, 1012, 12508, 1012, 5552, 5302, 9247, 8569, 23891, 2099, 2004, 3491, 1999, 1996, 1056, 2546, 3995, 3191, 4168, 1012, 102, 101, 2174, 1010, 2017, 2074, 2031, 2000, 14817, 2013, 1996, 10629, 1996, 7953, 2173, 14528, 1998, 2059, 5438, 1996, 2897, 2478, 2009, 1012, 102, 101, 2292, 1005, 1055, 6814, 2017, 15612, 2115, 2944, 4214, 2009, 2026, 1035, 2944, 1998, 26610, 2009, 2007, 1996, 6415, 6415, 1012, 102, 101, 2036, 2292, 1005, 1055, 6814, 2008, 2115, 7953, 2173, 14528, 2003, 2315, 1000, 2173, 14528, 1000, 1012, 102, 101, 9308, 1010, 2017, 2031, 2000, 2113, 1996, 2171, 1997, 2115, 6434, 13045, 1012, 102, 101, 2292, 1005, 1055, 2655, 2009, 6434, 1013, 13045, 1013, 4130, 1013, 6728, 1012, 102, 101, 2059, 2115, 3642, 2323, 2298, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 52, 76, 99, 118, 133, 148, 157, 171], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51556659", "vertexSet": [[{"sent_id": 6, "name": "tf.contrib", "pos": [156, 162]}], [{"sent_id": 6, "name": "tf.contrib.data", "pos": [156, 164]}], [{"sent_id": 6, "name": "tf.contrib.data.group_by_window", "pos": [156, 170]}]], "sents": ["I don't think your solution could work, if I understand it correctly, because sample_from_dataset expects a list of values for its weights, not a Tensor.", "However if you don't mind having 1000 Datasets as in your proposed solution, then I would suggest to simply", "create one Dataset per class,.", "batch each of these datasets \u2014 each batch has samples from a single class,.", "zip all of them into one big Dataset of batches,.", "shuffle this Dataset \u2014 the shuffling will occur on the batches, not on the samples, so it won't change the fact that batches are single class..", "A more sophisticated way is to rely on tf.contrib.data.group_by_window.", "Let me illustrate that with a synthetic example.", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2123, 1005, 1056, 2228, 2115, 5576, 2071, 2147, 1010, 2065, 1045, 3305, 2009, 11178, 1010, 2138, 7099, 1035, 2013, 1035, 2951, 13462, 24273, 1037, 2862, 1997, 5300, 2005, 2049, 15871, 1010, 2025, 1037, 23435, 1012, 102, 101, 2174, 2065, 2017, 2123, 1005, 1056, 2568, 2383, 6694, 2951, 13462, 2015, 2004, 1999, 2115, 3818, 5576, 1010, 2059, 1045, 2052, 6592, 2000, 3432, 102, 101, 3443, 2028, 2951, 13462, 2566, 2465, 1010, 1012, 102, 101, 14108, 2169, 1997, 2122, 2951, 13462, 2015, 1517, 2169, 14108, 2038, 8168, 2013, 1037, 2309, 2465, 1010, 1012, 102, 101, 14101, 2035, 1997, 2068, 2046, 2028, 2502, 2951, 13462, 1997, 14108, 2229, 1010, 1012, 102, 101, 23046, 2023, 2951, 13462, 1517, 1996, 24770, 2097, 5258, 2006, 1996, 14108, 2229, 1010, 2025, 2006, 1996, 8168, 1010, 2061, 2009, 2180, 1005, 1056, 2689, 1996, 2755, 2008, 14108, 2229, 2024, 2309, 2465, 1012, 1012, 102, 101, 1037, 2062, 12138, 2126, 2003, 2000, 11160, 2006, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 2177, 1035, 2011, 1035, 3332, 1012, 102, 101, 2292, 2033, 19141, 2008, 2007, 1037, 12553, 2742, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 64, 74, 94, 110, 147, 172, 183, 197], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63118424", "vertexSet": [[{"sent_id": 1, "name": "tf.tanh", "pos": [46, 51]}], [{"sent_id": 0, "name": "tf.matmul", "pos": [9, 15]}], [{"sent_id": 1, "name": "tf.reduce_sum", "pos": [39, 45]}]], "sents": ["Luong-style attention:    scores = tf.matmul(query, key, transpose_b=True)", "Bahdanau-style attention: scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)"], "sent_idxs": [101, 11320, 5063, 1011, 2806, 3086, 1024, 7644, 1027, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 23032, 1010, 3145, 1010, 9099, 20688, 1035, 1038, 1027, 2995, 1007, 102, 101, 8670, 14945, 5162, 2226, 1011, 2806, 3086, 1024, 7644, 1027, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1056, 2546, 1012, 9092, 2232, 1006, 23032, 1009, 3643, 1007, 1010, 8123, 1027, 1011, 1015, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 63], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62707829", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [17, 22]}], [{"sent_id": 1, "name": "tf.keras.callbacks", "pos": [17, 25]}], [{"sent_id": 1, "name": "tf.keras.callbacks.callback", "pos": [17, 28]}]], "sents": ["This could be easily implemented using a callback.", "When inheriting from tf.keras.callbacks.Callback, you can override the on_epoch_end method which you can change to display plots after an epoch ends.", "The method takes two arguments - epoch, and logs.", "on_epoch_end is called with the current epoch number starting from 0.", "So, you can easily add a conditional with this.", "Below is some boilerplate code.", "<code>Code Snippet</code>.", "Then, you can pass this callback into your callbacks list when training."], "sent_idxs": [101, 2023, 2071, 2022, 4089, 7528, 2478, 1037, 2655, 5963, 1012, 102, 101, 2043, 22490, 2075, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 2655, 12221, 1012, 2655, 5963, 1010, 2017, 2064, 2058, 15637, 1996, 2006, 1035, 25492, 1035, 2203, 4118, 2029, 2017, 2064, 2689, 2000, 4653, 14811, 2044, 2019, 25492, 4515, 1012, 102, 101, 1996, 4118, 3138, 2048, 9918, 1011, 25492, 1010, 1998, 15664, 1012, 102, 101, 2006, 1035, 25492, 1035, 2203, 2003, 2170, 2007, 1996, 2783, 25492, 2193, 3225, 2013, 1014, 1012, 102, 101, 2061, 1010, 2017, 2064, 4089, 5587, 1037, 18462, 2007, 2023, 1012, 102, 101, 2917, 2003, 2070, 15635, 15725, 3642, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1010, 2017, 2064, 3413, 2023, 2655, 5963, 2046, 2115, 2655, 12221, 2862, 2043, 2731, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 53, 66, 84, 97, 106, 120, 138], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45569067", "vertexSet": [[{"sent_id": 2, "name": "tf.concat", "pos": [59, 64]}], [{"sent_id": 0, "name": "tf.layers", "pos": [1, 5]}], [{"sent_id": 0, "name": "tf.layers.dense", "pos": [1, 7]}]], "sents": ["tf.layers.dense expects tensor input.", "It is a list ([self.final_time_input, self.final_request_input, self.final_stream_input]) in your case.", "You need to concatenate them by using tf.concat such that", "<code>Code Snippet</code>.", "Assuming that input tensors have a shape of [batch_size, feature_size] where feature_size can be different."], "sent_idxs": [101, 1056, 2546, 1012, 9014, 1012, 9742, 24273, 23435, 7953, 1012, 102, 101, 2009, 2003, 1037, 2862, 1006, 1031, 2969, 1012, 2345, 1035, 2051, 1035, 7953, 1010, 2969, 1012, 2345, 1035, 5227, 1035, 7953, 1010, 2969, 1012, 2345, 1035, 5460, 1035, 7953, 1033, 1007, 1999, 2115, 2553, 1012, 102, 101, 2017, 2342, 2000, 9530, 16280, 12556, 2068, 2011, 2478, 1056, 2546, 1012, 9530, 11266, 2107, 2008, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10262, 2008, 7953, 23435, 2015, 2031, 1037, 4338, 1997, 1031, 14108, 1035, 2946, 1010, 3444, 1035, 2946, 1033, 2073, 3444, 1035, 2946, 2064, 2022, 2367, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 49, 67, 81, 109], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45920284", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [19, 25]}, {"sent_id": 2, "name": "tf.contrib", "pos": [62, 68]}], [{"sent_id": 14, "name": "tf.estimator", "pos": [246, 252]}], [{"sent_id": 0, "name": "tf.contrib.data", "pos": [19, 27]}, {"sent_id": 2, "name": "tf.contrib.data", "pos": [62, 70]}], [{"sent_id": 14, "name": "tf.estimator.estimator", "pos": [246, 256]}]], "sents": ["Short version: convert your CSV file to a tfrecords and then use tf.contrib.data.TFRecordDataset.", "Long version: see code See the question/accepted answer here (copied below for convenience).", "Check out the tf.contrib.data.Dataset API.", "I suspect you'll be best off converting your CSVs to TfRecord files and using TfRecordDataset.", "There's a thorough tutorial here.", "Step 1: Convert to csv data to tfrecords data.", "Example code below.", "<code>Code Snippet</code>.", "This assumes the labels are integers in the last column with float features in the preceding columns.", "This only needs to be run once.", "Step 2: Write a dataset the decodes these record files.", "<code>Code Snippet</code>.", "To test (independent of the estimator):", "<code>Code Snippet</code>.", "For use with tf.estimator.Estimator:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2460, 2544, 1024, 10463, 2115, 20116, 2615, 5371, 2000, 1037, 1056, 19699, 8586, 8551, 2015, 1998, 2059, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1012, 102, 101, 2146, 2544, 1024, 2156, 3642, 2156, 1996, 3160, 1013, 3970, 3437, 2182, 1006, 15826, 2917, 2005, 15106, 1007, 1012, 102, 101, 4638, 2041, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 2951, 13462, 17928, 1012, 102, 101, 1045, 8343, 2017, 1005, 2222, 2022, 2190, 2125, 16401, 2115, 20116, 15088, 2000, 1056, 19699, 8586, 8551, 6764, 1998, 2478, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1012, 102, 101, 2045, 1005, 1055, 1037, 16030, 14924, 4818, 2182, 1012, 102, 101, 3357, 1015, 1024, 10463, 2000, 20116, 2615, 2951, 2000, 1056, 19699, 8586, 8551, 2015, 2951, 1012, 102, 101, 2742, 3642, 2917, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 15980, 1996, 10873, 2024, 24028, 1999, 1996, 2197, 5930, 2007, 14257, 2838, 1999, 1996, 11003, 7753, 1012, 102, 101, 2023, 2069, 3791, 2000, 2022, 2448, 2320, 1012, 102, 101, 3357, 1016, 1024, 4339, 1037, 2951, 13462, 1996, 21933, 6155, 2122, 2501, 6764, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 3231, 1006, 2981, 1997, 1996, 9765, 9581, 4263, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 2224, 2007, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 37, 58, 76, 106, 117, 135, 141, 155, 175, 185, 201, 215, 228, 242, 258, 272], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60777752", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator.linearregressor", "pos": [28, 40]}], [{"sent_id": 0, "name": "tf.estimator.linearclassifier", "pos": [10, 20]}]], "sents": ["It was a simple mistake, I was using tf.estimator.LinearClassifier where in fact I should have been using tf.estimator.LinearRegressor as it was a float value I was trying to predict.", "Solution was to change the tf estimator type and also remove n_classes from the model creation."], "sent_idxs": [101, 2009, 2001, 1037, 3722, 6707, 1010, 1045, 2001, 2478, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 7399, 26266, 18095, 2073, 1999, 2755, 1045, 2323, 2031, 2042, 2478, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 7399, 2890, 17603, 24137, 2099, 2004, 2009, 2001, 1037, 14257, 3643, 1045, 2001, 2667, 2000, 16014, 1012, 102, 101, 5576, 2001, 2000, 2689, 1996, 1056, 2546, 9765, 9581, 4263, 2828, 1998, 2036, 6366, 1050, 1035, 4280, 2013, 1996, 2944, 4325, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 53, 77], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57549853", "vertexSet": [[{"sent_id": 7, "name": "tf.exp", "pos": [146, 151]}, {"sent_id": 7, "name": "tf.exp", "pos": [152, 157]}], [{"sent_id": 7, "name": "tf.split", "pos": [123, 127]}], [{"sent_id": 7, "name": "tf.slice", "pos": [118, 122]}], [{"sent_id": 7, "name": "tf.concat", "pos": [128, 133]}]], "sents": ["Your understanding of tensorRt is incorrect.", "The major reason that tensorRt inference may be faster than tensorflow inference comes from:", "Aggressively fusing several layer kernels into one single kernel.", "i.e., https://miro.medium.com/max/1155/0*7WA6t51EZ46355m6..", "Auto tuning and selecting the fastest kernel implementation for your input sizes..", "Reduced precision inference.", ".", "If your network has a lot of memory heavy operations: tf.slice, tf.split, tf.concat, or a lot of elementwise operations (e.g, tf.exp(tf.exp( a + b - c) ), there is little that can be done by tensorRt since the fused kernel is not implemented (cannot fuse two consecutive exp ops) or there is little to optimize for memory operations.", "That said, it also depends on the input size into your model.", "Launching a cuda kernel would incur overhead (say 0.1 ms).", "If the input size is 10x10 instead of 1000x1000, then computation time is trivial compared to kernel launch time.", "You would not see a lot of gains from using tensorRt if the amount of required computation is small.", "I converted your model and run inference on it with TensoRt5.1.5 on a TitanXp (FP32).", "Here is what i got:", "<code>Code Snippet</code>.", "Image size is 1x3x1200x1920 (batch x channels x height x width)."], "sent_idxs": [101, 2115, 4824, 1997, 23435, 5339, 2003, 16542, 1012, 102, 101, 1996, 2350, 3114, 2008, 23435, 5339, 28937, 2089, 2022, 5514, 2084, 23435, 12314, 28937, 3310, 2013, 1024, 102, 101, 24663, 11865, 7741, 2195, 6741, 16293, 2015, 2046, 2028, 2309, 16293, 1012, 102, 101, 1045, 1012, 1041, 1012, 1010, 16770, 1024, 1013, 1013, 14719, 2080, 1012, 5396, 1012, 4012, 1013, 4098, 1013, 10630, 2629, 1013, 1014, 1008, 1021, 4213, 2575, 2102, 22203, 9351, 21472, 19481, 2629, 2213, 2575, 1012, 1012, 102, 101, 8285, 17372, 1998, 17739, 1996, 7915, 16293, 7375, 2005, 2115, 7953, 10826, 1012, 1012, 102, 101, 4359, 11718, 28937, 1012, 102, 101, 1012, 102, 101, 2065, 2115, 2897, 2038, 1037, 2843, 1997, 3638, 3082, 3136, 1024, 1056, 2546, 1012, 14704, 1010, 1056, 2546, 1012, 3975, 1010, 1056, 2546, 1012, 9530, 11266, 1010, 2030, 1037, 2843, 1997, 5783, 14244, 3136, 1006, 1041, 1012, 1043, 1010, 1056, 2546, 1012, 4654, 2361, 1006, 1056, 2546, 1012, 4654, 2361, 1006, 1037, 1009, 1038, 1011, 1039, 1007, 1007, 1010, 2045, 2003, 2210, 2008, 2064, 2022, 2589, 2011, 23435, 5339, 2144, 1996, 19660, 16293, 2003, 2025, 7528, 1006, 3685, 19976, 2048, 5486, 4654, 2361, 23092, 1007, 2030, 2045, 2003, 2210, 2000, 23569, 27605, 4371, 2005, 3638, 3136, 1012, 102, 101, 2008, 2056, 1010, 2009, 2036, 9041, 2006, 1996, 7953, 2946, 2046, 2115, 2944, 1012, 102, 101, 12106, 1037, 12731, 2850, 16293, 2052, 4297, 3126, 8964, 1006, 2360, 1014, 1012, 1015, 5796, 1007, 1012, 102, 101, 2065, 1996, 7953, 2946, 2003, 2184, 2595, 10790, 2612, 1997, 6694, 2595, 18613, 2692, 1010, 2059, 22334, 2051, 2003, 20610, 4102, 2000, 16293, 4888, 2051, 1012, 102, 101, 2017, 2052, 2025, 2156, 1037, 2843, 1997, 12154, 2013, 2478, 23435, 5339, 2065, 1996, 3815, 1997, 3223, 22334, 2003, 2235, 1012, 102, 101, 1045, 4991, 2115, 2944, 1998, 2448, 28937, 2006, 2009, 2007, 23435, 2102, 2629, 1012, 1015, 1012, 1019, 2006, 1037, 16537, 2595, 2361, 1006, 1042, 2361, 16703, 1007, 1012, 102, 101, 2182, 2003, 2054, 1045, 2288, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3746, 2946, 2003, 1015, 2595, 2509, 2595, 12521, 8889, 2595, 16147, 11387, 1006, 14108, 1060, 6833, 1060, 4578, 1060, 9381, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 10, 29, 43, 81, 97, 103, 106, 205, 221, 240, 268, 291, 321, 329, 343, 367], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 2, 2, 2, 2, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49101717", "vertexSet": [[{"sent_id": 6, "name": "tf.image", "pos": [79, 83]}, {"sent_id": 7, "name": "tf.image", "pos": [103, 107]}], [{"sent_id": 6, "name": "tf.image.decode_png", "pos": [79, 89]}], [{"sent_id": 7, "name": "tf.image.decode_jpeg", "pos": [103, 113]}]], "sents": ["I faced this issue recently,", "<code>Code Snippet</code>.", "doesn't return tensor with shape, but my images were all in jpeg format.", "So I used", "<code>Code Snippet</code>.", "it returns tensor with the shape and that solved the problem.", "Note there is tf.image.decode_png() too.", "More info can be found here Tensorflow tf.image.decode_jpeg documentation"], "sent_idxs": [101, 1045, 4320, 2023, 3277, 3728, 1010, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2987, 1005, 1056, 2709, 23435, 2007, 4338, 1010, 2021, 2026, 4871, 2020, 2035, 1999, 16545, 13910, 4289, 1012, 102, 101, 2061, 1045, 2109, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2009, 5651, 23435, 2007, 1996, 4338, 1998, 2008, 13332, 1996, 3291, 1012, 102, 101, 3602, 2045, 2003, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 1052, 3070, 1006, 1007, 2205, 1012, 102, 101, 2062, 18558, 2064, 2022, 2179, 2182, 23435, 12314, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 12653, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 8, 22, 42, 47, 61, 75, 94, 115], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0]}, {"title": "56796660", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [1, 6]}], [{"sent_id": 0, "name": "tf.keras.backend", "pos": [1, 9]}], [{"sent_id": 0, "name": "tf.keras.backend.eval", "pos": [1, 12]}]], "sents": ["tf.keras.backend.eval is useful for evaluating small expressions.", "<code>Code Snippet</code>.", "TF 1.x and TF 2.0 compatible.", "Minimal Verifiable Example", "<code>Code Snippet</code>.", "This is useful because you do not have to explicitly create a Session or InteractiveSession."], "sent_idxs": [101, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 9345, 2140, 2003, 6179, 2005, 23208, 2235, 11423, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1015, 1012, 1060, 1998, 1056, 2546, 1016, 1012, 1014, 11892, 1012, 102, 101, 10124, 2310, 3089, 22749, 3468, 2742, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2003, 6179, 2138, 2017, 2079, 2025, 2031, 2000, 12045, 3443, 1037, 5219, 2030, 9123, 8583, 10992, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 34, 49, 57, 71, 91], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55145960", "vertexSet": [[{"sent_id": 4, "name": "tf.shape", "pos": [105, 109]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [18, 22]}, {"sent_id": 4, "name": "tf.tensor", "pos": [88, 92]}], [{"sent_id": 0, "name": "tf.tensorshape", "pos": [18, 24]}]], "sents": ["The .shape attribute gives you the shape known at graph construction time, which is a tf.TensorShape structure.", "If the shape of x were fully known, you could get your code to work as follows:", "<code>Code Snippet</code>.", "However, in your case x has an unknown first dimension.", "In order to use the actual tensor shape as a regular tf.Tensor (with value only known at runtime), you can use tf.shape:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 1012, 4338, 17961, 3957, 2017, 1996, 4338, 2124, 2012, 10629, 2810, 2051, 1010, 2029, 2003, 1037, 1056, 2546, 1012, 23435, 7377, 5051, 3252, 1012, 102, 101, 2065, 1996, 4338, 1997, 1060, 2020, 3929, 2124, 1010, 2017, 2071, 2131, 2115, 3642, 2000, 2147, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 1010, 1999, 2115, 2553, 1060, 2038, 2019, 4242, 2034, 9812, 1012, 102, 101, 1999, 2344, 2000, 2224, 1996, 5025, 23435, 4338, 2004, 1037, 3180, 1056, 2546, 1012, 23435, 1006, 2007, 3643, 2069, 2124, 2012, 2448, 7292, 1007, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 4338, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 48, 62, 76, 111, 125], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59587988", "vertexSet": [[{"sent_id": 4, "name": "tf.summary", "pos": [64, 68]}, {"sent_id": 7, "name": "tf.summary", "pos": [115, 119]}, {"sent_id": 9, "name": "tf.summary", "pos": [167, 171]}, {"sent_id": 10, "name": "tf.summary", "pos": [207, 211]}], [{"sent_id": 7, "name": "tf.summary.scalar", "pos": [115, 122]}, {"sent_id": 9, "name": "tf.summary.scalar", "pos": [167, 174]}, {"sent_id": 10, "name": "tf.summary.scalar", "pos": [207, 214]}], [{"sent_id": 4, "name": "tf.summary.create_file_writer", "pos": [64, 74]}]], "sents": ["That's the intended behavior.", "If you want to log custom scalars such as a dynamic learning rate, you need to use the TensorFlow Summary API.", "Retrain the regression model and log a custom learning rate.", "Here's how:", "Create a file writer, using tf.summary.create_file_writer()..", "Define a custom learning rate function.", "This will be passed to the Keras LearningRateScheduler callback..", "Inside the learning rate function, use tf.summary.scalar() to log the custom learning rate..", "Pass the LearningRateScheduler callback to Model.fit()..", "In general, to log a custom scalar, you need to use tf.summary.scalar() with a file writer.", "The file writer is responsible for writing data for this run to the specified directory and is implicitly used when you use the tf.summary.scalar().", "<code>Code Snippet</code>."], "sent_idxs": [101, 2008, 1005, 1055, 1996, 3832, 5248, 1012, 102, 101, 2065, 2017, 2215, 2000, 8833, 7661, 26743, 2869, 2107, 2004, 1037, 8790, 4083, 3446, 1010, 2017, 2342, 2000, 2224, 1996, 23435, 12314, 12654, 17928, 1012, 102, 101, 2128, 23654, 1996, 26237, 2944, 1998, 8833, 1037, 7661, 4083, 3446, 1012, 102, 101, 2182, 1005, 1055, 2129, 1024, 102, 101, 3443, 1037, 5371, 3213, 1010, 2478, 1056, 2546, 1012, 12654, 1012, 3443, 1035, 5371, 1035, 3213, 1006, 1007, 1012, 1012, 102, 101, 9375, 1037, 7661, 4083, 3446, 3853, 1012, 102, 101, 2023, 2097, 2022, 2979, 2000, 1996, 17710, 8180, 4083, 20370, 7690, 9307, 2099, 2655, 5963, 1012, 1012, 102, 101, 2503, 1996, 4083, 3446, 3853, 1010, 2224, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1006, 1007, 2000, 8833, 1996, 7661, 4083, 3446, 1012, 1012, 102, 101, 3413, 1996, 4083, 20370, 7690, 9307, 2099, 2655, 5963, 2000, 2944, 1012, 4906, 1006, 1007, 1012, 1012, 102, 101, 1999, 2236, 1010, 2000, 8833, 1037, 7661, 26743, 2099, 1010, 2017, 2342, 2000, 2224, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1006, 1007, 2007, 1037, 5371, 3213, 1012, 102, 101, 1996, 5371, 3213, 2003, 3625, 2005, 3015, 2951, 2005, 2023, 2448, 2000, 1996, 9675, 14176, 1998, 2003, 24655, 2135, 2109, 2043, 2017, 2224, 1996, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1006, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 36, 50, 57, 79, 88, 107, 133, 152, 182, 218, 232], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48347663", "vertexSet": [[{"sent_id": 0, "name": "tf.image.decode_image", "pos": [10, 19]}, {"sent_id": 2, "name": "tf.image.decode_image", "pos": [57, 66]}], [{"sent_id": 4, "name": "tf.image.decode_png", "pos": [121, 131]}], [{"sent_id": 4, "name": "tf.image.decode_jpeg", "pos": [110, 120]}]], "sents": ["The issue here actually comes from the fact that tf.image.decode_image doesn't return the shape of the image.", "This was explained in these two GitHub issues: issue1, issue2.", "The problem comes from the fact that tf.image.decode_image also handles .gif, which returns a 4D tensor, whereas .jpg and .png return 3D images.", "Therefore, the correct shape cannot be returned.", "The solution is to simply use tf.image.decode_jpeg or tf.image.decode_png (both work the same and can be used on .png and .jpg images).", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3277, 2182, 2941, 3310, 2013, 1996, 2755, 2008, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 3746, 2987, 1005, 1056, 2709, 1996, 4338, 1997, 1996, 3746, 1012, 102, 101, 2023, 2001, 4541, 1999, 2122, 2048, 21025, 2705, 12083, 3314, 1024, 3277, 2487, 1010, 3277, 2475, 1012, 102, 101, 1996, 3291, 3310, 2013, 1996, 2755, 2008, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 3746, 2036, 16024, 1012, 21025, 2546, 1010, 2029, 5651, 1037, 1018, 2094, 23435, 1010, 6168, 1012, 16545, 2290, 1998, 1012, 1052, 3070, 2709, 7605, 4871, 1012, 102, 101, 3568, 1010, 1996, 6149, 4338, 3685, 2022, 2513, 1012, 102, 101, 1996, 5576, 2003, 2000, 3432, 2224, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 2030, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 1052, 3070, 1006, 2119, 2147, 1996, 2168, 1998, 2064, 2022, 2109, 2006, 1012, 1052, 3070, 1998, 1012, 16545, 2290, 4871, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 2, 3, 4]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 2, 3, 4]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 30, 49, 92, 103, 152, 166], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44913058", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [75, 80]}], [{"sent_id": 2, "name": "tf.nn.relu", "pos": [75, 83]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [16, 22]}]], "sents": ["They are essentially the same, the later calling the former.", "However tf.contrib.fully_connected adds a few functionalities on top of dense, in particular the possibility to pass a normalization and an activation in the parameters, \u00e0 la Keras.", "As noted by @wordforthewise, mind that the later defaults to tf.nn.relu.", "More generally, the TF API proposes (and mixes somewhat confusingly) low- and hi-level APIs; more on that here."], "sent_idxs": [101, 2027, 2024, 7687, 1996, 2168, 1010, 1996, 2101, 4214, 1996, 2280, 1012, 102, 101, 2174, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3929, 1035, 4198, 9909, 1037, 2261, 8360, 6447, 2006, 2327, 1997, 9742, 1010, 1999, 3327, 1996, 6061, 2000, 3413, 1037, 3671, 3989, 1998, 2019, 13791, 1999, 1996, 11709, 1010, 1037, 2474, 17710, 8180, 1012, 102, 101, 2004, 3264, 2011, 1030, 2773, 15628, 7974, 5562, 1010, 2568, 2008, 1996, 2101, 12398, 2015, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 2128, 7630, 1012, 102, 101, 2062, 3227, 1010, 1996, 1056, 2546, 17928, 17146, 1006, 1998, 21109, 5399, 16801, 2135, 1007, 2659, 1011, 1998, 7632, 1011, 2504, 17928, 2015, 1025, 2062, 2006, 2008, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 58, 85, 116], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56014787", "vertexSet": [[{"sent_id": 2, "name": "tf.data", "pos": [34, 38]}], [{"sent_id": 2, "name": "tf.data.experimental", "pos": [34, 40]}], [{"sent_id": 2, "name": "tf.data.experimental.make_csv_dataset", "pos": [34, 48]}]], "sents": ["You don't need this line", "<code>Code Snippet</code>.", "Function you're using to create dataset, tf.data.experimental.make_csv_dataset alredy batched it.", "You can use repeat though"], "sent_idxs": [101, 2017, 2123, 1005, 1056, 2342, 2023, 2240, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3853, 2017, 1005, 2128, 2478, 2000, 3443, 2951, 13462, 1010, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 2191, 1035, 20116, 2615, 1035, 2951, 13462, 2632, 5596, 2100, 14108, 2098, 2009, 1012, 102, 101, 2017, 2064, 2224, 9377, 2295, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 23, 56, 63], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57791552", "vertexSet": [[{"sent_id": 2, "name": "tf.compat", "pos": [46, 52]}], [{"sent_id": 2, "name": "tf.compat.v1", "pos": [46, 55]}], [{"sent_id": 3, "name": "tf.get_default_graph", "pos": [68, 76]}], [{"sent_id": 2, "name": "tf.compat.v1.get_default_graph", "pos": [46, 61]}]], "sents": ["Another cause due to which this is happening is that in tensorflow_backend.py", "located  in : lib/python3.6/site-packages/keras/backend/", "uses tf.compat.v1.get_default_graph for obtaining graph", "instead of tf.get_default_graph.", "By replacing this in the directory this problem can be solved successfully."], "sent_idxs": [101, 2178, 3426, 2349, 2000, 2029, 2023, 2003, 6230, 2003, 2008, 1999, 23435, 12314, 1035, 2067, 10497, 1012, 1052, 2100, 102, 101, 2284, 1999, 1024, 5622, 2497, 1013, 18750, 2509, 1012, 1020, 1013, 2609, 1011, 14555, 1013, 17710, 8180, 1013, 2067, 10497, 1013, 102, 101, 3594, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 2131, 1035, 12398, 1035, 10629, 2005, 11381, 10629, 102, 101, 2612, 1997, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1012, 102, 101, 2011, 6419, 2023, 1999, 1996, 14176, 2023, 3291, 2064, 2022, 13332, 5147, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 21, 44, 65, 78, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47120911", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [2, 7]}], [{"sent_id": 0, "name": "tf.identity", "pos": [52, 56]}], [{"sent_id": 0, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [2, 19]}]], "sents": ["Since tf.nn.softmax_cross_entropy_with_logits computes internally the softmax (in a numerically stable way) of its input, you have to define your network in order to use the linear activation function: tf.identity", "<code>Code Snippet</code>.", "Moreover, once the network has been trained and you want to use use the model for inference, you have to replace the activation with the softmax.", "Thus, introduce in your code a is_training python boolean variable, and use it to change your model definition when you're training or testing.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2144, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 24134, 2015, 16058, 1996, 3730, 17848, 1006, 1999, 1037, 15973, 2135, 6540, 2126, 1007, 1997, 2049, 7953, 1010, 2017, 2031, 2000, 9375, 2115, 2897, 1999, 2344, 2000, 2224, 1996, 7399, 13791, 3853, 1024, 1056, 2546, 1012, 4767, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 9308, 1010, 2320, 1996, 2897, 2038, 2042, 4738, 1998, 2017, 2215, 2000, 2224, 2224, 1996, 2944, 2005, 28937, 1010, 2017, 2031, 2000, 5672, 1996, 13791, 2007, 1996, 3730, 17848, 1012, 102, 101, 2947, 1010, 8970, 1999, 2115, 3642, 1037, 2003, 1035, 2731, 18750, 22017, 20898, 8023, 1010, 1998, 2224, 2009, 2000, 2689, 2115, 2944, 6210, 2043, 2017, 1005, 2128, 2731, 2030, 5604, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 57, 71, 103, 136, 150], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48546187", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [1, 5]}, {"sent_id": 4, "name": "tf.train", "pos": [78, 82]}, {"sent_id": 5, "name": "tf.train", "pos": [119, 123]}, {"sent_id": 10, "name": "tf.train", "pos": [252, 256]}], [{"sent_id": 4, "name": "tf.session", "pos": [95, 99]}], [{"sent_id": 0, "name": "tf.train.server", "pos": [1, 7]}, {"sent_id": 4, "name": "tf.train.server", "pos": [78, 84]}, {"sent_id": 10, "name": "tf.train.server", "pos": [252, 258]}], [{"sent_id": 5, "name": "tf.train.clusterspec", "pos": [119, 127]}]], "sents": ["tf.train.Server is designed for distributed computation within a cluster, when there is a need to communicate between different nodes.", "This is especially useful when training is distributed across multiple machines or in some cases across multiple GPUs on a single machine.", "From the documentation:", "An in-process TensorFlow server, for use in distributed training.", "A tf.train.Server instance encapsulates a set of devices and a tf.Session target that can participate in distributed training.", "A server belongs to a cluster (specified by a tf.train.ClusterSpec), and corresponds to a particular task in a named job.", "The server can communicate with any other server in the same cluster.", "Spawning multiple processes with multiprocessing.Process isn't a cluster in Tensorflow sense, because the child processes aren't interacting with each other.", "This method is easier to setup, but it's limited to a single machine.", "Since you say you have just one machine, this might not be a strong argument, but if you ever plan to scale to a cluster of machines, you'll have to redesign the whole approach.", "tf.train.Server is thus a more universal and scalable solution.", "Besides, it allows to organize complex training with some non-trivial communications, e.g., async gradient updates.", "Whether it is faster to train or not greatly depends on a task, I don't think there will be a significant difference on one shared GPU.", "Just for the reference, here's how the code looks like with the server (between graph replication example):", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 3345, 1012, 8241, 2003, 2881, 2005, 5500, 22334, 2306, 1037, 9324, 1010, 2043, 2045, 2003, 1037, 2342, 2000, 10639, 2090, 2367, 14164, 1012, 102, 101, 2023, 2003, 2926, 6179, 2043, 2731, 2003, 5500, 2408, 3674, 6681, 2030, 1999, 2070, 3572, 2408, 3674, 14246, 2271, 2006, 1037, 2309, 3698, 1012, 102, 101, 2013, 1996, 12653, 1024, 102, 101, 2019, 1999, 1011, 2832, 23435, 12314, 8241, 1010, 2005, 2224, 1999, 5500, 2731, 1012, 102, 101, 1037, 1056, 2546, 1012, 3345, 1012, 8241, 6013, 4372, 17695, 23722, 8520, 1037, 2275, 1997, 5733, 1998, 1037, 1056, 2546, 1012, 5219, 4539, 2008, 2064, 5589, 1999, 5500, 2731, 1012, 102, 101, 1037, 8241, 7460, 2000, 1037, 9324, 1006, 9675, 2011, 1037, 1056, 2546, 1012, 3345, 1012, 12906, 5051, 2278, 1007, 1010, 1998, 14788, 2000, 1037, 3327, 4708, 1999, 1037, 2315, 3105, 1012, 102, 101, 1996, 8241, 2064, 10639, 2007, 2151, 2060, 8241, 1999, 1996, 2168, 9324, 1012, 102, 101, 27957, 3674, 6194, 2007, 4800, 21572, 9623, 7741, 1012, 2832, 3475, 1005, 1056, 1037, 9324, 1999, 23435, 12314, 3168, 1010, 2138, 1996, 2775, 6194, 4995, 1005, 1056, 21935, 2007, 2169, 2060, 1012, 102, 101, 2023, 4118, 2003, 6082, 2000, 16437, 1010, 2021, 2009, 1005, 1055, 3132, 2000, 1037, 2309, 3698, 1012, 102, 101, 2144, 2017, 2360, 2017, 2031, 2074, 2028, 3698, 1010, 2023, 2453, 2025, 2022, 1037, 2844, 6685, 1010, 2021, 2065, 2017, 2412, 2933, 2000, 4094, 2000, 1037, 9324, 1997, 6681, 1010, 2017, 1005, 2222, 2031, 2000, 25136, 1996, 2878, 3921, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 8241, 2003, 2947, 1037, 2062, 5415, 1998, 26743, 3468, 5576, 1012, 102, 101, 4661, 1010, 2009, 4473, 2000, 10939, 3375, 2731, 2007, 2070, 2512, 1011, 20610, 4806, 1010, 1041, 1012, 1043, 1012, 1010, 2004, 6038, 2278, 17978, 14409, 1012, 102, 101, 3251, 2009, 2003, 5514, 2000, 3345, 2030, 2025, 6551, 9041, 2006, 1037, 4708, 1010, 1045, 2123, 1005, 1056, 2228, 2045, 2097, 2022, 1037, 3278, 4489, 2006, 2028, 4207, 14246, 2226, 1012, 102, 101, 2074, 2005, 1996, 4431, 1010, 2182, 1005, 1055, 2129, 1996, 3642, 3504, 2066, 2007, 1996, 8241, 1006, 2090, 10629, 21647, 2742, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 28, 54, 60, 76, 108, 141, 156, 190, 209, 251, 269, 297, 330, 355, 369], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54902812", "vertexSet": [[{"sent_id": 9, "name": "tf.contrib", "pos": [233, 239]}], [{"sent_id": 9, "name": "tf.contrib.predictor", "pos": [233, 242]}], [{"sent_id": 9, "name": "tf.contrib.predictor.from_saved_model", "pos": [233, 248]}]], "sents": ["Tensorflow detection API supports different input formats during exporting as discribed in documentation of file export_inference_graph.py:", "image_tensor: Accepts a uint8 4-D tensor of shape [None, None, None, 3].", "encoded_image_string_tensor: Accepts a 1-D string tensor of shape [None]\ncontaining encoded PNG or JPEG images.", "Image resolutions are expected to be\nthe same if more than 1 image is provided..", "tf_example: Accepts a 1-D string tensor of shape [None] containing\nserialized TFExample protos.", "Image resolutions are expected to be the same\nif more than 1 image is provided..", "So you should check that you use image_tensor input_type.", "The chosen input node will be named as \"inputs\" in exported model.", "So I suppose that replacing image_tensor:0 with inputs (or maybe inputs:0) will solve your problem.", "Also I would like to recommend a useful tool to run exported models with several lines of code: tf.contrib.predictor.from_saved_model.", "Here is example of how to use it:", "<code>Code Snippet</code>."], "sent_idxs": [101, 23435, 12314, 10788, 17928, 6753, 2367, 7953, 11630, 2076, 9167, 2075, 2004, 5860, 3089, 8270, 1999, 12653, 1997, 5371, 9167, 1035, 28937, 1035, 10629, 1012, 1052, 2100, 1024, 102, 101, 3746, 1035, 23435, 1024, 13385, 1037, 21318, 3372, 2620, 1018, 1011, 1040, 23435, 1997, 4338, 1031, 3904, 1010, 3904, 1010, 3904, 1010, 1017, 1033, 1012, 102, 101, 12359, 1035, 3746, 1035, 5164, 1035, 23435, 1024, 13385, 1037, 1015, 1011, 1040, 5164, 23435, 1997, 4338, 1031, 3904, 1033, 4820, 12359, 1052, 3070, 2030, 16545, 13910, 4871, 1012, 102, 101, 3746, 18853, 2024, 3517, 2000, 2022, 1996, 2168, 2065, 2062, 2084, 1015, 3746, 2003, 3024, 1012, 1012, 102, 101, 1056, 2546, 1035, 2742, 1024, 13385, 1037, 1015, 1011, 1040, 5164, 23435, 1997, 4338, 1031, 3904, 1033, 4820, 27289, 1056, 7959, 18684, 23344, 15053, 2015, 1012, 102, 101, 3746, 18853, 2024, 3517, 2000, 2022, 1996, 2168, 2065, 2062, 2084, 1015, 3746, 2003, 3024, 1012, 1012, 102, 101, 2061, 2017, 2323, 4638, 2008, 2017, 2224, 3746, 1035, 23435, 7953, 1035, 2828, 1012, 102, 101, 1996, 4217, 7953, 13045, 2097, 2022, 2315, 2004, 1000, 20407, 1000, 1999, 15612, 2944, 1012, 102, 101, 2061, 1045, 6814, 2008, 6419, 3746, 1035, 23435, 1024, 1014, 2007, 20407, 1006, 2030, 2672, 20407, 1024, 1014, 1007, 2097, 9611, 2115, 3291, 1012, 102, 101, 2036, 1045, 2052, 2066, 2000, 16755, 1037, 6179, 6994, 2000, 2448, 15612, 4275, 2007, 2195, 3210, 1997, 3642, 1024, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 16014, 2953, 1012, 2013, 1035, 5552, 1035, 2944, 1012, 102, 101, 2182, 2003, 2742, 1997, 2129, 2000, 2224, 2009, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 30, 57, 88, 107, 135, 154, 170, 187, 213, 250, 261, 275], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53390725", "vertexSet": [[{"sent_id": 0, "name": "tf.norm", "pos": [7, 11]}, {"sent_id": 7, "name": "tf.norm", "pos": [82, 86]}], [{"sent_id": 0, "name": "tf.linalg", "pos": [12, 18]}], [{"sent_id": 0, "name": "tf.linalg.norm", "pos": [12, 20]}]], "sents": ["You can achieve that simply with tf.norm/tf.linalg.norm:", "<code>Code Snippet</code>.", "For example:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "EDIT:", "If you cannot use tf.norm, the following is an equivalent implementation:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 6162, 2008, 3432, 2007, 1056, 2546, 1012, 13373, 1013, 1056, 2546, 1012, 27022, 2140, 2290, 1012, 13373, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10086, 1024, 102, 101, 2065, 2017, 3685, 2224, 1056, 2546, 1012, 13373, 1010, 1996, 2206, 2003, 2019, 5662, 7375, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 36, 41, 55, 59, 73, 77, 95, 109], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40943823", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [28, 32]}, {"sent_id": 1, "name": "tf.train", "pos": [64, 68]}], [{"sent_id": 0, "name": "tf.session", "pos": [8, 12]}], [{"sent_id": 1, "name": "tf.train.supervisor", "pos": [64, 70]}], [{"sent_id": 0, "name": "tf.train.start_queue_runners", "pos": [28, 38]}]], "sents": ["If you just use code of \nwith tf.Session() as sess:,\nyou must open the thread explicitly with \nthreads = tf.train.start_queue_runners().", "But in ptb_word_lm.py, It uses the codes like this \nsv = tf.train.Supervisor()  with sv.managed_session() as sess:,\nthe Supervisor() function contains something which starts the thread implicitly"], "sent_idxs": [101, 2065, 2017, 2074, 2224, 3642, 1997, 2007, 1056, 2546, 1012, 5219, 1006, 1007, 2004, 7367, 4757, 1024, 1010, 2017, 2442, 2330, 1996, 11689, 12045, 2007, 16457, 1027, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 1007, 1012, 102, 101, 2021, 1999, 13866, 2497, 1035, 2773, 1035, 1048, 2213, 1012, 1052, 2100, 1010, 2009, 3594, 1996, 9537, 2066, 2023, 17917, 1027, 1056, 2546, 1012, 3345, 1012, 12366, 1006, 1007, 2007, 17917, 1012, 3266, 1035, 5219, 1006, 1007, 2004, 7367, 4757, 1024, 1010, 1996, 12366, 1006, 1007, 3853, 3397, 2242, 2029, 4627, 1996, 11689, 24655, 2135, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 42, 99], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43576298", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.max_pool", "pos": [50, 59]}], [{"sent_id": 1, "name": "tf.reduce_max", "pos": [39, 45]}]], "sents": ["It seems like you simply want to find the largest value over one of the dimensions which may be of dynamic size.", "If that is the case, you are probably better off using the tf.reduce_max() function instead of tf.nn.max_pool().", "<code>Code Snippet</code>.", "I set keep_dims=True because it corresponds to what you would get if max pooling worked, but it is probably easier to work with the result if you set keep_dims=False."], "sent_idxs": [101, 2009, 3849, 2066, 2017, 3432, 2215, 2000, 2424, 1996, 2922, 3643, 2058, 2028, 1997, 1996, 9646, 2029, 2089, 2022, 1997, 8790, 2946, 1012, 102, 101, 2065, 2008, 2003, 1996, 2553, 1010, 2017, 2024, 2763, 2488, 2125, 2478, 1996, 1056, 2546, 1012, 5547, 1035, 4098, 1006, 1007, 3853, 2612, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 4098, 1035, 4770, 1006, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 2275, 2562, 1035, 11737, 2015, 1027, 2995, 2138, 2009, 14788, 2000, 2054, 2017, 2052, 2131, 2065, 4098, 4770, 2075, 2499, 1010, 2021, 2009, 2003, 2763, 6082, 2000, 2147, 2007, 1996, 2765, 2065, 2017, 2275, 2562, 1035, 11737, 2015, 1027, 6270, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 25, 63, 77, 121], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35164907", "vertexSet": [[{"sent_id": 6, "name": "tf.squeeze", "pos": [160, 164]}], [{"sent_id": 1, "name": "tf.transpose", "pos": [21, 26]}, {"sent_id": 3, "name": "tf.transpose", "pos": [84, 89]}], [{"sent_id": 4, "name": "tf.expand_dims", "pos": [101, 108]}, {"sent_id": 8, "name": "tf.expand_dims", "pos": [240, 247]}]], "sents": ["There are three relevant ops for implementing Theano's dimshuffle in TensorFlow:", "tf.transpose() is used to permute the dimensions of a tensor.", "If the pattern specified in the arguments to dimshuffle is a permutation of the input tensor's dimensions (i.e.", "there is no 'x' or missing dimension) you can use tf.transpose() to implement dimshuffle().", "tf.expand_dims() is used to add one or more size-1 dimensions to a tensor.", "This handles the case where 'x' is specified as part of the dimshuffle() pattern, but does not reorder the existing dimensions.", "tf.squeeze() is used to remove one or more size-1 dimensions from a tensor.", "This handles the case where a dimension is omitted from a dimshuffle() pattern, but it does not reorder the existing dimensions.", "Assuming that the input is a vector, your example (dimshuffle(0, 'x')) can be expressed using tf.expand_dims() only:", "<code>Code Snippet</code>.", "Taking a more complicated example, dimshuffle(1, 'x', 0) applied to a matrix would be:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2024, 2093, 7882, 23092, 2005, 14972, 1996, 6761, 1005, 1055, 11737, 14235, 18142, 1999, 23435, 12314, 1024, 102, 101, 1056, 2546, 1012, 9099, 20688, 1006, 1007, 2003, 2109, 2000, 2566, 26746, 1996, 9646, 1997, 1037, 23435, 1012, 102, 101, 2065, 1996, 5418, 9675, 1999, 1996, 9918, 2000, 11737, 14235, 18142, 2003, 1037, 2566, 28120, 3370, 1997, 1996, 7953, 23435, 1005, 1055, 9646, 1006, 1045, 1012, 1041, 1012, 102, 101, 2045, 2003, 2053, 1005, 1060, 1005, 2030, 4394, 9812, 1007, 2017, 2064, 2224, 1056, 2546, 1012, 9099, 20688, 1006, 1007, 2000, 10408, 11737, 14235, 18142, 1006, 1007, 1012, 102, 101, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 1006, 1007, 2003, 2109, 2000, 5587, 2028, 2030, 2062, 2946, 1011, 1015, 9646, 2000, 1037, 23435, 1012, 102, 101, 2023, 16024, 1996, 2553, 2073, 1005, 1060, 1005, 2003, 9675, 2004, 2112, 1997, 1996, 11737, 14235, 18142, 1006, 1007, 5418, 1010, 2021, 2515, 2025, 2128, 8551, 2121, 1996, 4493, 9646, 1012, 102, 101, 1056, 2546, 1012, 11025, 1006, 1007, 2003, 2109, 2000, 6366, 2028, 2030, 2062, 2946, 1011, 1015, 9646, 2013, 1037, 23435, 1012, 102, 101, 2023, 16024, 1996, 2553, 2073, 1037, 9812, 2003, 16647, 2013, 1037, 11737, 14235, 18142, 1006, 1007, 5418, 1010, 2021, 2009, 2515, 2025, 2128, 8551, 2121, 1996, 4493, 9646, 1012, 102, 101, 10262, 2008, 1996, 7953, 2003, 1037, 9207, 1010, 2115, 2742, 1006, 11737, 14235, 18142, 1006, 1014, 1010, 1005, 1060, 1005, 1007, 1007, 2064, 2022, 5228, 2478, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 1006, 1007, 2069, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2635, 1037, 2062, 8552, 2742, 1010, 11737, 14235, 18142, 1006, 1015, 1010, 1005, 1060, 1005, 1010, 1014, 1007, 4162, 2000, 1037, 8185, 2052, 2022, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 40, 70, 100, 126, 159, 182, 213, 252, 266, 293, 307], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56040520", "vertexSet": [[{"sent_id": 2, "name": "tf.compat.v1.py_func", "pos": [71, 86]}], [{"sent_id": 3, "name": "tf.py_function", "pos": [103, 110]}], [{"sent_id": 0, "name": "tf.data", "pos": [3, 7]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [3, 10]}]], "sents": ["No, tf.data.Dataset.from_generator won't be deprecated in TensorFlow 2.0.", "What you see is a warning message, it's used to inform users about future changes.", "In case you need to use py_func directly, the most straightforward way is to use tf.compat.v1.py_func.", "TF2.0 has it's own wrapper, called tf.py_function."], "sent_idxs": [101, 2053, 1010, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 13103, 2180, 1005, 1056, 2022, 2139, 28139, 12921, 1999, 23435, 12314, 1016, 1012, 1014, 1012, 102, 101, 2054, 2017, 2156, 2003, 1037, 5432, 4471, 1010, 2009, 1005, 1055, 2109, 2000, 12367, 5198, 2055, 2925, 3431, 1012, 102, 101, 1999, 2553, 2017, 2342, 2000, 2224, 1052, 2100, 1035, 4569, 2278, 3495, 1010, 1996, 2087, 19647, 2126, 2003, 2000, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 1052, 2100, 1035, 4569, 2278, 1012, 102, 101, 1056, 2546, 2475, 1012, 1014, 2038, 2009, 1005, 1055, 2219, 10236, 4842, 1010, 2170, 1056, 2546, 1012, 1052, 2100, 1035, 3853, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2, 3]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 29, 50, 88, 112], "sent_pos": [0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "56414647", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [12, 16]}, {"sent_id": 0, "name": "tf.train", "pos": [33, 37]}], [{"sent_id": 0, "name": "tf.train.sessionrunhook", "pos": [12, 21]}], [{"sent_id": 0, "name": "tf.train.sessionrunargs", "pos": [33, 42]}]], "sents": ["The intended solution for something like this is to subclass tf.train.SessionRunHook and override the before_run method to return a suitable tf.train.SessionRunArgs.", "This will allow you to feed values at train time and add fetches to the session.run call.", "Your class will have to carry a reference to the placeholder and the loss state in-between the calls.", "Then you simply instantiate the class and add the hook to the hooks parameter in your estimator.train call or in this case your train_spec.", "If you wish to use the evaluation loss instead of the training loss then this can be achieved by adding another hook to the eval_spec that reads out the values in the after_run method."], "sent_idxs": [101, 1996, 3832, 5576, 2005, 2242, 2066, 2023, 2003, 2000, 4942, 26266, 1056, 2546, 1012, 3345, 1012, 5219, 15532, 6806, 6559, 1998, 2058, 15637, 1996, 2077, 1035, 2448, 4118, 2000, 2709, 1037, 7218, 1056, 2546, 1012, 3345, 1012, 5219, 26605, 10623, 2015, 1012, 102, 101, 2023, 2097, 3499, 2017, 2000, 5438, 5300, 2012, 3345, 2051, 1998, 5587, 18584, 2229, 2000, 1996, 5219, 1012, 2448, 2655, 1012, 102, 101, 2115, 2465, 2097, 2031, 2000, 4287, 1037, 4431, 2000, 1996, 2173, 14528, 1998, 1996, 3279, 2110, 1999, 1011, 2090, 1996, 4455, 1012, 102, 101, 2059, 2017, 3432, 7107, 13143, 1996, 2465, 1998, 5587, 1996, 8103, 2000, 1996, 18008, 16381, 1999, 2115, 9765, 9581, 4263, 1012, 3345, 2655, 2030, 1999, 2023, 2553, 2115, 3345, 1035, 28699, 1012, 102, 101, 2065, 2017, 4299, 2000, 2224, 1996, 9312, 3279, 2612, 1997, 1996, 2731, 3279, 2059, 2023, 2064, 2022, 4719, 2011, 5815, 2178, 8103, 2000, 1996, 9345, 2140, 1035, 28699, 2008, 9631, 2041, 1996, 5300, 1999, 1996, 2044, 1035, 2448, 4118, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 44, 67, 91, 125, 167], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45096525", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [41, 46]}], [{"sent_id": 11, "name": "tf.matmul", "pos": [219, 225]}], [{"sent_id": 1, "name": "tf.sparse_matmul", "pos": [24, 32]}, {"sent_id": 12, "name": "tf.sparse_matmul", "pos": [268, 276]}], [{"sent_id": 1, "name": "tf.sparse_tensor_dense_matmul", "pos": [9, 21]}], [{"sent_id": 1, "name": "tf.nn.embedding_lookup_sparse", "pos": [41, 55]}]], "sents": ["TL;DR.", "Use tf.sparse_tensor_dense_matmul in place of tf.sparse_matmul; look at the documentation for an alternative using tf.nn.embedding_lookup_sparse.", "About sparse matrices and SparseTensors.", "The problem is not specific to sparse_placeholder, but due to a confusion in tensorflow's terminology.", "You have sparse matrices.", "And then you have SparseTensor.", "Both are related but different concept.", "A SparseTensor is a structure that indexes its values and can represent sparse matrices or tensors efficiently..", "A sparse matrix is a matrix filled mostly with 0.", "In tensorflow's documentation, it often does not refer to a SparseTensor but to a plain old Tensor filled mostly with 0s..", "It is therefore important to look at the expected type of a function's argument to figure out.", "So for example, in the documentation of tf.matmul, operands need to be plain Tensors and not SparseTensors, independently of the value of the xxx_is_sparse flags, which explains your error.", "When these flags are True, what tf.sparse_matmul actually expects really is a (dense) Tensor.", "In other words, these flags serve some optimization purposes and not input type constraints.", "(Those optimizations seem to be useful only for rather larger matrices by the way)."], "sent_idxs": [101, 1056, 2140, 1025, 2852, 1012, 102, 101, 2224, 1056, 2546, 1012, 20288, 1035, 23435, 1035, 9742, 1035, 13523, 12274, 2140, 1999, 2173, 1997, 1056, 2546, 1012, 20288, 1035, 13523, 12274, 2140, 1025, 2298, 2012, 1996, 12653, 2005, 2019, 4522, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 7861, 8270, 4667, 1035, 2298, 6279, 1035, 20288, 1012, 102, 101, 2055, 20288, 21520, 1998, 20288, 25808, 5668, 1012, 102, 101, 1996, 3291, 2003, 2025, 3563, 2000, 20288, 1035, 2173, 14528, 1010, 2021, 2349, 2000, 1037, 6724, 1999, 23435, 12314, 1005, 1055, 18444, 1012, 102, 101, 2017, 2031, 20288, 21520, 1012, 102, 101, 1998, 2059, 2017, 2031, 20288, 25808, 2953, 1012, 102, 101, 2119, 2024, 3141, 2021, 2367, 4145, 1012, 102, 101, 1037, 20288, 25808, 2953, 2003, 1037, 3252, 2008, 5950, 2229, 2049, 5300, 1998, 2064, 5050, 20288, 21520, 2030, 23435, 2015, 18228, 1012, 1012, 102, 101, 1037, 20288, 8185, 2003, 1037, 8185, 3561, 3262, 2007, 1014, 1012, 102, 101, 1999, 23435, 12314, 1005, 1055, 12653, 1010, 2009, 2411, 2515, 2025, 6523, 2000, 1037, 20288, 25808, 2953, 2021, 2000, 1037, 5810, 2214, 23435, 3561, 3262, 2007, 1014, 2015, 1012, 1012, 102, 101, 2009, 2003, 3568, 2590, 2000, 2298, 2012, 1996, 3517, 2828, 1997, 1037, 3853, 1005, 1055, 6685, 2000, 3275, 2041, 1012, 102, 101, 2061, 2005, 2742, 1010, 1999, 1996, 12653, 1997, 1056, 2546, 1012, 13523, 12274, 2140, 1010, 3850, 18376, 2342, 2000, 2022, 5810, 23435, 2015, 1998, 2025, 20288, 25808, 5668, 1010, 9174, 1997, 1996, 3643, 1997, 1996, 22038, 2595, 1035, 2003, 1035, 20288, 9245, 1010, 2029, 7607, 2115, 7561, 1012, 102, 101, 2043, 2122, 9245, 2024, 2995, 1010, 2054, 1056, 2546, 1012, 20288, 1035, 13523, 12274, 2140, 2941, 24273, 2428, 2003, 1037, 1006, 9742, 1007, 23435, 1012, 102, 101, 1999, 2060, 2616, 1010, 2122, 9245, 3710, 2070, 20600, 5682, 1998, 2025, 7953, 2828, 14679, 1012, 102, 101, 1006, 2216, 20600, 2015, 4025, 2000, 2022, 6179, 2069, 2005, 2738, 3469, 21520, 2011, 1996, 2126, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 7, 57, 67, 92, 99, 109, 118, 143, 156, 188, 210, 260, 287, 305, 325], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61032135", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [38, 43]}, {"sent_id": 3, "name": "tf.keras", "pos": [96, 101]}], [{"sent_id": 1, "name": "tf.keras.metrics", "pos": [38, 46]}, {"sent_id": 3, "name": "tf.keras.metrics", "pos": [96, 104]}], [{"sent_id": 1, "name": "tf.keras.metrics.sparsecategoricalaccuracy", "pos": [38, 55]}, {"sent_id": 3, "name": "tf.keras.metrics.sparsecategoricalaccuracy", "pos": [96, 113]}]], "sents": ["When you use built-in loss function, you can use 'accuracy' as metric .", "Under the hood, tensorflow will select appropriate accuracy function (in your case it is tf.keras.metrics.SparseCategoricalAccuracy()).", "When you define custom_loss function, then tensorflow doesn't know which accuracy function to use.", "In this case, you need to explicitly specify that it is tf.keras.metrics.SparseCategoricalAccuracy().", "Please check the gist hub gist here.", "The code modification and the output is as follows.", "<code>Code Snippet</code>.", "output.", "<code>Code Snippet</code>.", "Hope this helps"], "sent_idxs": [101, 2043, 2017, 2224, 2328, 1011, 1999, 3279, 3853, 1010, 2017, 2064, 2224, 1005, 10640, 1005, 2004, 12046, 1012, 102, 101, 2104, 1996, 7415, 1010, 23435, 12314, 2097, 7276, 6413, 10640, 3853, 1006, 1999, 2115, 2553, 2009, 2003, 1056, 2546, 1012, 17710, 8180, 1012, 12046, 2015, 1012, 20288, 16280, 20255, 7476, 6305, 10841, 22648, 2100, 1006, 1007, 1007, 1012, 102, 101, 2043, 2017, 9375, 7661, 1035, 3279, 3853, 1010, 2059, 23435, 12314, 2987, 1005, 1056, 2113, 2029, 10640, 3853, 2000, 2224, 1012, 102, 101, 1999, 2023, 2553, 1010, 2017, 2342, 2000, 12045, 20648, 2008, 2009, 2003, 1056, 2546, 1012, 17710, 8180, 1012, 12046, 2015, 1012, 20288, 16280, 20255, 7476, 6305, 10841, 22648, 2100, 1006, 1007, 1012, 102, 101, 3531, 4638, 1996, 21025, 3367, 9594, 21025, 3367, 2182, 1012, 102, 101, 1996, 3642, 14080, 1998, 1996, 6434, 2003, 2004, 4076, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3246, 2023, 7126, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 60, 83, 117, 129, 141, 155, 159, 173, 178], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51486057", "vertexSet": [[{"sent_id": 1, "name": "tf.losses", "pos": [30, 34]}], [{"sent_id": 1, "name": "tf.reduce_mean", "pos": [23, 29]}], [{"sent_id": 1, "name": "tf.losses.mean_squared_error", "pos": [30, 40]}]], "sents": ["I tried your code and I think you should change cost function.", "If I change it to cost = tf.reduce_mean(tf.losses.mean_squared_error(labels = Y, predictions = Z2)) then it works better.", "EDIT:\nAnd when I didn't transpose your input and output data it reduces cost to 0 in under 200 epochs."], "sent_idxs": [101, 1045, 2699, 2115, 3642, 1998, 1045, 2228, 2017, 2323, 2689, 3465, 3853, 1012, 102, 101, 2065, 1045, 2689, 2009, 2000, 3465, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1056, 2546, 1012, 6409, 1012, 2812, 1035, 19942, 1035, 7561, 1006, 10873, 1027, 1061, 1010, 20932, 1027, 1062, 2475, 1007, 1007, 2059, 2009, 2573, 2488, 1012, 102, 101, 10086, 1024, 1998, 2043, 1045, 2134, 1005, 1056, 9099, 20688, 2115, 7953, 1998, 6434, 2951, 2009, 13416, 3465, 2000, 1014, 1999, 2104, 3263, 25492, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 57, 85], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42897970", "vertexSet": [[{"sent_id": 3, "name": "tf.train", "pos": [106, 110]}, {"sent_id": 3, "name": "tf.train", "pos": [134, 138]}], [{"sent_id": 7, "name": "tf.reset_default_graph", "pos": [247, 255]}], [{"sent_id": 3, "name": "tf.train.latest_checkpoint", "pos": [134, 142]}], [{"sent_id": 3, "name": "tf.train.import_meta_graph", "pos": [106, 116]}]], "sents": ["Although, the selected answer tells us what should be done but doesn't explain why exactly you are getting the unexpected answer.", "I am explaining for anyone who comes here later.", "In Tensorflow, if you already have a graph and you again import the same graph after saving it, your graph operations will not be replaced, rather they Tensorflow is designed to create new variables by adding suffix like _1, _2 etc.", "For example, in your case, before you did:\n    saver = tf.train.import_meta_graph('model.ckpt.meta')\n    saver.restore(sess, tf.train.latest_checkpoint('./'))\nYour graph had a variable called v1.", "After you import the same graph, your variable v1 is not going to be replaced, rather, a new variable v1_1 will be added to the graph.", "So, the size of the graph will be doubled.", "Since v1 has not changed by loading the graph you still get v1's old value(all 1).", "If you want to reset the graph, you have to use tf.reset_default_graph()\n before importing the graph again as explained in the documentation.", "If you do import after this and print v1, you will get an all 0 v1."], "sent_idxs": [101, 2348, 1010, 1996, 3479, 3437, 4136, 2149, 2054, 2323, 2022, 2589, 2021, 2987, 1005, 1056, 4863, 2339, 3599, 2017, 2024, 2893, 1996, 9223, 3437, 1012, 102, 101, 1045, 2572, 9990, 2005, 3087, 2040, 3310, 2182, 2101, 1012, 102, 101, 1999, 23435, 12314, 1010, 2065, 2017, 2525, 2031, 1037, 10629, 1998, 2017, 2153, 12324, 1996, 2168, 10629, 2044, 7494, 2009, 1010, 2115, 10629, 3136, 2097, 2025, 2022, 2999, 1010, 2738, 2027, 23435, 12314, 2003, 2881, 2000, 3443, 2047, 10857, 2011, 5815, 16809, 2066, 1035, 1015, 1010, 1035, 1016, 4385, 1012, 102, 101, 2005, 2742, 1010, 1999, 2115, 2553, 1010, 2077, 2017, 2106, 1024, 3828, 2099, 1027, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1005, 2944, 1012, 23616, 13876, 1012, 18804, 1005, 1007, 3828, 2099, 1012, 9239, 1006, 7367, 4757, 1010, 1056, 2546, 1012, 3345, 1012, 6745, 1035, 26520, 1006, 1005, 1012, 1013, 1005, 1007, 1007, 2115, 10629, 2018, 1037, 8023, 2170, 1058, 2487, 1012, 102, 101, 2044, 2017, 12324, 1996, 2168, 10629, 1010, 2115, 8023, 1058, 2487, 2003, 2025, 2183, 2000, 2022, 2999, 1010, 2738, 1010, 1037, 2047, 8023, 1058, 2487, 1035, 1015, 2097, 2022, 2794, 2000, 1996, 10629, 1012, 102, 101, 2061, 1010, 1996, 2946, 1997, 1996, 10629, 2097, 2022, 11515, 1012, 102, 101, 2144, 1058, 2487, 2038, 2025, 2904, 2011, 10578, 1996, 10629, 2017, 2145, 2131, 1058, 2487, 1005, 1055, 2214, 3643, 1006, 2035, 1015, 1007, 1012, 102, 101, 2065, 2017, 2215, 2000, 25141, 1996, 10629, 1010, 2017, 2031, 2000, 2224, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2077, 12324, 2075, 1996, 10629, 2153, 2004, 4541, 1999, 1996, 12653, 1012, 102, 101, 2065, 2017, 2079, 12324, 2044, 2023, 1998, 6140, 1058, 2487, 1010, 2017, 2097, 2131, 2019, 2035, 1014, 1058, 2487, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 27, 39, 91, 159, 195, 208, 234, 270, 292], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50975398", "vertexSet": [[{"sent_id": 1, "name": "tf.variable", "pos": [26, 30]}, {"sent_id": 3, "name": "tf.variable", "pos": [64, 68]}], [{"sent_id": 3, "name": "tf.get_variable", "pos": [77, 83]}], [{"sent_id": 1, "name": "tf.variable_scope", "pos": [26, 32]}], [{"sent_id": 5, "name": "tf.get_default_graph", "pos": [133, 141]}]], "sents": ["You can force the reuse of a scope by adding a '/' after the name i.e.", ": tf.variable_scope(\"foo/\", reuse=True):", "However that won't solve your problem.", "In the case of variables, calling tf.Variable will always create a new variable, whereas calling tf.get_variable will reuse it if it already exists.", "But with Placeholders there is no tf.get_placeholder.", "What you can do is define your placeholders outside of foo, only once, and get them by name using tf.get_default_graph().get_tensor_by_name(name) or directly using the python variable whenever you need them.", "example with get_tensor_by_name:", "<code>Code Snippet</code>.", "Note that placeholders, unlike variables, do not maintain a state that can be reused or not.", "They are merely a \"pointer\" to a tensor which will be fed later.", "They should not be part of your model, but an input to it, so you should not be creating them several times anyway."], "sent_idxs": [101, 2017, 2064, 2486, 1996, 2128, 8557, 1997, 1037, 9531, 2011, 5815, 1037, 1005, 1013, 1005, 2044, 1996, 2171, 1045, 1012, 1041, 1012, 102, 101, 1024, 1056, 2546, 1012, 8023, 1035, 9531, 1006, 1000, 29379, 1013, 1000, 1010, 2128, 8557, 1027, 2995, 1007, 1024, 102, 101, 2174, 2008, 2180, 1005, 1056, 9611, 2115, 3291, 1012, 102, 101, 1999, 1996, 2553, 1997, 10857, 1010, 4214, 1056, 2546, 1012, 8023, 2097, 2467, 3443, 1037, 2047, 8023, 1010, 6168, 4214, 1056, 2546, 1012, 2131, 1035, 8023, 2097, 2128, 8557, 2009, 2065, 2009, 2525, 6526, 1012, 102, 101, 2021, 2007, 2173, 17794, 2045, 2003, 2053, 1056, 2546, 1012, 2131, 1035, 2173, 14528, 1012, 102, 101, 2054, 2017, 2064, 2079, 2003, 9375, 2115, 2173, 17794, 2648, 1997, 29379, 1010, 2069, 2320, 1010, 1998, 2131, 2068, 2011, 2171, 2478, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 2171, 1007, 2030, 3495, 2478, 1996, 18750, 8023, 7188, 2017, 2342, 2068, 1012, 102, 101, 2742, 2007, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 2173, 17794, 1010, 4406, 10857, 1010, 2079, 2025, 5441, 1037, 2110, 2008, 2064, 2022, 26513, 2030, 2025, 1012, 102, 101, 2027, 2024, 6414, 1037, 1000, 20884, 1000, 2000, 1037, 23435, 2029, 2097, 2022, 7349, 2101, 1012, 102, 101, 2027, 2323, 2025, 2022, 2112, 1997, 2115, 2944, 1010, 2021, 2019, 7953, 2000, 2009, 1010, 2061, 2017, 2323, 2025, 2022, 4526, 2068, 2195, 2335, 4312, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 24, 45, 56, 93, 110, 166, 178, 192, 214, 232, 260], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57077079", "vertexSet": [[{"sent_id": 3, "name": "tf.nn", "pos": [76, 81]}, {"sent_id": 6, "name": "tf.nn", "pos": [152, 157]}, {"sent_id": 10, "name": "tf.nn", "pos": [259, 264]}, {"sent_id": 11, "name": "tf.nn", "pos": [291, 296]}], [{"sent_id": 6, "name": "tf.cast", "pos": [174, 178]}], [{"sent_id": 10, "name": "tf.nn.softmax", "pos": [259, 267]}], [{"sent_id": 3, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [76, 95]}, {"sent_id": 6, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [152, 171]}, {"sent_id": 11, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [291, 310]}]], "sents": ["There are following problems in your code:", "You forgot to rescale your data:  x_train, x_test = x_train / 255.0, x_test / 255.0.", "w and b in line: tape.gradient(loss, [w, b]) are not defined..", "Valid labels dtype in tf.nn.sparse_softmax_cross_entropy_with_logits should be int32 or int64, while for logits, it should be float16, float32, or float64.", "In your case, it's uint8 for labels.", "Convert it to int32 before passing, like below", "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.cast(y, dtype=tf.int32), logits=y_pred)", "As per the official documentation,", "WARNING: This op expects unscaled logits, since it performs a softmax\n  on logits internally for efficiency.", "Do not call this op with the\n  output of softmax, as it will produce incorrect results.", "So, remove tf.nn.softmax from the output of mlp function, as it performs softmax on logits internally.", "For more info on tf.nn.sparse_softmax_cross_entropy_with_logits, check this.", "You should  modify your grad function and For loop to something like below:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2024, 2206, 3471, 1999, 2115, 3642, 1024, 102, 101, 2017, 9471, 2000, 24501, 9289, 2063, 2115, 2951, 1024, 1060, 1035, 3345, 1010, 1060, 1035, 3231, 1027, 1060, 1035, 3345, 1013, 20637, 1012, 1014, 1010, 1060, 1035, 3231, 1013, 20637, 1012, 1014, 1012, 102, 101, 1059, 1998, 1038, 1999, 2240, 1024, 6823, 1012, 17978, 1006, 3279, 1010, 1031, 1059, 1010, 1038, 1033, 1007, 2024, 2025, 4225, 1012, 1012, 102, 101, 9398, 10873, 26718, 18863, 1999, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2323, 2022, 20014, 16703, 2030, 20014, 21084, 1010, 2096, 2005, 8833, 12762, 1010, 2009, 2323, 2022, 14257, 16048, 1010, 14257, 16703, 1010, 2030, 14257, 21084, 1012, 102, 101, 1999, 2115, 2553, 1010, 2009, 1005, 1055, 21318, 3372, 2620, 2005, 10873, 1012, 102, 101, 10463, 2009, 2000, 20014, 16703, 2077, 4458, 1010, 2066, 2917, 102, 101, 3279, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 10873, 1027, 1056, 2546, 1012, 3459, 1006, 1061, 1010, 26718, 18863, 1027, 1056, 2546, 1012, 20014, 16703, 1007, 1010, 8833, 12762, 1027, 1061, 1035, 3653, 2094, 1007, 102, 101, 2004, 2566, 1996, 2880, 12653, 1010, 102, 101, 5432, 1024, 2023, 6728, 24273, 4895, 15782, 3709, 8833, 12762, 1010, 2144, 2009, 10438, 1037, 3730, 17848, 2006, 8833, 12762, 16058, 2005, 8122, 1012, 102, 101, 2079, 2025, 2655, 2023, 6728, 2007, 1996, 6434, 1997, 3730, 17848, 1010, 2004, 2009, 2097, 3965, 16542, 3463, 1012, 102, 101, 2061, 1010, 6366, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 2013, 1996, 6434, 1997, 19875, 2361, 3853, 1010, 2004, 2009, 10438, 3730, 17848, 2006, 8833, 12762, 16058, 1012, 102, 101, 2005, 2062, 18558, 2006, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1010, 4638, 2023, 1012, 102, 101, 2017, 2323, 19933, 2115, 24665, 4215, 3853, 1998, 2005, 7077, 2000, 2242, 2066, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 10, 45, 70, 122, 137, 149, 200, 208, 234, 255, 286, 315, 332, 346], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37748225", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [4, 9]}, {"sent_id": 2, "name": "tf.nn", "pos": [54, 59]}, {"sent_id": 3, "name": "tf.nn", "pos": [74, 79]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [4, 12]}, {"sent_id": 2, "name": "tf.nn.softmax", "pos": [54, 62]}, {"sent_id": 3, "name": "tf.nn.softmax", "pos": [74, 82]}]], "sents": ["You can use tf.nn.softmax in the code if you want, but then you will have to compute the loss yourself:", "<code>Code Snippet</code>.", "In practice, you don't use tf.nn.softmax for computing the loss.", "However you need to use tf.nn.softmax if for instance you want to compute the predictions of your algorithm and compare them to the true labels (to compute accuracy)."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1999, 1996, 3642, 2065, 2017, 2215, 1010, 2021, 2059, 2017, 2097, 2031, 2000, 24134, 1996, 3279, 4426, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 3218, 1010, 2017, 2123, 1005, 1056, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 2005, 9798, 1996, 3279, 1012, 102, 101, 2174, 2017, 2342, 2000, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 2065, 2005, 6013, 2017, 2215, 2000, 24134, 1996, 20932, 1997, 2115, 9896, 1998, 12826, 2068, 2000, 1996, 2995, 10873, 1006, 2000, 24134, 10640, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 31, 45, 68, 108], "sent_pos": [0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48348544", "vertexSet": [[{"sent_id": 8, "name": "tf.contrib", "pos": [104, 110]}], [{"sent_id": 8, "name": "tf.contrib.distributions", "pos": [104, 112]}], [{"sent_id": 8, "name": "tf.contrib.distributions.negativebinomial", "pos": [104, 117]}]], "sents": ["you can either do:", "<code>Code Snippet</code>.", "and", "<code>Code Snippet</code>.", "or you can (as suggested in Name the output of an expression in Tensorflow ):", "<code>Code Snippet</code>.", "and", "<code>Code Snippet</code>.", "Also, should i pack this method into a sub-class of \"tf.contrib.distributions.NegativeBinomial\" or should I create some utils.py file for it?", "I would not put it into any 'official' namespace such as tf but use your own file such that it is clear that this is functionality added by you / your project and not by Tensorflow (unless you need to fix / override some standard tensorflow functionality)."], "sent_idxs": [101, 2017, 2064, 2593, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 2017, 2064, 1006, 2004, 4081, 1999, 2171, 1996, 6434, 1997, 2019, 3670, 1999, 23435, 12314, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2036, 1010, 2323, 1045, 5308, 2023, 4118, 2046, 1037, 4942, 1011, 2465, 1997, 1000, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 20611, 1012, 4997, 21891, 10092, 2140, 1000, 2030, 2323, 1045, 3443, 2070, 21183, 12146, 1012, 1052, 2100, 5371, 2005, 2009, 1029, 102, 101, 1045, 2052, 2025, 2404, 2009, 2046, 2151, 1005, 2880, 1005, 3415, 15327, 2107, 2004, 1056, 2546, 2021, 2224, 2115, 2219, 5371, 2107, 2008, 2009, 2003, 3154, 2008, 2023, 2003, 15380, 2794, 2011, 2017, 1013, 2115, 2622, 1998, 2025, 2011, 23435, 12314, 1006, 4983, 2017, 2342, 2000, 8081, 1013, 2058, 15637, 2070, 3115, 23435, 12314, 15380, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 7, 21, 24, 38, 58, 72, 75, 89, 133, 192], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57675489", "vertexSet": [[{"sent_id": 0, "name": "tf.layers", "pos": [12, 16]}], [{"sent_id": 3, "name": "tf.reshape", "pos": [101, 107]}], [{"sent_id": 0, "name": "tf.layers.dense", "pos": [12, 18]}]], "sents": ["I faced the same issue while using the dense layer (tf.layers.dense API).", "The reason for the issue is that there is a reshape operation being applied to weights (introduced by tf.layer.dense API).", "The converter misinterprets it as part of the model execution and hence tries to convert to a layer which it can't since there are no input layers to it.", "You can use reshape(tf.reshape API) between a convolution and fully connected to flatten the tensor and it will work fine."], "sent_idxs": [101, 1045, 4320, 1996, 2168, 3277, 2096, 2478, 1996, 9742, 6741, 1006, 1056, 2546, 1012, 9014, 1012, 9742, 17928, 1007, 1012, 102, 101, 1996, 3114, 2005, 1996, 3277, 2003, 2008, 2045, 2003, 1037, 24501, 3270, 5051, 3169, 2108, 4162, 2000, 15871, 1006, 3107, 2011, 1056, 2546, 1012, 6741, 1012, 9742, 17928, 1007, 1012, 102, 101, 1996, 10463, 2121, 28616, 18447, 2121, 28139, 3215, 2009, 2004, 2112, 1997, 1996, 2944, 7781, 1998, 6516, 5363, 2000, 10463, 2000, 1037, 6741, 2029, 2009, 2064, 1005, 1056, 2144, 2045, 2024, 2053, 7953, 9014, 2000, 2009, 1012, 102, 101, 2017, 2064, 2224, 24501, 3270, 5051, 1006, 1056, 2546, 1012, 24501, 3270, 5051, 17928, 1007, 2090, 1037, 9530, 6767, 7630, 3508, 1998, 3929, 4198, 2000, 4257, 6528, 1996, 23435, 1998, 2009, 2097, 2147, 2986, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 54, 93, 130], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57990643", "vertexSet": [[{"sent_id": 1, "name": "tf.shape", "pos": [38, 42]}], [{"sent_id": 1, "name": "tf.range", "pos": [27, 31]}], [{"sent_id": 3, "name": "tf.random", "pos": [71, 75]}], [{"sent_id": 3, "name": "tf.random.shuffle", "pos": [71, 77]}]], "sents": ["Instead of shuffling x and y , its much easier to shuffle their indices, so first generate a list of indices", "indices = tf.range(start=0, limit=tf.shape(x_data)[0], dtype=tf.int32)", "then shuffle these indices", "idx = tf.random.shuffle(indices)", "and use these indices to shuffle the data", "<code>Code Snippet</code>.", "and youll have shuffled data"], "sent_idxs": [101, 2612, 1997, 24770, 1060, 1998, 1061, 1010, 2049, 2172, 6082, 2000, 23046, 2037, 29299, 1010, 2061, 2034, 9699, 1037, 2862, 1997, 29299, 102, 101, 29299, 1027, 1056, 2546, 1012, 2846, 1006, 2707, 1027, 1014, 1010, 5787, 1027, 1056, 2546, 1012, 4338, 1006, 1060, 1035, 2951, 1007, 1031, 1014, 1033, 1010, 26718, 18863, 1027, 1056, 2546, 1012, 20014, 16703, 1007, 102, 101, 2059, 23046, 2122, 29299, 102, 101, 8909, 2595, 1027, 1056, 2546, 1012, 6721, 1012, 23046, 1006, 29299, 1007, 102, 101, 1998, 2224, 2122, 29299, 2000, 23046, 1996, 2951, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2017, 3363, 2031, 18764, 2951, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 24, 61, 67, 81, 91, 105, 113], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56738981", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [4, 8]}], [{"sent_id": 0, "name": "tf.get_variable", "pos": [9, 15]}], [{"sent_id": 2, "name": "tf.global_variables_initializer", "pos": [69, 78]}]], "sents": ["You can use tf.Variable / tf.get_variable with trainable=False and validate_shape=False.", "You can use a value depending on a placeholder for the shape as initial value.", "Then, when you initialize the variable (either using the initializer attribute or something more common like tf.global_variables_initializer), you just have to give the shape for initialization.", "After the initialization, the value of the variable will be kept the same for the whole session, as long as it is not initialized again or assigned a different value.", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 8023, 1013, 1056, 2546, 1012, 2131, 1035, 8023, 2007, 3345, 3085, 1027, 6270, 1998, 9398, 3686, 1035, 4338, 1027, 6270, 1012, 102, 101, 2017, 2064, 2224, 1037, 3643, 5834, 2006, 1037, 2173, 14528, 2005, 1996, 4338, 2004, 3988, 3643, 1012, 102, 101, 2059, 1010, 2043, 2017, 3988, 4697, 1996, 8023, 1006, 2593, 2478, 1996, 3988, 17629, 17961, 2030, 2242, 2062, 2691, 2066, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1007, 1010, 2017, 2074, 2031, 2000, 2507, 1996, 4338, 2005, 3988, 3989, 1012, 102, 101, 2044, 1996, 3988, 3989, 1010, 1996, 3643, 1997, 1996, 8023, 2097, 2022, 2921, 1996, 2168, 2005, 1996, 2878, 5219, 1010, 2004, 2146, 2004, 2009, 2003, 2025, 3988, 3550, 2153, 2030, 4137, 1037, 2367, 3643, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 48, 92, 129, 143, 147, 161], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55143329", "vertexSet": [[{"sent_id": 0, "name": "tf.compat", "pos": [20, 26]}], [{"sent_id": 0, "name": "tf.session", "pos": [35, 39]}], [{"sent_id": 0, "name": "tf.compat.v1", "pos": [20, 29]}], [{"sent_id": 0, "name": "tf.compat.v1.session", "pos": [20, 31]}]], "sents": ["According to TF 1:1 Symbols Map, in TF 2.0 you should use tf.compat.v1.Session() instead of tf.Session()", "https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0", "To get TF 1.x like behaviour in TF 2.0 one can run", "<code>Code Snippet</code>.", "but then one cannot benefit of many improvements made in TF 2.0.", "For more details please refer to the migration guide \nhttps://www.tensorflow.org/guide/migrate"], "sent_idxs": [101, 2429, 2000, 1056, 2546, 1015, 1024, 1015, 9255, 4949, 1010, 1999, 1056, 2546, 1016, 1012, 1014, 2017, 2323, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 5219, 1006, 1007, 2612, 1997, 1056, 2546, 1012, 5219, 1006, 1007, 102, 101, 16770, 1024, 1013, 1013, 9986, 2015, 1012, 8224, 1012, 4012, 1013, 20861, 21030, 3215, 1013, 1040, 1013, 1015, 10258, 2546, 3501, 23858, 2290, 2581, 7962, 2361, 2575, 3501, 6806, 2094, 2595, 2629, 4160, 2620, 2497, 2094, 21600, 2102, 2912, 2546, 4160, 1035, 22889, 22269, 25311, 26493, 18902, 2063, 4160, 1013, 10086, 1001, 21025, 2094, 1027, 1014, 102, 101, 2000, 2131, 1056, 2546, 1015, 1012, 1060, 2066, 9164, 1999, 1056, 2546, 1016, 1012, 1014, 2028, 2064, 2448, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2021, 2059, 2028, 3685, 5770, 1997, 2116, 8377, 2081, 1999, 1056, 2546, 1016, 1012, 1014, 1012, 102, 101, 2005, 2062, 4751, 3531, 6523, 2000, 1996, 9230, 5009, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 22806, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 42, 100, 120, 134, 152, 177], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49115512", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [11, 15]}], [{"sent_id": 1, "name": "tf.assign_add", "pos": [38, 44]}], [{"sent_id": 2, "name": "tf.random_normal", "pos": [72, 78]}]], "sents": ["You can define epoch as a non-trainable tf.Variable in your graph and increment it at the end of each epoch.", "You can define an operation with tf.assign_add to do the incrementation and run it end of each epoch.", "Instead of rnd.normal you will also need to use tf.random_normal then.", "Example:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 9375, 25492, 2004, 1037, 2512, 1011, 3345, 3085, 1056, 2546, 1012, 8023, 1999, 2115, 10629, 1998, 4297, 28578, 4765, 2009, 2012, 1996, 2203, 1997, 2169, 25492, 1012, 102, 101, 2017, 2064, 9375, 2019, 3169, 2007, 1056, 2546, 1012, 23911, 1035, 5587, 2000, 2079, 1996, 4297, 28578, 19304, 1998, 2448, 2009, 2203, 1997, 2169, 25492, 1012, 102, 101, 2612, 1997, 29300, 2094, 1012, 3671, 2017, 2097, 2036, 2342, 2000, 2224, 1056, 2546, 1012, 6721, 1035, 3671, 2059, 1012, 102, 101, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 59, 81, 85, 99], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43633250", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [17, 22]}], [{"sent_id": 2, "name": "tf.matmul", "pos": [55, 61]}], [{"sent_id": 2, "name": "tf.multiply", "pos": [73, 78]}], [{"sent_id": 0, "name": "tf.reduce_sum", "pos": [7, 13]}, {"sent_id": 2, "name": "tf.reduce_sum", "pos": [66, 72]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [17, 25]}]], "sents": ["The problem arises because you call tf.reduce_sum on the argument of tf.nn.softmax.", "As a result, the softmax function fails because a scalar is not a valid input argument.", "Did you mean to use tf.matmul instead of the combination of tf.reduce_sum and tf.multiply?", "Edit: Tensorflow does not provide an equivalent of np.dot out of the box.", "If you want to compute the dot product of a matrix and a vector, you need to sum over indices explicitly:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3291, 18653, 2138, 2017, 2655, 1056, 2546, 1012, 5547, 1035, 7680, 2006, 1996, 6685, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1012, 102, 101, 2004, 1037, 2765, 1010, 1996, 3730, 17848, 3853, 11896, 2138, 1037, 26743, 2099, 2003, 2025, 1037, 9398, 7953, 6685, 1012, 102, 101, 2106, 2017, 2812, 2000, 2224, 1056, 2546, 1012, 13523, 12274, 2140, 2612, 1997, 1996, 5257, 1997, 1056, 2546, 1012, 5547, 1035, 7680, 1998, 1056, 2546, 1012, 4800, 22086, 1029, 102, 101, 10086, 1024, 23435, 12314, 2515, 2025, 3073, 2019, 5662, 1997, 27937, 1012, 11089, 2041, 1997, 1996, 3482, 1012, 102, 101, 2065, 2017, 2215, 2000, 24134, 1996, 11089, 4031, 1997, 1037, 8185, 1998, 1037, 9207, 1010, 2017, 2342, 2000, 7680, 2058, 29299, 12045, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 27, 49, 80, 100, 125, 139], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44486867", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [39, 44]}, {"sent_id": 6, "name": "tf.nn", "pos": [179, 184]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [19, 25]}, {"sent_id": 2, "name": "tf.contrib", "pos": [59, 65]}, {"sent_id": 6, "name": "tf.contrib", "pos": [164, 170]}], [{"sent_id": 1, "name": "tf.nn.l2_loss", "pos": [39, 49]}, {"sent_id": 6, "name": "tf.nn.l2_loss", "pos": [179, 189]}], [{"sent_id": 1, "name": "tf.contrib.layers", "pos": [19, 27]}, {"sent_id": 2, "name": "tf.contrib.layers", "pos": [59, 67]}, {"sent_id": 6, "name": "tf.contrib.layers", "pos": [164, 172]}], [{"sent_id": 1, "name": "tf.contrib.layers.l2_regularizer", "pos": [19, 33]}, {"sent_id": 2, "name": "tf.contrib.layers.l2_regularizer", "pos": [59, 73]}, {"sent_id": 6, "name": "tf.contrib.layers.l2_regularizer", "pos": [164, 178]}]], "sents": ["They do the same thing (at least now).", "The only difference is that tf.contrib.layers.l2_regularizer multiplies the result of tf.nn.l2_loss by scale.", "Look at the implementation of tf.contrib.layers.l2_regularizer[https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/layers/python/layers/regularizers.py]:", "<code>Code Snippet</code>.", "The line you are interested in is:", "<code>Code Snippet</code>.", "So in practice, tf.contrib.layers.l2_regularizer calls tf.nn.l2_loss internally and simply multiplies the result by the scale parameter."], "sent_idxs": [101, 2027, 2079, 1996, 2168, 2518, 1006, 2012, 2560, 2085, 1007, 1012, 102, 101, 1996, 2069, 4489, 2003, 2008, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 1048, 2475, 1035, 3180, 17629, 4800, 24759, 3111, 1996, 2765, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 1048, 2475, 1035, 3279, 2011, 4094, 1012, 102, 101, 2298, 2012, 1996, 7375, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 1048, 2475, 1035, 3180, 17629, 1031, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 1054, 2487, 1012, 1015, 1013, 23435, 12314, 1013, 9530, 18886, 2497, 1013, 9014, 1013, 18750, 1013, 9014, 1013, 3180, 17629, 2015, 1012, 1052, 2100, 1033, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2240, 2017, 2024, 4699, 1999, 2003, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2061, 1999, 3218, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 1048, 2475, 1035, 3180, 17629, 4455, 1056, 2546, 1012, 1050, 2078, 1012, 1048, 2475, 1035, 3279, 16058, 1998, 3432, 4800, 24759, 3111, 1996, 2765, 2011, 1996, 4094, 16381, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 13, 53, 121, 135, 145, 159, 203], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55898429", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [20, 25]}], [{"sent_id": 1, "name": "tf.keras.backend", "pos": [20, 28]}], [{"sent_id": 1, "name": "tf.keras.backend.get_value", "pos": [20, 32]}]], "sents": ["Use K.get_value(x) to get scalar of a tensor.", "tf.keras.backend.get_value"], "sent_idxs": [101, 2224, 1047, 1012, 2131, 1035, 3643, 1006, 1060, 1007, 2000, 2131, 26743, 2099, 1997, 1037, 23435, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 2131, 1035, 3643, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 33], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]}, {"title": "54117754", "vertexSet": [[{"sent_id": 3, "name": "tf.data", "pos": [132, 136]}], [{"sent_id": 2, "name": "tf.keras", "pos": [71, 76]}, {"sent_id": 3, "name": "tf.keras", "pos": [109, 114]}, {"sent_id": 3, "name": "tf.keras", "pos": [147, 152]}, {"sent_id": 3, "name": "tf.keras", "pos": [160, 165]}, {"sent_id": 3, "name": "tf.keras", "pos": [174, 179]}, {"sent_id": 4, "name": "tf.keras", "pos": [197, 202]}, {"sent_id": 5, "name": "tf.keras", "pos": [255, 260]}, {"sent_id": 7, "name": "tf.keras", "pos": [315, 320]}, {"sent_id": 8, "name": "tf.keras", "pos": [357, 362]}], [{"sent_id": 3, "name": "tf.keras.estimator", "pos": [174, 183]}], [{"sent_id": 3, "name": "tf.keras.estimator.model_to_estimator", "pos": [174, 191]}]], "sents": ["You are mixing things up:", "Keras (https://keras.io/) is a library independent from TensorFlow, which specifies a high-level API for building and training neural networks and is capable of using one of multiple backends (among which, TensorFlow) for low-level tensor computation..", "tf.keras (https://www.tensorflow.org/guide/keras) implements the Keras API specification within TensorFlow.", "In addition, the tf.keras API is optimized to work well with other TensorFlow modules: you can pass a tf.data Dataset to the .fit() method of a tf.keras model, for instance, or convert a tf.keras model to a TensorFlow estimator with tf.keras.estimator.model_to_estimator.", "Currently, the tf.keras API is the high-level API to look for when building models within TensorFlow, and the integration with other TensorFlow features will continue in the future..", "So to answer your question: no, you don't need to convert Keras code to tf.keras code.", "Keras code uses the Keras library, potentially even runs on top of a different backend than TensorFlow, and will continue to work just fine in the future.", "Even more, it's important to not just mix up Keras and tf.keras objects within the same script, since this might produce incompatabilities, as you can see for example in this question.", "Update: Keras will be abandoned in favor of tf.keras: https://twitter.com/fchollet/status/1174019423541157888"], "sent_idxs": [101, 2017, 2024, 6809, 2477, 2039, 1024, 102, 101, 17710, 8180, 1006, 16770, 1024, 1013, 1013, 17710, 8180, 1012, 22834, 1013, 1007, 2003, 1037, 3075, 2981, 2013, 23435, 12314, 1010, 2029, 27171, 1037, 2152, 1011, 2504, 17928, 2005, 2311, 1998, 2731, 15756, 6125, 1998, 2003, 5214, 1997, 2478, 2028, 1997, 3674, 2067, 10497, 2015, 1006, 2426, 2029, 1010, 23435, 12314, 1007, 2005, 2659, 1011, 2504, 23435, 22334, 1012, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1006, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 17710, 8180, 1007, 22164, 1996, 17710, 8180, 17928, 12827, 2306, 23435, 12314, 1012, 102, 101, 1999, 2804, 1010, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 2003, 23569, 27605, 5422, 2000, 2147, 2092, 2007, 2060, 23435, 12314, 14184, 1024, 2017, 2064, 3413, 1037, 1056, 2546, 1012, 2951, 2951, 13462, 2000, 1996, 1012, 4906, 1006, 1007, 4118, 1997, 1037, 1056, 2546, 1012, 17710, 8180, 2944, 1010, 2005, 6013, 1010, 2030, 10463, 1037, 1056, 2546, 1012, 17710, 8180, 2944, 2000, 1037, 23435, 12314, 9765, 9581, 4263, 2007, 1056, 2546, 1012, 17710, 8180, 1012, 9765, 9581, 4263, 1012, 2944, 1035, 2000, 1035, 9765, 9581, 4263, 1012, 102, 101, 2747, 1010, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 2003, 1996, 2152, 1011, 2504, 17928, 2000, 2298, 2005, 2043, 2311, 4275, 2306, 23435, 12314, 1010, 1998, 1996, 8346, 2007, 2060, 23435, 12314, 2838, 2097, 3613, 1999, 1996, 2925, 1012, 1012, 102, 101, 2061, 2000, 3437, 2115, 3160, 1024, 2053, 1010, 2017, 2123, 1005, 1056, 2342, 2000, 10463, 17710, 8180, 3642, 2000, 1056, 2546, 1012, 17710, 8180, 3642, 1012, 102, 101, 17710, 8180, 3642, 3594, 1996, 17710, 8180, 3075, 1010, 9280, 2130, 3216, 2006, 2327, 1997, 1037, 2367, 2067, 10497, 2084, 23435, 12314, 1010, 1998, 2097, 3613, 2000, 2147, 2074, 2986, 1999, 1996, 2925, 1012, 102, 101, 2130, 2062, 1010, 2009, 1005, 1055, 2590, 2000, 2025, 2074, 4666, 2039, 17710, 8180, 1998, 1056, 2546, 1012, 17710, 8180, 5200, 2306, 1996, 2168, 5896, 1010, 2144, 2023, 2453, 3965, 4297, 25377, 6790, 14680, 1010, 2004, 2017, 2064, 2156, 2005, 2742, 1999, 2023, 3160, 1012, 102, 101, 10651, 1024, 17710, 8180, 2097, 2022, 4704, 1999, 5684, 1997, 1056, 2546, 1012, 17710, 8180, 1024, 16770, 1024, 1013, 1013, 10474, 1012, 4012, 1013, 4429, 14854, 7485, 1013, 3570, 1013, 12567, 12740, 16147, 20958, 19481, 23632, 16068, 2581, 2620, 2620, 2620, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 8, 70, 104, 193, 235, 263, 299, 346, 389], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36211137", "vertexSet": [[{"sent_id": 2, "name": "tf.cast", "pos": [94, 98]}], [{"sent_id": 0, "name": "tf.matmul", "pos": [2, 8]}, {"sent_id": 1, "name": "tf.matmul", "pos": [45, 51]}], [{"sent_id": 4, "name": "tf.constant", "pos": [156, 160]}]], "sents": ["The tf.matmul() op does not perform automatic type conversions, so both of its inputs must have the same element type.", "The error message you are seeing indicates that you have a call to tf.matmul() where the first argument has type tf.float32, and the second argument has type tf.float64.", "You must convert one of the inputs to match the other, for example using tf.cast(x, tf.float32).", "Looking at your code, I don't see anywhere that a tf.float64 tensor is explicitly created (the default dtype for floating-point values in the TensorFlow Python API\u2014e.g.", "for tf.constant(37.0)\u2014is tf.float32).", "I would guess that the errors are caused by the np.loadtxt(filename) calls, which might be loading an np.float64 array.", "You can explicitly change them to load np.float32 arrays (which are converted to tf.float32 tensors) as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1007, 6728, 2515, 2025, 4685, 6882, 2828, 25834, 1010, 2061, 2119, 1997, 2049, 20407, 2442, 2031, 1996, 2168, 5783, 2828, 1012, 102, 101, 1996, 7561, 4471, 2017, 2024, 3773, 7127, 2008, 2017, 2031, 1037, 2655, 2000, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1007, 2073, 1996, 2034, 6685, 2038, 2828, 1056, 2546, 1012, 14257, 16703, 1010, 1998, 1996, 2117, 6685, 2038, 2828, 1056, 2546, 1012, 14257, 21084, 1012, 102, 101, 2017, 2442, 10463, 2028, 1997, 1996, 20407, 2000, 2674, 1996, 2060, 1010, 2005, 2742, 2478, 1056, 2546, 1012, 3459, 1006, 1060, 1010, 1056, 2546, 1012, 14257, 16703, 1007, 1012, 102, 101, 2559, 2012, 2115, 3642, 1010, 1045, 2123, 1005, 1056, 2156, 5973, 2008, 1037, 1056, 2546, 1012, 14257, 21084, 23435, 2003, 12045, 2580, 1006, 1996, 12398, 26718, 18863, 2005, 8274, 1011, 2391, 5300, 1999, 1996, 23435, 12314, 18750, 17928, 1517, 1041, 1012, 1043, 1012, 102, 101, 2005, 1056, 2546, 1012, 5377, 1006, 4261, 1012, 1014, 1007, 1517, 2003, 1056, 2546, 1012, 14257, 16703, 1007, 1012, 102, 101, 1045, 2052, 3984, 2008, 1996, 10697, 2024, 3303, 2011, 1996, 27937, 1012, 7170, 2102, 18413, 1006, 5371, 18442, 1007, 4455, 1010, 2029, 2453, 2022, 10578, 2019, 27937, 1012, 14257, 21084, 9140, 1012, 102, 101, 2017, 2064, 12045, 2689, 2068, 2000, 7170, 27937, 1012, 14257, 16703, 27448, 1006, 2029, 2024, 4991, 2000, 1056, 2546, 1012, 14257, 16703, 23435, 2015, 1007, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 78, 109, 154, 175, 209, 239, 253], "sent_pos": [0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44461648", "vertexSet": [[{"sent_id": 3, "name": "tf.nn", "pos": [82, 87]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [8, 14]}], [{"sent_id": 3, "name": "tf.nn.dropout", "pos": [82, 90]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [8, 17]}], [{"sent_id": 0, "name": "tf.contrib.rnn.dropoutwrapper", "pos": [8, 23]}]], "sents": ["If you structure your code to use tf.contrib.rnn.DropoutWrapper, you can set variational_recurrent=True in your wrapper, which causes the same dropout mask to be used at all steps, i.e.", "the dropout mask will be constant.", "Is that what you want?", "Setting the seed parameter in tf.nn.dropout will just make sure that you get the same sequence of dropout masks every time you run with that seed.", "That does not mean the dropout mask will be constant, just that you'll always see the same dropout mask at a particular iteration.", "The mask will be different for every iteration."], "sent_idxs": [101, 2065, 2017, 3252, 2115, 3642, 2000, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 4530, 5833, 13088, 29098, 2121, 1010, 2017, 2064, 2275, 8386, 2389, 1035, 28667, 29264, 1027, 2995, 1999, 2115, 10236, 4842, 1010, 2029, 5320, 1996, 2168, 4530, 5833, 7308, 2000, 2022, 2109, 2012, 2035, 4084, 1010, 1045, 1012, 1041, 1012, 102, 101, 1996, 4530, 5833, 7308, 2097, 2022, 5377, 1012, 102, 101, 2003, 2008, 2054, 2017, 2215, 1029, 102, 101, 4292, 1996, 6534, 16381, 1999, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2097, 2074, 2191, 2469, 2008, 2017, 2131, 1996, 2168, 5537, 1997, 4530, 5833, 15806, 2296, 2051, 2017, 2448, 2007, 2008, 6534, 1012, 102, 101, 2008, 2515, 2025, 2812, 1996, 4530, 5833, 7308, 2097, 2022, 5377, 1010, 2074, 2008, 2017, 1005, 2222, 2467, 2156, 1996, 2168, 4530, 5833, 7308, 2012, 1037, 3327, 27758, 1012, 102, 101, 1996, 7308, 2097, 2022, 2367, 2005, 2296, 27758, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 58, 68, 76, 113, 144, 155], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47768798", "vertexSet": [[{"sent_id": 3, "name": "tf.layers", "pos": [123, 127]}], [{"sent_id": 1, "name": "tf.multiply", "pos": [36, 41]}], [{"sent_id": 3, "name": "tf.layers.dropout", "pos": [123, 130]}]], "sents": ["I'd suggest creating a TensorFlow constant with zeros at the locations you want to zero out and ones everywhere else.", "Then you could create an op that uses tf.multiply to do elementwise multiplication of the constant and dy_dx.", "Depending on the structure of your graph, you might need to feed the result to dy_dx in your next call to session.run; you can replace any Tensor with feed data, including variables and constants.", "Incidentally, if you just want to apply dropout to the input layer you can use tf.layers.dropout"], "sent_idxs": [101, 1045, 1005, 1040, 6592, 4526, 1037, 23435, 12314, 5377, 2007, 5717, 2015, 2012, 1996, 5269, 2017, 2215, 2000, 5717, 2041, 1998, 3924, 7249, 2842, 1012, 102, 101, 2059, 2017, 2071, 3443, 2019, 6728, 2008, 3594, 1056, 2546, 1012, 4800, 22086, 2000, 2079, 5783, 14244, 24856, 1997, 1996, 5377, 1998, 1040, 2100, 1035, 1040, 2595, 1012, 102, 101, 5834, 2006, 1996, 3252, 1997, 2115, 10629, 1010, 2017, 2453, 2342, 2000, 5438, 1996, 2765, 2000, 1040, 2100, 1035, 1040, 2595, 1999, 2115, 2279, 2655, 2000, 5219, 1012, 2448, 1025, 2017, 2064, 5672, 2151, 23435, 2007, 5438, 2951, 1010, 2164, 10857, 1998, 5377, 2015, 1012, 102, 101, 5043, 3973, 1010, 2065, 2017, 2074, 2215, 2000, 6611, 4530, 5833, 2000, 1996, 7953, 6741, 2017, 2064, 2224, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 57, 104, 131], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0]}, {"title": "54852056", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [28, 33]}], [{"sent_id": 1, "name": "tf.keras.layers", "pos": [28, 35]}], [{"sent_id": 1, "name": "tf.keras.layers.dense", "pos": [28, 37]}]], "sents": ["The problem is in the input shape of your dense layer.", "It should match shape of your input tensor, which is 1. \ntf.keras.layers.Dense(N_OUTPUTS, input_shape=(features_shape,))", "Also you might encounter problems defining model.fit() steps_per_epoch parameter, it should be of type int.", "model.fit(dataset, steps_per_epoch=int(N_SAMPLES/BATCH_SIZE), epochs=EPOCHS)", "Edit 1:\nIn case you need multiple labels, you can do", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3291, 2003, 1999, 1996, 7953, 4338, 1997, 2115, 9742, 6741, 1012, 102, 101, 2009, 2323, 2674, 4338, 1997, 2115, 7953, 23435, 1010, 2029, 2003, 1015, 1012, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 1050, 1035, 27852, 1010, 7953, 1035, 4338, 1027, 1006, 2838, 1035, 4338, 1010, 1007, 1007, 102, 101, 2036, 2017, 2453, 8087, 3471, 12854, 2944, 1012, 4906, 1006, 1007, 4084, 1035, 2566, 1035, 25492, 16381, 1010, 2009, 2323, 2022, 1997, 2828, 20014, 1012, 102, 101, 2944, 1012, 4906, 1006, 2951, 13462, 1010, 4084, 1035, 2566, 1035, 25492, 1027, 20014, 1006, 1050, 1035, 8168, 1013, 14108, 1035, 2946, 1007, 1010, 25492, 2015, 1027, 25492, 2015, 1007, 102, 101, 10086, 1015, 1024, 1999, 2553, 2017, 2342, 3674, 10873, 1010, 2017, 2064, 2079, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 54, 81, 113, 128, 142], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41600007", "vertexSet": [[{"sent_id": 3, "name": "tf.image.crop_and_resize", "pos": [83, 94]}], [{"sent_id": 0, "name": "tf.slice", "pos": [4, 8]}], [{"sent_id": 0, "name": "tf.image.extract_glimpse", "pos": [24, 32]}]], "sents": ["Instead of using tf.slice (which doesn't let you operate on a batch), I recommend using tf.image.extract_glimpse.", "Here is a toy sample program that operates in a batch:", "<code>Code Snippet</code>.", "If you would like to extract multiple sizes (and even multiple glimpses per image), check out tf.image.crop_and_resize.", "Docs: https://www.tensorflow.org/api_docs/python/image/cropping#extract_glimpse"], "sent_idxs": [101, 2612, 1997, 2478, 1056, 2546, 1012, 14704, 1006, 2029, 2987, 1005, 1056, 2292, 2017, 5452, 2006, 1037, 14108, 1007, 1010, 1045, 16755, 2478, 1056, 2546, 1012, 3746, 1012, 14817, 1035, 12185, 1012, 102, 101, 2182, 2003, 1037, 9121, 7099, 2565, 2008, 5748, 1999, 1037, 14108, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2052, 2066, 2000, 14817, 3674, 10826, 1006, 1998, 2130, 3674, 12185, 2015, 2566, 3746, 1007, 1010, 4638, 2041, 1056, 2546, 1012, 3746, 1012, 10416, 1035, 1998, 1035, 24501, 4697, 1012, 102, 101, 9986, 2015, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 3746, 1013, 10416, 4691, 1001, 14817, 1035, 12185, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 3]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 34, 48, 62, 96, 127], "sent_pos": [0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50505684", "vertexSet": [[{"sent_id": 2, "name": "tf.layers", "pos": [44, 48]}], [{"sent_id": 4, "name": "tf.reshape", "pos": [80, 86]}], [{"sent_id": 10, "name": "tf.estimator", "pos": [213, 219]}], [{"sent_id": 2, "name": "tf.layers.dense", "pos": [44, 50]}], [{"sent_id": 10, "name": "tf.estimator.estimator", "pos": [213, 223]}]], "sents": ["First of all, there are some problems with your code:", "It seems you have only 2 classes (0 and 1), yet your network has 10 outputs (c.f.", "logits = tf.layers.dense(inputs=dropout, units=10)).", "Your last dense layer probably should have only 2 units.", "logits=tf.reshape(logits,[10,10])) has no effects (since you're not using logits afterwards).", "It can simply be removed.", "Second, using the mock load_images() pasted below, I am not getting any errors training this model (trace pasted below too).", "It is thus possible your problem comes:", "Either from your own load_images() function or from your dataset itself ;.", "Or from always reloading the model saved in /tmp/convnet_model (c.f.", "classifier = tf.estimator.Estimator(model_fn=define_model, model_dir=\"/tmp/convnet_model\"), as this model may have been corrupted from previous training attempts..", "<code>Code Snippet</code>.", "Trace:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2034, 1997, 2035, 1010, 2045, 2024, 2070, 3471, 2007, 2115, 3642, 1024, 102, 101, 2009, 3849, 2017, 2031, 2069, 1016, 4280, 1006, 1014, 1998, 1015, 1007, 1010, 2664, 2115, 2897, 2038, 2184, 27852, 1006, 1039, 1012, 1042, 1012, 102, 101, 8833, 12762, 1027, 1056, 2546, 1012, 9014, 1012, 9742, 1006, 20407, 1027, 4530, 5833, 1010, 3197, 1027, 2184, 1007, 1007, 1012, 102, 101, 2115, 2197, 9742, 6741, 2763, 2323, 2031, 2069, 1016, 3197, 1012, 102, 101, 8833, 12762, 1027, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 8833, 12762, 1010, 1031, 2184, 1010, 2184, 1033, 1007, 1007, 2038, 2053, 3896, 1006, 2144, 2017, 1005, 2128, 2025, 2478, 8833, 12762, 5728, 1007, 1012, 102, 101, 2009, 2064, 3432, 2022, 3718, 1012, 102, 101, 2117, 1010, 2478, 1996, 12934, 7170, 1035, 4871, 1006, 1007, 19351, 2094, 2917, 1010, 1045, 2572, 2025, 2893, 2151, 10697, 2731, 2023, 2944, 1006, 7637, 19351, 2094, 2917, 2205, 1007, 1012, 102, 101, 2009, 2003, 2947, 2825, 2115, 3291, 3310, 1024, 102, 101, 2593, 2013, 2115, 2219, 7170, 1035, 4871, 1006, 1007, 3853, 2030, 2013, 2115, 2951, 13462, 2993, 1025, 1012, 102, 101, 2030, 2013, 2467, 2128, 18570, 1996, 2944, 5552, 1999, 1013, 1056, 8737, 1013, 9530, 16022, 3388, 1035, 2944, 1006, 1039, 1012, 1042, 1012, 102, 101, 2465, 18095, 1027, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1006, 2944, 1035, 1042, 2078, 1027, 9375, 1035, 2944, 1010, 2944, 1035, 16101, 1027, 1000, 1013, 1056, 8737, 1013, 9530, 16022, 3388, 1035, 2944, 1000, 1007, 1010, 2004, 2023, 2944, 2089, 2031, 2042, 27279, 2013, 3025, 2731, 4740, 1012, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 7637, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 14, 40, 63, 76, 113, 121, 154, 164, 184, 209, 264, 278, 282, 296], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47672475", "vertexSet": [[{"sent_id": 7, "name": "tf.sigmoid", "pos": [171, 177]}], [{"sent_id": 0, "name": "tf.variable", "pos": [29, 33]}, {"sent_id": 2, "name": "tf.variable", "pos": [57, 61]}], [{"sent_id": 0, "name": "tf.clip_by_value", "pos": [15, 23]}]], "sents": ["The proper way to do this would be to pass the clipping function tf.clip_by_value as the constraint argument to the tf.Variable constructor:", "<code>Code Snippet</code>.", "From the docs of tf.Variable:", "constraint: An optional projection function to be applied to the\n  variable after being updated by an Optimizer (e.g.", "used to implement\n  norm constraints or value constraints for layer weights).", "The function\n  must take as input the unprojected Tensor representing the value of\n  the variable and return the Tensor for the projected value (which must\n  have the same shape).", "Constraints are not safe to use when doing\n  asynchronous distributed training.", "Or you might want to consider simply adding a nonlinearity tf.sigmoid on top of your variable.", "<code>Code Snippet</code>.", "This will transform your variable to range between 0 and 1.", "Read more about activation functions here."], "sent_idxs": [101, 1996, 5372, 2126, 2000, 2079, 2023, 2052, 2022, 2000, 3413, 1996, 12528, 4691, 3853, 1056, 2546, 1012, 12528, 1035, 2011, 1035, 3643, 2004, 1996, 27142, 6685, 2000, 1996, 1056, 2546, 1012, 8023, 9570, 2953, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2013, 1996, 9986, 2015, 1997, 1056, 2546, 1012, 8023, 1024, 102, 101, 27142, 1024, 2019, 11887, 13996, 3853, 2000, 2022, 4162, 2000, 1996, 8023, 2044, 2108, 7172, 2011, 2019, 23569, 27605, 6290, 1006, 1041, 1012, 1043, 1012, 102, 101, 2109, 2000, 10408, 13373, 14679, 2030, 3643, 14679, 2005, 6741, 15871, 1007, 1012, 102, 101, 1996, 3853, 2442, 2202, 2004, 7953, 1996, 4895, 21572, 24455, 23435, 5052, 1996, 3643, 1997, 1996, 8023, 1998, 2709, 1996, 23435, 2005, 1996, 11310, 3643, 1006, 2029, 2442, 2031, 1996, 2168, 4338, 1007, 1012, 102, 101, 14679, 2024, 2025, 3647, 2000, 2224, 2043, 2725, 2004, 6038, 2818, 4948, 3560, 5500, 2731, 1012, 102, 101, 2030, 2017, 2453, 2215, 2000, 5136, 3432, 5815, 1037, 27400, 3012, 1056, 2546, 1012, 9033, 21693, 9314, 2006, 2327, 1997, 2115, 8023, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2097, 10938, 2115, 8023, 2000, 2846, 2090, 1014, 1998, 1015, 1012, 102, 101, 3191, 2062, 2055, 13791, 4972, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 51, 63, 90, 105, 141, 159, 184, 198, 212, 221], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52145089", "vertexSet": [[{"sent_id": 3, "name": "tf.train", "pos": [63, 67]}, {"sent_id": 3, "name": "tf.train", "pos": [74, 78]}], [{"sent_id": 3, "name": "tf.train.adamoptimizer", "pos": [63, 73]}], [{"sent_id": 3, "name": "tf.train.gradientdescentoptimizer", "pos": [74, 86]}]], "sents": ["You don't need to re-initialize the graph.", "Tensorflow is not explicitly aware of whether you're doing the training or testing phase when you request the output tensor.", "Rather, back propagation happens whenever you evaluate the optimizer's (e.g.", "tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, etc) minimize operation.", "Normally you don't do this during testing, since you're trying to predict how well it performs on unseen data.", "But if you want to add that data to the training set after making the initial prediction, you're free to do so as long as you have a way to get the true value so it can compute the error tensor.", "Even in non-online learning, it's common to intersperse training and testing passes: if you use a validation set to determine early stopping, you alternate between training batches, where you do do backprop, and validation batches, where you don't."], "sent_idxs": [101, 2017, 2123, 1005, 1056, 2342, 2000, 2128, 1011, 3988, 4697, 1996, 10629, 1012, 102, 101, 23435, 12314, 2003, 2025, 12045, 5204, 1997, 3251, 2017, 1005, 2128, 2725, 1996, 2731, 2030, 5604, 4403, 2043, 2017, 5227, 1996, 6434, 23435, 1012, 102, 101, 2738, 1010, 2067, 20594, 6433, 7188, 2017, 16157, 1996, 23569, 27605, 6290, 1005, 1055, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 4205, 7361, 3775, 4328, 6290, 1010, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1010, 4385, 1007, 18478, 3169, 1012, 102, 101, 5373, 2017, 2123, 1005, 1056, 2079, 2023, 2076, 5604, 1010, 2144, 2017, 1005, 2128, 2667, 2000, 16014, 2129, 2092, 2009, 10438, 2006, 16100, 2951, 1012, 102, 101, 2021, 2065, 2017, 2215, 2000, 5587, 2008, 2951, 2000, 1996, 2731, 2275, 2044, 2437, 1996, 3988, 17547, 1010, 2017, 1005, 2128, 2489, 2000, 2079, 2061, 2004, 2146, 2004, 2017, 2031, 1037, 2126, 2000, 2131, 1996, 2995, 3643, 2061, 2009, 2064, 24134, 1996, 7561, 23435, 1012, 102, 101, 2130, 1999, 2512, 1011, 3784, 4083, 1010, 2009, 1005, 1055, 2691, 2000, 6970, 17668, 3366, 2731, 1998, 5604, 5235, 1024, 2065, 2017, 2224, 1037, 27354, 2275, 2000, 5646, 2220, 7458, 1010, 2017, 6585, 2090, 2731, 14108, 2229, 1010, 2073, 2017, 2079, 2079, 2067, 21572, 2361, 1010, 1998, 27354, 14108, 2229, 1010, 2073, 2017, 2123, 1005, 1056, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 41, 62, 93, 120, 167, 226], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46987611", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [35, 39]}], [{"sent_id": 0, "name": "tf.summary", "pos": [19, 23]}], [{"sent_id": 0, "name": "tf.summary.merge_all", "pos": [19, 27]}], [{"sent_id": 0, "name": "tf.train.summarysaverhook", "pos": [35, 45]}]], "sents": ["1) Create your summaries in your model_fn\n2) Merge them with tf.summary.merge_all\n3) Pass the merged summary to a tf.train.SummarySaverHook\n4) Pass this hook to the training_hooks param (As a List) of the EstimatorSpec that you return with the your model_fn", "Note: The summaries defined in the model_fn cannot be passed outside the scpoe of the function.", "Hence, you cannot use the hooks param of the estimator.train method to pass in your hooks."], "sent_idxs": [101, 1015, 1007, 3443, 2115, 7680, 7849, 3111, 1999, 2115, 2944, 1035, 1042, 2078, 1016, 1007, 13590, 2068, 2007, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 1017, 1007, 3413, 1996, 5314, 12654, 2000, 1037, 1056, 2546, 1012, 3345, 1012, 12654, 3736, 6299, 6806, 6559, 1018, 1007, 3413, 2023, 8103, 2000, 1996, 2731, 1035, 18008, 11498, 2213, 1006, 2004, 1037, 2862, 1007, 1997, 1996, 9765, 9581, 6591, 5051, 2278, 2008, 2017, 2709, 2007, 1996, 2115, 2944, 1035, 1042, 2078, 102, 101, 3602, 1024, 1996, 7680, 7849, 3111, 4225, 1999, 1996, 2944, 1035, 1042, 2078, 3685, 2022, 2979, 2648, 1996, 8040, 6873, 2063, 1997, 1996, 3853, 1012, 102, 101, 6516, 1010, 2017, 3685, 2224, 1996, 18008, 11498, 2213, 1997, 1996, 9765, 9581, 4263, 1012, 3345, 4118, 2000, 3413, 1999, 2115, 18008, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 80, 107, 132], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44206972", "vertexSet": [[{"sent_id": 3, "name": "tf.variable", "pos": [73, 77]}], [{"sent_id": 1, "name": "tf.constant", "pos": [15, 19]}, {"sent_id": 2, "name": "tf.constant", "pos": [59, 63]}, {"sent_id": 3, "name": "tf.constant", "pos": [81, 85]}], [{"sent_id": 2, "name": "tf.random_normal", "pos": [42, 48]}]], "sents": ["Someone else has answered this question on another thread.", "Essentially, tf.constant() takes a NumPy array as an argument or some sort of array or just a value.", "tf.random_normal() returns a Tensor which cannot be an argument to tf.constant().", "To fix this, use tf.Variable() instead of tf.constant().", "See the answer from the link.", "The person explains it better."], "sent_idxs": [101, 2619, 2842, 2038, 4660, 2023, 3160, 2006, 2178, 11689, 1012, 102, 101, 7687, 1010, 1056, 2546, 1012, 5377, 1006, 1007, 3138, 1037, 16371, 8737, 2100, 9140, 2004, 2019, 6685, 2030, 2070, 4066, 1997, 9140, 2030, 2074, 1037, 3643, 1012, 102, 101, 1056, 2546, 1012, 6721, 1035, 3671, 1006, 1007, 5651, 1037, 23435, 2029, 3685, 2022, 2019, 6685, 2000, 1056, 2546, 1012, 5377, 1006, 1007, 1012, 102, 101, 2000, 8081, 2023, 1010, 2224, 1056, 2546, 1012, 8023, 1006, 1007, 2612, 1997, 1056, 2546, 1012, 5377, 1006, 1007, 1012, 102, 101, 2156, 1996, 3437, 2013, 1996, 4957, 1012, 102, 101, 1996, 2711, 7607, 2009, 2488, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 41, 67, 89, 98, 106], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51356461", "vertexSet": [[{"sent_id": 1, "name": "tf.constant", "pos": [49, 53]}], [{"sent_id": 3, "name": "tf.placeholder", "pos": [123, 128]}], [{"sent_id": 2, "name": "tf.graphdef", "pos": [99, 105]}]], "sents": ["https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays", "Note that the above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations.", "This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.", "As an alternative, you can define the Dataset in terms of tf.placeholder() tensors, and feed the NumPy arrays when you initialize an Iterator over the dataset.", "Instead of using", "<code>Code Snippet</code>.", "Use", "<code>Code Snippet</code>."], "sent_idxs": [101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 2951, 13462, 2015, 1001, 15077, 1035, 16371, 8737, 2100, 1035, 27448, 102, 101, 3602, 2008, 1996, 2682, 3642, 1055, 3490, 29519, 2097, 7861, 8270, 1996, 2838, 1998, 10873, 27448, 1999, 2115, 23435, 12314, 10629, 2004, 1056, 2546, 1012, 5377, 1006, 1007, 3136, 1012, 102, 101, 2023, 2573, 2092, 2005, 1037, 2235, 2951, 13462, 1010, 2021, 5949, 2015, 3638, 1011, 1011, 1011, 2138, 1996, 8417, 1997, 1996, 9140, 2097, 2022, 15826, 3674, 2335, 1011, 1011, 1011, 1998, 2064, 2448, 2046, 1996, 1016, 18259, 5787, 2005, 1996, 1056, 2546, 1012, 10629, 3207, 2546, 8778, 17698, 1012, 102, 101, 2004, 2019, 4522, 1010, 2017, 2064, 9375, 1996, 2951, 13462, 1999, 3408, 1997, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23435, 2015, 1010, 1998, 5438, 1996, 16371, 8737, 2100, 27448, 2043, 2017, 3988, 4697, 2019, 2009, 6906, 4263, 2058, 1996, 2951, 13462, 1012, 102, 101, 2612, 1997, 2478, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2224, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 58, 109, 154, 159, 173, 176, 190], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57918427", "vertexSet": [[{"sent_id": 3, "name": "tf.keras", "pos": [71, 76]}], [{"sent_id": 0, "name": "tf.py_func", "pos": [14, 22]}], [{"sent_id": 0, "name": "tf.numpy_function", "pos": [4, 12]}]], "sents": ["You can use tf.numpy_function, or tf.py_func to wrap a python function and use it as a TensorFlow op.", "Here is an example which I used:", "<code>Code Snippet</code>.", "Then I used this loss function in training a tf.keras model:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 16371, 8737, 2100, 1035, 3853, 1010, 2030, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 2000, 10236, 1037, 18750, 3853, 1998, 2224, 2009, 2004, 1037, 23435, 12314, 6728, 1012, 102, 101, 2182, 2003, 2019, 2742, 2029, 1045, 2109, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1045, 2109, 2023, 3279, 3853, 1999, 2731, 1037, 1056, 2546, 1012, 17710, 8180, 2944, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 47, 61, 79, 93], "sent_pos": [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44571038", "vertexSet": [[{"sent_id": 1, "name": "tf.equal", "pos": [40, 44]}], [{"sent_id": 1, "name": "tf.argmax", "pos": [47, 53]}], [{"sent_id": 1, "name": "tf.boolean_mask", "pos": [21, 28]}]], "sents": ["You can do that in Tensorflow.", "The best way I see is using a mask and tf.boolean_mask k times, with the i-th mask being given by tf.equal(i, tf.argmax(z, axis=-1))", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2079, 2008, 1999, 23435, 12314, 1012, 102, 101, 1996, 2190, 2126, 1045, 2156, 2003, 2478, 1037, 7308, 1998, 1056, 2546, 1012, 22017, 20898, 1035, 7308, 1047, 2335, 1010, 2007, 1996, 1045, 1011, 16215, 7308, 2108, 2445, 2011, 1056, 2546, 1012, 5020, 1006, 1045, 1010, 1056, 2546, 1012, 12098, 21693, 8528, 1006, 1062, 1010, 8123, 1027, 1011, 1015, 1007, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 63, 77], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54680951", "vertexSet": [[{"sent_id": 7, "name": "tf.nn", "pos": [113, 118]}], [{"sent_id": 9, "name": "tf.data", "pos": [177, 181]}], [{"sent_id": 7, "name": "tf.keras", "pos": [91, 96]}, {"sent_id": 9, "name": "tf.keras", "pos": [160, 165]}], [{"sent_id": 9, "name": "tf.data.dataset", "pos": [177, 184]}], [{"sent_id": 7, "name": "tf.keras.backend", "pos": [91, 99]}], [{"sent_id": 7, "name": "tf.keras.backend.categorical_crossentropy", "pos": [91, 107]}], [{"sent_id": 7, "name": "tf.nn.softmax_cross_entropy_with_logits_v2", "pos": [113, 133]}]], "sents": ["Replacing the low-level TF loss function", "<code>Code Snippet</code>.", "by its Keras equivalent", "<code>Code Snippet</code>.", "does the trick.", "Now the low-level TensorFlow training loop behaves just like model.fit().", "However, I don't know why this is.", "If anyone knows why tf.keras.backend.categorical_crossentropy() behaves well while tf.nn.softmax_cross_entropy_with_logits_v2() doesn't work at all, please post an answer.", "Another important note:", "In order to train a tf.keras model with a low-level TF training loop and a tf.data.Dataset object, one generally shouldn't call the model on the iterator output.", "That is, one shouldn't do this:", "<code>Code Snippet</code>.", "Instead, one should create a model in which the input layer is set to build on the iterator output instead of creating a placeholder, like so:", "<code>Code Snippet</code>.", "This doesn't matter in this example, but it becomes relevant if any layers in the model have internal updates that need to be run during the training (e.g.", "BatchNormalization)."], "sent_idxs": [101, 6419, 1996, 2659, 1011, 2504, 1056, 2546, 3279, 3853, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2011, 2049, 17710, 8180, 5662, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2515, 1996, 7577, 1012, 102, 101, 2085, 1996, 2659, 1011, 2504, 23435, 12314, 2731, 7077, 16582, 2015, 2074, 2066, 2944, 1012, 4906, 1006, 1007, 1012, 102, 101, 2174, 1010, 1045, 2123, 1005, 1056, 2113, 2339, 2023, 2003, 1012, 102, 101, 2065, 3087, 4282, 2339, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 4937, 27203, 1035, 2892, 4765, 18981, 2100, 1006, 1007, 16582, 2015, 2092, 2096, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1035, 1058, 2475, 1006, 1007, 2987, 1005, 1056, 2147, 2012, 2035, 1010, 3531, 2695, 2019, 3437, 1012, 102, 101, 2178, 2590, 3602, 1024, 102, 101, 1999, 2344, 2000, 3345, 1037, 1056, 2546, 1012, 17710, 8180, 2944, 2007, 1037, 2659, 1011, 2504, 1056, 2546, 2731, 7077, 1998, 1037, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 4874, 1010, 2028, 3227, 5807, 1005, 1056, 2655, 1996, 2944, 2006, 1996, 2009, 6906, 4263, 6434, 1012, 102, 101, 2008, 2003, 1010, 2028, 5807, 1005, 1056, 2079, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2612, 1010, 2028, 2323, 3443, 1037, 2944, 1999, 2029, 1996, 7953, 6741, 2003, 2275, 2000, 3857, 2006, 1996, 2009, 6906, 4263, 6434, 2612, 1997, 4526, 1037, 2173, 14528, 1010, 2066, 2061, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2987, 1005, 1056, 3043, 1999, 2023, 2742, 1010, 2021, 2009, 4150, 7882, 2065, 2151, 9014, 1999, 1996, 2944, 2031, 4722, 14409, 2008, 2342, 2000, 2022, 2448, 2076, 1996, 2731, 1006, 1041, 1012, 1043, 1012, 102, 101, 14108, 12131, 9067, 3989, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 11, 25, 32, 46, 52, 73, 86, 148, 154, 202, 214, 228, 262, 276, 313, 321], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58792474", "vertexSet": [[{"sent_id": 1, "name": "tf.data", "pos": [6, 10]}], [{"sent_id": 1, "name": "tf.data.experimental", "pos": [6, 12]}], [{"sent_id": 1, "name": "tf.data.experimental.cardinality", "pos": [6, 15]}]], "sents": ["UPDATE:", "Use tf.data.experimental.cardinality - see here.", "In case of tensorflow datasets you can use _, info = tfds.load(with_info=True).", "Then you may call info.splits['train'].num_examples.", "But even in this case it doesn't work properly if you define your own split.", "So you may either count your files or iterate over the dataset (like described in other answers):", "<code>Code Snippet</code>."], "sent_idxs": [101, 10651, 1024, 102, 101, 2224, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 7185, 3012, 1011, 2156, 2182, 1012, 102, 101, 1999, 2553, 1997, 23435, 12314, 2951, 13462, 2015, 2017, 2064, 2224, 1035, 1010, 18558, 1027, 1056, 2546, 5104, 1012, 7170, 1006, 2007, 1035, 18558, 1027, 2995, 1007, 1012, 102, 101, 2059, 2017, 2089, 2655, 18558, 1012, 19584, 1031, 1005, 3345, 1005, 1033, 1012, 16371, 2213, 1035, 4973, 1012, 102, 101, 2021, 2130, 1999, 2023, 2553, 2009, 2987, 1005, 1056, 2147, 7919, 2065, 2017, 9375, 2115, 2219, 3975, 1012, 102, 101, 2061, 2017, 2089, 2593, 4175, 2115, 6764, 2030, 2009, 22139, 2058, 1996, 2951, 13462, 1006, 2066, 2649, 1999, 2060, 6998, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 4, 20, 50, 70, 90, 114, 128], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57043917", "vertexSet": [[{"sent_id": 0, "name": "tf.set_random_seed", "pos": [7, 15]}], [{"sent_id": 0, "name": "tf.random.set_seed", "pos": [21, 29]}]], "sents": ["In TensorFlow 2.0 tf.set_random_seed(42) has changed to tf.random.set_seed(42).", "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/set_seed", "That should be the only seed necessary if just using TensorFlow."], "sent_idxs": [101, 1999, 23435, 12314, 1016, 1012, 1014, 1056, 2546, 1012, 2275, 1035, 6721, 1035, 6534, 1006, 4413, 1007, 2038, 2904, 2000, 1056, 2546, 1012, 6721, 1012, 2275, 1035, 6534, 1006, 4413, 1007, 1012, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 4617, 1013, 1054, 2475, 1012, 1014, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 6721, 1013, 2275, 1035, 6534, 102, 101, 2008, 2323, 2022, 1996, 2069, 6534, 4072, 2065, 2074, 2478, 23435, 12314, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 34, 69, 84], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49970811", "vertexSet": [[{"sent_id": 2, "name": "tf.data", "pos": [97, 101]}, {"sent_id": 8, "name": "tf.data", "pos": [248, 252]}], [{"sent_id": 0, "name": "tf.keras", "pos": [1, 6]}, {"sent_id": 4, "name": "tf.keras", "pos": [130, 135]}, {"sent_id": 7, "name": "tf.keras", "pos": [229, 234]}, {"sent_id": 10, "name": "tf.keras", "pos": [315, 320]}, {"sent_id": 14, "name": "tf.keras", "pos": [421, 426]}], [{"sent_id": 3, "name": "tf.summary", "pos": [109, 113]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [8, 14]}], [{"sent_id": 0, "name": "tf.contrib.keras", "pos": [8, 17]}]], "sents": ["tf.keras (formerly tf.contrib.keras) is an implementation of keras 2 implemented exclusively with/for tensorflow.", "It is hosted on the tensorflow repo and has a distinct code base than the official repo (the last commit there in the tf-keras branch dates back from May 2017).", "As a rule of thumb, if your code use any tensorflow-specific code,  say anything in tf.data.", "* for providing inputs or tf.summary.", "* for visualization in tensorboard, it is simpler to just use tf.keras.", "(Some may even recommend not using the reference Keras implementation with TF because of occasional problems it has with this toolkit).", "On the other hand, if you plan to actively maintain a framework-agnostic code, using keras' own package is your only choice.", "If you don't care much about being framework-agnostic but don't use tensorflow-specific code, I would probably advise to go with tf.keras and start using tensorflow-specific code, esp.", "tf.data which is a game-changer in my opinion.", "EDIT", "I attended a talk by Chollet on TF2 (couldn't find a recording online) in which he basically said that support for frameworks other than TF would eventually drop and future developments of Keras would happen exclusively in tf.keras.", "From what I can see, this is already happening, as Keras' commit stream is getting thin these days.", "It makes a lot of sense since, as of now, the only other popular DL framework is pytorch, which is not supported by Keras.", "Keeping Keras code \"agnostic\" to tensorflow -- the only major framework it is supporting -- makes less and less sense.", "So today, my answer would be to use tf.keras by default, and keep Keras for legacy projects that would be hard to migrate -- that is the future-proof choice for Keras."], "sent_idxs": [101, 1056, 2546, 1012, 17710, 8180, 1006, 3839, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 17710, 8180, 1007, 2003, 2019, 7375, 1997, 17710, 8180, 1016, 7528, 7580, 2007, 1013, 2005, 23435, 12314, 1012, 102, 101, 2009, 2003, 4354, 2006, 1996, 23435, 12314, 16360, 2080, 1998, 2038, 1037, 5664, 3642, 2918, 2084, 1996, 2880, 16360, 2080, 1006, 1996, 2197, 10797, 2045, 1999, 1996, 1056, 2546, 1011, 17710, 8180, 3589, 5246, 2067, 2013, 2089, 2418, 1007, 1012, 102, 101, 2004, 1037, 3627, 1997, 7639, 1010, 2065, 2115, 3642, 2224, 2151, 23435, 12314, 1011, 3563, 3642, 1010, 2360, 2505, 1999, 1056, 2546, 1012, 2951, 1012, 102, 101, 1008, 2005, 4346, 20407, 2030, 1056, 2546, 1012, 12654, 1012, 102, 101, 1008, 2005, 5107, 3989, 1999, 23435, 6277, 1010, 2009, 2003, 16325, 2000, 2074, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 1006, 2070, 2089, 2130, 16755, 2025, 2478, 1996, 4431, 17710, 8180, 7375, 2007, 1056, 2546, 2138, 1997, 8138, 3471, 2009, 2038, 2007, 2023, 6994, 23615, 1007, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 2065, 2017, 2933, 2000, 8851, 5441, 1037, 7705, 1011, 12943, 28199, 3642, 1010, 2478, 17710, 8180, 1005, 2219, 7427, 2003, 2115, 2069, 3601, 1012, 102, 101, 2065, 2017, 2123, 1005, 1056, 2729, 2172, 2055, 2108, 7705, 1011, 12943, 28199, 2021, 2123, 1005, 1056, 2224, 23435, 12314, 1011, 3563, 3642, 1010, 1045, 2052, 2763, 18012, 2000, 2175, 2007, 1056, 2546, 1012, 17710, 8180, 1998, 2707, 2478, 23435, 12314, 1011, 3563, 3642, 1010, 9686, 2361, 1012, 102, 101, 1056, 2546, 1012, 2951, 2029, 2003, 1037, 2208, 1011, 2689, 2099, 1999, 2026, 5448, 1012, 102, 101, 10086, 102, 101, 1045, 3230, 1037, 2831, 2011, 16480, 22592, 2006, 1056, 2546, 2475, 1006, 2481, 1005, 1056, 2424, 1037, 3405, 3784, 1007, 1999, 2029, 2002, 10468, 2056, 2008, 2490, 2005, 7705, 2015, 2060, 2084, 1056, 2546, 2052, 2776, 4530, 1998, 2925, 8973, 1997, 17710, 8180, 2052, 4148, 7580, 1999, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 2013, 2054, 1045, 2064, 2156, 1010, 2023, 2003, 2525, 6230, 1010, 2004, 17710, 8180, 1005, 10797, 5460, 2003, 2893, 4857, 2122, 2420, 1012, 102, 101, 2009, 3084, 1037, 2843, 1997, 3168, 2144, 1010, 2004, 1997, 2085, 1010, 1996, 2069, 2060, 2759, 21469, 7705, 2003, 1052, 22123, 2953, 2818, 1010, 2029, 2003, 2025, 3569, 2011, 17710, 8180, 1012, 102, 101, 4363, 17710, 8180, 3642, 1000, 12943, 28199, 1000, 2000, 23435, 12314, 1011, 1011, 1996, 2069, 2350, 7705, 2009, 2003, 4637, 1011, 1011, 3084, 2625, 1998, 2625, 3168, 1012, 102, 101, 2061, 2651, 1010, 2026, 3437, 2052, 2022, 2000, 2224, 1056, 2546, 1012, 17710, 8180, 2011, 12398, 1010, 1998, 2562, 17710, 8180, 2005, 8027, 3934, 2008, 2052, 2022, 2524, 2000, 22806, 1011, 1011, 2008, 2003, 1996, 2925, 1011, 6947, 3601, 2005, 17710, 8180, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 34, 76, 103, 115, 137, 166, 197, 247, 264, 267, 322, 347, 381, 411, 456], "sent_pos": [0, 2, 2, 2, 2, 2, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51465838", "vertexSet": [[{"sent_id": 4, "name": "tf.placeholder", "pos": [99, 104]}], [{"sent_id": 4, "name": "tf.trainable_variables", "pos": [84, 91]}, {"sent_id": 6, "name": "tf.trainable_variables", "pos": [171, 178]}], [{"sent_id": 7, "name": "tf.get_default_session", "pos": [192, 200]}]], "sents": ["Note that the solution from @Maxim will create new operations in the graph each time it runs.", "If you run the function very frequently this will cause your code to get slower and slower.", "Two solutions to work around this problem:", "Create the assign operations at the same time as the rest of the graph and reuse them:", "assign_ops = []\nfor var_name in tf.trainable_variables():\n  assign_placeholder = tf.placeholder(var.dtype, shape=value.shape)\n  assign_op = var.assign(assign_placeholder)\n  assign_ops.append(assign_op)", "Use the load function on the variables, I prefer this one as it removes the need for the code above:", "self.params = tf.trainable_variables()", "def get_weights(self):\n    values = tf.get_default_session().run(self.params)\n    return values", "def set_weights(self, weights):\n    for i, value in enumerate(weights):\n        value = np.asarray(value)\n        self.params[i].load(value, self.sess)", "(I can't comment so I put this as an answer instead)"], "sent_idxs": [101, 3602, 2008, 1996, 5576, 2013, 1030, 20446, 2097, 3443, 2047, 3136, 1999, 1996, 10629, 2169, 2051, 2009, 3216, 1012, 102, 101, 2065, 2017, 2448, 1996, 3853, 2200, 4703, 2023, 2097, 3426, 2115, 3642, 2000, 2131, 12430, 1998, 12430, 1012, 102, 101, 2048, 7300, 2000, 2147, 2105, 2023, 3291, 1024, 102, 101, 3443, 1996, 23911, 3136, 2012, 1996, 2168, 2051, 2004, 1996, 2717, 1997, 1996, 10629, 1998, 2128, 8557, 2068, 1024, 102, 101, 23911, 1035, 23092, 1027, 1031, 1033, 2005, 13075, 1035, 2171, 1999, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 1024, 23911, 1035, 2173, 14528, 1027, 1056, 2546, 1012, 2173, 14528, 1006, 13075, 1012, 26718, 18863, 1010, 4338, 1027, 3643, 1012, 4338, 1007, 23911, 1035, 6728, 1027, 13075, 1012, 23911, 1006, 23911, 1035, 2173, 14528, 1007, 23911, 1035, 23092, 1012, 10439, 10497, 1006, 23911, 1035, 6728, 1007, 102, 101, 2224, 1996, 7170, 3853, 2006, 1996, 10857, 1010, 1045, 9544, 2023, 2028, 2004, 2009, 20362, 1996, 2342, 2005, 1996, 3642, 2682, 1024, 102, 101, 2969, 1012, 11498, 5244, 1027, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 102, 101, 13366, 2131, 1035, 15871, 1006, 2969, 1007, 1024, 5300, 1027, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 5219, 1006, 1007, 1012, 2448, 1006, 2969, 1012, 11498, 5244, 1007, 2709, 5300, 102, 101, 13366, 2275, 1035, 15871, 1006, 2969, 1010, 15871, 1007, 1024, 2005, 1045, 1010, 3643, 1999, 4372, 17897, 11657, 1006, 15871, 1007, 1024, 3643, 1027, 27937, 1012, 17306, 11335, 2100, 1006, 3643, 1007, 2969, 1012, 11498, 5244, 1031, 1045, 1033, 1012, 7170, 1006, 3643, 1010, 2969, 1012, 7367, 4757, 1007, 102, 101, 1006, 1045, 2064, 1005, 1056, 7615, 2061, 1045, 2404, 2023, 2004, 2019, 3437, 2612, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 41, 51, 72, 141, 165, 181, 213, 264, 281], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57205629", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [17, 22]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [17, 24]}], [{"sent_id": 0, "name": "tf.keras.layers.lstm", "pos": [17, 28]}]], "sents": ["Going to answer this myself...it seems like there's problems with tf.keras.layers.LSTM as of 7/24 as seen here.", "I changed the model to the following", "<code>Code Snippet</code>."], "sent_idxs": [101, 2183, 2000, 3437, 2023, 2870, 1012, 1012, 1012, 2009, 3849, 2066, 2045, 1005, 1055, 3471, 2007, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 1048, 3367, 2213, 2004, 1997, 1021, 1013, 2484, 2004, 2464, 2182, 1012, 102, 101, 1045, 2904, 1996, 2944, 2000, 1996, 2206, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 47, 61], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56419842", "vertexSet": [[{"sent_id": 3, "name": "tf.random", "pos": [62, 66]}], [{"sent_id": 3, "name": "tf.distributions", "pos": [34, 38]}], [{"sent_id": 3, "name": "tf.random.normal", "pos": [62, 68]}], [{"sent_id": 3, "name": "tf.distributions.normal", "pos": [34, 40]}]], "sents": ["I recently looked at tf probability, the new place for tf distributions.", "This is my understanding:", "They are not the same.", "tf.distributions.Normal will give you a distribution object from which you can sample (this will be same as evaluating the tensor returned by tf.random.normal function call for the same mean and loc values).", "But, a distribution additionally allows you to evaluate probability of a sample that you provide and all the aspects of having access to a distribution.", "For example, you could do the following:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 3728, 2246, 2012, 1056, 2546, 9723, 1010, 1996, 2047, 2173, 2005, 1056, 2546, 20611, 1012, 102, 101, 2023, 2003, 2026, 4824, 1024, 102, 101, 2027, 2024, 2025, 1996, 2168, 1012, 102, 101, 1056, 2546, 1012, 20611, 1012, 3671, 2097, 2507, 2017, 1037, 4353, 4874, 2013, 2029, 2017, 2064, 7099, 1006, 2023, 2097, 2022, 2168, 2004, 23208, 1996, 23435, 2513, 2011, 1056, 2546, 1012, 6721, 1012, 3671, 3853, 2655, 2005, 1996, 2168, 2812, 1998, 8840, 2278, 5300, 1007, 1012, 102, 101, 2021, 1010, 1037, 4353, 5678, 4473, 2017, 2000, 16157, 9723, 1997, 1037, 7099, 2008, 2017, 3073, 1998, 2035, 1996, 5919, 1997, 2383, 3229, 2000, 1037, 4353, 1012, 102, 101, 2005, 2742, 1010, 2017, 2071, 2079, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 18, 25, 33, 81, 110, 121, 135], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44848050", "vertexSet": [[{"sent_id": 4, "name": "tf.device", "pos": [107, 111]}], [{"sent_id": 6, "name": "tf.session", "pos": [172, 176]}], [{"sent_id": 6, "name": "tf.configproto", "pos": [145, 153]}]], "sents": ["There are 3 ways to achieve this:", "Using CUDA_VISIBLE_DEVICES environment variable.", "by setting environment variable CUDA_VISIBLE_DEVICES=\"1\" makes only device 1 visible and by setting CUDA_VISIBLE_DEVICES=\"0,1\" makes devices 0 and 1 visible.", "You can do this in python by having a line os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" after importing os package.", "Using with tf.device('/gpu:2') and creating the graph.", "Then it will use GPU device 2 to run.", "Using config = tf.ConfigProto(device_count = {'GPU': 1}) and then sess = tf.Session(config=config).", "This will use GPU device 1."], "sent_idxs": [101, 2045, 2024, 1017, 3971, 2000, 6162, 2023, 1024, 102, 101, 2478, 12731, 2850, 1035, 5710, 1035, 5733, 4044, 8023, 1012, 102, 101, 2011, 4292, 4044, 8023, 12731, 2850, 1035, 5710, 1035, 5733, 1027, 1000, 1015, 1000, 3084, 2069, 5080, 1015, 5710, 1998, 2011, 4292, 12731, 2850, 1035, 5710, 1035, 5733, 1027, 1000, 1014, 1010, 1015, 1000, 3084, 5733, 1014, 1998, 1015, 5710, 1012, 102, 101, 2017, 2064, 2079, 2023, 1999, 18750, 2011, 2383, 1037, 2240, 9808, 1012, 4372, 21663, 2239, 1031, 1000, 12731, 2850, 1035, 5710, 1035, 5733, 1000, 1033, 1027, 1000, 1014, 1010, 1015, 1000, 2044, 12324, 2075, 9808, 7427, 1012, 102, 101, 2478, 2007, 1056, 2546, 1012, 5080, 1006, 1005, 1013, 14246, 2226, 1024, 1016, 1005, 1007, 1998, 4526, 1996, 10629, 1012, 102, 101, 2059, 2009, 2097, 2224, 14246, 2226, 5080, 1016, 2000, 2448, 1012, 102, 101, 2478, 9530, 8873, 2290, 1027, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 1006, 5080, 1035, 4175, 1027, 1063, 1005, 14246, 2226, 1005, 1024, 1015, 1065, 1007, 1998, 2059, 7367, 4757, 1027, 1056, 2546, 1012, 5219, 1006, 9530, 8873, 2290, 1027, 9530, 8873, 2290, 1007, 1012, 102, 101, 2023, 2097, 2224, 14246, 2226, 5080, 1015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 22, 65, 104, 126, 139, 187, 197], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58361716", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [64, 69]}, {"sent_id": 2, "name": "tf.keras", "pos": [114, 119]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [9, 15]}], [{"sent_id": 2, "name": "tf.keras.model", "pos": [114, 121]}], [{"sent_id": 1, "name": "tf.keras.layers", "pos": [64, 71]}], [{"sent_id": 1, "name": "tf.keras.layers.dense", "pos": [64, 73]}]], "sents": ["In TensorFlow 2.0 the package tf.contrib has been removed (and this was a good choice since the whole package was a huge mix of different projects all placed inside the same box), so you can't use it.", "In TensorFlow 2.0 we need to use tf.keras.layers.Dense to create a fully connected layer, but more importantly, you have to migrate your codebase to Keras.", "In fact, you can't define a layer and use it, without creating a tf.keras.Model object that uses it."], "sent_idxs": [101, 1999, 23435, 12314, 1016, 1012, 1014, 1996, 7427, 1056, 2546, 1012, 9530, 18886, 2497, 2038, 2042, 3718, 1006, 1998, 2023, 2001, 1037, 2204, 3601, 2144, 1996, 2878, 7427, 2001, 1037, 4121, 4666, 1997, 2367, 3934, 2035, 2872, 2503, 1996, 2168, 3482, 1007, 1010, 2061, 2017, 2064, 1005, 1056, 2224, 2009, 1012, 102, 101, 1999, 23435, 12314, 1016, 1012, 1014, 2057, 2342, 2000, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 2000, 3443, 1037, 3929, 4198, 6741, 1010, 2021, 2062, 14780, 1010, 2017, 2031, 2000, 22806, 2115, 3642, 15058, 2000, 17710, 8180, 1012, 102, 101, 1999, 2755, 1010, 2017, 2064, 1005, 1056, 9375, 1037, 6741, 1998, 2224, 2009, 1010, 2302, 4526, 1037, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 4874, 2008, 3594, 2009, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 53, 96, 127], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0]}, {"title": "47761718", "vertexSet": [[{"sent_id": 4, "name": "tf.sigmoid ", "pos": [68, 74]}], [{"sent_id": 4, "name": "tf.nn.softmax", "pos": [55, 63]}]], "sents": ["Using a sigmoid activation on two outputs doesn't give you a probability distribution:", "<code>Code Snippet</code>.", "Prints:", "<code>Code Snippet</code>.", "Instead of tf.nn.softmax, you could also use tf.sigmoid on a single logit, then set the other output to one minus that."], "sent_idxs": [101, 2478, 1037, 9033, 21693, 9314, 13791, 2006, 2048, 27852, 2987, 1005, 1056, 2507, 2017, 1037, 9723, 4353, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 11204, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2612, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1010, 2017, 2071, 2036, 2224, 1056, 2546, 1012, 9033, 21693, 9314, 2006, 1037, 2309, 8833, 4183, 1010, 2059, 2275, 1996, 2060, 6434, 2000, 2028, 15718, 2008, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [4]}], "na_triple": [], "sent_ends": [0, 20, 34, 38, 52, 91], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51225193", "vertexSet": [[{"sent_id": 4, "name": "tf.data", "pos": [63, 67]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [21, 27]}], [{"sent_id": 2, "name": "tf.contrib.learn", "pos": [21, 29]}], [{"sent_id": 2, "name": "tf.contrib.learn.preprocessing", "pos": [21, 34]}]], "sents": ["I've not solved it yet either.", "One set of explanation is here:", "tf.contrib.learn.preprocessing: Deprecated.", "The python-only\n  preprocessing functions are not a good fit for TensorFlow.", "Please use\n  tf.data, and consider tensorflow/transform for more complex use cases.", "https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/learn/README.md", "This example should help... but it doesn't explain much\nhttps://github.com/tensorflow/transform/blob/master/examples/sentiment_example.py", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 1005, 2310, 2025, 13332, 2009, 2664, 2593, 1012, 102, 101, 2028, 2275, 1997, 7526, 2003, 2182, 1024, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 17463, 3217, 9623, 7741, 1024, 2139, 28139, 12921, 1012, 102, 101, 1996, 18750, 1011, 2069, 17463, 3217, 9623, 7741, 4972, 2024, 2025, 1037, 2204, 4906, 2005, 23435, 12314, 1012, 102, 101, 3531, 2224, 1056, 2546, 1012, 2951, 1010, 1998, 5136, 23435, 12314, 1013, 10938, 2005, 2062, 3375, 2224, 3572, 1012, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 1054, 2487, 1012, 1022, 1013, 23435, 12314, 1013, 9530, 18886, 2497, 1013, 4553, 1013, 3191, 4168, 1012, 9108, 102, 101, 2023, 2742, 2323, 2393, 1012, 1012, 1012, 2021, 2009, 2987, 1005, 1056, 4863, 2172, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 10938, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 4973, 1013, 15792, 1035, 2742, 1012, 1052, 2100, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 11, 20, 40, 60, 81, 121, 166, 180], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38538835", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [82, 87]}], [{"sent_id": 1, "name": "tf.maximum", "pos": [35, 39]}], [{"sent_id": 1, "name": "tf.clip_by_value", "pos": [49, 57]}], [{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [82, 99]}]], "sents": ["The issue you are having is because log(pred) is not defined for pred = 0.", "The \"hacky\" way around this is to use tf.maximum(pred, 1e-15) or tf.clip_by_value(pred, 1e-15, 1.0).", "An even better solution, however, is using tf.nn.softmax_cross_entropy_with_logits(pred) instead of applying softmax and cross-entropy separately, which deals with edge cases like this (hence all your problems) automatically!", "For further reading, I'd recommend this great answer:\nhttps://stackoverflow.com/a/34243720/5829427"], "sent_idxs": [101, 1996, 3277, 2017, 2024, 2383, 2003, 2138, 8833, 1006, 3653, 2094, 1007, 2003, 2025, 4225, 2005, 3653, 2094, 1027, 1014, 1012, 102, 101, 1996, 1000, 20578, 2100, 1000, 2126, 2105, 2023, 2003, 2000, 2224, 1056, 2546, 1012, 4555, 1006, 3653, 2094, 1010, 1015, 2063, 1011, 2321, 1007, 2030, 1056, 2546, 1012, 12528, 1035, 2011, 1035, 3643, 1006, 3653, 2094, 1010, 1015, 2063, 1011, 2321, 1010, 1015, 1012, 1014, 1007, 1012, 102, 101, 2019, 2130, 2488, 5576, 1010, 2174, 1010, 2003, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 3653, 2094, 1007, 2612, 1997, 11243, 3730, 17848, 1998, 2892, 1011, 23077, 10329, 1010, 2029, 9144, 2007, 3341, 3572, 2066, 2023, 1006, 6516, 2035, 2115, 3471, 1007, 8073, 999, 102, 101, 2005, 2582, 3752, 1010, 1045, 1005, 1040, 16755, 2023, 2307, 3437, 1024, 16770, 1024, 1013, 1013, 9991, 7840, 12314, 1012, 4012, 1013, 1037, 1013, 4090, 18827, 24434, 11387, 1013, 5388, 24594, 20958, 2581, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 23, 72, 130, 165], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56262468", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [14, 19]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [14, 21]}], [{"sent_id": 0, "name": "tf.keras.layers.dropout", "pos": [14, 24]}]], "sents": ["This depreciation warning is due to the Dropout layer in tf.keras.layers.Dropout.To avoid this warning, you need to clearly specify rate= in Dropout as: Dropout(rate=0.2).", "Earlier it was keep_prob and it is now deprecated to rate i.e.", "rate = 1-keep_prob.", "For more, you can check out this tensorflow documentation."], "sent_idxs": [101, 2023, 2139, 28139, 23247, 5432, 2003, 2349, 2000, 1996, 4530, 5833, 6741, 1999, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 4530, 5833, 1012, 2000, 4468, 2023, 5432, 1010, 2017, 2342, 2000, 4415, 20648, 3446, 1027, 1999, 4530, 5833, 2004, 1024, 4530, 5833, 1006, 3446, 1027, 1014, 1012, 1016, 1007, 1012, 102, 101, 3041, 2009, 2001, 2562, 1035, 4013, 2497, 1998, 2009, 2003, 2085, 2139, 28139, 12921, 2000, 3446, 1045, 1012, 1041, 1012, 102, 101, 3446, 1027, 1015, 1011, 2562, 1035, 4013, 2497, 1012, 102, 101, 2005, 2062, 1010, 2017, 2064, 4638, 2041, 2023, 23435, 12314, 12653, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 53, 75, 86, 100], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35929502", "vertexSet": [[{"sent_id": 4, "name": "tf.op_scope", "pos": [80, 86]}, {"sent_id": 9, "name": "tf.op_scope", "pos": [219, 225]}], [{"sent_id": 2, "name": "tf.name_scope", "pos": [40, 46]}, {"sent_id": 4, "name": "tf.name_scope", "pos": [88, 94]}, {"sent_id": 10, "name": "tf.name_scope", "pos": [259, 265]}], [{"sent_id": 8, "name": "tf.get_variable", "pos": [202, 208]}], [{"sent_id": 3, "name": "tf.variable_scope", "pos": [59, 65]}, {"sent_id": 5, "name": "tf.variable_scope", "pos": [118, 124]}, {"sent_id": 8, "name": "tf.variable_scope", "pos": [193, 199]}, {"sent_id": 10, "name": "tf.variable_scope", "pos": [266, 272]}, {"sent_id": 11, "name": "tf.variable_scope", "pos": [276, 282]}], [{"sent_id": 5, "name": "tf.variable_op_scope", "pos": [108, 116]}, {"sent_id": 8, "name": "tf.variable_op_scope", "pos": [184, 192]}, {"sent_id": 9, "name": "tf.variable_op_scope", "pos": [226, 234]}]], "sents": ["Namespaces is a way to organize names for variables and operators in hierarchical manner (e.g.", "\"scopeA/scopeB/scopeC/op1\")", "tf.name_scope creates namespace for operators in the default graph..", "tf.variable_scope creates namespace for both variables and operators in the default graph.", "tf.op_scope same as tf.name_scope, but for the graph in which specified variables were created.", "tf.variable_op_scope same as tf.variable_scope, but for the graph in which specified variables were created.", "Links to the sources above help to disambiguate this documentation issue.", "This example shows that all types of scopes define namespaces for both variables and operators with following differences:", "scopes defined by tf.variable_op_scope or tf.variable_scope are compatible with tf.get_variable (it ignores two other scopes).", "tf.op_scope and tf.variable_op_scope just select a graph from a list of specified variables to create a scope for.", "Other than than their behavior equal to tf.name_scope and tf.variable_scope accordingly.", "tf.variable_scope and variable_op_scope add specified or default initializer.", "."], "sent_idxs": [101, 3415, 15327, 2015, 2003, 1037, 2126, 2000, 10939, 3415, 2005, 10857, 1998, 9224, 1999, 25835, 5450, 1006, 1041, 1012, 1043, 1012, 102, 101, 1000, 9531, 2050, 1013, 9531, 2497, 1013, 9531, 2278, 1013, 6728, 2487, 1000, 1007, 102, 101, 1056, 2546, 1012, 2171, 1035, 9531, 9005, 3415, 15327, 2005, 9224, 1999, 1996, 12398, 10629, 1012, 1012, 102, 101, 1056, 2546, 1012, 8023, 1035, 9531, 9005, 3415, 15327, 2005, 2119, 10857, 1998, 9224, 1999, 1996, 12398, 10629, 1012, 102, 101, 1056, 2546, 1012, 6728, 1035, 9531, 2168, 2004, 1056, 2546, 1012, 2171, 1035, 9531, 1010, 2021, 2005, 1996, 10629, 1999, 2029, 9675, 10857, 2020, 2580, 1012, 102, 101, 1056, 2546, 1012, 8023, 1035, 6728, 1035, 9531, 2168, 2004, 1056, 2546, 1012, 8023, 1035, 9531, 1010, 2021, 2005, 1996, 10629, 1999, 2029, 9675, 10857, 2020, 2580, 1012, 102, 101, 6971, 2000, 1996, 4216, 2682, 2393, 2000, 4487, 21559, 5638, 19696, 2618, 2023, 12653, 3277, 1012, 102, 101, 2023, 2742, 3065, 2008, 2035, 4127, 1997, 9531, 2015, 9375, 3415, 15327, 2015, 2005, 2119, 10857, 1998, 9224, 2007, 2206, 5966, 1024, 102, 101, 9531, 2015, 4225, 2011, 1056, 2546, 1012, 8023, 1035, 6728, 1035, 9531, 2030, 1056, 2546, 1012, 8023, 1035, 9531, 2024, 11892, 2007, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 2009, 26663, 2048, 2060, 9531, 2015, 1007, 1012, 102, 101, 1056, 2546, 1012, 6728, 1035, 9531, 1998, 1056, 2546, 1012, 8023, 1035, 6728, 1035, 9531, 2074, 7276, 1037, 10629, 2013, 1037, 2862, 1997, 9675, 10857, 2000, 3443, 1037, 9531, 2005, 1012, 102, 101, 2060, 2084, 2084, 2037, 5248, 5020, 2000, 1056, 2546, 1012, 2171, 1035, 9531, 1998, 1056, 2546, 1012, 8023, 1035, 9531, 11914, 1012, 102, 101, 1056, 2546, 1012, 8023, 1035, 9531, 1998, 8023, 1035, 6728, 1035, 9531, 5587, 9675, 2030, 12398, 3988, 17629, 1012, 102, 101, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 23, 39, 58, 79, 107, 137, 155, 179, 218, 251, 275, 296, 299], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63172238", "vertexSet": [[{"sent_id": 2, "name": "tf.tensor", "pos": [78, 82]}], [{"sent_id": 7, "name": "tf.function", "pos": [192, 196]}], [{"sent_id": 0, "name": "tf.numpy_function", "pos": [31, 39]}, {"sent_id": 2, "name": "tf.numpy_function", "pos": [68, 76]}], [{"sent_id": 2, "name": "tf.convert_to_tensor", "pos": [57, 65]}]], "sents": ["EDIT: Your code should work if instead of using .numpy() in tf_func you pass params and input directly to tf.numpy_function:", "<code>Code Snippet</code>.", "The tf.convert_to_tensor are there because tf.numpy_function expects strictly  tf.Tensor objects, so if you directly use params, which will be a variable passed from myLayer, it will not work as expected.", "For some reason, the code still gives an error about shapes after this.", "I got it to run properly changing the shape of the param weight to [1, 1]:", "<code>Code Snippet</code>.", "You can pass run_eagerly=True to compile to force Keras to use eager mode (i.e.", "without tf.function) for training:", "<code>Code Snippet</code>."], "sent_idxs": [101, 10086, 1024, 2115, 3642, 2323, 2147, 2065, 2612, 1997, 2478, 1012, 16371, 8737, 2100, 1006, 1007, 1999, 1056, 2546, 1035, 4569, 2278, 2017, 3413, 11498, 5244, 1998, 7953, 3495, 2000, 1056, 2546, 1012, 16371, 8737, 2100, 1035, 3853, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 1056, 2546, 1012, 10463, 1035, 2000, 1035, 23435, 2024, 2045, 2138, 1056, 2546, 1012, 16371, 8737, 2100, 1035, 3853, 24273, 9975, 1056, 2546, 1012, 23435, 5200, 1010, 2061, 2065, 2017, 3495, 2224, 11498, 5244, 1010, 2029, 2097, 2022, 1037, 8023, 2979, 2013, 2026, 24314, 1010, 2009, 2097, 2025, 2147, 2004, 3517, 1012, 102, 101, 2005, 2070, 3114, 1010, 1996, 3642, 2145, 3957, 2019, 7561, 2055, 10466, 2044, 2023, 1012, 102, 101, 1045, 2288, 2009, 2000, 2448, 7919, 5278, 1996, 4338, 1997, 1996, 11498, 2213, 3635, 2000, 1031, 1015, 1010, 1015, 1033, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 3413, 2448, 1035, 17858, 1027, 2995, 2000, 4012, 22090, 2000, 2486, 17710, 8180, 2000, 2224, 9461, 5549, 1006, 1045, 1012, 1041, 1012, 102, 101, 2302, 1056, 2546, 1012, 3853, 1007, 2005, 2731, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 41, 55, 110, 127, 150, 164, 190, 201, 215], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46671829", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [32, 38]}], [{"sent_id": 1, "name": "tf.contrib.legacy_seq2seq", "pos": [32, 46]}], [{"sent_id": 1, "name": "tf.contrib.legacy_seq2seq.sequence_loss_by_example", "pos": [32, 54]}]], "sents": ["You can output an index of a word (per example), thus avoid one-hot word representation (which is indeed very big).", "Use tf.contrib.legacy_seq2seq.sequence_loss_by_example:", "Weighted cross-entropy loss for a sequence of logits (per example).", "logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].", ".", "targets: List of 1D batch-sized int32 Tensors of the same length as logits..", "weights: List of 1D batch-sized\n  float-Tensors of the same length as logits..", "Note that it doesn't reduce the size of your model, but it saves a lot of memory by computing the loss from sparsely encoded labels.", "A complete example of a word-rnn implementation can be found here, and they use exactly this approach."], "sent_idxs": [101, 2017, 2064, 6434, 2019, 5950, 1997, 1037, 2773, 1006, 2566, 2742, 1007, 1010, 2947, 4468, 2028, 1011, 2980, 2773, 6630, 1006, 2029, 2003, 5262, 2200, 2502, 1007, 1012, 102, 101, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 8027, 1035, 7367, 4160, 2475, 3366, 4160, 1012, 5537, 1035, 3279, 1035, 2011, 1035, 2742, 1024, 102, 101, 18215, 2892, 1011, 23077, 3279, 2005, 1037, 5537, 1997, 8833, 12762, 1006, 2566, 2742, 1007, 1012, 102, 101, 8833, 12762, 1024, 2862, 1997, 14134, 23435, 2015, 1997, 4338, 1031, 14108, 1035, 2946, 1060, 16371, 2213, 1035, 21933, 4063, 1035, 9255, 1033, 1012, 102, 101, 1012, 102, 101, 7889, 1024, 2862, 1997, 1015, 2094, 14108, 1011, 7451, 20014, 16703, 23435, 2015, 1997, 1996, 2168, 3091, 2004, 8833, 12762, 1012, 1012, 102, 101, 15871, 1024, 2862, 1997, 1015, 2094, 14108, 1011, 7451, 14257, 1011, 23435, 2015, 1997, 1996, 2168, 3091, 2004, 8833, 12762, 1012, 1012, 102, 101, 3602, 2008, 2009, 2987, 1005, 1056, 5547, 1996, 2946, 1997, 2115, 2944, 1010, 2021, 2009, 13169, 1037, 2843, 1997, 3638, 2011, 9798, 1996, 3279, 2013, 24961, 12359, 10873, 1012, 102, 101, 1037, 3143, 2742, 1997, 1037, 2773, 1011, 29300, 2078, 7375, 2064, 2022, 2179, 2182, 1010, 1998, 2027, 2224, 3599, 2023, 3921, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 30, 56, 74, 100, 103, 127, 151, 182, 206], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59993751", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [38, 43]}, {"sent_id": 2, "name": "tf.keras", "pos": [68, 73]}], [{"sent_id": 1, "name": "tf.distribute", "pos": [24, 28]}], [{"sent_id": 1, "name": "tf.distribute.mirroredstrategy", "pos": [24, 33]}]], "sents": ["This answer is based on a comment on OP's question.", "When conducting multi-gpu training with tf.distribute.MirroredStrategy, one should use the tf.keras API and not the tensorflow backend of the keras package.", "In general, it is best not to mix tf.keras and keras."], "sent_idxs": [101, 2023, 3437, 2003, 2241, 2006, 1037, 7615, 2006, 6728, 1005, 1055, 3160, 1012, 102, 101, 2043, 9283, 4800, 1011, 14246, 2226, 2731, 2007, 1056, 2546, 1012, 16062, 1012, 22243, 20528, 2618, 6292, 1010, 2028, 2323, 2224, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 1998, 2025, 1996, 23435, 12314, 2067, 10497, 1997, 1996, 17710, 8180, 7427, 1012, 102, 101, 1999, 2236, 1010, 2009, 2003, 2190, 2025, 2000, 4666, 1056, 2546, 1012, 17710, 8180, 1998, 17710, 8180, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 58, 78], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}, {"title": "34594851", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [2, 6]}, {"sent_id": 1, "name": "tf.train", "pos": [35, 39]}, {"sent_id": 4, "name": "tf.train", "pos": [177, 181]}, {"sent_id": 5, "name": "tf.train", "pos": [212, 216]}, {"sent_id": 6, "name": "tf.train", "pos": [244, 248]}], [{"sent_id": 1, "name": "tf.randomshufflequeue", "pos": [47, 55]}], [{"sent_id": 0, "name": "tf.train.shuffle_batch", "pos": [2, 10]}, {"sent_id": 1, "name": "tf.train.shuffle_batch", "pos": [35, 43]}, {"sent_id": 4, "name": "tf.train.shuffle_batch", "pos": [177, 185]}, {"sent_id": 5, "name": "tf.train.shuffle_batch", "pos": [212, 220]}], [{"sent_id": 6, "name": "tf.train.start_queue_runners", "pos": [244, 254]}]], "sents": ["The tf.train.shuffle_batch() function can be used to produce (one or more) tensors containing a batch of inputs.", "Internally, tf.train.shuffle_batch() creates a tf.RandomShuffleQueue, on which it calls q.enqueue() with the image and label tensors to enqueue a single element (image-label pair).", "It then returns the result of q.dequeue_many(batch_size), which concatenates batch_size randomly selected elements (image-label pairs) into a batch of images and a batch of labels.", "Note that, although it looks from the code like read_input and filename_queue have a functional relationship, there is an additional wrinkle.", "Simply evaluating the result of tf.train.shuffle_batch() will block forever, because no elements have been added to the internal queue.", "To simplify this, when you call tf.train.shuffle_batch(), TensorFlow will add a QueueRunner to an internal collection in the graph.", "A later call to tf.train.start_queue_runners() (e.g.", "here in cifar10_train.py) will start a thread that adds elements to the queue, and enables training to proceed.", "The Threading and Queues HOWTO has more information on how this works."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 3853, 2064, 2022, 2109, 2000, 3965, 1006, 2028, 2030, 2062, 1007, 23435, 2015, 4820, 1037, 14108, 1997, 20407, 1012, 102, 101, 16058, 1010, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 9005, 1037, 1056, 2546, 1012, 6721, 14235, 18142, 4226, 5657, 1010, 2006, 2029, 2009, 4455, 1053, 1012, 4372, 4226, 5657, 1006, 1007, 2007, 1996, 3746, 1998, 3830, 23435, 2015, 2000, 4372, 4226, 5657, 1037, 2309, 5783, 1006, 3746, 1011, 3830, 3940, 1007, 1012, 102, 101, 2009, 2059, 5651, 1996, 2765, 1997, 1053, 1012, 2139, 4226, 5657, 1035, 2116, 1006, 14108, 1035, 2946, 1007, 1010, 2029, 9530, 16280, 12556, 2015, 14108, 1035, 2946, 18154, 3479, 3787, 1006, 3746, 1011, 3830, 7689, 1007, 2046, 1037, 14108, 1997, 4871, 1998, 1037, 14108, 1997, 10873, 1012, 102, 101, 3602, 2008, 1010, 2348, 2009, 3504, 2013, 1996, 3642, 2066, 3191, 1035, 7953, 1998, 5371, 18442, 1035, 24240, 2031, 1037, 8360, 3276, 1010, 2045, 2003, 2019, 3176, 23277, 19839, 2571, 1012, 102, 101, 3432, 23208, 1996, 2765, 1997, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 2097, 3796, 5091, 1010, 2138, 2053, 3787, 2031, 2042, 2794, 2000, 1996, 4722, 24240, 1012, 102, 101, 2000, 21934, 28250, 2023, 1010, 2043, 2017, 2655, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 1010, 23435, 12314, 2097, 5587, 1037, 24240, 23195, 2000, 2019, 4722, 3074, 1999, 1996, 10629, 1012, 102, 101, 1037, 2101, 2655, 2000, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 1007, 1006, 1041, 1012, 1043, 1012, 102, 101, 2182, 1999, 25022, 14971, 10790, 1035, 3345, 1012, 1052, 2100, 1007, 2097, 2707, 1037, 11689, 2008, 9909, 3787, 2000, 1996, 24240, 1010, 1998, 12939, 2731, 2000, 10838, 1012, 102, 101, 1996, 11689, 2075, 1998, 24240, 2015, 2129, 3406, 2038, 2062, 2592, 2006, 2129, 2023, 2573, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 32, 89, 138, 171, 203, 239, 262, 292, 310], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38794027", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [44, 48]}, {"sent_id": 5, "name": "tf.train", "pos": [152, 156]}], [{"sent_id": 2, "name": "tf.device", "pos": [39, 43]}, {"sent_id": 5, "name": "tf.device", "pos": [147, 151]}, {"sent_id": 5, "name": "tf.device", "pos": [178, 182]}], [{"sent_id": 2, "name": "tf.train.replica_device_setter", "pos": [44, 55]}, {"sent_id": 5, "name": "tf.train.replica_device_setter", "pos": [152, 163]}]], "sents": ["The problem stems from the definition of your global_step variable:", "<code>Code Snippet</code>.", "This definition is outside the scope of the with tf.device(tf.train.replica_device_setter(...)): block above, so no device is assigned to global_step.", "In replicated training, this is often a source of error (because if different replicas decide to place the variable on a different device, they won't share the same value), so TensorFlow includes a sanity check that prevents this.", "Fortunately, the solution is simple.", "You can either define global_step inside the with tf.device(tf.train.replica_device_setter(...)): block above, or add a small with tf.device(\"/job:ps/task:0\"): block as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3291, 12402, 2013, 1996, 6210, 1997, 2115, 3795, 1035, 3357, 8023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 6210, 2003, 2648, 1996, 9531, 1997, 1996, 2007, 1056, 2546, 1012, 5080, 1006, 1056, 2546, 1012, 3345, 1012, 15059, 1035, 5080, 1035, 2275, 3334, 1006, 1012, 1012, 1012, 1007, 1007, 1024, 3796, 2682, 1010, 2061, 2053, 5080, 2003, 4137, 2000, 3795, 1035, 3357, 1012, 102, 101, 1999, 28024, 2094, 2731, 1010, 2023, 2003, 2411, 1037, 3120, 1997, 7561, 1006, 2138, 2065, 2367, 15059, 2015, 5630, 2000, 2173, 1996, 8023, 2006, 1037, 2367, 5080, 1010, 2027, 2180, 1005, 1056, 3745, 1996, 2168, 3643, 1007, 1010, 2061, 23435, 12314, 2950, 1037, 20039, 4638, 2008, 16263, 2023, 1012, 102, 101, 14599, 1010, 1996, 5576, 2003, 3722, 1012, 102, 101, 2017, 2064, 2593, 9375, 3795, 1035, 3357, 2503, 1996, 2007, 1056, 2546, 1012, 5080, 1006, 1056, 2546, 1012, 3345, 1012, 15059, 1035, 5080, 1035, 2275, 3334, 1006, 1012, 1012, 1012, 1007, 1007, 1024, 3796, 2682, 1010, 2030, 5587, 1037, 2235, 2007, 1056, 2546, 1012, 5080, 1006, 1000, 1013, 3105, 1024, 8827, 1013, 4708, 1024, 1014, 1000, 1007, 1024, 3796, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 29, 76, 127, 136, 200, 214], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39783314", "vertexSet": [[{"sent_id": 5, "name": "tf.nn.weighted_cross_entropy_with_logits", "pos": [192, 208]}], [{"sent_id": 0, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [7, 24]}, {"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [82, 99]}, {"sent_id": 8, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [266, 283]}], [{"sent_id": 5, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [172, 190]}]], "sents": ["The warning just informs you that tf.nn.softmax_cross_entropy_with_logits will apply a softmax on the input logits, before computing cross-entropy.", "This warning seems really to avoid applying softmax twice, as the cross-entropy results would be very different.", "Here is a comment in the relevant source code, about the function that implements tf.nn.softmax_cross_entropy_with_logits:", "<code>Code Snippet</code>.", "As the warning states, this implementation is for improving performance, with the caveat that you should not put your own softmax layer as input (which is somewhat convenient, in practice).", "If the forced softmax hinders your computation, perhaps another API could help: tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits.", "The implementation does not seem to indicate, though, that any scaling will impact the result.", "I guess a linear scaling function should be fine, as long as it preserves the original logits repartition.", "But whatever is applied on the input logits, tf.nn.softmax_cross_entropy_with_logits will apply a softmax before computing the cross-entropy."], "sent_idxs": [101, 1996, 5432, 2074, 15670, 2017, 2008, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2097, 6611, 1037, 3730, 17848, 2006, 1996, 7953, 8833, 12762, 1010, 2077, 9798, 2892, 1011, 23077, 1012, 102, 101, 2023, 5432, 3849, 2428, 2000, 4468, 11243, 3730, 17848, 3807, 1010, 2004, 1996, 2892, 1011, 23077, 3463, 2052, 2022, 2200, 2367, 1012, 102, 101, 2182, 2003, 1037, 7615, 1999, 1996, 7882, 3120, 3642, 1010, 2055, 1996, 3853, 2008, 22164, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2004, 1996, 5432, 2163, 1010, 2023, 7375, 2003, 2005, 9229, 2836, 1010, 2007, 1996, 5430, 4017, 2008, 2017, 2323, 2025, 2404, 2115, 2219, 3730, 17848, 6741, 2004, 7953, 1006, 2029, 2003, 5399, 14057, 1010, 1999, 3218, 1007, 1012, 102, 101, 2065, 1996, 3140, 3730, 17848, 17666, 2545, 2115, 22334, 1010, 3383, 2178, 17928, 2071, 2393, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2030, 2672, 1056, 2546, 1012, 1050, 2078, 1012, 18215, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1012, 102, 101, 1996, 7375, 2515, 2025, 4025, 2000, 5769, 1010, 2295, 1010, 2008, 2151, 25169, 2097, 4254, 1996, 2765, 1012, 102, 101, 1045, 3984, 1037, 7399, 25169, 3853, 2323, 2022, 2986, 1010, 2004, 2146, 2004, 2009, 18536, 1996, 2434, 8833, 12762, 16360, 8445, 22753, 1012, 102, 101, 2021, 3649, 2003, 4162, 2006, 1996, 7953, 8833, 12762, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2097, 6611, 1037, 3730, 17848, 2077, 9798, 1996, 2892, 1011, 23077, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 4, 5, 6, 7, 8]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 4, 5, 6, 7, 8]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1, 4, 5, 8]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1, 4, 5, 8]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 42, 66, 101, 115, 155, 210, 230, 255, 296], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61429987", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [9, 14]}, {"sent_id": 10, "name": "tf.keras", "pos": [245, 250]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [46, 50]}], [{"sent_id": 0, "name": "tf.variable", "pos": [26, 30]}, {"sent_id": 2, "name": "tf.variable", "pos": [88, 92]}]], "sents": ["The reason for the bug is that the tf.keras optimizers apply gradients to variable objects (of type tf.Variable), while you are trying to apply gradients to tensors (of type tf.Tensor).", "Tensor objects are not mutable in TensorFlow, thus the optimizer cannot apply gradients to it.", "You should initialize the variable img as a tf.Variable.", "This is how your code should be:", "<code>Code Snippet</code>.", "Also, it is recommended to calculate the gradients outside the tape's context.", "This is because keeping it in will lead to the tape tracking the gradient calculation itself, leading to higher memory usage.", "This is only desirable if you want to calculate higher-order gradients.", "Since you don't need those, I have kept them outside.", "Note I have changed the line y = model(img)[:, :, :, filter] to y = model(img.value())[:, :, :, filter].", "This is because tf.keras models need tensors as input, not variables (bug, or feature?", ")."], "sent_idxs": [101, 1996, 3114, 2005, 1996, 11829, 2003, 2008, 1996, 1056, 2546, 1012, 17710, 8180, 23569, 27605, 16750, 6611, 17978, 2015, 2000, 8023, 5200, 1006, 1997, 2828, 1056, 2546, 1012, 8023, 1007, 1010, 2096, 2017, 2024, 2667, 2000, 6611, 17978, 2015, 2000, 23435, 2015, 1006, 1997, 2828, 1056, 2546, 1012, 23435, 1007, 1012, 102, 101, 23435, 5200, 2024, 2025, 14163, 10880, 1999, 23435, 12314, 1010, 2947, 1996, 23569, 27605, 6290, 3685, 6611, 17978, 2015, 2000, 2009, 1012, 102, 101, 2017, 2323, 3988, 4697, 1996, 8023, 10047, 2290, 2004, 1037, 1056, 2546, 1012, 8023, 1012, 102, 101, 2023, 2003, 2129, 2115, 3642, 2323, 2022, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2036, 1010, 2009, 2003, 6749, 2000, 18422, 1996, 17978, 2015, 2648, 1996, 6823, 1005, 1055, 6123, 1012, 102, 101, 2023, 2003, 2138, 4363, 2009, 1999, 2097, 2599, 2000, 1996, 6823, 9651, 1996, 17978, 17208, 2993, 1010, 2877, 2000, 3020, 3638, 8192, 1012, 102, 101, 2023, 2003, 2069, 16166, 2065, 2017, 2215, 2000, 18422, 3020, 1011, 2344, 17978, 2015, 1012, 102, 101, 2144, 2017, 2123, 1005, 1056, 2342, 2216, 1010, 1045, 2031, 2921, 2068, 2648, 1012, 102, 101, 3602, 1045, 2031, 2904, 1996, 2240, 1061, 1027, 2944, 1006, 10047, 2290, 1007, 1031, 1024, 1010, 1024, 1010, 1024, 1010, 11307, 1033, 2000, 1061, 1027, 2944, 1006, 10047, 2290, 1012, 3643, 1006, 1007, 1007, 1031, 1024, 1010, 1024, 1010, 1024, 1010, 11307, 1033, 1012, 102, 101, 2023, 2003, 2138, 1056, 2546, 1012, 17710, 8180, 4275, 2342, 23435, 2015, 2004, 7953, 1010, 2025, 10857, 1006, 11829, 1010, 2030, 3444, 1029, 102, 101, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 53, 77, 94, 104, 118, 137, 162, 179, 195, 241, 266, 270], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49432044", "vertexSet": [[{"sent_id": 0, "name": "tf.losses", "pos": [31, 35]}], [{"sent_id": 0, "name": "tf.sigmoid", "pos": [9, 15]}, {"sent_id": 2, "name": "tf.sigmoid", "pos": [66, 72]}], [{"sent_id": 0, "name": "tf.losses.sigmoid_cross_entropy", "pos": [31, 43]}]], "sents": ["It appears that I was applying loss = tf.sigmoid(logits) (as in original Torch model) and then feeding loss to tf.losses.sigmoid_cross_entropy.", "This brought gradients to nearly zero and the weights were not updated properly.", "When I removed tf.sigmoid function the gradients increased the weights started moving.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2009, 3544, 2008, 1045, 2001, 11243, 3279, 1027, 1056, 2546, 1012, 9033, 21693, 9314, 1006, 8833, 12762, 1007, 1006, 2004, 1999, 2434, 12723, 2944, 1007, 1998, 2059, 8521, 3279, 2000, 1056, 2546, 1012, 6409, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1012, 102, 101, 2023, 2716, 17978, 2015, 2000, 3053, 5717, 1998, 1996, 15871, 2020, 2025, 7172, 7919, 1012, 102, 101, 2043, 1045, 3718, 1056, 2546, 1012, 9033, 21693, 9314, 3853, 1996, 17978, 2015, 3445, 1996, 15871, 2318, 3048, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 45, 62, 83, 97], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59316498", "vertexSet": [[{"sent_id": 1, "name": "tf.random", "pos": [15, 19]}, {"sent_id": 1, "name": "tf.random", "pos": [22, 26]}], [{"sent_id": 1, "name": "tf.random.shuffle", "pos": [22, 28]}], [{"sent_id": 1, "name": "tf.random_shuffle", "pos": [15, 21]}]], "sents": ["I have the same issue.", "And it is resolved by changing tf.random_shuffle to tf.random.shuffle"], "sent_idxs": [101, 1045, 2031, 1996, 2168, 3277, 1012, 102, 101, 1998, 2009, 2003, 10395, 2011, 5278, 1056, 2546, 1012, 6721, 1035, 23046, 2000, 1056, 2546, 1012, 6721, 1012, 23046, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 8, 29], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0]}, {"title": "41131155", "vertexSet": [[{"sent_id": 3, "name": "tf.train", "pos": [86, 90]}], [{"sent_id": 7, "name": "tf.errors", "pos": [148, 152]}], [{"sent_id": 3, "name": "tf.train.saver", "pos": [86, 93]}], [{"sent_id": 7, "name": "tf.errors.outofrangeerror", "pos": [148, 158]}]], "sents": ["Alas, currently there is no good answer to this question.", "The typical evaluation workflow involves running a separate process that periodically does the following (e.g.", "evaluate() in cifar10_eval.py):", "Build a graph that includes an input pipeline that knows about the evaluation set, a copy of the model, the evaluation ops (if any), and a tf.train.Saver..", "Create a new session..", "Restore the latest checkpoint written by the training process in that session..", "Run the test op (e.g.", "ce in your question) and accumulate the results in Python, until you get a tf.errors.OutOfRangeError..", "We're currently working on improved input pipelines that will make it easier to iterate over files many times, and reuse the same session."], "sent_idxs": [101, 21862, 2015, 1010, 2747, 2045, 2003, 2053, 2204, 3437, 2000, 2023, 3160, 1012, 102, 101, 1996, 5171, 9312, 2147, 12314, 7336, 2770, 1037, 3584, 2832, 2008, 18043, 2515, 1996, 2206, 1006, 1041, 1012, 1043, 1012, 102, 101, 16157, 1006, 1007, 1999, 25022, 14971, 10790, 1035, 9345, 2140, 1012, 1052, 2100, 1007, 1024, 102, 101, 3857, 1037, 10629, 2008, 2950, 2019, 7953, 13117, 2008, 4282, 2055, 1996, 9312, 2275, 1010, 1037, 6100, 1997, 1996, 2944, 1010, 1996, 9312, 23092, 1006, 2065, 2151, 1007, 1010, 1998, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 1012, 1012, 102, 101, 3443, 1037, 2047, 5219, 1012, 1012, 102, 101, 9239, 1996, 6745, 26520, 2517, 2011, 1996, 2731, 2832, 1999, 2008, 5219, 1012, 1012, 102, 101, 2448, 1996, 3231, 6728, 1006, 1041, 1012, 1043, 1012, 102, 101, 8292, 1999, 2115, 3160, 1007, 1998, 27598, 1996, 3463, 1999, 18750, 1010, 2127, 2017, 2131, 1037, 1056, 2546, 1012, 10697, 1012, 2041, 11253, 24388, 11510, 29165, 1012, 1012, 102, 101, 2057, 1005, 2128, 2747, 2551, 2006, 5301, 7953, 13117, 2015, 2008, 2097, 2191, 2009, 6082, 2000, 2009, 22139, 2058, 6764, 2116, 2335, 1010, 1998, 2128, 8557, 1996, 2168, 5219, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 15, 37, 54, 96, 104, 120, 131, 161, 193], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64202673", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [10, 15]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [10, 17]}], [{"sent_id": 0, "name": "tf.keras.layers.conv1d", "pos": [10, 22]}]], "sents": ["For the completion, here is the documentation of tf.keras.layers.Conv1D that explain what each parameter is for.", "There is no such flow!", "This is one of the issues in deep learning, there is no \"magic\" way of selecting the best hyper-parameters to fit your problem.", "Once you are more experienced you might be able to make an educational guess which will work fairly good.", "A way to address it, is just setting multiple possible valid options for each of the hyper-parameter you wish to tune and iterate them efficently.", "One way you can do it in keras is using the GridSearchCV here is a couple of good starting links:", "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html", "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/", "https://www.kaggle.com/shujunge/gridsearchcv-with-keras"], "sent_idxs": [101, 2005, 1996, 6503, 1010, 2182, 2003, 1996, 12653, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 2008, 4863, 2054, 2169, 16381, 2003, 2005, 1012, 102, 101, 2045, 2003, 2053, 2107, 4834, 999, 102, 101, 2023, 2003, 2028, 1997, 1996, 3314, 1999, 2784, 4083, 1010, 2045, 2003, 2053, 1000, 3894, 1000, 2126, 1997, 17739, 1996, 2190, 23760, 1011, 11709, 2000, 4906, 2115, 3291, 1012, 102, 101, 2320, 2017, 2024, 2062, 5281, 2017, 2453, 2022, 2583, 2000, 2191, 2019, 4547, 3984, 2029, 2097, 2147, 7199, 2204, 1012, 102, 101, 1037, 2126, 2000, 4769, 2009, 1010, 2003, 2074, 4292, 3674, 2825, 9398, 7047, 2005, 2169, 1997, 1996, 23760, 1011, 16381, 2017, 4299, 2000, 8694, 1998, 2009, 22139, 2068, 1041, 26989, 13013, 2135, 1012, 102, 101, 2028, 2126, 2017, 2064, 2079, 2009, 1999, 17710, 8180, 2003, 2478, 1996, 8370, 17310, 11140, 2278, 2615, 2182, 2003, 1037, 3232, 1997, 2204, 3225, 6971, 1024, 102, 101, 16770, 1024, 1013, 1013, 16596, 23615, 1011, 4553, 1012, 8917, 1013, 6540, 1013, 14184, 1013, 7013, 1013, 15315, 19738, 6826, 1012, 2944, 1035, 4989, 1012, 8370, 17310, 11140, 2278, 2615, 1012, 16129, 102, 101, 16770, 1024, 1013, 1013, 3698, 19738, 6826, 2075, 8706, 2100, 1012, 4012, 1013, 8370, 1011, 3945, 1011, 23760, 28689, 22828, 2015, 1011, 2784, 1011, 4083, 1011, 4275, 1011, 18750, 1011, 17710, 8180, 1013, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 10556, 24679, 1012, 4012, 1013, 18454, 19792, 3351, 1013, 8370, 17310, 11140, 2278, 2615, 1011, 2007, 1011, 17710, 8180, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 39, 70, 92, 127, 155, 189, 224, 251], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40809042", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [9, 14]}], [{"sent_id": 0, "name": "tf.nn.top_k", "pos": [9, 18]}], [{"sent_id": 2, "name": "tf.greater_equal", "pos": [57, 63]}]], "sents": ["You can do it using built-in tf.nn.top_k function:", "<code>Code Snippet</code>.", "To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2079, 2009, 2478, 2328, 1011, 1999, 1056, 2546, 1012, 1050, 2078, 1012, 2327, 1035, 1047, 3853, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 2131, 22017, 20898, 2995, 1013, 6270, 5300, 1010, 2017, 2064, 2034, 2131, 1996, 1047, 1011, 16215, 3643, 1998, 2059, 2224, 1056, 2546, 1012, 3618, 1035, 5020, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 35, 65, 79], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48329165", "vertexSet": [[{"sent_id": 2, "name": "tf.estimator", "pos": [115, 121]}], [{"sent_id": 2, "name": "tf.estimator.export", "pos": [115, 123]}], [{"sent_id": 2, "name": "tf.estimator.export.predictoutput", "pos": [115, 127]}]], "sents": ["The function export_savedmodel requires the argument serving_input_receiver_fn, that is a function without arguments, which defines the input from the model and the predictor.", "Therefore, you must create your own serving_input_receiver_fn, where the model input type match with the model input in the training script, and the predictor input type match with the predictor input in the testing script.", "On the other hand, if you create a custom model, you must define the export_outputs,  defined by the function tf.estimator.export.PredictOutput, which input is a dictionary that define the name that has to match with the name of the predictor output in the testing script.", "For example:", "TRAINING SCRIPT.", "<code>Code Snippet</code>.", "TESTING SCRIPT.", "<code>Code Snippet</code>.", "(Code tested in Python 3.6.3, Tensorflow 1.4.0)"], "sent_idxs": [101, 1996, 3853, 9167, 1035, 5552, 5302, 9247, 5942, 1996, 6685, 3529, 1035, 7953, 1035, 8393, 1035, 1042, 2078, 1010, 2008, 2003, 1037, 3853, 2302, 9918, 1010, 2029, 11859, 1996, 7953, 2013, 1996, 2944, 1998, 1996, 16014, 2953, 1012, 102, 101, 3568, 1010, 2017, 2442, 3443, 2115, 2219, 3529, 1035, 7953, 1035, 8393, 1035, 1042, 2078, 1010, 2073, 1996, 2944, 7953, 2828, 2674, 2007, 1996, 2944, 7953, 1999, 1996, 2731, 5896, 1010, 1998, 1996, 16014, 2953, 7953, 2828, 2674, 2007, 1996, 16014, 2953, 7953, 1999, 1996, 5604, 5896, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 2065, 2017, 3443, 1037, 7661, 2944, 1010, 2017, 2442, 9375, 1996, 9167, 1035, 27852, 1010, 4225, 2011, 1996, 3853, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9167, 1012, 16014, 5833, 18780, 1010, 2029, 7953, 2003, 1037, 9206, 2008, 9375, 1996, 2171, 2008, 2038, 2000, 2674, 2007, 1996, 2171, 1997, 1996, 16014, 2953, 6434, 1999, 1996, 5604, 5896, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 2731, 5896, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5604, 5896, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1006, 3642, 7718, 1999, 18750, 1017, 1012, 1020, 1012, 1017, 1010, 23435, 12314, 1015, 1012, 1018, 1012, 1014, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 40, 90, 155, 160, 165, 179, 184, 198, 219], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51813781", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [19, 24]}, {"sent_id": 3, "name": "tf.nn", "pos": [80, 85]}, {"sent_id": 8, "name": "tf.nn", "pos": [223, 228]}, {"sent_id": 8, "name": "tf.nn", "pos": [256, 261]}], [{"sent_id": 1, "name": "tf.cast", "pos": [34, 38]}, {"sent_id": 3, "name": "tf.cast", "pos": [105, 109]}], [{"sent_id": 3, "name": "tf.equal", "pos": [61, 65]}], [{"sent_id": 8, "name": "tf.nn.tanh", "pos": [223, 231]}, {"sent_id": 8, "name": "tf.nn.tanh", "pos": [256, 264]}], [{"sent_id": 3, "name": "tf.reshape", "pos": [66, 72]}], [{"sent_id": 8, "name": "tf.name_scope", "pos": [187, 193]}], [{"sent_id": 3, "name": "tf.nn.sigmoid", "pos": [80, 89]}], [{"sent_id": 1, "name": "tf.nn.in_top_k", "pos": [19, 30]}], [{"sent_id": 3, "name": "tf.greater_equal", "pos": [73, 79]}]], "sents": ["I'm not really clear what you're trying to achieve with", "correct = tf.nn.in_top_k(logits, tf.cast(y,tf.int32), 1)", "I'd recommend using", "correct = tf.equal(\n    tf.reshape(\n        tf.greater_equal(tf.nn.sigmoid(logits),0.5),[-1]\n    ), \n    tf.cast(y,tf.bool)\n)", "Edited: I noticed that the accuracy is stuck at 0.5 in the given solution.", "I was able to get this solution to work (accuracy: 100.0) by making the following changes.", "Changed the network to the following.", "(using tanh, using two hidden layers)", "with tf.name_scope('dnn'):\n    hidden1 = neuron_layer(X, n_hidden, name='hidden1', activation=tf.nn.tanh)\n    hidden2 = neuron_layer(hidden1, n_hidden, name='hidden2', activation=tf.nn.tanh)\n    logits = neuron_layer(hidden2, n_outputs, name='outputs')", "and n_hidden = 7 , n_epochs = 5", "Note: I'm not really sure why it needs two hidden layers.", "But apparently it needs to to get it to work in this settings."], "sent_idxs": [101, 1045, 1005, 1049, 2025, 2428, 3154, 2054, 2017, 1005, 2128, 2667, 2000, 6162, 2007, 102, 101, 6149, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 1999, 1035, 2327, 1035, 1047, 1006, 8833, 12762, 1010, 1056, 2546, 1012, 3459, 1006, 1061, 1010, 1056, 2546, 1012, 20014, 16703, 1007, 1010, 1015, 1007, 102, 101, 1045, 1005, 1040, 16755, 2478, 102, 101, 6149, 1027, 1056, 2546, 1012, 5020, 1006, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1056, 2546, 1012, 3618, 1035, 5020, 1006, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1006, 8833, 12762, 1007, 1010, 1014, 1012, 1019, 1007, 1010, 1031, 1011, 1015, 1033, 1007, 1010, 1056, 2546, 1012, 3459, 1006, 1061, 1010, 1056, 2546, 1012, 22017, 2140, 1007, 1007, 102, 101, 5493, 1024, 1045, 4384, 2008, 1996, 10640, 2003, 5881, 2012, 1014, 1012, 1019, 1999, 1996, 2445, 5576, 1012, 102, 101, 1045, 2001, 2583, 2000, 2131, 2023, 5576, 2000, 2147, 1006, 10640, 1024, 2531, 1012, 1014, 1007, 2011, 2437, 1996, 2206, 3431, 1012, 102, 101, 2904, 1996, 2897, 2000, 1996, 2206, 1012, 102, 101, 1006, 2478, 9092, 2232, 1010, 2478, 2048, 5023, 9014, 1007, 102, 101, 2007, 1056, 2546, 1012, 2171, 1035, 9531, 1006, 1005, 1040, 10695, 1005, 1007, 1024, 5023, 2487, 1027, 11265, 21017, 1035, 6741, 1006, 1060, 1010, 1050, 1035, 5023, 1010, 2171, 1027, 1005, 5023, 2487, 1005, 1010, 13791, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 9092, 2232, 1007, 5023, 2475, 1027, 11265, 21017, 1035, 6741, 1006, 5023, 2487, 1010, 1050, 1035, 5023, 1010, 2171, 1027, 1005, 5023, 2475, 1005, 1010, 13791, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 9092, 2232, 1007, 8833, 12762, 1027, 11265, 21017, 1035, 6741, 1006, 5023, 2475, 1010, 1050, 1035, 27852, 1010, 2171, 1027, 1005, 27852, 1005, 1007, 102, 101, 1998, 1050, 1035, 5023, 1027, 1021, 1010, 1050, 1035, 25492, 2015, 1027, 1019, 102, 101, 3602, 1024, 1045, 1005, 1049, 2025, 2428, 2469, 2339, 2009, 3791, 2048, 5023, 9014, 1012, 102, 101, 2021, 4593, 2009, 3791, 2000, 2000, 2131, 2009, 2000, 2147, 1999, 2023, 10906, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 8], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [5, 8], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [6, 8], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 8], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7]], "sent_ends": [0, 16, 51, 58, 120, 140, 164, 173, 185, 287, 302, 319, 335], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 0, 9, 9, 9, 9, 9, 9, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61680259", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [16, 20]}], [{"sent_id": 2, "name": "tf.py_function", "pos": [53, 60]}, {"sent_id": 3, "name": "tf.py_function", "pos": [84, 91]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [16, 23]}]], "sents": ["Here is an example of how to get string part of a tensor in the tf.data.Dataset.map function.", "Below are the steps I have implemented in the code to achieve this.", "You have to decorate the map function with tf.py_function(get_path, [x], [tf.string]).", "You can find more about tf.py_function here..", "You can get your string part by using bytes.decode(file_path.numpy()) in map function..", "Code -", "<code>Code Snippet</code>.", "Output -", "<code>Code Snippet</code>.", "Hope this answers your question."], "sent_idxs": [101, 2182, 2003, 2019, 2742, 1997, 2129, 2000, 2131, 5164, 2112, 1997, 1037, 23435, 1999, 1996, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 4949, 3853, 1012, 102, 101, 2917, 2024, 1996, 4084, 1045, 2031, 7528, 1999, 1996, 3642, 2000, 6162, 2023, 1012, 102, 101, 2017, 2031, 2000, 29460, 1996, 4949, 3853, 2007, 1056, 2546, 1012, 1052, 2100, 1035, 3853, 1006, 2131, 1035, 4130, 1010, 1031, 1060, 1033, 1010, 1031, 1056, 2546, 1012, 5164, 1033, 1007, 1012, 102, 101, 2017, 2064, 2424, 2062, 2055, 1056, 2546, 1012, 1052, 2100, 1035, 3853, 2182, 1012, 1012, 102, 101, 2017, 2064, 2131, 2115, 5164, 2112, 2011, 2478, 27507, 1012, 21933, 3207, 1006, 5371, 1035, 4130, 1012, 16371, 8737, 2100, 1006, 1007, 1007, 1999, 4949, 3853, 1012, 1012, 102, 101, 3642, 1011, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1011, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3246, 2023, 6998, 2115, 3160, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 44, 78, 95, 125, 129, 143, 147, 161, 169], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60121483", "vertexSet": [[{"sent_id": 0, "name": "tf.compat", "pos": [3, 9]}, {"sent_id": 0, "name": "tf.compat", "pos": [39, 45]}], [{"sent_id": 0, "name": "tf.compat.v2", "pos": [39, 48]}], [{"sent_id": 0, "name": "tf.compat.v1", "pos": [3, 12]}], [{"sent_id": 0, "name": "tf.compat.v2.nn", "pos": [39, 51]}], [{"sent_id": 0, "name": "tf.compat.v1.graphkeys", "pos": [3, 16]}], [{"sent_id": 0, "name": "tf.compat.v2.nn.avg_pool2d", "pos": [39, 58]}]], "sents": ["This code tf.compat.v1.GraphKeys.UPDATE_OPS is not available on Tensorflow==1.9.0, this is the same for tf.compat.v2.nn.avg_pool2d.", "To have those features update your version to 1.15 with conda install tensorflow=1.15.", "That will match the tutorial's version.", "As obtained from it's repository it uses tensorflow-gpu==1.15.2."], "sent_idxs": [101, 2023, 3642, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 2003, 2025, 2800, 2006, 23435, 12314, 1027, 1027, 1015, 1012, 1023, 1012, 1014, 1010, 2023, 2003, 1996, 2168, 2005, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2475, 1012, 1050, 2078, 1012, 20704, 2290, 1035, 4770, 2475, 2094, 1012, 102, 101, 2000, 2031, 2216, 2838, 10651, 2115, 2544, 2000, 1015, 1012, 2321, 2007, 9530, 2850, 16500, 23435, 12314, 1027, 1015, 1012, 2321, 1012, 102, 101, 2008, 2097, 2674, 1996, 14924, 4818, 1005, 1055, 2544, 1012, 102, 101, 2004, 4663, 2013, 2009, 1005, 1055, 22409, 2009, 3594, 23435, 12314, 1011, 14246, 2226, 1027, 1027, 1015, 1012, 2321, 1012, 1016, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 60, 84, 96, 120], "sent_pos": [0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43135048", "vertexSet": [[{"sent_id": 1, "name": "tf.add", "pos": [11, 15]}, {"sent_id": 2, "name": "tf.add", "pos": [38, 42]}], [{"sent_id": 2, "name": "tf.nn.bias_add", "pos": [49, 58]}]], "sents": ["From the tensorflow documentation here:", "Unlike tf.add, the type of bias is allowed to differ from value in the case where both types are quantized.", "tf.add is a general addition operation, while tf.nn.bias_add is to be used specifically for adding bias to the weights, which raises an exception if the dtypes aren't same."], "sent_idxs": [101, 2013, 1996, 23435, 12314, 12653, 2182, 1024, 102, 101, 4406, 1056, 2546, 1012, 5587, 1010, 1996, 2828, 1997, 13827, 2003, 3039, 2000, 11234, 2013, 3643, 1999, 1996, 2553, 2073, 2119, 4127, 2024, 24110, 23355, 1012, 102, 101, 1056, 2546, 1012, 5587, 2003, 1037, 2236, 2804, 3169, 1010, 2096, 1056, 2546, 1012, 1050, 2078, 1012, 13827, 1035, 5587, 2003, 2000, 2022, 2109, 4919, 2005, 5815, 13827, 2000, 1996, 15871, 1010, 2029, 13275, 2019, 6453, 2065, 1996, 26718, 18863, 2015, 4995, 1005, 1056, 2168, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2]}], "na_triple": [], "sent_ends": [0, 9, 37, 85], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50827140", "vertexSet": [[{"sent_id": 0, "name": "tf.summary", "pos": [5, 9]}, {"sent_id": 0, "name": "tf.summary", "pos": [20, 24]}, {"sent_id": 2, "name": "tf.summary", "pos": [54, 58]}], [{"sent_id": 2, "name": "tf.placeholder", "pos": [65, 70]}], [{"sent_id": 2, "name": "tf.summary.scalar", "pos": [54, 61]}], [{"sent_id": 0, "name": "tf.summary.filewriter", "pos": [20, 27]}]], "sents": ["You can manually create tf.Summary object that stores the scalar value and pass it to tf.summary.FileWriter like in the following example:", "<code>Code Snippet</code>.", "Alternatively, you can define tf.summary.scalar() operation using tf.placeholder as a tensor and feed the actual value at run time:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 21118, 3443, 1056, 2546, 1012, 12654, 4874, 2008, 5324, 1996, 26743, 2099, 3643, 1998, 3413, 2009, 2000, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 2066, 1999, 1996, 2206, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 14084, 1010, 2017, 2064, 9375, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1006, 1007, 3169, 2478, 1056, 2546, 1012, 2173, 14528, 2004, 1037, 23435, 1998, 5438, 1996, 5025, 3643, 2012, 2448, 2051, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 34, 48, 83, 97], "sent_pos": [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51865645", "vertexSet": [[{"sent_id": 7, "name": "tf.saved_model", "pos": [119, 125]}], [{"sent_id": 9, "name": "tf.get_default_graph", "pos": [167, 175]}], [{"sent_id": 7, "name": "tf.saved_model.loader", "pos": [119, 128]}], [{"sent_id": 7, "name": "tf.saved_model.loader.load", "pos": [119, 130]}]], "sents": ["So after a lot of stumbling I realize that I was just loading the meta graph.", "Not the whole graph with variables.", "Here is code that does so:", "<code>Code Snippet</code>.", "I realized that I was just loading the meta graph.", "I would love if someone could confirm my understanding of what was failing.", "With compat.as_bytes I was just loading it as a meta graph.", "Is there a way of integrating variables after doing that kind of loading or should I stick with tf.saved_model.loader.load() ?", "My attempt of loading was completely wrong as it wasn't even calling the folder of variables.", "Another question:  with [n.name for n in tf.get_default_graph().as_graph_def().node] I am putting all nodes into the output_nodes, should I just put the last node?", "It works with just the last node.", "What is the difference?"], "sent_idxs": [101, 2061, 2044, 1037, 2843, 1997, 19730, 1045, 5382, 2008, 1045, 2001, 2074, 10578, 1996, 18804, 10629, 1012, 102, 101, 2025, 1996, 2878, 10629, 2007, 10857, 1012, 102, 101, 2182, 2003, 3642, 2008, 2515, 2061, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 3651, 2008, 1045, 2001, 2074, 10578, 1996, 18804, 10629, 1012, 102, 101, 1045, 2052, 2293, 2065, 2619, 2071, 12210, 2026, 4824, 1997, 2054, 2001, 7989, 1012, 102, 101, 2007, 4012, 4502, 2102, 1012, 2004, 1035, 27507, 1045, 2001, 2074, 10578, 2009, 2004, 1037, 18804, 10629, 1012, 102, 101, 2003, 2045, 1037, 2126, 1997, 22380, 10857, 2044, 2725, 2008, 2785, 1997, 10578, 2030, 2323, 1045, 6293, 2007, 1056, 2546, 1012, 5552, 1035, 2944, 1012, 7170, 2121, 1012, 7170, 1006, 1007, 1029, 102, 101, 2026, 3535, 1997, 10578, 2001, 3294, 3308, 2004, 2009, 2347, 1005, 1056, 2130, 4214, 1996, 19622, 1997, 10857, 1012, 102, 101, 2178, 3160, 1024, 2007, 1031, 1050, 1012, 2171, 2005, 1050, 1999, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 2004, 1035, 10629, 1035, 13366, 1006, 1007, 1012, 13045, 1033, 1045, 2572, 5128, 2035, 14164, 2046, 1996, 6434, 1035, 14164, 1010, 2323, 1045, 2074, 2404, 1996, 2197, 13045, 1029, 102, 101, 2009, 2573, 2007, 2074, 1996, 2197, 13045, 1012, 102, 101, 2054, 2003, 1996, 4489, 1029, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 19, 28, 37, 51, 64, 80, 100, 134, 155, 208, 218, 225], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55001597", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [18, 24]}, {"sent_id": 2, "name": "tf.contrib", "pos": [60, 66]}], [{"sent_id": 0, "name": "tf.contrib.distributions", "pos": [18, 26]}], [{"sent_id": 0, "name": "tf.contrib.distributions.percentile", "pos": [18, 29]}]], "sents": ["The correct way of doing this would be computing the 90 percentile, for example with tf.contrib.distributions.percentile:", "<code>Code Snippet</code>.", "If you want to be ready for TensorFlow 2.x, where tf.contrib will be removed, you can instead use TensorFlow Probability, which is where the percentile function will be permanently in the future.", "EDIT: If you want to do the filtering per channel, you can modify the code slightly like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 6149, 2126, 1997, 2725, 2023, 2052, 2022, 9798, 1996, 3938, 3867, 9463, 1010, 2005, 2742, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 20611, 1012, 3867, 9463, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 2000, 2022, 3201, 2005, 23435, 12314, 1016, 1012, 1060, 1010, 2073, 1056, 2546, 1012, 9530, 18886, 2497, 2097, 2022, 3718, 1010, 2017, 2064, 2612, 2224, 23435, 12314, 9723, 1010, 2029, 2003, 2073, 1996, 3867, 9463, 3853, 2097, 2022, 8642, 1999, 1996, 2925, 1012, 102, 101, 10086, 1024, 2065, 2017, 2215, 2000, 2079, 1996, 22910, 2566, 3149, 1010, 2017, 2064, 19933, 1996, 3642, 3621, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 45, 93, 116, 130], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55413120", "vertexSet": [[{"sent_id": 1, "name": " tf.keras.layers", "pos": [60, 67]}, {"sent_id": 2, "name": " tf.keras.layers", "pos": [81, 88]}], [{"sent_id": 1, "name": "tf.layers", "pos": [52, 56]}]], "sents": ["First of all, Layers API is deprecated and will be removed from TF 2.0. keras.layers is a direct substitute, because it will be the main high level api for future version.", "As per official docs, tf.layers are wrappers around tf.keras.layers.", "Convolutional layers in Layers API inherit from tf.keras.layers.", "From tensorflow/python/layers/convolutional.py:", "<code>Code Snippet</code>.", "TensorFlow layers cannot be used directly within a Keras model, as it they miss some attributes required by the Keras API.", "However, it is possible to use them  with Keras Lambda layer."], "sent_idxs": [101, 2034, 1997, 2035, 1010, 9014, 17928, 2003, 2139, 28139, 12921, 1998, 2097, 2022, 3718, 2013, 1056, 2546, 1016, 1012, 1014, 1012, 17710, 8180, 1012, 9014, 2003, 1037, 3622, 7681, 1010, 2138, 2009, 2097, 2022, 1996, 2364, 2152, 2504, 17928, 2005, 2925, 2544, 1012, 102, 101, 2004, 2566, 2880, 9986, 2015, 1010, 1056, 2546, 1012, 9014, 2024, 10236, 7347, 2105, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 102, 101, 9530, 6767, 7630, 3508, 2389, 9014, 1999, 9014, 17928, 22490, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 102, 101, 2013, 23435, 12314, 1013, 18750, 1013, 9014, 1013, 9530, 6767, 7630, 3508, 2389, 1012, 1052, 2100, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 23435, 12314, 9014, 3685, 2022, 2109, 3495, 2306, 1037, 17710, 8180, 2944, 1010, 2004, 2009, 2027, 3335, 2070, 12332, 3223, 2011, 1996, 17710, 8180, 17928, 1012, 102, 101, 2174, 1010, 2009, 2003, 2825, 2000, 2224, 2068, 2007, 17710, 8180, 23375, 6741, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 45, 69, 90, 109, 123, 151, 167], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55076101", "vertexSet": [[{"sent_id": 3, "name": "tf.get_collection", "pos": [101, 107]}], [{"sent_id": 3, "name": "tf.add_to_collection", "pos": [86, 94]}], [{"sent_id": 4, "name": "tf.trainable_variables", "pos": [144, 151]}]], "sents": ["Regardless of whether you have a convolutional or a dense layer, and whether you have finished your training or not, you can access the values of your variables via session interface (once you have initialized them).", "Consider following example:", "<code>Code Snippet</code>.", "If you want access to specific variables in you network you may add them to collection via tf.add_to_collection() and later access them via tf.get_collection() OR you can just filter by variable name from the list of all variables (e.g.", "[v if 'conv' in v.name for v in tf.trainable_variables()])"], "sent_idxs": [101, 7539, 1997, 3251, 2017, 2031, 1037, 9530, 6767, 7630, 3508, 2389, 2030, 1037, 9742, 6741, 1010, 1998, 3251, 2017, 2031, 2736, 2115, 2731, 2030, 2025, 1010, 2017, 2064, 3229, 1996, 5300, 1997, 2115, 10857, 3081, 5219, 8278, 1006, 2320, 2017, 2031, 3988, 3550, 2068, 1007, 1012, 102, 101, 5136, 2206, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 3229, 2000, 3563, 10857, 1999, 2017, 2897, 2017, 2089, 5587, 2068, 2000, 3074, 3081, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1006, 1007, 1998, 2101, 3229, 2068, 3081, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1007, 2030, 2017, 2064, 2074, 11307, 2011, 8023, 2171, 2013, 1996, 2862, 1997, 2035, 10857, 1006, 1041, 1012, 1043, 1012, 102, 101, 1031, 1058, 2065, 1005, 9530, 2615, 1005, 1999, 1058, 1012, 2171, 2005, 1058, 1999, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 1033, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 48, 54, 68, 129, 156], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0]}, {"title": "62411311", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [15, 20]}], [{"sent_id": 1, "name": "tf.keras.preprocessing", "pos": [15, 25]}], [{"sent_id": 1, "name": "tf.keras.preprocessing.image_dataset_from_directory", "pos": [15, 34]}]], "sents": ["It has been addressed under this issue.", "The specific function (tf.keras.preprocessing.image_dataset_from_directory) is not available under TensorFlow v2.1.x or v2.2.0 yet.", "It is only available with the tf-nightly builds and is existent in the source code of the master branch.", "Too bad they didn't indicate it anywhere on site.", "Better to use flow_from_directory for now.", "Or switch to tf-nightly and carry on."], "sent_idxs": [101, 2009, 2038, 2042, 8280, 2104, 2023, 3277, 1012, 102, 101, 1996, 3563, 3853, 1006, 1056, 2546, 1012, 17710, 8180, 1012, 17463, 3217, 9623, 7741, 1012, 3746, 1035, 2951, 13462, 1035, 2013, 1035, 14176, 1007, 2003, 2025, 2800, 2104, 23435, 12314, 1058, 2475, 1012, 1015, 1012, 1060, 2030, 1058, 2475, 1012, 1016, 1012, 1014, 2664, 1012, 102, 101, 2009, 2003, 2069, 2800, 2007, 1996, 1056, 2546, 1011, 22390, 16473, 1998, 2003, 25953, 1999, 1996, 3120, 3642, 1997, 1996, 3040, 3589, 1012, 102, 101, 2205, 2919, 2027, 2134, 1005, 1056, 5769, 2009, 5973, 2006, 2609, 1012, 102, 101, 2488, 2000, 2224, 4834, 1035, 2013, 1035, 14176, 2005, 2085, 1012, 102, 101, 2030, 6942, 2000, 1056, 2546, 1011, 22390, 1998, 4287, 2006, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 57, 82, 96, 109, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57054373", "vertexSet": [[{"sent_id": 3, "name": "tf.tensor", "pos": [107, 111]}], [{"sent_id": 1, "name": "tf.session", "pos": [54, 58]}], [{"sent_id": 3, "name": "tf.variable", "pos": [88, 92]}]], "sents": ["I had the same problem, stumbled upon the same unanswered questions and managed to piece together a solution for creating a variable with a dynamic shape at graph creation time.", "Note that the shape has to be defined before, or with the first execution of tf.Session.run(...).", "<code>Code Snippet</code>.", "The trick is to create a tf.Variable with shape=None, validate_shape=False and hand over a tf.Tensor with unknown shape as initializer."], "sent_idxs": [101, 1045, 2018, 1996, 2168, 3291, 1010, 9845, 2588, 1996, 2168, 14477, 3619, 13777, 2098, 3980, 1998, 3266, 2000, 3538, 2362, 1037, 5576, 2005, 4526, 1037, 8023, 2007, 1037, 8790, 4338, 2012, 10629, 4325, 2051, 1012, 102, 101, 3602, 2008, 1996, 4338, 2038, 2000, 2022, 4225, 2077, 1010, 2030, 2007, 1996, 2034, 7781, 1997, 1056, 2546, 1012, 5219, 1012, 2448, 1006, 1012, 1012, 1012, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 7577, 2003, 2000, 3443, 1037, 1056, 2546, 1012, 8023, 2007, 4338, 1027, 3904, 1010, 9398, 3686, 1035, 4338, 1027, 6270, 1998, 2192, 2058, 1037, 1056, 2546, 1012, 23435, 2007, 4242, 4338, 2004, 3988, 17629, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 67, 81, 119], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38920063", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [135, 139]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [62, 68]}, {"sent_id": 2, "name": "tf.contrib", "pos": [103, 109]}], [{"sent_id": 2, "name": "tf.graphkeys", "pos": [86, 92]}], [{"sent_id": 2, "name": "tf.contrib.losses", "pos": [62, 70]}, {"sent_id": 2, "name": "tf.contrib.losses", "pos": [103, 111]}], [{"sent_id": 2, "name": "tf.train.optimizer", "pos": [135, 143]}], [{"sent_id": 2, "name": "tf.add_to_collection", "pos": [77, 85]}], [{"sent_id": 2, "name": "tf.contrib.losses.add_loss", "pos": [62, 74]}], [{"sent_id": 2, "name": "tf.contrib.losses.get_total_loss", "pos": [103, 117]}]], "sents": ["I expect that running two optimizers simultaneously will lead to inconsistent gradient updates on the common variables, and this might be causing your training not to converge.", "Instead, if you add the scalar loss from each sub-network to the \"losses collection\" (e.g.", "via tf.contrib.losses.add_loss() or tf.add_to_collection(tf.GraphKeys.LOSSES, ...), you can use tf.contrib.losses.get_total_loss() to get a single loss value that can be passed to a single standard TensorFlow tf.train.Optimizer subclass.", "TensorFlow will derive the appropriate back-prop computation for your split network.", "The get_total_loss() method simply computes an unweighted sum of the values that have been added to the losses collection.", "I'm not familiar with the literature on how or if you should scale these values, but you can use any arbitrary (differentiable) TensorFlow expression to combine the losses and pass the result to a single optimizer."], "sent_idxs": [101, 1045, 5987, 2008, 2770, 2048, 23569, 27605, 16750, 7453, 2097, 2599, 2000, 20316, 17978, 14409, 2006, 1996, 2691, 10857, 1010, 1998, 2023, 2453, 2022, 4786, 2115, 2731, 2025, 2000, 28314, 1012, 102, 101, 2612, 1010, 2065, 2017, 5587, 1996, 26743, 2099, 3279, 2013, 2169, 4942, 1011, 2897, 2000, 1996, 1000, 6409, 3074, 1000, 1006, 1041, 1012, 1043, 1012, 102, 101, 3081, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 6409, 1012, 5587, 1035, 3279, 1006, 1007, 2030, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1006, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 6409, 1010, 1012, 1012, 1012, 1007, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 6409, 1012, 2131, 1035, 2561, 1035, 3279, 1006, 1007, 2000, 2131, 1037, 2309, 3279, 3643, 2008, 2064, 2022, 2979, 2000, 1037, 2309, 3115, 23435, 12314, 1056, 2546, 1012, 3345, 1012, 23569, 27605, 6290, 4942, 26266, 1012, 102, 101, 23435, 12314, 2097, 18547, 1996, 6413, 2067, 1011, 17678, 22334, 2005, 2115, 3975, 2897, 1012, 102, 101, 1996, 2131, 1035, 2561, 1035, 3279, 1006, 1007, 4118, 3432, 24134, 2015, 2019, 4895, 11179, 2098, 7680, 1997, 1996, 5300, 2008, 2031, 2042, 2794, 2000, 1996, 6409, 3074, 1012, 102, 101, 1045, 1005, 1049, 2025, 5220, 2007, 1996, 3906, 2006, 2129, 2030, 2065, 2017, 2323, 4094, 2122, 5300, 1010, 2021, 2017, 2064, 2224, 2151, 15275, 1006, 2367, 19210, 1007, 23435, 12314, 3670, 2000, 11506, 1996, 6409, 1998, 3413, 1996, 2765, 2000, 1037, 2309, 23569, 27605, 6290, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 33, 60, 147, 164, 195, 243], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "33904534", "vertexSet": [[{"sent_id": 0, "name": "tf.where", "pos": [25, 29]}, {"sent_id": 4, "name": "tf.where", "pos": [128, 132]}], [{"sent_id": 1, "name": "tf.train", "pos": [91, 95]}, {"sent_id": 7, "name": "tf.train", "pos": [238, 242]}], [{"sent_id": 0, "name": "tf.gather", "pos": [45, 49]}, {"sent_id": 4, "name": "tf.gather", "pos": [147, 151]}], [{"sent_id": 1, "name": "tf.train.batch", "pos": [91, 97]}, {"sent_id": 7, "name": "tf.train.batch", "pos": [238, 244]}]], "sents": ["The most straightforward way to do this is to dequeue a batch, run them through the predicate test, use tf.where to produce a dense vector of the ones that match the predicate, and use tf.gather to collect the results, and enqueue that batch.", "If you want that to happen automatically, you can start a queue runner on the second queue - the easiest way to do that is to use tf.train.batch:", "Example:", "<code>Code Snippet</code>.", "The predicate produces a boolean vector;  the tf.where produces a dense vector of the indexes of the true values, and the tf.gather collects items from your original tensor based upon those indexes.", "A lot of things are hardcoded in this example that you'd need to make not-hardcoded in reality, of course, but hopefully it shows the structure of what you're trying to do (create a filtering pipeline).", "In practice, you'd want QueueRunners on there to keep things churning automatically.", "Using tf.train.batch is very useful to handle that automatically -- see Threading and Queues for more detail."], "sent_idxs": [101, 1996, 2087, 19647, 2126, 2000, 2079, 2023, 2003, 2000, 2139, 4226, 5657, 1037, 14108, 1010, 2448, 2068, 2083, 1996, 3653, 16467, 3231, 1010, 2224, 1056, 2546, 1012, 2073, 2000, 3965, 1037, 9742, 9207, 1997, 1996, 3924, 2008, 2674, 1996, 3653, 16467, 1010, 1998, 2224, 1056, 2546, 1012, 8587, 2000, 8145, 1996, 3463, 1010, 1998, 4372, 4226, 5657, 2008, 14108, 1012, 102, 101, 2065, 2017, 2215, 2008, 2000, 4148, 8073, 1010, 2017, 2064, 2707, 1037, 24240, 5479, 2006, 1996, 2117, 24240, 1011, 1996, 25551, 2126, 2000, 2079, 2008, 2003, 2000, 2224, 1056, 2546, 1012, 3345, 1012, 14108, 1024, 102, 101, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3653, 16467, 7137, 1037, 22017, 20898, 9207, 1025, 1996, 1056, 2546, 1012, 2073, 7137, 1037, 9742, 9207, 1997, 1996, 5950, 2229, 1997, 1996, 2995, 5300, 1010, 1998, 1996, 1056, 2546, 1012, 8587, 17427, 5167, 2013, 2115, 2434, 23435, 2241, 2588, 2216, 5950, 2229, 1012, 102, 101, 1037, 2843, 1997, 2477, 2024, 2524, 16044, 2094, 1999, 2023, 2742, 2008, 2017, 1005, 1040, 2342, 2000, 2191, 2025, 1011, 2524, 16044, 2094, 1999, 4507, 1010, 1997, 2607, 1010, 2021, 11504, 2009, 3065, 1996, 3252, 1997, 2054, 2017, 1005, 2128, 2667, 2000, 2079, 1006, 3443, 1037, 22910, 13117, 1007, 1012, 102, 101, 1999, 3218, 1010, 2017, 1005, 1040, 2215, 24240, 23195, 2015, 2006, 2045, 2000, 2562, 2477, 26765, 8073, 1012, 102, 101, 2478, 1056, 2546, 1012, 3345, 1012, 14108, 2003, 2200, 6179, 2000, 5047, 2008, 8073, 1011, 1011, 2156, 11689, 2075, 1998, 24240, 2015, 2005, 2062, 6987, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 62, 99, 103, 117, 164, 216, 236, 264], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36902201", "vertexSet": [[{"sent_id": 0, "name": "tf.scan", "pos": [36, 40]}], [{"sent_id": 0, "name": "tf.foldl", "pos": [17, 22]}], [{"sent_id": 0, "name": "tf.foldr", "pos": [24, 29]}], [{"sent_id": 0, "name": "tf.map_fn", "pos": [7, 14]}, {"sent_id": 2, "name": "tf.map_fn", "pos": [110, 117]}]], "sents": ["This is possible using the new tf.map_fn(), tf.foldl() tf.foldr() or (most generally) tf.scan() higher-order operators, which were added to TensorFlow in version 0.8.", "The particular operator that you would use depends on the computation that you want to perform.", "For example, if you wanted to perform the same function on each row of the tensor and pack the elements back into a single tensor, you would use tf.map_fn():", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2003, 2825, 2478, 1996, 2047, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 1010, 1056, 2546, 1012, 10671, 2140, 1006, 1007, 1056, 2546, 1012, 10671, 2099, 1006, 1007, 2030, 1006, 2087, 3227, 1007, 1056, 2546, 1012, 13594, 1006, 1007, 3020, 1011, 2344, 9224, 1010, 2029, 2020, 2794, 2000, 23435, 12314, 1999, 2544, 1014, 1012, 1022, 1012, 102, 101, 1996, 3327, 6872, 2008, 2017, 2052, 2224, 9041, 2006, 1996, 22334, 2008, 2017, 2215, 2000, 4685, 1012, 102, 101, 2005, 2742, 1010, 2065, 2017, 2359, 2000, 4685, 1996, 2168, 3853, 2006, 2169, 5216, 1997, 1996, 23435, 1998, 5308, 1996, 3787, 2067, 2046, 1037, 2309, 23435, 1010, 2017, 2052, 2224, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 60, 79, 121, 135], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45464078", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [41, 47]}], [{"sent_id": 1, "name": "tf.contrib.framework", "pos": [41, 49]}], [{"sent_id": 1, "name": "tf.trainable_variables", "pos": [31, 38]}], [{"sent_id": 1, "name": "tf.contrib.framework.filter_variables", "pos": [41, 53]}]], "sents": ["You can pass a list of variables to compute_gradients via the keyword argument var_list.", "You may also have a look at tf.trainable_variables() and tf.contrib.framework.filter_variables().", "Here's an example to compute the gradients for two different scopes:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 3413, 1037, 2862, 1997, 10857, 2000, 24134, 1035, 17978, 2015, 3081, 1996, 3145, 18351, 6685, 13075, 1035, 2862, 1012, 102, 101, 2017, 2089, 2036, 2031, 1037, 2298, 2012, 1056, 2546, 1012, 3345, 3085, 1035, 10857, 1006, 1007, 1998, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7705, 1012, 11307, 1035, 10857, 1006, 1007, 1012, 102, 101, 2182, 1005, 1055, 2019, 2742, 2000, 24134, 1996, 17978, 2015, 2005, 2048, 2367, 9531, 2015, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 23, 57, 75, 89], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50112267", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [19, 24]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [52, 58]}, {"sent_id": 2, "name": "tf.contrib", "pos": [67, 73]}], [{"sent_id": 0, "name": "tf.keras.model", "pos": [19, 26]}], [{"sent_id": 1, "name": "tf.contrib.eager", "pos": [52, 60]}, {"sent_id": 2, "name": "tf.contrib.eager", "pos": [67, 75]}], [{"sent_id": 1, "name": "tf.contrib.eager.checkpoint", "pos": [52, 62]}, {"sent_id": 2, "name": "tf.contrib.eager.checkpoint", "pos": [67, 77]}]], "sents": ["Eager execution in TensorFlow encourages encapsulating model state in objects, for example in tf.keras.Model objects.", "The state of these objects (the \"checkpointed\" values of variables) can then be saved and restored using tf.contrib.eager.Checkpoint", "Note that the tf.contrib.eager.Checkpoint class is compatible with both eager and graph execution.", "You'll see this used in examples in the tensorflow repository such as this and this", "Hope that helps."], "sent_idxs": [101, 9461, 7781, 1999, 23435, 12314, 16171, 4372, 17695, 23722, 5844, 2944, 2110, 1999, 5200, 1010, 2005, 2742, 1999, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 5200, 1012, 102, 101, 1996, 2110, 1997, 2122, 5200, 1006, 1996, 1000, 26520, 2098, 1000, 5300, 1997, 10857, 1007, 2064, 2059, 2022, 5552, 1998, 5854, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 26520, 102, 101, 3602, 2008, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 26520, 2465, 2003, 11892, 2007, 2119, 9461, 1998, 10629, 7781, 1012, 102, 101, 2017, 1005, 2222, 2156, 2023, 2109, 1999, 4973, 1999, 1996, 23435, 12314, 22409, 2107, 2004, 2023, 1998, 2023, 102, 101, 3246, 2008, 7126, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 29, 63, 88, 108, 114], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54033691", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [15, 21]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [15, 24]}], [{"sent_id": 0, "name": "tf.contrib.rnn.dropoutwrapper", "pos": [15, 30]}]], "sents": ["Your code just does dropout for input X, and you should use tf.contrib.rnn.DropoutWrapper(link).", "<code>Code Snippet</code>."], "sent_idxs": [101, 2115, 3642, 2074, 2515, 4530, 5833, 2005, 7953, 1060, 1010, 1998, 2017, 2323, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 4530, 5833, 13088, 29098, 2121, 1006, 4957, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 35, 49], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43430032", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [3, 8]}, {"sent_id": 4, "name": "tf.nn", "pos": [129, 134]}, {"sent_id": 4, "name": "tf.nn", "pos": [138, 143]}, {"sent_id": 9, "name": "tf.nn", "pos": [228, 233]}, {"sent_id": 9, "name": "tf.nn", "pos": [250, 255]}], [{"sent_id": 2, "name": "tf.log", "pos": [91, 95]}], [{"sent_id": 4, "name": "tf.nn.sigmoid", "pos": [138, 147]}, {"sent_id": 9, "name": "tf.nn.sigmoid", "pos": [228, 237]}, {"sent_id": 9, "name": "tf.nn.sigmoid", "pos": [250, 259]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [3, 11]}, {"sent_id": 4, "name": "tf.nn.softmax", "pos": [129, 137]}]], "sents": ["The function tf.nn.softmax expects the number of logits (last dimension) to be equal the number of classes (2 in your case {1,0}).", "since the last dimension in your case is 1, softmax will always return 1 (the probability of being in the only available class is always 1 since no other class exists).", "therefore h is a tensor filled with 1's and tf.log(1-h) will return negative infinity.", "Infinity multiplied by zero (1-y_i in some rows) returns NaN.", "You should replace tf.nn.softmax with tf.nn.sigmoid.", "A possible fix is:", "<code>Code Snippet</code>.", "or better, you can use tf.sigmoid_cross_entropy_with_logits\nin that case, it should be done as follows:", "<code>Code Snippet</code>.", "this function is more numerically stable than using tf.nn.sigmoid followed by the cross_entropy function which can return a NaN if tf.nn.sigmoid gets near 0 or 1 due to the imprecision of float32."], "sent_idxs": [101, 1996, 3853, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 24273, 1996, 2193, 1997, 8833, 12762, 1006, 2197, 9812, 1007, 2000, 2022, 5020, 1996, 2193, 1997, 4280, 1006, 1016, 1999, 2115, 2553, 1063, 1015, 1010, 1014, 1065, 1007, 1012, 102, 101, 2144, 1996, 2197, 9812, 1999, 2115, 2553, 2003, 1015, 1010, 3730, 17848, 2097, 2467, 2709, 1015, 1006, 1996, 9723, 1997, 2108, 1999, 1996, 2069, 2800, 2465, 2003, 2467, 1015, 2144, 2053, 2060, 2465, 6526, 1007, 1012, 102, 101, 3568, 1044, 2003, 1037, 23435, 3561, 2007, 1015, 1005, 1055, 1998, 1056, 2546, 1012, 8833, 1006, 1015, 1011, 1044, 1007, 2097, 2709, 4997, 15579, 1012, 102, 101, 15579, 28608, 2011, 5717, 1006, 1015, 1011, 1061, 1035, 1045, 1999, 2070, 10281, 1007, 5651, 16660, 1012, 102, 101, 2017, 2323, 5672, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 2007, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1012, 102, 101, 1037, 2825, 8081, 2003, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 2488, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1999, 2008, 2553, 1010, 2009, 2323, 2022, 2589, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 3853, 2003, 2062, 15973, 2135, 6540, 2084, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 2628, 2011, 1996, 2892, 1035, 23077, 3853, 2029, 2064, 2709, 1037, 16660, 2065, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 4152, 2379, 1014, 2030, 1015, 2349, 2000, 1996, 17727, 2890, 28472, 1997, 14257, 16703, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 41, 79, 106, 125, 149, 156, 170, 204, 218, 275], "sent_pos": [0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56231916", "vertexSet": [[{"sent_id": 3, "name": "tf.losses", "pos": [84, 88]}, {"sent_id": 4, "name": "tf.losses", "pos": [100, 104]}], [{"sent_id": 4, "name": "tf.losses.add_loss", "pos": [100, 108]}], [{"sent_id": 3, "name": "tf.losses.get_total_loss", "pos": [84, 94]}]], "sents": ["One would use this method to register the loss defined by user.", "Namely, if you have created a tensor that defines your loss, for example as my_loss = tf.mean(output) you can use this method to add it to loss collection.", "You might want to do that if you are not tracking all your losses manually.", "For example if you are using a method like tf.losses.get_total_loss().", "Inside tf.losses.add_loss is very much straightforward:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2028, 2052, 2224, 2023, 4118, 2000, 4236, 1996, 3279, 4225, 2011, 5310, 1012, 102, 101, 8419, 1010, 2065, 2017, 2031, 2580, 1037, 23435, 2008, 11859, 2115, 3279, 1010, 2005, 2742, 2004, 2026, 1035, 3279, 1027, 1056, 2546, 1012, 2812, 1006, 6434, 1007, 2017, 2064, 2224, 2023, 4118, 2000, 5587, 2009, 2000, 3279, 3074, 1012, 102, 101, 2017, 2453, 2215, 2000, 2079, 2008, 2065, 2017, 2024, 2025, 9651, 2035, 2115, 6409, 21118, 1012, 102, 101, 2005, 2742, 2065, 2017, 2024, 2478, 1037, 4118, 2066, 1056, 2546, 1012, 6409, 1012, 2131, 1035, 2561, 1035, 3279, 1006, 1007, 1012, 102, 101, 2503, 1056, 2546, 1012, 6409, 1012, 5587, 1035, 3279, 2003, 2200, 2172, 19647, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 56, 74, 98, 114, 128], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40348155", "vertexSet": [[{"sent_id": 6, "name": "tf.shape", "pos": [168, 172]}], [{"sent_id": 3, "name": "tf.train", "pos": [75, 79]}], [{"sent_id": 3, "name": "tf.train.shuffle_batch", "pos": [75, 83]}], [{"sent_id": 0, "name": "tf.placeholder_with_default", "pos": [8, 17]}]], "sents": ["One potential solution is to use a tf.placeholder_with_default() op to relax the shape requirement on the input op.", "For example:", "<code>Code Snippet</code>.", "If you run code that depends on input_placeholder but don't feed it, it will use the result of tf.train.shuffle_batch().", "Alternatively, if you feed a value for input_placeholder you can feed any 4-D tensor (with depth 3), so you can use any batch size or image size.", "Note, however, that doing this will disable some optimizations in training, because the shape of each batch can now vary, at least in principle.", "This prevents TensorFlow from considering some internal tf.shape() calls as constant values, which might mean that it needs to do more work on each training step.", "In the end, it may be better to build two separate graphs for training and inference, as these can be optimized separately."], "sent_idxs": [101, 2028, 4022, 5576, 2003, 2000, 2224, 1037, 1056, 2546, 1012, 2173, 14528, 1035, 2007, 1035, 12398, 1006, 1007, 6728, 2000, 9483, 1996, 4338, 9095, 2006, 1996, 7953, 6728, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2448, 3642, 2008, 9041, 2006, 7953, 1035, 2173, 14528, 2021, 2123, 1005, 1056, 5438, 2009, 1010, 2009, 2097, 2224, 1996, 2765, 1997, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 1012, 102, 101, 14084, 1010, 2065, 2017, 5438, 1037, 3643, 2005, 7953, 1035, 2173, 14528, 2017, 2064, 5438, 2151, 1018, 1011, 1040, 23435, 1006, 2007, 5995, 1017, 1007, 1010, 2061, 2017, 2064, 2224, 2151, 14108, 2946, 2030, 3746, 2946, 1012, 102, 101, 3602, 1010, 2174, 1010, 2008, 2725, 2023, 2097, 4487, 19150, 2070, 20600, 2015, 1999, 2731, 1010, 2138, 1996, 4338, 1997, 2169, 14108, 2064, 2085, 8137, 1010, 2012, 2560, 1999, 6958, 1012, 102, 101, 2023, 16263, 23435, 12314, 2013, 6195, 2070, 4722, 1056, 2546, 1012, 4338, 1006, 1007, 4455, 2004, 5377, 5300, 1010, 2029, 2453, 2812, 2008, 2009, 3791, 2000, 2079, 2062, 2147, 2006, 2169, 2731, 3357, 1012, 102, 101, 1999, 1996, 2203, 1010, 2009, 2089, 2022, 2488, 2000, 3857, 2048, 3584, 19287, 2005, 2731, 1998, 28937, 1010, 2004, 2122, 2064, 2022, 23569, 27605, 5422, 10329, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 31, 36, 50, 87, 126, 159, 195, 224], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56408894", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [7, 11]}, {"sent_id": 1, "name": "tf.data", "pos": [22, 26]}], [{"sent_id": 1, "name": "tf.data.dataset", "pos": [22, 29]}], [{"sent_id": 0, "name": "tf.data.experimental", "pos": [7, 13]}], [{"sent_id": 0, "name": "tf.data.experimental.parallel_interleave", "pos": [7, 19]}]], "sents": ["As per official docs on tf.data.experimental.parallel_interleave", "Unlike tf.data.Dataset.interleave, it gets elements from cycle_length\nnested datasets in parallel", "and", "cycle_length: The number of input Datasets to interleave from in\nparallel.", "So basically, a reasonable argument would be number of dataset elements, which would be processed in parallel.", "In this way, it has no relation to CPU cores/threads"], "sent_idxs": [101, 2004, 2566, 2880, 9986, 2015, 2006, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 5903, 1035, 6970, 19738, 3726, 102, 101, 4406, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 6970, 19738, 3726, 1010, 2009, 4152, 3787, 2013, 5402, 1035, 3091, 9089, 2098, 2951, 13462, 2015, 1999, 5903, 102, 101, 1998, 102, 101, 5402, 1035, 3091, 1024, 1996, 2193, 1997, 7953, 2951, 13462, 2015, 2000, 6970, 19738, 3726, 2013, 1999, 5903, 1012, 102, 101, 2061, 10468, 1010, 1037, 9608, 6685, 2052, 2022, 2193, 1997, 2951, 13462, 3787, 1010, 2029, 2052, 2022, 13995, 1999, 5903, 1012, 102, 101, 1999, 2023, 2126, 1010, 2009, 2038, 2053, 7189, 2000, 17368, 25562, 1013, 16457, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 20, 49, 52, 73, 96, 111], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55495991", "vertexSet": [[{"sent_id": 3, "name": "tf.assign", "pos": [130, 134]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [43, 49]}, {"sent_id": 3, "name": "tf.contrib", "pos": [112, 118]}], [{"sent_id": 2, "name": "tf.contrib.framework", "pos": [43, 51]}, {"sent_id": 3, "name": "tf.contrib.framework", "pos": [112, 120]}], [{"sent_id": 2, "name": "tf.contrib.framework.list_variables", "pos": [43, 55]}], [{"sent_id": 3, "name": "tf.contrib.framework.load_checkpoint", "pos": [112, 124]}]], "sents": ["You need to make sure that the variables have the same in the variable file and in the graph where you load the variables.", "You can write a script that will convert the variables names.", "With tf.contrib.framework.list_variables(ckpt), you can find out what variables of what shapes you have in the checkpoint and create respective variables with the new names (I believe, you can write a regex that will fix the names) and correct shape..", "Then you load the original variables with tf.contrib.framework.load_checkpoint(ckpt) assign ops tf.assign(var, loaded) that will assigning the variables with new names with the saved values..", "Runn the assign ops in a session..", "Save the new variables..", "Minimum example:", "Original model (variables in scope \"regression\"):", "<code>Code Snippet</code>.", "Renaming script:", "<code>Code Snippet</code>.", "Model where you load the renamed variables (the same variables in score \"foo/bar\"):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2342, 2000, 2191, 2469, 2008, 1996, 10857, 2031, 1996, 2168, 1999, 1996, 8023, 5371, 1998, 1999, 1996, 10629, 2073, 2017, 7170, 1996, 10857, 1012, 102, 101, 2017, 2064, 4339, 1037, 5896, 2008, 2097, 10463, 1996, 10857, 3415, 1012, 102, 101, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7705, 1012, 2862, 1035, 10857, 1006, 23616, 13876, 1007, 1010, 2017, 2064, 2424, 2041, 2054, 10857, 1997, 2054, 10466, 2017, 2031, 1999, 1996, 26520, 1998, 3443, 7972, 10857, 2007, 1996, 2047, 3415, 1006, 1045, 2903, 1010, 2017, 2064, 4339, 1037, 19723, 10288, 2008, 2097, 8081, 1996, 3415, 1007, 1998, 6149, 4338, 1012, 1012, 102, 101, 2059, 2017, 7170, 1996, 2434, 10857, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7705, 1012, 7170, 1035, 26520, 1006, 23616, 13876, 1007, 23911, 23092, 1056, 2546, 1012, 23911, 1006, 13075, 1010, 8209, 1007, 2008, 2097, 23911, 2075, 1996, 10857, 2007, 2047, 3415, 2007, 1996, 5552, 5300, 1012, 1012, 102, 101, 2448, 2078, 1996, 23911, 23092, 1999, 1037, 5219, 1012, 1012, 102, 101, 3828, 1996, 2047, 10857, 1012, 1012, 102, 101, 6263, 2742, 1024, 102, 101, 2434, 2944, 1006, 10857, 1999, 9531, 1000, 26237, 1000, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 24944, 5896, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2944, 2073, 2017, 7170, 1996, 4096, 10857, 1006, 1996, 2168, 10857, 1999, 3556, 1000, 29379, 1013, 3347, 1000, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 27, 41, 104, 155, 167, 175, 180, 193, 207, 212, 226, 248, 262], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49530094", "vertexSet": [[{"sent_id": 6, "name": "tf.contrib", "pos": [67, 73]}, {"sent_id": 6, "name": "tf.contrib", "pos": [89, 95]}], [{"sent_id": 5, "name": "tf.string_split", "pos": [51, 57]}], [{"sent_id": 6, "name": "tf.contrib.lookup", "pos": [67, 76]}, {"sent_id": 6, "name": "tf.contrib.lookup", "pos": [89, 98]}], [{"sent_id": 6, "name": "tf.contrib.lookup.string_to_index_table_from_file", "pos": [67, 88]}], [{"sent_id": 6, "name": "tf.contrib.lookup.string_to_index_table_from_tensor", "pos": [89, 110]}]], "sents": ["How about this?", "(I didn\u2019t implement this.", "but maybe this idea will work.)", "This method is based on BOW representation.", "Get your data as tf.string.", "Split it using tf.string_split.", "Find indexes of your words using tf.contrib.lookup.string_to_index_table_from_file or tf.contrib.lookup.string_to_index_table_from_tensor.", "Length of this tensor can vary..", "Find embeddings of your indexes.", ".", "<code>Code Snippet</code>.", "Sum up the embeddings.", "And you will get a tensor of fixed length(=embedding size).", "Maybe you can choose another method then sum.", "(avg, mean or something else).", "Maybe it\u2019s too late :) Good luck."], "sent_idxs": [101, 2129, 2055, 2023, 1029, 102, 101, 1006, 1045, 2134, 1521, 1056, 10408, 2023, 1012, 102, 101, 2021, 2672, 2023, 2801, 2097, 2147, 1012, 1007, 102, 101, 2023, 4118, 2003, 2241, 2006, 6812, 6630, 1012, 102, 101, 2131, 2115, 2951, 2004, 1056, 2546, 1012, 5164, 1012, 102, 101, 3975, 2009, 2478, 1056, 2546, 1012, 5164, 1035, 3975, 1012, 102, 101, 2424, 5950, 2229, 1997, 2115, 2616, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2298, 6279, 1012, 5164, 1035, 2000, 1035, 5950, 1035, 2795, 1035, 2013, 1035, 5371, 2030, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2298, 6279, 1012, 5164, 1035, 2000, 1035, 5950, 1035, 2795, 1035, 2013, 1035, 23435, 1012, 102, 101, 3091, 1997, 2023, 23435, 2064, 8137, 1012, 1012, 102, 101, 2424, 7861, 8270, 4667, 2015, 1997, 2115, 5950, 2229, 1012, 102, 101, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 7680, 2039, 1996, 7861, 8270, 4667, 2015, 1012, 102, 101, 1998, 2017, 2097, 2131, 1037, 23435, 1997, 4964, 3091, 1006, 1027, 7861, 8270, 4667, 2946, 1007, 1012, 102, 101, 2672, 2017, 2064, 5454, 2178, 4118, 2059, 7680, 1012, 102, 101, 1006, 20704, 2290, 1010, 2812, 2030, 2242, 2842, 1007, 1012, 102, 101, 2672, 2009, 1521, 1055, 2205, 2397, 1024, 1007, 2204, 6735, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 6, 16, 26, 36, 47, 59, 112, 122, 134, 137, 151, 161, 180, 191, 203, 216], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53078710", "vertexSet": [[{"sent_id": 10, "name": "tf.nn", "pos": [319, 324]}], [{"sent_id": 0, "name": "tf.varlenfeature", "pos": [10, 18]}], [{"sent_id": 6, "name": "tf.sparse_tensor_to_dense", "pos": [160, 170]}], [{"sent_id": 10, "name": "tf.nn.embedding_lookup_sparse", "pos": [319, 333]}]], "sents": ["I think you're looking for something like tf.VarLenFeature(), more specifically, you do not necessarily have to pad your rows prior to creating the tfrecord file.", "You can create the tf_example,", "<code>Code Snippet</code>.", "Do this for all of your rows, that can vary in length.", "You'll parse the tf_examples with something like,", "<code>Code Snippet</code>.", "Now, this will return your features as tf.SparseTensors, if you don't want to deal with that at this stage, and carry on using tensor ops as you would normally, you can simply use tf.sparse_tensor_to_dense() and carry on as you normally would with tensors.", "The returned dense tensors will be of varying lengths, so you shouldn't have to worry about selecting '-1's, there won't be any.", "Unless you convert the sparse tensors to dense in batches, in that case the batches will be padded to the length of the longest tensor in the batch, and the padding value can be set by the default_value parameter.", "That is in so far as your question about using varying length rows in tfrecords and getting back varying length tensors.", "With regards to the lookup op, I haven't used it myself, but I think tf.nn.embedding_lookup_sparse() might help you out here, it offers the ability to lookup the embeddings from the sparse tensor, forgoing the need to convert it to a dense tensor first, and also has a combiner parameter to specify a reduction op on those embeddings, which in your case would be 'mean'.", "I hope this helps in some way, good luck."], "sent_idxs": [101, 1045, 2228, 2017, 1005, 2128, 2559, 2005, 2242, 2066, 1056, 2546, 1012, 13075, 7770, 7959, 4017, 5397, 1006, 1007, 1010, 2062, 4919, 1010, 2017, 2079, 2025, 9352, 2031, 2000, 11687, 2115, 10281, 3188, 2000, 4526, 1996, 1056, 19699, 8586, 8551, 5371, 1012, 102, 101, 2017, 2064, 3443, 1996, 1056, 2546, 1035, 2742, 1010, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2079, 2023, 2005, 2035, 1997, 2115, 10281, 1010, 2008, 2064, 8137, 1999, 3091, 1012, 102, 101, 2017, 1005, 2222, 11968, 3366, 1996, 1056, 2546, 1035, 4973, 2007, 2242, 2066, 1010, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 1010, 2023, 2097, 2709, 2115, 2838, 2004, 1056, 2546, 1012, 20288, 25808, 5668, 1010, 2065, 2017, 2123, 1005, 1056, 2215, 2000, 3066, 2007, 2008, 2012, 2023, 2754, 1010, 1998, 4287, 2006, 2478, 23435, 23092, 2004, 2017, 2052, 5373, 1010, 2017, 2064, 3432, 2224, 1056, 2546, 1012, 20288, 1035, 23435, 1035, 2000, 1035, 9742, 1006, 1007, 1998, 4287, 2006, 2004, 2017, 5373, 2052, 2007, 23435, 2015, 1012, 102, 101, 1996, 2513, 9742, 23435, 2015, 2097, 2022, 1997, 9671, 10742, 1010, 2061, 2017, 5807, 1005, 1056, 2031, 2000, 4737, 2055, 17739, 1005, 1011, 1015, 1005, 1055, 1010, 2045, 2180, 1005, 1056, 2022, 2151, 1012, 102, 101, 4983, 2017, 10463, 1996, 20288, 23435, 2015, 2000, 9742, 1999, 14108, 2229, 1010, 1999, 2008, 2553, 1996, 14108, 2229, 2097, 2022, 20633, 2000, 1996, 3091, 1997, 1996, 6493, 23435, 1999, 1996, 14108, 1010, 1998, 1996, 11687, 4667, 3643, 2064, 2022, 2275, 2011, 1996, 12398, 1035, 3643, 16381, 1012, 102, 101, 2008, 2003, 1999, 2061, 2521, 2004, 2115, 3160, 2055, 2478, 9671, 3091, 10281, 1999, 1056, 19699, 8586, 8551, 2015, 1998, 2893, 2067, 9671, 3091, 23435, 2015, 1012, 102, 101, 2007, 12362, 2000, 1996, 2298, 6279, 6728, 1010, 1045, 4033, 1005, 1056, 2109, 2009, 2870, 1010, 2021, 1045, 2228, 1056, 2546, 1012, 1050, 2078, 1012, 7861, 8270, 4667, 1035, 2298, 6279, 1035, 20288, 1006, 1007, 2453, 2393, 2017, 2041, 2182, 1010, 2009, 4107, 1996, 3754, 2000, 2298, 6279, 1996, 7861, 8270, 4667, 2015, 2013, 1996, 20288, 23435, 1010, 2005, 26966, 1996, 2342, 2000, 10463, 2009, 2000, 1037, 9742, 23435, 2034, 1010, 1998, 2036, 2038, 1037, 11506, 2099, 16381, 2000, 20648, 1037, 7312, 6728, 2006, 2216, 7861, 8270, 4667, 2015, 1010, 2029, 1999, 2115, 2553, 2052, 2022, 1005, 2812, 1005, 1012, 102, 101, 1045, 3246, 2023, 7126, 1999, 2070, 2126, 1010, 2204, 6735, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 44, 55, 69, 85, 101, 115, 184, 220, 270, 299, 401, 414], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53919276", "vertexSet": [[{"sent_id": 7, "name": "tf.train", "pos": [138, 142]}], [{"sent_id": 4, "name": "tf.tensor", "pos": [87, 91]}], [{"sent_id": 3, "name": "tf.session", "pos": [56, 60]}], [{"sent_id": 4, "name": "tf.operation", "pos": [82, 86]}], [{"sent_id": 7, "name": "tf.train.saver", "pos": [138, 145]}], [{"sent_id": 3, "name": "tf.interactivesession", "pos": [61, 67]}], [{"sent_id": 0, "name": "tf.reset_default_graph", "pos": [1, 9]}]], "sents": ["tf.reset_default_graph will clear the default graph stack and resets the global default graph.", "NOTE: The default graph is a property of the current thread.", "This\n  function applies only to the current thread.", "Calling this function\n  while a tf.Session or tf.InteractiveSession is active will result in\n  undefined behavior.", "Using any previously created tf.Operation or\n  tf.Tensor objects after calling this function will result in undefined\n  behavior.", "You should specify Graph separately, and define all of these in the corresponding graph scope.", "<code>Code Snippet</code>.", "tf.train.Saver is another way to save model variables.", "Edit\nIf you get empty \"variable\", you should save model in graph:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 2097, 3154, 1996, 12398, 10629, 9991, 1998, 25141, 2015, 1996, 3795, 12398, 10629, 1012, 102, 101, 3602, 1024, 1996, 12398, 10629, 2003, 1037, 3200, 1997, 1996, 2783, 11689, 1012, 102, 101, 2023, 3853, 12033, 2069, 2000, 1996, 2783, 11689, 1012, 102, 101, 4214, 2023, 3853, 2096, 1037, 1056, 2546, 1012, 5219, 2030, 1056, 2546, 1012, 9123, 8583, 10992, 2003, 3161, 2097, 2765, 1999, 6151, 28344, 5248, 1012, 102, 101, 2478, 2151, 3130, 2580, 1056, 2546, 1012, 3169, 2030, 1056, 2546, 1012, 23435, 5200, 2044, 4214, 2023, 3853, 2097, 2765, 1999, 6151, 28344, 5248, 1012, 102, 101, 2017, 2323, 20648, 10629, 10329, 1010, 1998, 9375, 2035, 1997, 2122, 1999, 1996, 7978, 10629, 9531, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2003, 2178, 2126, 2000, 3828, 2944, 10857, 1012, 102, 101, 10086, 2065, 2017, 2131, 4064, 1000, 8023, 1000, 1010, 2017, 2323, 3828, 2944, 1999, 10629, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 24, 39, 50, 77, 104, 123, 137, 154, 172, 186], "sent_pos": [0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45500469", "vertexSet": [[{"sent_id": 3, "name": "tf.matmul", "pos": [107, 113]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [26, 32]}], [{"sent_id": 2, "name": "tf.contrib.learn", "pos": [26, 34]}], [{"sent_id": 2, "name": "tf.contrib.learn.linearclassifier", "pos": [26, 38]}]], "sents": ["Yes, of course.", "In fact you can find a lot of examples:", "There are some ready implementations like tf.contrib.learn.LinearClassifier in https://www.tensorflow.org/tutorials/wide", "Or something like this: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py where you use tf.matmul and appropriate activations.", "There is even something with gradient boosting: https://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html"], "sent_idxs": [101, 2748, 1010, 1997, 2607, 1012, 102, 101, 1999, 2755, 2017, 2064, 2424, 1037, 2843, 1997, 4973, 1024, 102, 101, 2045, 2024, 2070, 3201, 24977, 2066, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7399, 26266, 18095, 1999, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 14924, 26340, 1013, 2898, 102, 101, 2030, 2242, 2066, 2023, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 1037, 25219, 7277, 17130, 9013, 1013, 23435, 12314, 1011, 4973, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 4973, 1013, 1016, 1035, 3937, 5302, 9247, 2015, 1013, 8833, 6553, 1035, 26237, 1012, 1052, 2100, 2073, 2017, 2224, 1056, 2546, 1012, 13523, 12274, 2140, 1998, 6413, 13791, 2015, 1012, 102, 101, 2045, 2003, 2130, 2242, 2007, 17978, 12992, 2075, 1024, 16770, 1024, 1013, 1013, 12098, 22844, 27922, 22576, 1012, 21025, 2705, 12083, 1012, 22834, 1013, 2355, 1013, 5718, 1013, 5709, 1013, 17978, 1035, 12992, 2075, 1035, 14705, 1012, 16129, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 7, 19, 55, 119, 159], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45744465", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [24, 29]}], [{"sent_id": 3, "name": "tf.layers", "pos": [81, 85]}], [{"sent_id": 1, "name": "tf.nn.weighted_cross_entropy_with_logits", "pos": [24, 40]}]], "sents": ["There are couple of problems with the code:", "You are applying softmax on the last layer and then calling tf.nn.weighted_cross_entropy_with_logits which in turn applies sigmoid activation, so you are applying activation twice..", "For initialisation of the weights, use Xavier or Variance_scaling for faster convergence.", "Better to use tf.layers API in implementing your model as its default settings follows best practices.", "."], "sent_idxs": [101, 2045, 2024, 3232, 1997, 3471, 2007, 1996, 3642, 1024, 102, 101, 2017, 2024, 11243, 3730, 17848, 2006, 1996, 2197, 6741, 1998, 2059, 4214, 1056, 2546, 1012, 1050, 2078, 1012, 18215, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2029, 1999, 2735, 12033, 9033, 21693, 9314, 13791, 1010, 2061, 2017, 2024, 11243, 13791, 3807, 1012, 1012, 102, 101, 2005, 3988, 6648, 1997, 1996, 15871, 1010, 2224, 10062, 2030, 23284, 1035, 25169, 2005, 5514, 19143, 1012, 102, 101, 2488, 2000, 2224, 1056, 2546, 1012, 9014, 17928, 1999, 14972, 2115, 2944, 2004, 2049, 12398, 10906, 4076, 2190, 6078, 1012, 102, 101, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 11, 58, 77, 99, 102], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35367161", "vertexSet": [[{"sent_id": 0, "name": "tf.tile", "pos": [16, 20]}, {"sent_id": 2, "name": "tf.tile", "pos": [55, 59]}], [{"sent_id": 4, "name": "tf.gather", "pos": [87, 91]}], [{"sent_id": 0, "name": "tf.reshape", "pos": [23, 29]}]], "sents": ["You can achieve the effect of np.repeat() using a combination of tf.tile() and tf.reshape():", "<code>Code Snippet</code>.", "You can simply compute jdx using tf.tile():", "<code>Code Snippet</code>.", "For the indexing, you could try using tf.gather() to extract non-contiguous slices from the yp tensor:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 6162, 1996, 3466, 1997, 27937, 1012, 9377, 1006, 1007, 2478, 1037, 5257, 1997, 1056, 2546, 1012, 14090, 1006, 1007, 1998, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 3432, 24134, 26219, 2595, 2478, 1056, 2546, 1012, 14090, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 1996, 5950, 2075, 1010, 2017, 2071, 3046, 2478, 1056, 2546, 1012, 8587, 1006, 1007, 2000, 14817, 2512, 1011, 25177, 25609, 2013, 1996, 1061, 2361, 23435, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 47, 63, 77, 106, 120], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52514166", "vertexSet": [[{"sent_id": 2, "name": "tf.image", "pos": [27, 31]}, {"sent_id": 6, "name": "tf.image", "pos": [99, 103]}], [{"sent_id": 6, "name": "tf.image.decode_jpeg", "pos": [99, 109]}], [{"sent_id": 2, "name": "tf.image.rgb_to_grayscale", "pos": [27, 40]}]], "sents": ["The problem seems to be in the following line:", "<code>Code Snippet</code>.", "tf.image.rgb_to_grayscale expects the given image tensor to have a last dimension with size 3, representing the RGB channels.", "However, the line:", "<code>Code Snippet</code>.", "Can produce tensors with a different number of channels.", "This is because tf.image.decode_jpeg will, by default, make a tensor with the same number of channels than those in the JPEG data.", "So if you have an image that is already grayscale then the tensor will have only one channel and the program will fail.", "You can solve the problem by requesting to decode the JPEG data as an RGB image in all cases, setting the channels parameter to 3:", "<code>Code Snippet</code>.", "This will ensure that all your images are treated uniformly."], "sent_idxs": [101, 1996, 3291, 3849, 2000, 2022, 1999, 1996, 2206, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 3746, 1012, 1054, 18259, 1035, 2000, 1035, 3897, 15782, 2571, 24273, 1996, 2445, 3746, 23435, 2000, 2031, 1037, 2197, 9812, 2007, 2946, 1017, 1010, 5052, 1996, 1054, 18259, 6833, 1012, 102, 101, 2174, 1010, 1996, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2064, 3965, 23435, 2015, 2007, 1037, 2367, 2193, 1997, 6833, 1012, 102, 101, 2023, 2003, 2138, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 2097, 1010, 2011, 12398, 1010, 2191, 1037, 23435, 2007, 1996, 2168, 2193, 1997, 6833, 2084, 2216, 1999, 1996, 16545, 13910, 2951, 1012, 102, 101, 2061, 2065, 2017, 2031, 2019, 3746, 2008, 2003, 2525, 3897, 15782, 2571, 2059, 1996, 23435, 2097, 2031, 2069, 2028, 3149, 1998, 1996, 2565, 2097, 8246, 1012, 102, 101, 2017, 2064, 9611, 1996, 3291, 2011, 17942, 2000, 21933, 3207, 1996, 16545, 13910, 2951, 2004, 2019, 1054, 18259, 3746, 1999, 2035, 3572, 1010, 4292, 1996, 6833, 16381, 2000, 1017, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2097, 5676, 2008, 2035, 2115, 4871, 2024, 5845, 27423, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 26, 61, 68, 82, 95, 132, 160, 192, 206, 219], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56477844", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [11, 16]}], [{"sent_id": 2, "name": "tf.metrics", "pos": [57, 62]}], [{"sent_id": 2, "name": "tf.metrics.average_precision_at_k", "pos": [57, 70]}]], "sents": ["If you are using the keras API, through tf.keras, you can add evaluation functions as metrics in the model.fit function.", "Checkout the official documentation for a list of all available metrics.", "You might be interested be interested in tf.metrics.average_precision_at_k.", "If it doesn't do exactly what you need, you can also implement a custom metric."], "sent_idxs": [101, 2065, 2017, 2024, 2478, 1996, 17710, 8180, 17928, 1010, 2083, 1056, 2546, 1012, 17710, 8180, 1010, 2017, 2064, 5587, 9312, 4972, 2004, 12046, 2015, 1999, 1996, 2944, 1012, 4906, 3853, 1012, 102, 101, 4638, 5833, 1996, 2880, 12653, 2005, 1037, 2862, 1997, 2035, 2800, 12046, 2015, 1012, 102, 101, 2017, 2453, 2022, 4699, 2022, 4699, 1999, 1056, 2546, 1012, 12046, 2015, 1012, 2779, 1035, 11718, 1035, 2012, 1035, 1047, 1012, 102, 101, 2065, 2009, 2987, 1005, 1056, 2079, 3599, 2054, 2017, 2342, 1010, 2017, 2064, 2036, 10408, 1037, 7661, 12046, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 49, 72, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46432399", "vertexSet": [[{"sent_id": 7, "name": "tf.data", "pos": [205, 209]}], [{"sent_id": 7, "name": "tf.contrib", "pos": [193, 199]}], [{"sent_id": 7, "name": "tf.contrib.data", "pos": [193, 201]}]], "sents": ["Can you provide a minimal example?", "e.g., do you continue to have the memory leak if you just call the example parsing over and over again via multiple session.run calls, and not have any queues?", "The reason I ask is that the _store_sparse_tensors is hidden to that file for a reason; if you misuse it, you will hit a memory leak.", "Thus all callers of this function must be very careful to use it correctly.", "For every sparse tensor stored via _store_sparse_tensors, that same tensor must be restored via _restore_sparse_tensors.", "If it is not, you will leak memory.", "I'm considering a DT_VARIANT storage format to replace this wrapper, but for now I'd recommend against using these functions yourself.", "Instead, you can probably do what you want using the new tf.contrib.data (soon to be tf.data) libraries!"], "sent_idxs": [101, 2064, 2017, 3073, 1037, 10124, 2742, 1029, 102, 101, 1041, 1012, 1043, 1012, 1010, 2079, 2017, 3613, 2000, 2031, 1996, 3638, 17271, 2065, 2017, 2074, 2655, 1996, 2742, 11968, 7741, 2058, 1998, 2058, 2153, 3081, 3674, 5219, 1012, 2448, 4455, 1010, 1998, 2025, 2031, 2151, 24240, 2015, 1029, 102, 101, 1996, 3114, 1045, 3198, 2003, 2008, 1996, 1035, 3573, 1035, 20288, 1035, 23435, 2015, 2003, 5023, 2000, 2008, 5371, 2005, 1037, 3114, 1025, 2065, 2017, 28616, 8557, 2009, 1010, 2017, 2097, 2718, 1037, 3638, 17271, 1012, 102, 101, 2947, 2035, 20587, 2015, 1997, 2023, 3853, 2442, 2022, 2200, 6176, 2000, 2224, 2009, 11178, 1012, 102, 101, 2005, 2296, 20288, 23435, 8250, 3081, 1035, 3573, 1035, 20288, 1035, 23435, 2015, 1010, 2008, 2168, 23435, 2442, 2022, 5854, 3081, 1035, 9239, 1035, 20288, 1035, 23435, 2015, 1012, 102, 101, 2065, 2009, 2003, 2025, 1010, 2017, 2097, 17271, 3638, 1012, 102, 101, 1045, 1005, 1049, 6195, 1037, 26718, 1035, 8349, 5527, 4289, 2000, 5672, 2023, 10236, 4842, 1010, 2021, 2005, 2085, 1045, 1005, 1040, 16755, 2114, 2478, 2122, 4972, 4426, 1012, 102, 101, 2612, 1010, 2017, 2064, 2763, 2079, 2054, 2017, 2215, 2478, 1996, 2047, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1006, 2574, 2000, 2022, 1056, 2546, 1012, 2951, 1007, 8860, 999, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 50, 88, 106, 137, 149, 180, 213], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]}, {"title": "33763208", "vertexSet": [[{"sent_id": 4, "name": "tf.graphkeys", "pos": [206, 212]}], [{"sent_id": 1, "name": "tf.import_graph_def", "pos": [57, 65]}], [{"sent_id": 4, "name": "tf.add_to_collection", "pos": [197, 205]}]], "sents": ["There are two parts to the model, the model definition, saved by Supervisor as graph.pbtxt in the model directory and the numerical values of tensors, saved into checkpoint files like model.ckpt-1003418.", "The model definition can be restored using tf.import_graph_def, and the weights are restored using Saver.", "However, Saver uses special collection holding list of variables that's attached to the model Graph, and this collection is not initialized using import_graph_def, so you can't use the two together at the moment (it's on our roadmap to fix).", "For now, you have to use approach of Ryan Sepassi -- manually construct a graph with identical node names, and use Saver to load the weights into it.", "(Alternatively you could hack it by using by using import_graph_def, creating variables manually, and using tf.add_to_collection(tf.GraphKeys.VARIABLES, variable) for each variable, then using Saver)"], "sent_idxs": [101, 2045, 2024, 2048, 3033, 2000, 1996, 2944, 1010, 1996, 2944, 6210, 1010, 5552, 2011, 12366, 2004, 10629, 1012, 1052, 19279, 18413, 1999, 1996, 2944, 14176, 1998, 1996, 15973, 5300, 1997, 23435, 2015, 1010, 5552, 2046, 26520, 6764, 2066, 2944, 1012, 23616, 13876, 1011, 2531, 22022, 15136, 1012, 102, 101, 1996, 2944, 6210, 2064, 2022, 5854, 2478, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1010, 1998, 1996, 15871, 2024, 5854, 2478, 3828, 2099, 1012, 102, 101, 2174, 1010, 3828, 2099, 3594, 2569, 3074, 3173, 2862, 1997, 10857, 2008, 1005, 1055, 4987, 2000, 1996, 2944, 10629, 1010, 1998, 2023, 3074, 2003, 2025, 3988, 3550, 2478, 12324, 1035, 10629, 1035, 13366, 1010, 2061, 2017, 2064, 1005, 1056, 2224, 1996, 2048, 2362, 2012, 1996, 2617, 1006, 2009, 1005, 1055, 2006, 2256, 2346, 2863, 2361, 2000, 8081, 1007, 1012, 102, 101, 2005, 2085, 1010, 2017, 2031, 2000, 2224, 3921, 1997, 4575, 19802, 12054, 2072, 1011, 1011, 21118, 9570, 1037, 10629, 2007, 7235, 13045, 3415, 1010, 1998, 2224, 3828, 2099, 2000, 7170, 1996, 15871, 2046, 2009, 1012, 102, 101, 1006, 14084, 2017, 2071, 20578, 2009, 2011, 2478, 2011, 2478, 12324, 1035, 10629, 1035, 13366, 1010, 4526, 10857, 21118, 1010, 1998, 2478, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1006, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10857, 1010, 8023, 1007, 2005, 2169, 8023, 1010, 2059, 2478, 3828, 2099, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 49, 76, 137, 174, 227], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51849045", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [36, 42]}], [{"sent_id": 2, "name": "tf.contrib.layers", "pos": [36, 44]}], [{"sent_id": 2, "name": "tf.contrib.layers.variance_scaling_initializer", "pos": [36, 51]}]], "sents": ["<code>Code Snippet</code>.", "This will give you He / MRSA initialization.", "The documentation states that the default arguments for tf.contrib.layers.variance_scaling_initializer correspond to He initialization and that changing the arguments can yield Xavier initialization (this is what is done in TF's internal implementation for Xavier initialization).", "Example usage:", "<code>Code Snippet</code>.", "or", "<code>Code Snippet</code>."], "sent_idxs": [101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2097, 2507, 2017, 2002, 1013, 3680, 2050, 3988, 3989, 1012, 102, 101, 1996, 12653, 2163, 2008, 1996, 12398, 9918, 2005, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 23284, 1035, 25169, 1035, 3988, 17629, 17254, 2000, 2002, 3988, 3989, 1998, 2008, 5278, 1996, 9918, 2064, 10750, 10062, 3988, 3989, 1006, 2023, 2003, 2054, 2003, 2589, 1999, 1056, 2546, 1005, 1055, 4722, 7375, 2005, 10062, 3988, 3989, 1007, 1012, 102, 101, 2742, 8192, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 27, 86, 91, 105, 108, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43507519", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [17, 22]}, {"sent_id": 4, "name": "tf.nn", "pos": [103, 108]}], [{"sent_id": 0, "name": "tf.nn.dynamic_rnn", "pos": [17, 27]}, {"sent_id": 4, "name": "tf.nn.dynamic_rnn", "pos": [103, 113]}], [{"sent_id": 3, "name": "tf.reset_default_graph", "pos": [76, 84]}]], "sents": ["I ran into a similar issue in TensorFlow v1.0.1 using tf.nn.dynamic_rnn.", "It turned out that the error only arose if I had to re-train or cancel in the middle of training and restart my training process.", "Basically the graph was not being reset.", "Long story short, throw a tf.reset_default_graph() at the start of your code and it should help.", "At least when using tf.nn.dynamic_rnn and retraining."], "sent_idxs": [101, 1045, 2743, 2046, 1037, 2714, 3277, 1999, 23435, 12314, 1058, 2487, 1012, 1014, 1012, 1015, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1012, 102, 101, 2009, 2357, 2041, 2008, 1996, 7561, 2069, 10375, 2065, 1045, 2018, 2000, 2128, 1011, 3345, 2030, 17542, 1999, 1996, 2690, 1997, 2731, 1998, 23818, 2026, 2731, 2832, 1012, 102, 101, 10468, 1996, 10629, 2001, 2025, 2108, 25141, 1012, 102, 101, 2146, 2466, 2460, 1010, 5466, 1037, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2012, 1996, 2707, 1997, 2115, 3642, 1998, 2009, 2323, 2393, 1012, 102, 101, 2012, 2560, 2043, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1998, 2128, 23654, 2075, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 59, 69, 98, 119], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0]}, {"title": "41533416", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [33, 39]}, {"sent_id": 1, "name": "tf.contrib", "pos": [47, 53]}], [{"sent_id": 1, "name": "tf.contrib.learn", "pos": [33, 41]}, {"sent_id": 1, "name": "tf.contrib.learn", "pos": [47, 55]}], [{"sent_id": 1, "name": "tf.contrib.learn.skcompat", "pos": [33, 46]}], [{"sent_id": 1, "name": "tf.contrib.learn.dnnclassifier", "pos": [47, 60]}]], "sents": ["You could try to \"wrap\" all in SKCompat.", "I had a simile issue, and I have partially solved by (example)tf.contrib.learn.SKCompat(tf.contrib.learn.DNNClassifier( ...).fit(...).", "Otherwise you could use input_fn() as explained in tensorflow.org"], "sent_idxs": [101, 2017, 2071, 3046, 2000, 1000, 10236, 1000, 2035, 1999, 15315, 9006, 4502, 2102, 1012, 102, 101, 1045, 2018, 1037, 28684, 2571, 3277, 1010, 1998, 1045, 2031, 6822, 13332, 2011, 1006, 2742, 1007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 15315, 9006, 4502, 2102, 1006, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 1040, 10695, 26266, 18095, 1006, 1012, 1012, 1012, 1007, 1012, 4906, 1006, 1012, 1012, 1012, 1007, 1012, 102, 101, 4728, 2017, 2071, 2224, 7953, 1035, 1042, 2078, 1006, 1007, 2004, 4541, 1999, 23435, 12314, 1012, 8917, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 74, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46459075", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [3, 9]}], [{"sent_id": 4, "name": "tf.py_func", "pos": [112, 120]}], [{"sent_id": 7, "name": "tf.estimator", "pos": [212, 218]}], [{"sent_id": 3, "name": "tf.placeholder", "pos": [79, 84]}], [{"sent_id": 0, "name": "tf.contrib.data", "pos": [3, 11]}], [{"sent_id": 7, "name": "tf.estimator.estimator", "pos": [212, 222]}]], "sents": ["The new tf.contrib.data.Dataset.from_generator() can potentially speed up your input pipeline by overlapping the data preparation with training.", "However, you will tend to get the best performance by switching over to TensorFlow ops in your input pipeline wherever possible.", "To answer your specific questions:", "The Keras TensorFlow backend uses tf.placeholder() to represent compiled function inputs, and feed_dict to pass arguments to a function.", "With the recent optimizations to tf.py_func() and feed_dict copy overhead, I suspect the amount of time spent in memcpy() will be the same.", "However, you can more easily use Dataset.from_generator() with Dataset.prefetch() to overlap the training on one batch with preprocessing on the next batch.", "It sounds like you can define a separate iterator for the prediction phase.", "The tf.estimator.Estimator class does something similar by instantiating different \"input functions\" with different signatures for training and evaluation, then building a separate graph for each role.", "Alternatively, you could add a dummy output to your training iterator (for the batch_z values) and switch between training and evaluation iterators using a \"feedable iterator\"."], "sent_idxs": [101, 1996, 2047, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 13103, 1006, 1007, 2064, 9280, 3177, 2039, 2115, 7953, 13117, 2011, 20567, 1996, 2951, 7547, 2007, 2731, 1012, 102, 101, 2174, 1010, 2017, 2097, 7166, 2000, 2131, 1996, 2190, 2836, 2011, 11991, 2058, 2000, 23435, 12314, 23092, 1999, 2115, 7953, 13117, 11210, 2825, 1012, 102, 101, 2000, 3437, 2115, 3563, 3980, 1024, 102, 101, 1996, 17710, 8180, 23435, 12314, 2067, 10497, 3594, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 2000, 5050, 9227, 3853, 20407, 1010, 1998, 5438, 1035, 4487, 6593, 2000, 3413, 9918, 2000, 1037, 3853, 1012, 102, 101, 2007, 1996, 3522, 20600, 2015, 2000, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 1006, 1007, 1998, 5438, 1035, 4487, 6593, 6100, 8964, 1010, 1045, 8343, 1996, 3815, 1997, 2051, 2985, 1999, 2033, 12458, 7685, 1006, 1007, 2097, 2022, 1996, 2168, 1012, 102, 101, 2174, 1010, 2017, 2064, 2062, 4089, 2224, 2951, 13462, 1012, 2013, 1035, 13103, 1006, 1007, 2007, 2951, 13462, 1012, 3653, 7959, 10649, 1006, 1007, 2000, 17702, 1996, 2731, 2006, 2028, 14108, 2007, 17463, 3217, 9623, 7741, 2006, 1996, 2279, 14108, 1012, 102, 101, 2009, 4165, 2066, 2017, 2064, 9375, 1037, 3584, 2009, 6906, 4263, 2005, 1996, 17547, 4403, 1012, 102, 101, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 2465, 2515, 2242, 2714, 2011, 7107, 15370, 2367, 1000, 7953, 4972, 1000, 2007, 2367, 16442, 2005, 2731, 1998, 9312, 1010, 2059, 2311, 1037, 3584, 10629, 2005, 2169, 2535, 1012, 102, 101, 14084, 1010, 2017, 2071, 5587, 1037, 24369, 6434, 2000, 2115, 2731, 2009, 6906, 4263, 1006, 2005, 1996, 14108, 1035, 1062, 5300, 1007, 1998, 6942, 2090, 2731, 1998, 9312, 2009, 6906, 6591, 2478, 1037, 1000, 5438, 3085, 2009, 6906, 4263, 1000, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 36, 62, 70, 105, 149, 192, 210, 252, 295], "sent_pos": [0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37464246", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [10, 14]}, {"sent_id": 1, "name": "tf.train", "pos": [58, 62]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [28, 32]}], [{"sent_id": 1, "name": "tf.placeholder", "pos": [81, 86]}], [{"sent_id": 0, "name": "tf.train.batch", "pos": [10, 16]}, {"sent_id": 1, "name": "tf.train.batch", "pos": [58, 64]}]], "sents": ["As nessuno points out, the results of tf.train.batch()\u2014image_batch and label_batch\u2014are tf.Tensor objects, so you can't use these as a value to feed into a subgraph.", "The typical way to use tf.train.batch() is to use it to define the inputs to a pipeline (rather than using tf.placeholder() for x and y_),  so that batching and prefetching will be handled inside the TensorFlow graph.", "Here's an approximate restructuring of the first part of your program that performs batching as desired:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 23384, 27819, 2685, 2041, 1010, 1996, 3463, 1997, 1056, 2546, 1012, 3345, 1012, 14108, 1006, 1007, 1517, 3746, 1035, 14108, 1998, 3830, 1035, 14108, 1517, 2024, 1056, 2546, 1012, 23435, 5200, 1010, 2061, 2017, 2064, 1005, 1056, 2224, 2122, 2004, 1037, 3643, 2000, 5438, 2046, 1037, 4942, 14413, 1012, 102, 101, 1996, 5171, 2126, 2000, 2224, 1056, 2546, 1012, 3345, 1012, 14108, 1006, 1007, 2003, 2000, 2224, 2009, 2000, 9375, 1996, 20407, 2000, 1037, 13117, 1006, 2738, 2084, 2478, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 2005, 1060, 1998, 1061, 1035, 1007, 1010, 2061, 2008, 14108, 2075, 1998, 3653, 7959, 10649, 2075, 2097, 2022, 8971, 2503, 1996, 23435, 12314, 10629, 1012, 102, 101, 2182, 1005, 1055, 2019, 15796, 18322, 1997, 1996, 2034, 2112, 1997, 2115, 2565, 2008, 10438, 14108, 2075, 2004, 9059, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 52, 114, 136, 150], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63889813", "vertexSet": [[{"sent_id": 5, "name": "tf.math", "pos": [99, 103]}], [{"sent_id": 4, "name": "tf.keras", "pos": [70, 75]}], [{"sent_id": 5, "name": "tf.math.exp", "pos": [99, 106]}], [{"sent_id": 4, "name": "tf.keras.model", "pos": [70, 77]}]], "sents": ["The input_shape should not include the batch dimension.", "Use input_shape=(70,).", "<code>Code Snippet</code>.", "You can set the batch size when you call model.fit(..., batch_size=10).", "See the documentation on tf.keras.Model.fit.", "There was another error in the original post, due to passing an int32 value to tf.math.exp.", "That line should should read", "<code>Code Snippet</code>.", "to solve that error.", "Note the 0. values, which evaluate to floats instead of ints."], "sent_idxs": [101, 1996, 7953, 1035, 4338, 2323, 2025, 2421, 1996, 14108, 9812, 1012, 102, 101, 2224, 7953, 1035, 4338, 1027, 1006, 3963, 1010, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2275, 1996, 14108, 2946, 2043, 2017, 2655, 2944, 1012, 4906, 1006, 1012, 1012, 1012, 1010, 14108, 1035, 2946, 1027, 2184, 1007, 1012, 102, 101, 2156, 1996, 12653, 2006, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1012, 4906, 1012, 102, 101, 2045, 2001, 2178, 7561, 1999, 1996, 2434, 2695, 1010, 2349, 2000, 4458, 2019, 20014, 16703, 3643, 2000, 1056, 2546, 1012, 8785, 1012, 4654, 2361, 1012, 102, 101, 2008, 2240, 2323, 2323, 3191, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 9611, 2008, 7561, 1012, 102, 101, 3602, 1996, 1014, 1012, 5300, 1010, 2029, 16157, 2000, 24885, 2612, 1997, 20014, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 13, 25, 39, 65, 81, 108, 115, 129, 136, 153], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51808342", "vertexSet": [[{"sent_id": 5, "name": "tf.cast", "pos": [107, 111]}], [{"sent_id": 3, "name": "tf.size", "pos": [71, 75]}, {"sent_id": 5, "name": "tf.size", "pos": [112, 116]}], [{"sent_id": 0, "name": "tf.cond", "pos": [9, 14]}]], "sents": ["You should be able to do this via tf.cond, which executes one of two branches depending on some condition.", "I haven't tested the below code so please report whether it works.", "<code>Code Snippet</code>.", "The idea is to check whether result contains any items via tf.size (should return 0 if result is empty).", "You might need to convert it to a boolean condition explicitly, i.e.", "use tf.cast(tf.size(result), tf.bool) instead."], "sent_idxs": [101, 2017, 2323, 2022, 2583, 2000, 2079, 2023, 3081, 1056, 2546, 1012, 9530, 2094, 1010, 2029, 15389, 2015, 2028, 1997, 2048, 5628, 5834, 2006, 2070, 4650, 1012, 102, 101, 1045, 4033, 1005, 1056, 7718, 1996, 2917, 3642, 2061, 3531, 3189, 3251, 2009, 2573, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2801, 2003, 2000, 4638, 3251, 2765, 3397, 2151, 5167, 3081, 1056, 2546, 1012, 2946, 1006, 2323, 2709, 1014, 2065, 2765, 2003, 4064, 1007, 1012, 102, 101, 2017, 2453, 2342, 2000, 10463, 2009, 2000, 1037, 22017, 20898, 4650, 12045, 1010, 1045, 1012, 1041, 1012, 102, 101, 2224, 1056, 2546, 1012, 3459, 1006, 1056, 2546, 1012, 2946, 1006, 2765, 1007, 1010, 1056, 2546, 1012, 22017, 2140, 1007, 2612, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 45, 59, 86, 105, 129], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "33717109", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [55, 60]}], [{"sent_id": 2, "name": "tf.matmul", "pos": [64, 70]}], [{"sent_id": 2, "name": "tf.nn.softmax", "pos": [55, 63]}]], "sents": ["As @dga suggested, you need to run your new instance of the data though your already predicted model.", "Here is an example:", "Assume you went though the first tutorial and calculated the accuracy of your model (the model is this: y = tf.nn.softmax(tf.matmul(x, W) + b)).", "Now you grab your model and apply the new data point to it.", "In the following code I calculate the vector, getting the position of the maximum value.", "Show the image and print that maximum position.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 1030, 1040, 3654, 4081, 1010, 2017, 2342, 2000, 2448, 2115, 2047, 6013, 1997, 1996, 2951, 2295, 2115, 2525, 10173, 2944, 1012, 102, 101, 2182, 2003, 2019, 2742, 1024, 102, 101, 7868, 2017, 2253, 2295, 1996, 2034, 14924, 4818, 1998, 10174, 1996, 10640, 1997, 2115, 2944, 1006, 1996, 2944, 2003, 2023, 1024, 1061, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1060, 1010, 1059, 1007, 1009, 1038, 1007, 1007, 1012, 102, 101, 2085, 2017, 6723, 2115, 2944, 1998, 6611, 1996, 2047, 2951, 2391, 2000, 2009, 1012, 102, 101, 1999, 1996, 2206, 3642, 1045, 18422, 1996, 9207, 1010, 2893, 1996, 2597, 1997, 1996, 4555, 3643, 1012, 102, 101, 2265, 1996, 3746, 1998, 6140, 2008, 4555, 2597, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 31, 81, 97, 116, 127, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54997975", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [8, 14]}, {"sent_id": 4, "name": "tf.contrib", "pos": [79, 85]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [8, 17]}, {"sent_id": 4, "name": "tf.contrib.rnn", "pos": [79, 88]}], [{"sent_id": 0, "name": "tf.contrib.rnn.lstmblockfusedcell", "pos": [8, 26]}], [{"sent_id": 4, "name": "tf.contrib.rnn.stack_bidirectional_dynamic_rnn", "pos": [79, 100]}]], "sents": ["first\uff0cyou should use two independent tf.contrib.rnn.LSTMBlockFusedCell for fw and bw, change the code below", "<code>Code Snippet</code>.", "to this:", "<code>Code Snippet</code>.", "second,in tf's tf.contrib.rnn.stack_bidirectional_dynamic_rnn api, it says", "The combined forward and backward layer outputs are used as input of\n  the next layer.", "so the code below", "<code>Code Snippet</code>.", "should be change to:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2034, 1989, 2017, 2323, 2224, 2048, 2981, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 1048, 3367, 14905, 7878, 25608, 2098, 29109, 2140, 2005, 1042, 2860, 1998, 1038, 2860, 1010, 2689, 1996, 3642, 2917, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2117, 1010, 1999, 1056, 2546, 1005, 1055, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 9991, 1035, 7226, 7442, 7542, 2389, 1035, 8790, 1035, 29300, 2078, 17928, 1010, 2009, 2758, 102, 101, 1996, 4117, 2830, 1998, 8848, 6741, 27852, 2024, 2109, 2004, 7953, 1997, 1996, 2279, 6741, 1012, 102, 101, 2061, 1996, 3642, 2917, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2323, 2022, 2689, 2000, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 38, 52, 57, 71, 105, 123, 129, 143, 150, 164], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36320000", "vertexSet": [[{"sent_id": 2, "name": "tf.device", "pos": [52, 56]}], [{"sent_id": 7, "name": "tf.session", "pos": [166, 170]}], [{"sent_id": 7, "name": "tf.configproto", "pos": [154, 162]}]], "sents": ["Splitting a large model across multiple GPUs is certainly possible in TensorFlow, but doing it optimally is a hard research problem.", "In general, you will need to do the following:", "Wrap large contiguous regions of your code in a with tf.device(...): block, naming the different GPUs:", "<code>Code Snippet</code>.", "When building your optimizer, pass the optional argument colocate_gradients_with_ops=True to the optimizer.minimize() method:", "<code>Code Snippet</code>.", "(Optionally.)", "You may need to enable \"soft placement\" in the tf.ConfigProto when you create your tf.Session, if any of the operations in your model cannot run on GPU:", "<code>Code Snippet</code>."], "sent_idxs": [101, 14541, 1037, 2312, 2944, 2408, 3674, 14246, 2271, 2003, 5121, 2825, 1999, 23435, 12314, 1010, 2021, 2725, 2009, 15502, 2135, 2003, 1037, 2524, 2470, 3291, 1012, 102, 101, 1999, 2236, 1010, 2017, 2097, 2342, 2000, 2079, 1996, 2206, 1024, 102, 101, 10236, 2312, 25177, 4655, 1997, 2115, 3642, 1999, 1037, 2007, 1056, 2546, 1012, 5080, 1006, 1012, 1012, 1012, 1007, 1024, 3796, 1010, 10324, 1996, 2367, 14246, 2271, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2043, 2311, 2115, 23569, 27605, 6290, 1010, 3413, 1996, 11887, 6685, 8902, 24755, 2618, 1035, 17978, 2015, 1035, 2007, 1035, 23092, 1027, 2995, 2000, 1996, 23569, 27605, 6290, 1012, 18478, 1006, 1007, 4118, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1006, 11887, 2135, 1012, 1007, 102, 101, 2017, 2089, 2342, 2000, 9585, 1000, 3730, 11073, 1000, 1999, 1996, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 2043, 2017, 3443, 2115, 1056, 2546, 1012, 5219, 1010, 2065, 2151, 1997, 1996, 3136, 1999, 2115, 2944, 3685, 2448, 2006, 14246, 2226, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 41, 71, 85, 121, 135, 142, 186, 200], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61771538", "vertexSet": [[{"sent_id": 0, "name": "tf.losses", "pos": [11, 15]}, {"sent_id": 0, "name": "tf.losses", "pos": [25, 29]}], [{"sent_id": 0, "name": "tf.losses.softmax_cross_entropy", "pos": [11, 22]}], [{"sent_id": 0, "name": "tf.losses.compute_weighted_loss", "pos": [25, 35]}]], "sents": ["You can just average the result (which is what tf.losses.softmax_cross_entropy did anyway through tf.losses.compute_weighted_loss):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2074, 2779, 1996, 2765, 1006, 2029, 2003, 2054, 1056, 2546, 1012, 6409, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 2106, 4312, 2083, 1056, 2546, 1012, 6409, 1012, 24134, 1035, 18215, 1035, 3279, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 52], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42792589", "vertexSet": [[{"sent_id": 4, "name": "tf.cond", "pos": [125, 130]}, {"sent_id": 9, "name": "tf.cond", "pos": [255, 260]}], [{"sent_id": 7, "name": "tf.where", "pos": [219, 223]}, {"sent_id": 9, "name": "tf.where", "pos": [247, 251]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [2, 6]}], [{"sent_id": 4, "name": "tf.map_fn", "pos": [108, 115]}, {"sent_id": 7, "name": "tf.map_fn", "pos": [207, 214]}, {"sent_id": 9, "name": "tf.map_fn", "pos": [264, 271]}], [{"sent_id": 1, "name": "tf.variable", "pos": [47, 51]}]], "sents": ["A tf.Tensor in TensorFlow is a read-only value\u2014in fact, a symbolic expression for computing a read-only value\u2014so you cannot in general assign values to it.", "(The main exceptions are tf.Variable objects.)", "This means that you are encourage to use \"functional\" operations to define your tensor.", "For example, there are several ways to generate the weights tensor functionally:", "Since weights is defined as an element-wise transformation of labels, you can use tf.map_fn() to create a new tensor (containing a tf.cond() to replace the if statement) by mapping a function across it:", "<code>Code Snippet</code>.", "This version allows you to apply an arbitrarily complicated function to each element of labels.", "However, since the function is simple, cheap to compute, and representable using simple TensorFlow ops, you can avoid using tf.map_fn() and instead use tf.where():", "<code>Code Snippet</code>.", "(You could also use tf.where() instead of tf.cond() in the tf.map_fn() version.)"], "sent_idxs": [101, 1037, 1056, 2546, 1012, 23435, 1999, 23435, 12314, 2003, 1037, 3191, 1011, 2069, 3643, 1517, 1999, 2755, 1010, 1037, 12613, 3670, 2005, 9798, 1037, 3191, 1011, 2069, 3643, 1517, 2061, 2017, 3685, 1999, 2236, 23911, 5300, 2000, 2009, 1012, 102, 101, 1006, 1996, 2364, 11790, 2024, 1056, 2546, 1012, 8023, 5200, 1012, 1007, 102, 101, 2023, 2965, 2008, 2017, 2024, 8627, 2000, 2224, 1000, 8360, 1000, 3136, 2000, 9375, 2115, 23435, 1012, 102, 101, 2005, 2742, 1010, 2045, 2024, 2195, 3971, 2000, 9699, 1996, 15871, 23435, 8360, 2135, 1024, 102, 101, 2144, 15871, 2003, 4225, 2004, 2019, 5783, 1011, 7968, 8651, 1997, 10873, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2000, 3443, 1037, 2047, 23435, 1006, 4820, 1037, 1056, 2546, 1012, 9530, 2094, 1006, 1007, 2000, 5672, 1996, 2065, 4861, 1007, 2011, 12375, 1037, 3853, 2408, 2009, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2544, 4473, 2017, 2000, 6611, 2019, 12098, 16313, 19848, 6588, 8552, 3853, 2000, 2169, 5783, 1997, 10873, 1012, 102, 101, 2174, 1010, 2144, 1996, 3853, 2003, 3722, 1010, 10036, 2000, 24134, 1010, 1998, 5050, 3085, 2478, 3722, 23435, 12314, 23092, 1010, 2017, 2064, 4468, 2478, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 1998, 2612, 2224, 1056, 2546, 1012, 2073, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1006, 2017, 2071, 2036, 2224, 1056, 2546, 1012, 2073, 1006, 1007, 2612, 1997, 1056, 2546, 1012, 9530, 2094, 1006, 1007, 1999, 1996, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 2544, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 41, 55, 74, 91, 146, 160, 181, 227, 241, 277], "sent_pos": [0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]}, {"title": "48722354", "vertexSet": [[{"sent_id": 9, "name": "tf.train", "pos": [230, 234]}, {"sent_id": 9, "name": "tf.train", "pos": [237, 241]}], [{"sent_id": 9, "name": "tf.train.batch", "pos": [230, 236]}], [{"sent_id": 9, "name": "tf.train.shuffle_batch", "pos": [237, 245]}]], "sents": ["From Threading and Queues tutorial:", "For example, a typical input architecture is to use a\n  RandomShuffleQueue to prepare inputs for training a model:", "Multiple threads prepare training examples and push them in the queue..", "A training thread executes a training op that dequeues mini-batches\n  from the queue..", "The TensorFlow Session object is multithreaded, so multiple threads\n  can easily use the same session and run ops in parallel.", "The idea is that data pipeline is usually I/O intensive: the data may be fetched from the disk or even streamed from the network.", "It is quite possible for a GPU not to be a bottleneck in computation, simply because the data isn't fed fast enough to saturate it.", "Reading in multiple threads solves this problem: while one thread is waiting for I/O task, the other thread already has some data for the GPU.", "When this data is processed, the first thread hopefully received and prepared its batch, and so on.", "That's why tf.train.batch, tf.train.shuffle_batch and other functions, support multi-thread data processing.", "Setting num_threads = 1 makes the batching deterministic, but if there are multiple threads, the order of data in the queue is not guaranteed."], "sent_idxs": [101, 2013, 11689, 2075, 1998, 24240, 2015, 14924, 4818, 1024, 102, 101, 2005, 2742, 1010, 1037, 5171, 7953, 4294, 2003, 2000, 2224, 1037, 6721, 14235, 18142, 4226, 5657, 2000, 7374, 20407, 2005, 2731, 1037, 2944, 1024, 102, 101, 3674, 16457, 7374, 2731, 4973, 1998, 5245, 2068, 1999, 1996, 24240, 1012, 1012, 102, 101, 1037, 2731, 11689, 15389, 2015, 1037, 2731, 6728, 2008, 2139, 4226, 15808, 7163, 1011, 14108, 2229, 2013, 1996, 24240, 1012, 1012, 102, 101, 1996, 23435, 12314, 5219, 4874, 2003, 4800, 2705, 16416, 5732, 1010, 2061, 3674, 16457, 2064, 4089, 2224, 1996, 2168, 5219, 1998, 2448, 23092, 1999, 5903, 1012, 102, 101, 1996, 2801, 2003, 2008, 2951, 13117, 2003, 2788, 1045, 1013, 1051, 11806, 1024, 1996, 2951, 2089, 2022, 18584, 2098, 2013, 1996, 9785, 2030, 2130, 18498, 2013, 1996, 2897, 1012, 102, 101, 2009, 2003, 3243, 2825, 2005, 1037, 14246, 2226, 2025, 2000, 2022, 1037, 5835, 18278, 1999, 22334, 1010, 3432, 2138, 1996, 2951, 3475, 1005, 1056, 7349, 3435, 2438, 2000, 2938, 4648, 2618, 2009, 1012, 102, 101, 3752, 1999, 3674, 16457, 9611, 2015, 2023, 3291, 1024, 2096, 2028, 11689, 2003, 3403, 2005, 1045, 1013, 1051, 4708, 1010, 1996, 2060, 11689, 2525, 2038, 2070, 2951, 2005, 1996, 14246, 2226, 1012, 102, 101, 2043, 2023, 2951, 2003, 13995, 1010, 1996, 2034, 11689, 11504, 2363, 1998, 4810, 2049, 14108, 1010, 1998, 2061, 2006, 1012, 102, 101, 2008, 1005, 1055, 2339, 1056, 2546, 1012, 3345, 1012, 14108, 1010, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1998, 2060, 4972, 1010, 2490, 4800, 1011, 11689, 2951, 6364, 1012, 102, 101, 4292, 16371, 2213, 1035, 16457, 1027, 1015, 3084, 1996, 14108, 2075, 28283, 25300, 10074, 1010, 2021, 2065, 2045, 2024, 3674, 16457, 1010, 1996, 2344, 1997, 2951, 1999, 1996, 24240, 2003, 2025, 12361, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 11, 37, 52, 75, 103, 134, 169, 203, 225, 257, 292], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58365921", "vertexSet": [[{"sent_id": 3, "name": "tf.io", "pos": [80, 84]}], [{"sent_id": 2, "name": "tf.gfile", "pos": [29, 35]}], [{"sent_id": 3, "name": "tf.io.gfile", "pos": [80, 88]}], [{"sent_id": 2, "name": "tf.gfile.gfile", "pos": [29, 39]}], [{"sent_id": 3, "name": "tf.io.gfile.gfile", "pos": [80, 92]}]], "sents": ["It's not called that in TensorFlow 2.", "You might be using a TensorFlow 1 tutorial.", "Version 1\ntf.gfile.GFile\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/io/gfile/GFile", "Version 2\ntf.io.gfile.GFile\nhttps://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile"], "sent_idxs": [101, 2009, 1005, 1055, 2025, 2170, 2008, 1999, 23435, 12314, 1016, 1012, 102, 101, 2017, 2453, 2022, 2478, 1037, 23435, 12314, 1015, 14924, 4818, 1012, 102, 101, 2544, 1015, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 1043, 8873, 2571, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 4617, 1013, 1054, 2487, 1012, 2321, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 22834, 1013, 1043, 8873, 2571, 1013, 1043, 8873, 2571, 102, 101, 2544, 1016, 1056, 2546, 1012, 22834, 1012, 1043, 8873, 2571, 1012, 1043, 8873, 2571, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 22834, 1013, 1043, 8873, 2571, 1013, 1043, 8873, 2571, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 13, 26, 77, 123], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56772187", "vertexSet": [[{"sent_id": 4, "name": "tf.keras", "pos": [88, 93]}], [{"sent_id": 4, "name": "tf.compat", "pos": [105, 111]}], [{"sent_id": 4, "name": "tf.compat.v1", "pos": [105, 114]}], [{"sent_id": 4, "name": "tf.keras.layers", "pos": [88, 95]}], [{"sent_id": 4, "name": "tf.compat.v1.keras", "pos": [105, 117]}], [{"sent_id": 4, "name": "tf.compat.v1.keras.layers", "pos": [105, 119]}], [{"sent_id": 4, "name": "tf.keras.layers.cudnnlstm", "pos": [88, 101]}], [{"sent_id": 4, "name": "tf.compat.v1.keras.layers.cudnnlstm", "pos": [105, 125]}]], "sents": ["In general, in TensorFlow 2.0 we should just use:", "<code>Code Snippet</code>.", "which, despite the warning, will use the GPU.", "The warning message incorrectly existed in the 2.0.0-alpha0 version but has since been removed in 2.0.0-beta1", "If for some reason you specifically need the original implementation of tf.keras.layers.CuDNNLSTM then you can use tf.compat.v1.keras.layers.CuDNNLSTM but this would be an edge case."], "sent_idxs": [101, 1999, 2236, 1010, 1999, 23435, 12314, 1016, 1012, 1014, 2057, 2323, 2074, 2224, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2029, 1010, 2750, 1996, 5432, 1010, 2097, 2224, 1996, 14246, 2226, 1012, 102, 101, 1996, 5432, 4471, 19721, 5839, 1999, 1996, 1016, 1012, 1014, 1012, 1014, 1011, 6541, 2692, 2544, 2021, 2038, 2144, 2042, 3718, 1999, 1016, 1012, 1014, 1012, 1014, 1011, 8247, 2487, 102, 101, 2065, 2005, 2070, 3114, 2017, 4919, 2342, 1996, 2434, 7375, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 12731, 2094, 10695, 4877, 21246, 2059, 2017, 2064, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 17710, 8180, 1012, 9014, 1012, 12731, 2094, 10695, 4877, 21246, 2021, 2023, 2052, 2022, 2019, 3341, 2553, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 16, 30, 44, 76, 134], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50956478", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [44, 50]}], [{"sent_id": 2, "name": "tf.contrib.distributions", "pos": [44, 52]}], [{"sent_id": 2, "name": "tf.contrib.distributions.percentile", "pos": [44, 55]}]], "sents": ["We can modify BlueSun's solution to be much faster on GPUs:", "<code>Code Snippet</code>.", "This is as fast as (in my experience) using tf.contrib.distributions.percentile(v, 50.0), and returns one of the actual elements."], "sent_idxs": [101, 2057, 2064, 19933, 5132, 4609, 1005, 1055, 5576, 2000, 2022, 2172, 5514, 2006, 14246, 2271, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2003, 2004, 3435, 2004, 1006, 1999, 2026, 3325, 1007, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 20611, 1012, 3867, 9463, 1006, 1058, 1010, 2753, 1012, 1014, 1007, 1010, 1998, 5651, 2028, 1997, 1996, 5025, 3787, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 18, 32, 72], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58195903", "vertexSet": [[{"sent_id": 2, "name": "tf.strings.regex_replace", "pos": [67, 76]}], [{"sent_id": 1, "name": "tf.regex_replace", "pos": [35, 42]}]], "sents": ["We can use tf.regex.replace to rename string.", "So, in place of python string replacement, use:file_path_mask = tf.regex_replace(file_path, \"img\", \"mask\").", "For TF 2.0, use tf.strings.regex_replace."], "sent_idxs": [101, 2057, 2064, 2224, 1056, 2546, 1012, 19723, 10288, 1012, 5672, 2000, 14916, 14074, 5164, 1012, 102, 101, 2061, 1010, 1999, 2173, 1997, 18750, 5164, 6110, 1010, 2224, 1024, 5371, 1035, 4130, 1035, 7308, 1027, 1056, 2546, 1012, 19723, 10288, 1035, 5672, 1006, 5371, 1035, 4130, 1010, 1000, 10047, 2290, 1000, 1010, 1000, 7308, 1000, 1007, 1012, 102, 101, 2005, 1056, 2546, 1016, 1012, 1014, 1010, 2224, 1056, 2546, 1012, 7817, 1012, 19723, 10288, 1035, 5672, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 17, 58, 78], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}, {"title": "50266170", "vertexSet": [[{"sent_id": 4, "name": "tf.data", "pos": [114, 118]}, {"sent_id": 10, "name": "tf.data", "pos": [291, 295]}], [{"sent_id": 6, "name": "tf.contrib", "pos": [188, 194]}, {"sent_id": 10, "name": "tf.contrib", "pos": [303, 309]}], [{"sent_id": 6, "name": "tf.contrib.data", "pos": [188, 196]}, {"sent_id": 10, "name": "tf.contrib.data", "pos": [303, 311]}], [{"sent_id": 4, "name": "tf.data.dataset", "pos": [114, 121]}, {"sent_id": 10, "name": "tf.data.dataset", "pos": [291, 298]}], [{"sent_id": 10, "name": "tf.contrib.data.map_and_batch", "pos": [303, 317]}], [{"sent_id": 6, "name": "tf.contrib.data.batch_and_drop_remainder", "pos": [188, 204]}]], "sents": ["It seems that some operation in your graph (from the error message, likely sparse_softmax_cross_entropy_loss), is expecting a fixed batch size.", "It may be your code (not part of the input_fn) that is enforcing this (e.g.", "passing batch_size as the shape of some tensor that is used in an op), or it may be one of the TF libraries.", "This is not always a problem per se.", "However, the fact that the documented behavior of tf.data.Dataset.batch is:", "NOTE: If the number of elements (N) in this dataset is not an exact\n  multiple of batch_size, the final batch contain smaller tensors with\n  shape N % batch_size in the batch dimension.", "If your program depends\n  on the batches having the same shape, consider using the\n  tf.contrib.data.batch_and_drop_remainder transformation instead.", "As currently written your (non-input_fn) code is in the category of depending on the batch with the same shape.", "Your options are to track down where the code is passing through a static batch size or to \"drop the remainder\".", "I believe the former is preferable, but more work.", "If you choose the latter, note that you are not actually using tf.data.Dataset.batch, but rather tf.contrib.data.map_and_batch which accepts a drop_remainder parameter."], "sent_idxs": [101, 2009, 3849, 2008, 2070, 3169, 1999, 2115, 10629, 1006, 2013, 1996, 7561, 4471, 1010, 3497, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 3279, 1007, 1010, 2003, 8074, 1037, 4964, 14108, 2946, 1012, 102, 101, 2009, 2089, 2022, 2115, 3642, 1006, 2025, 2112, 1997, 1996, 7953, 1035, 1042, 2078, 1007, 2008, 2003, 27455, 2023, 1006, 1041, 1012, 1043, 1012, 102, 101, 4458, 14108, 1035, 2946, 2004, 1996, 4338, 1997, 2070, 23435, 2008, 2003, 2109, 1999, 2019, 6728, 1007, 1010, 2030, 2009, 2089, 2022, 2028, 1997, 1996, 1056, 2546, 8860, 1012, 102, 101, 2023, 2003, 2025, 2467, 1037, 3291, 2566, 7367, 1012, 102, 101, 2174, 1010, 1996, 2755, 2008, 1996, 8832, 5248, 1997, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 14108, 2003, 1024, 102, 101, 3602, 1024, 2065, 1996, 2193, 1997, 3787, 1006, 1050, 1007, 1999, 2023, 2951, 13462, 2003, 2025, 2019, 6635, 3674, 1997, 14108, 1035, 2946, 1010, 1996, 2345, 14108, 5383, 3760, 23435, 2015, 2007, 4338, 1050, 1003, 14108, 1035, 2946, 1999, 1996, 14108, 9812, 1012, 102, 101, 2065, 2115, 2565, 9041, 2006, 1996, 14108, 2229, 2383, 1996, 2168, 4338, 1010, 5136, 2478, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 14108, 1035, 1998, 1035, 4530, 1035, 6893, 8651, 2612, 1012, 102, 101, 2004, 2747, 2517, 2115, 1006, 2512, 1011, 7953, 1035, 1042, 2078, 1007, 3642, 2003, 1999, 1996, 4696, 1997, 5834, 2006, 1996, 14108, 2007, 1996, 2168, 4338, 1012, 102, 101, 2115, 7047, 2024, 2000, 2650, 2091, 2073, 1996, 3642, 2003, 4458, 2083, 1037, 10763, 14108, 2946, 2030, 2000, 1000, 4530, 1996, 6893, 1000, 1012, 102, 101, 1045, 2903, 1996, 2280, 2003, 9544, 3085, 1010, 2021, 2062, 2147, 1012, 102, 101, 2065, 2017, 5454, 1996, 3732, 1010, 3602, 2008, 2017, 2024, 2025, 2941, 2478, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 14108, 1010, 2021, 2738, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 4949, 1035, 1998, 1035, 14108, 2029, 13385, 1037, 4530, 1035, 6893, 16381, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 36, 62, 93, 104, 126, 171, 208, 237, 263, 277, 326], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43001802", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [24, 30]}, {"sent_id": 0, "name": "tf.contrib", "pos": [41, 47]}, {"sent_id": 0, "name": "tf.contrib", "pos": [61, 67]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [24, 33]}, {"sent_id": 0, "name": "tf.contrib.rnn", "pos": [41, 50]}, {"sent_id": 0, "name": "tf.contrib.rnn", "pos": [61, 70]}], [{"sent_id": 0, "name": "tf.contrib.rnn.basicrnncell", "pos": [24, 38]}]], "sents": ["The answer of ruoho ruotsi is almost correct:\nYet, the definition of linear is not located in tf.contrib.rnn.basicRNNCell, but in tf.contrib.rnn.python.ops.rnn_cell, or tf.contrib.rnn.python.ops.core_rnn_cell_impl, respectively.", "You can find their source code here and here."], "sent_idxs": [101, 1996, 3437, 1997, 21766, 11631, 2080, 21766, 12868, 2072, 2003, 2471, 6149, 1024, 2664, 1010, 1996, 6210, 1997, 7399, 2003, 2025, 2284, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 3937, 6826, 5897, 3363, 1010, 2021, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 18750, 1012, 23092, 1012, 29300, 2078, 1035, 3526, 1010, 2030, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 18750, 1012, 23092, 1012, 4563, 1035, 29300, 2078, 1035, 3526, 1035, 17727, 2140, 1010, 4414, 1012, 102, 101, 2017, 2064, 2424, 2037, 3120, 3642, 2182, 1998, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 88, 100], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39142367", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [39, 44]}, {"sent_id": 3, "name": "tf.nn", "pos": [85, 90]}, {"sent_id": 3, "name": "tf.nn", "pos": [96, 101]}], [{"sent_id": 2, "name": "tf.nn.rnn_cell", "pos": [39, 49]}], [{"sent_id": 3, "name": "tf.nn.dynamic_rnn", "pos": [85, 95]}], [{"sent_id": 2, "name": "tf.nn.rnn_cell.multirnncell", "pos": [39, 54]}]], "sents": ["Here is how I do it in a translation model with GRU cells.", "You can just replace the GRU with an LSTM.", "It is really easy just use tf.nn.rnn_cell.MultiRNNCell with a list of the multiple cells it should wrap.", "In the code bellow I am manually unrolling it but you can pass it to tf.nn.dynamic_rnn or tf.nn.rnn as well.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2182, 2003, 2129, 1045, 2079, 2009, 1999, 1037, 5449, 2944, 2007, 24665, 2226, 4442, 1012, 102, 101, 2017, 2064, 2074, 5672, 1996, 24665, 2226, 2007, 2019, 1048, 3367, 2213, 1012, 102, 101, 2009, 2003, 2428, 3733, 2074, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 4800, 6826, 5897, 3363, 2007, 1037, 2862, 1997, 1996, 3674, 4442, 2009, 2323, 10236, 1012, 102, 101, 1999, 1996, 3642, 4330, 5004, 1045, 2572, 21118, 4895, 28402, 2075, 2009, 2021, 2017, 2064, 3413, 2009, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 2030, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 2004, 2092, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 17, 32, 66, 108, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46037859", "vertexSet": [[{"sent_id": 5, "name": "tf.graph", "pos": [146, 150]}], [{"sent_id": 9, "name": "tf.contrib", "pos": [221, 227]}, {"sent_id": 10, "name": "tf.contrib", "pos": [253, 259]}], [{"sent_id": 9, "name": "tf.contrib.graph_editor", "pos": [221, 231]}, {"sent_id": 10, "name": "tf.contrib.graph_editor", "pos": [253, 263]}], [{"sent_id": 3, "name": "tf.placeholder_with_default", "pos": [79, 88]}], [{"sent_id": 10, "name": "tf.contrib.graph_editor.swap_inputs", "pos": [253, 267]}]], "sents": ["There is no straightforward way of doing something like that.", "In general, the computation graph can be augmented with new operations, but the existing nodes cannot be modified.", "There are three possible paths you can follow:", "The easiest thing would be to leave the dropout layer as it is, and simply pass a constant keep_prob or 1 (for example using a tf.placeholder_with_default).", "You will still have some minor overhead (I think, I don't know if the implementation of dropout bypasses the operation with a probability of 1), but it will probably be unnoticeable..", "Make a copy of the graph in another tf.Graph object without the dropout layers and then copy the variable values from the session in the first one to a session in the new one (e.g.", "with load())..", "Actually edit the graph.", "Although not its main intended usage, it is possible to edit the graph to some extent.", "The model tf.contrib.graph_editor implements a number of operations to that end.", "In your case, you are probably looking for something like tf.contrib.graph_editor.swap_inputs.", "The drawback here is that these operations must be done \"offline\", that is, with no active sessions using the graph.", "That means that variable values would in principle not be saved.", "You can either checkpoint the model, manually save the variable values to NumPy arrays and restore them after the graph is modified or, if you are done training and only intend to use your model for inference, you can also freeze the graph.", "In any case, you have to take care of it.."], "sent_idxs": [101, 2045, 2003, 2053, 19647, 2126, 1997, 2725, 2242, 2066, 2008, 1012, 102, 101, 1999, 2236, 1010, 1996, 22334, 10629, 2064, 2022, 19335, 2007, 2047, 3136, 1010, 2021, 1996, 4493, 14164, 3685, 2022, 6310, 1012, 102, 101, 2045, 2024, 2093, 2825, 10425, 2017, 2064, 3582, 1024, 102, 101, 1996, 25551, 2518, 2052, 2022, 2000, 2681, 1996, 4530, 5833, 6741, 2004, 2009, 2003, 1010, 1998, 3432, 3413, 1037, 5377, 2562, 1035, 4013, 2497, 2030, 1015, 1006, 2005, 2742, 2478, 1037, 1056, 2546, 1012, 2173, 14528, 1035, 2007, 1035, 12398, 1007, 1012, 102, 101, 2017, 2097, 2145, 2031, 2070, 3576, 8964, 1006, 1045, 2228, 1010, 1045, 2123, 1005, 1056, 2113, 2065, 1996, 7375, 1997, 4530, 5833, 11826, 2229, 1996, 3169, 2007, 1037, 9723, 1997, 1015, 1007, 1010, 2021, 2009, 2097, 2763, 2022, 4895, 17048, 6610, 3085, 1012, 1012, 102, 101, 2191, 1037, 6100, 1997, 1996, 10629, 1999, 2178, 1056, 2546, 1012, 10629, 4874, 2302, 1996, 4530, 5833, 9014, 1998, 2059, 6100, 1996, 8023, 5300, 2013, 1996, 5219, 1999, 1996, 2034, 2028, 2000, 1037, 5219, 1999, 1996, 2047, 2028, 1006, 1041, 1012, 1043, 1012, 102, 101, 2007, 7170, 1006, 1007, 1007, 1012, 1012, 102, 101, 2941, 10086, 1996, 10629, 1012, 102, 101, 2348, 2025, 2049, 2364, 3832, 8192, 1010, 2009, 2003, 2825, 2000, 10086, 1996, 10629, 2000, 2070, 6698, 1012, 102, 101, 1996, 2944, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 10629, 1035, 3559, 22164, 1037, 2193, 1997, 3136, 2000, 2008, 2203, 1012, 102, 101, 1999, 2115, 2553, 1010, 2017, 2024, 2763, 2559, 2005, 2242, 2066, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 10629, 1035, 3559, 1012, 19948, 1035, 20407, 1012, 102, 101, 1996, 4009, 5963, 2182, 2003, 2008, 2122, 3136, 2442, 2022, 2589, 1000, 2125, 4179, 1000, 1010, 2008, 2003, 1010, 2007, 2053, 3161, 6521, 2478, 1996, 10629, 1012, 102, 101, 2008, 2965, 2008, 8023, 5300, 2052, 1999, 6958, 2025, 2022, 5552, 1012, 102, 101, 2017, 2064, 2593, 26520, 1996, 2944, 1010, 21118, 3828, 1996, 8023, 5300, 2000, 16371, 8737, 2100, 27448, 1998, 9239, 2068, 2044, 1996, 10629, 2003, 6310, 2030, 1010, 2065, 2017, 2024, 2589, 2731, 1998, 2069, 13566, 2000, 2224, 2115, 2944, 2005, 28937, 1010, 2017, 2064, 2036, 13184, 1996, 10629, 1012, 102, 101, 1999, 2151, 2553, 1010, 2017, 2031, 2000, 2202, 2729, 1997, 2009, 1012, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 13, 36, 47, 91, 137, 182, 191, 198, 218, 241, 269, 298, 312, 363, 378], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61386745", "vertexSet": [[{"sent_id": 6, "name": "tf.keras", "pos": [184, 189]}], [{"sent_id": 6, "name": "tf.keras.optimizers", "pos": [184, 193]}], [{"sent_id": 6, "name": "tf.keras.optimizers.adam", "pos": [184, 195]}]], "sents": ["The main problem is in this line: ids = inputs[0][1].", "Actually, the ids are the first element of inputs[0]; so it should be ids = inputs[0][0].", "But there is also another problem which might result in inconsistent validation accuracy: you should fit the LabelEncoder only one time to construct the label mapping; so you should use the transform method, instead of fit_transform, on validation labels.", "Another point is that you might need to use a lower learning rate for the optimizer.", "The default learning rate of Adam optimizer is 1e-3, which might be too high considering that you are fine-tuning a pretrained model.", "Try a lower learning rate, say 1e-4 or 1e-5; e.g.", "tf.keras.optimizers.Adam(learning_rate=1e-4).", "A high learning rate for fine-tuning a pretrained model might destroy the learned weights and disrupts fine-tuning process (due to the large gradient values generated, especially at the start of fine-tuning process)."], "sent_idxs": [101, 1996, 2364, 3291, 2003, 1999, 2023, 2240, 1024, 8909, 2015, 1027, 20407, 1031, 1014, 1033, 1031, 1015, 1033, 1012, 102, 101, 2941, 1010, 1996, 8909, 2015, 2024, 1996, 2034, 5783, 1997, 20407, 1031, 1014, 1033, 1025, 2061, 2009, 2323, 2022, 8909, 2015, 1027, 20407, 1031, 1014, 1033, 1031, 1014, 1033, 1012, 102, 101, 2021, 2045, 2003, 2036, 2178, 3291, 2029, 2453, 2765, 1999, 20316, 27354, 10640, 1024, 2017, 2323, 4906, 1996, 3830, 2368, 16044, 2099, 2069, 2028, 2051, 2000, 9570, 1996, 3830, 12375, 1025, 2061, 2017, 2323, 2224, 1996, 10938, 4118, 1010, 2612, 1997, 4906, 1035, 10938, 1010, 2006, 27354, 10873, 1012, 102, 101, 2178, 2391, 2003, 2008, 2017, 2453, 2342, 2000, 2224, 1037, 2896, 4083, 3446, 2005, 1996, 23569, 27605, 6290, 1012, 102, 101, 1996, 12398, 4083, 3446, 1997, 4205, 23569, 27605, 6290, 2003, 1015, 2063, 1011, 1017, 1010, 2029, 2453, 2022, 2205, 2152, 6195, 2008, 2017, 2024, 2986, 1011, 17372, 1037, 3653, 23654, 2098, 2944, 1012, 102, 101, 3046, 1037, 2896, 4083, 3446, 1010, 2360, 1015, 2063, 1011, 1018, 2030, 1015, 2063, 1011, 1019, 1025, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 23569, 27605, 16750, 1012, 4205, 1006, 4083, 1035, 3446, 1027, 1015, 2063, 1011, 1018, 1007, 1012, 102, 101, 1037, 2152, 4083, 3446, 2005, 2986, 1011, 17372, 1037, 3653, 23654, 2098, 2944, 2453, 6033, 1996, 4342, 15871, 1998, 23217, 2015, 2986, 1011, 17372, 2832, 1006, 2349, 2000, 1996, 2312, 17978, 5300, 7013, 1010, 2926, 2012, 1996, 2707, 1997, 2986, 1011, 17372, 2832, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 53, 104, 125, 160, 183, 207, 254], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47179468", "vertexSet": [[{"sent_id": 2, "name": "tf.get_variable", "pos": [76, 82]}], [{"sent_id": 5, "name": "tf.variable_scope", "pos": [134, 140]}], [{"sent_id": 7, "name": "tf.fixed_size_partitioner", "pos": [175, 184]}]], "sents": ["Huge tensorflow variables can be sharded across several machines (see this discussion).", "Partitioner is the mechanism, through which tensorflow distributes and assembles the tensors back, so that the rest of the program doesn't know these implementation details and works with tensors the usual way.", "You can specify the partitioner per one variable via tf.get_variable:", "If a partitioner is provided, a PartitionedVariable is returned.", "Accessing this object as a Tensor returns the shards concatenated along the partition axis.", "Or you define the default partitioner for the whole scope via tf.variable_scope, which will affect all variables defined in it.", "See the list of available partitioners in tensorflow 1.3 on this page.", "The simplest one is tf.fixed_size_partitioner, which shards the tensor along the specified axis.", "Here's an example usage (from this question):", "<code>Code Snippet</code>."], "sent_idxs": [101, 4121, 23435, 12314, 10857, 2064, 2022, 21146, 25547, 2094, 2408, 2195, 6681, 1006, 2156, 2023, 6594, 1007, 1012, 102, 101, 13571, 2121, 2003, 1996, 7337, 1010, 2083, 2029, 23435, 12314, 16062, 2015, 1998, 21365, 2015, 1996, 23435, 2015, 2067, 1010, 2061, 2008, 1996, 2717, 1997, 1996, 2565, 2987, 1005, 1056, 2113, 2122, 7375, 4751, 1998, 2573, 2007, 23435, 2015, 1996, 5156, 2126, 1012, 102, 101, 2017, 2064, 20648, 1996, 13571, 2121, 2566, 2028, 8023, 3081, 1056, 2546, 1012, 2131, 1035, 8023, 1024, 102, 101, 2065, 1037, 13571, 2121, 2003, 3024, 1010, 1037, 13571, 2098, 10755, 19210, 2003, 2513, 1012, 102, 101, 3229, 2075, 2023, 4874, 2004, 1037, 23435, 5651, 1996, 23327, 9530, 16280, 23854, 2247, 1996, 13571, 8123, 1012, 102, 101, 2030, 2017, 9375, 1996, 12398, 13571, 2121, 2005, 1996, 2878, 9531, 3081, 1056, 2546, 1012, 8023, 1035, 9531, 1010, 2029, 2097, 7461, 2035, 10857, 4225, 1999, 2009, 1012, 102, 101, 2156, 1996, 2862, 1997, 2800, 13571, 2545, 1999, 23435, 12314, 1015, 1012, 1017, 2006, 2023, 3931, 1012, 102, 101, 1996, 21304, 2028, 2003, 1056, 2546, 1012, 4964, 1035, 2946, 1035, 13571, 2121, 1010, 2029, 23327, 1996, 23435, 2247, 1996, 9675, 8123, 1012, 102, 101, 2182, 1005, 1055, 2019, 2742, 8192, 1006, 2013, 2023, 3160, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 65, 84, 101, 121, 151, 170, 195, 209, 223], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59443661", "vertexSet": [[{"sent_id": 3, "name": "tf.keras", "pos": [25, 30]}], [{"sent_id": 3, "name": "tf.keras.models", "pos": [25, 32]}], [{"sent_id": 3, "name": "tf.keras.models.load_model", "pos": [25, 36]}]], "sents": ["Try to replace", "<code>Code Snippet</code>.", "with", "model = tf.keras.models.load_model(model_path)", "It works for me, and I am using:\ntensorflow version: 2.0.0\nkeras version: 2.3.1", "You can check the following:\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/load_model?version=stable"], "sent_idxs": [101, 3046, 2000, 5672, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2007, 102, 101, 2944, 1027, 1056, 2546, 1012, 17710, 8180, 1012, 4275, 1012, 7170, 1035, 2944, 1006, 2944, 1035, 4130, 1007, 102, 101, 2009, 2573, 2005, 2033, 1010, 1998, 1045, 2572, 2478, 1024, 23435, 12314, 2544, 1024, 1016, 1012, 1014, 1012, 1014, 17710, 8180, 2544, 1024, 1016, 1012, 1017, 1012, 1015, 102, 101, 2017, 2064, 4638, 1996, 2206, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 17710, 8180, 1013, 4275, 1013, 7170, 1035, 2944, 1029, 2544, 1027, 6540, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 5, 19, 22, 42, 72, 113], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61630018", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [21, 26]}, {"sent_id": 2, "name": "tf.keras", "pos": [57, 62]}, {"sent_id": 3, "name": "tf.keras", "pos": [115, 120]}], [{"sent_id": 3, "name": "tf.train", "pos": [102, 106]}], [{"sent_id": 2, "name": "tf.losses", "pos": [43, 47]}], [{"sent_id": 2, "name": "tf.keras.losses", "pos": [57, 64]}], [{"sent_id": 3, "name": "tf.keras.optimizers", "pos": [115, 124]}], [{"sent_id": 3, "name": "tf.keras.optimizers.sgd", "pos": [115, 127]}], [{"sent_id": 3, "name": "tf.train.momentumoptimizer", "pos": [102, 112]}], [{"sent_id": 2, "name": "tf.losses.softmax_cross_entropy", "pos": [43, 54]}], [{"sent_id": 2, "name": "tf.keras.losses.categoricalcrossentropy", "pos": [57, 71]}]], "sents": ["For converting from TF1 to TF2  code, you can do it easily by using the tf.keras api in TF2.", "For example the functions your provided.", "tf.losses.softmax_cross_entropy() to\ntf.keras.losses.CategoricalCrossentropy(from_logits=True)\nfrom_logits parameter here specifies whether y_pred is expected to be a logits tensor.", "tf.train.MomentumOptimizer() to tf.keras.optimizers.SGD(momentum=...)\nby providing the momentum parameter.", "As for the final function its still being used in the TF2.0 documentation so maybe there isn't an equivalent version in 2.0.", "You can check the this guide from the TF2.0 website whic would provide a great refrence for you.", "Migrate your TensorFlow 1 code to TensorFlow 2"], "sent_idxs": [101, 2005, 16401, 2013, 1056, 2546, 2487, 2000, 1056, 2546, 2475, 3642, 1010, 2017, 2064, 2079, 2009, 4089, 2011, 2478, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 1999, 1056, 2546, 2475, 1012, 102, 101, 2005, 2742, 1996, 4972, 2115, 3024, 1012, 102, 101, 1056, 2546, 1012, 6409, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1006, 1007, 2000, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 4937, 27203, 16458, 4765, 18981, 2100, 1006, 2013, 1035, 8833, 12762, 1027, 2995, 1007, 2013, 1035, 8833, 12762, 16381, 2182, 27171, 3251, 1061, 1035, 3653, 2094, 2003, 3517, 2000, 2022, 1037, 8833, 12762, 23435, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 11071, 7361, 3775, 4328, 6290, 1006, 1007, 2000, 1056, 2546, 1012, 17710, 8180, 1012, 23569, 27605, 16750, 1012, 22214, 2094, 1006, 11071, 1027, 1012, 1012, 1012, 1007, 2011, 4346, 1996, 11071, 16381, 1012, 102, 101, 2004, 2005, 1996, 2345, 3853, 2049, 2145, 2108, 2109, 1999, 1996, 1056, 2546, 2475, 1012, 1014, 12653, 2061, 2672, 2045, 3475, 1005, 1056, 2019, 5662, 2544, 1999, 1016, 1012, 1014, 1012, 102, 101, 2017, 2064, 4638, 1996, 2023, 5009, 2013, 1996, 1056, 2546, 2475, 1012, 1014, 4037, 1059, 16066, 2052, 3073, 1037, 2307, 25416, 24413, 2005, 2017, 1012, 102, 101, 22806, 2115, 23435, 12314, 1015, 3642, 2000, 23435, 12314, 1016, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 8], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [5, 8], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [6, 8], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 8], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7]], "sent_ends": [0, 33, 42, 101, 141, 174, 201, 213], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49299942", "vertexSet": [[{"sent_id": 0, "name": "tf.layers", "pos": [4, 8]}, {"sent_id": 3, "name": "tf.layers", "pos": [60, 64]}], [{"sent_id": 3, "name": "tf.layers.conv1d", "pos": [60, 69]}], [{"sent_id": 0, "name": "tf.layers.max_pooling1d", "pos": [4, 15]}]], "sents": ["You can try tf.layers.max_pooling1d.", "It supports 3-rank inputs which has the format of (batch, length, channels).", "Or you can specify your own format with data_format parameter.", "And it supports the output of tf.layers.conv1d."], "sent_idxs": [101, 2017, 2064, 3046, 1056, 2546, 1012, 9014, 1012, 4098, 1035, 4770, 2075, 2487, 2094, 1012, 102, 101, 2009, 6753, 1017, 1011, 4635, 20407, 2029, 2038, 1996, 4289, 1997, 1006, 14108, 1010, 3091, 1010, 6833, 1007, 1012, 102, 101, 2030, 2017, 2064, 20648, 2115, 2219, 4289, 2007, 2951, 1035, 4289, 16381, 1012, 102, 101, 1998, 2009, 6753, 1996, 6434, 1997, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 17, 38, 53, 71], "sent_pos": [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "59613938", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [58, 63]}, {"sent_id": 2, "name": "tf.keras", "pos": [106, 111]}], [{"sent_id": 2, "name": "tf.keras.losses", "pos": [58, 65]}, {"sent_id": 2, "name": "tf.keras.losses", "pos": [106, 113]}], [{"sent_id": 2, "name": "tf.keras.losses.binarycrossentropy", "pos": [58, 71]}], [{"sent_id": 2, "name": "tf.keras.losses.binary_crossentropy", "pos": [106, 120]}]], "sents": ["Thanks, I find the reasons of the inconsistent accuracy:", "The shape of outputs in the model is (None, 1), but the feeded label is (None, ), which cause a wrong meaning with python's broadcast mechanism.", "In the source code of tf.keras.losses.BinaryCrossentropy(), while calculating the loss, both y_pred and y_true are processed through a function called squeeze_or_expand_dimensions, which is lacked in tf.keras.losses.binary_crossentropy.", "Note: Take care that whether the shape is consistent between input data and model outputs."], "sent_idxs": [101, 4283, 1010, 1045, 2424, 1996, 4436, 1997, 1996, 20316, 10640, 1024, 102, 101, 1996, 4338, 1997, 27852, 1999, 1996, 2944, 2003, 1006, 3904, 1010, 1015, 1007, 1010, 2021, 1996, 5438, 2098, 3830, 2003, 1006, 3904, 1010, 1007, 1010, 2029, 3426, 1037, 3308, 3574, 2007, 18750, 1005, 1055, 3743, 7337, 1012, 102, 101, 1999, 1996, 3120, 3642, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 12441, 16458, 4765, 18981, 2100, 1006, 1007, 1010, 2096, 20177, 1996, 3279, 1010, 2119, 1061, 1035, 3653, 2094, 1998, 1061, 1035, 2995, 2024, 13995, 2083, 1037, 3853, 2170, 11025, 1035, 2030, 1035, 7818, 1035, 9646, 1010, 2029, 2003, 10858, 1999, 1056, 2546, 1012, 17710, 8180, 1012, 6409, 1012, 12441, 1035, 2892, 4765, 18981, 2100, 1012, 102, 101, 3602, 1024, 2202, 2729, 2008, 3251, 1996, 4338, 2003, 8335, 2090, 7953, 2951, 1998, 2944, 27852, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 13, 52, 122, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62503826", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [1, 6]}], [{"sent_id": 0, "name": "tf.keras.utils", "pos": [1, 9]}], [{"sent_id": 0, "name": "tf.keras.utils.normalize", "pos": [1, 12]}]], "sents": ["tf.keras.utils.normalize uses the algorithm described here., so it just makes the data along the specified axis a unit vector with respect to your favorite lp norm.", "Whether this is preferable to sklearn.StandardScaler() depends on the problem.", "For many time series problems, you want to detrend them, so make the mean 0., so StandardScaler is appropriate.", "If you want the inputs reasonably similarly scaled, both methods are equivalent, more or less."], "sent_idxs": [101, 1056, 2546, 1012, 17710, 8180, 1012, 21183, 12146, 1012, 3671, 4697, 3594, 1996, 9896, 2649, 2182, 1012, 1010, 2061, 2009, 2074, 3084, 1996, 2951, 2247, 1996, 9675, 8123, 1037, 3131, 9207, 2007, 4847, 2000, 2115, 5440, 6948, 13373, 1012, 102, 101, 3251, 2023, 2003, 9544, 3085, 2000, 15315, 19738, 6826, 1012, 4781, 9289, 2121, 1006, 1007, 9041, 2006, 1996, 3291, 1012, 102, 101, 2005, 2116, 2051, 2186, 3471, 1010, 2017, 2215, 2000, 20010, 7389, 2094, 2068, 1010, 2061, 2191, 1996, 2812, 1014, 1012, 1010, 2061, 4781, 9289, 2121, 2003, 6413, 1012, 102, 101, 2065, 2017, 2215, 1996, 20407, 16286, 6660, 18953, 1010, 2119, 4725, 2024, 5662, 1010, 2062, 2030, 2625, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 41, 63, 93, 113], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59979830", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [7, 12]}], [{"sent_id": 0, "name": "tf.keras.backend", "pos": [7, 15]}], [{"sent_id": 0, "name": "tf.keras.backend.print_tensor", "pos": [7, 19]}]], "sents": ["I could execute the output of tf.keras.backend.print_tensor successfully in Google Colab with Tensorflow Version 2.1.", "Mentioned below is the code:", "<code>Code Snippet</code>.", "Output is shown below:", "<code>Code Snippet</code>.", "Here is the Link of the Google Colab Gist."], "sent_idxs": [101, 1045, 2071, 15389, 1996, 6434, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 6140, 1035, 23435, 5147, 1999, 8224, 15270, 2497, 2007, 23435, 12314, 2544, 1016, 1012, 1015, 1012, 102, 101, 3855, 2917, 2003, 1996, 3642, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 2003, 3491, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2182, 2003, 1996, 4957, 1997, 1996, 8224, 15270, 2497, 21025, 3367, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 41, 55, 62, 76, 90], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51159374", "vertexSet": [[{"sent_id": 3, "name": "tf.nn", "pos": [80, 85]}, {"sent_id": 5, "name": "tf.nn", "pos": [175, 180]}], [{"sent_id": 4, "name": "tf.log", "pos": [160, 164]}], [{"sent_id": 1, "name": "tf.cast", "pos": [33, 37]}], [{"sent_id": 3, "name": "tf.losses", "pos": [101, 105]}], [{"sent_id": 3, "name": "tf.nn.sigmoid", "pos": [80, 89]}, {"sent_id": 5, "name": "tf.nn.sigmoid", "pos": [175, 184]}], [{"sent_id": 3, "name": "tf.losses.sigmoid_cross_entropy", "pos": [101, 113]}], [{"sent_id": 3, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [80, 98]}]], "sents": ["This error likely comes from trying to multiply integer-type labels with float-type logits.", "You can explicity cast the labels to float via tf.cast(labels, dtype=tf.float32).", "Unfortunately your question does not reveal whether you tried this specific casting.", "However, for reasons of numerical stability I would advise you to use tf.nn.sigmoid_cross_entropy_with_logits instead (or tf.losses.sigmoid_cross_entropy).", "This is also a good idea for correctness; cross-entropy uses log-probabilities, but you are already putting in logits (which are log unnormalized probabilities) so the extra tf.log is actually wrong.", "You could also add a tf.nn.sigmoid activation to the output layer to make it correct, however the built-in functions are still preferrable for stability."], "sent_idxs": [101, 2023, 7561, 3497, 3310, 2013, 2667, 2000, 4800, 22086, 16109, 1011, 2828, 10873, 2007, 14257, 1011, 2828, 8833, 12762, 1012, 102, 101, 2017, 2064, 13216, 2100, 3459, 1996, 10873, 2000, 14257, 3081, 1056, 2546, 1012, 3459, 1006, 10873, 1010, 26718, 18863, 1027, 1056, 2546, 1012, 14257, 16703, 1007, 1012, 102, 101, 6854, 2115, 3160, 2515, 2025, 7487, 3251, 2017, 2699, 2023, 3563, 9179, 1012, 102, 101, 2174, 1010, 2005, 4436, 1997, 15973, 9211, 1045, 2052, 18012, 2017, 2000, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2612, 1006, 2030, 1056, 2546, 1012, 6409, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1007, 1012, 102, 101, 2023, 2003, 2036, 1037, 2204, 2801, 2005, 6149, 2791, 1025, 2892, 1011, 23077, 3594, 8833, 1011, 4013, 3676, 14680, 1010, 2021, 2017, 2024, 2525, 5128, 1999, 8833, 12762, 1006, 2029, 2024, 8833, 4895, 12131, 9067, 3550, 4013, 3676, 14680, 1007, 2061, 1996, 4469, 1056, 2546, 1012, 8833, 2003, 2941, 3308, 1012, 102, 101, 2017, 2071, 2036, 5587, 1037, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 13791, 2000, 1996, 6434, 6741, 2000, 2191, 2009, 6149, 1010, 2174, 1996, 2328, 1011, 1999, 4972, 2024, 2145, 9544, 16670, 2005, 9211, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 22, 51, 66, 116, 169, 208], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37382135", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [6, 11]}, {"sent_id": 0, "name": "tf.nn", "pos": [17, 22]}], [{"sent_id": 0, "name": "tf.nn.lrn", "pos": [6, 14]}], [{"sent_id": 0, "name": "tf.nn.local_response_normalization", "pos": [17, 29]}]], "sents": ["As nessuno mentioned, tf.nn.lrn is short for tf.nn.local_response_normalization (documentation)", "Further, this question provides good resources for more information into response normalization layers.", "From: http://caffe.berkeleyvision.org/tutorial/layers.html#data-layers", "\"The local response normalization layer performs a kind of \u201clateral inhibition\u201d by normalizing over local input regions.", "In ACROSS_CHANNELS mode, the local regions extend across nearby channels, but have no spatial extent (i.e., they have shape local_size x 1 x 1).", "In WITHIN_CHANNEL mode, the local regions extend spatially, but are in separate channels (i.e., they have shape 1 x local_size x local_size).", "Each input value is divided by (1+(\u03b1/n)\u2211ix2i)\u03b2, where n is the size of each local region, and the sum is taken over the region centered at that value (zero padding is added where necessary).\"", "These layers have fallen out of favor because they had very little impact on results, and other techniques proved to be more beneficial."], "sent_idxs": [101, 2004, 23384, 27819, 3855, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 1048, 6826, 2003, 2460, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 2334, 1035, 3433, 1035, 3671, 3989, 1006, 12653, 1007, 102, 101, 2582, 1010, 2023, 3160, 3640, 2204, 4219, 2005, 2062, 2592, 2046, 3433, 3671, 3989, 9014, 1012, 102, 101, 2013, 1024, 8299, 1024, 1013, 1013, 24689, 7959, 1012, 8256, 17084, 1012, 8917, 1013, 14924, 4818, 1013, 9014, 1012, 16129, 1001, 2951, 1011, 9014, 102, 101, 1000, 1996, 2334, 3433, 3671, 3989, 6741, 10438, 1037, 2785, 1997, 1523, 11457, 23586, 1524, 2011, 3671, 6026, 2058, 2334, 7953, 4655, 1012, 102, 101, 1999, 2408, 1035, 6833, 5549, 1010, 1996, 2334, 4655, 7949, 2408, 3518, 6833, 1010, 2021, 2031, 2053, 13589, 6698, 1006, 1045, 1012, 1041, 1012, 1010, 2027, 2031, 4338, 2334, 1035, 2946, 1060, 1015, 1060, 1015, 1007, 1012, 102, 101, 1999, 2306, 1035, 3149, 5549, 1010, 1996, 2334, 4655, 7949, 13589, 2135, 1010, 2021, 2024, 1999, 3584, 6833, 1006, 1045, 1012, 1041, 1012, 1010, 2027, 2031, 4338, 1015, 1060, 2334, 1035, 2946, 1060, 2334, 1035, 2946, 1007, 1012, 102, 101, 2169, 7953, 3643, 2003, 4055, 2011, 1006, 1015, 1009, 1006, 1155, 1013, 1050, 1007, 100, 1007, 1156, 1010, 2073, 1050, 2003, 1996, 2946, 1997, 2169, 2334, 2555, 1010, 1998, 1996, 7680, 2003, 2579, 2058, 1996, 2555, 8857, 2012, 2008, 3643, 1006, 5717, 11687, 4667, 2003, 2794, 2073, 4072, 1007, 1012, 1000, 102, 101, 2122, 9014, 2031, 5357, 2041, 1997, 5684, 2138, 2027, 2018, 2200, 2210, 4254, 2006, 3463, 1010, 1998, 2060, 5461, 4928, 2000, 2022, 2062, 15189, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 33, 51, 77, 102, 141, 181, 234, 261], "sent_pos": [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55680074", "vertexSet": [[{"sent_id": 2, "name": "tf.slice", "pos": [54, 58]}], [{"sent_id": 8, "name": "tf.map_fn", "pos": [188, 195]}], [{"sent_id": 2, "name": "tf.while_loop", "pos": [38, 44]}], [{"sent_id": 2, "name": "tf.tensorarray", "pos": [47, 53]}]], "sents": ["TensorArray object can store tensors of different shapes.", "However, it is still not that simple.", "Take a look at this example that does what you want using tf.while_loop() with tf.TensorArray and tf.slice() function:", "<code>Code Snippet</code>.", "Note", "We still had to use batch_size to extract elements one by one from first_n TensorArray using read() method.", "We can't use any other method that returns Tensor because we have rows of different sizes (except TensorArray.concat method but it will return all elements stacked in one dimension).", "If TensorArray will have less elements than index you pass to TensorArray.read(index) you will get InvalidArgumentError.", "You can't use tf.map_fn because it returns a tensor that must have all elements of the same shape.", "The task is simpler if you only need to compute variances of the first n elements of each row (without actually gather elements of different sizes together).", "In this case we could directly compute variance of sliced tensor, put it to TensorArray and then stack it to tensor:", "<code>Code Snippet</code>."], "sent_idxs": [101, 23435, 2906, 9447, 4874, 2064, 3573, 23435, 2015, 1997, 2367, 10466, 1012, 102, 101, 2174, 1010, 2009, 2003, 2145, 2025, 2008, 3722, 1012, 102, 101, 2202, 1037, 2298, 2012, 2023, 2742, 2008, 2515, 2054, 2017, 2215, 2478, 1056, 2546, 1012, 2096, 1035, 7077, 1006, 1007, 2007, 1056, 2546, 1012, 23435, 2906, 9447, 1998, 1056, 2546, 1012, 14704, 1006, 1007, 3853, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 102, 101, 2057, 2145, 2018, 2000, 2224, 14108, 1035, 2946, 2000, 14817, 3787, 2028, 2011, 2028, 2013, 2034, 1035, 1050, 23435, 2906, 9447, 2478, 3191, 1006, 1007, 4118, 1012, 102, 101, 2057, 2064, 1005, 1056, 2224, 2151, 2060, 4118, 2008, 5651, 23435, 2138, 2057, 2031, 10281, 1997, 2367, 10826, 1006, 3272, 23435, 2906, 9447, 1012, 9530, 11266, 4118, 2021, 2009, 2097, 2709, 2035, 3787, 16934, 1999, 2028, 9812, 1007, 1012, 102, 101, 2065, 23435, 2906, 9447, 2097, 2031, 2625, 3787, 2084, 5950, 2017, 3413, 2000, 23435, 2906, 9447, 1012, 3191, 1006, 5950, 1007, 2017, 2097, 2131, 19528, 2906, 22850, 29110, 29165, 1012, 102, 101, 2017, 2064, 1005, 1056, 2224, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 2138, 2009, 5651, 1037, 23435, 2008, 2442, 2031, 2035, 3787, 1997, 1996, 2168, 4338, 1012, 102, 101, 1996, 4708, 2003, 16325, 2065, 2017, 2069, 2342, 2000, 24134, 23284, 2015, 1997, 1996, 2034, 1050, 3787, 1997, 2169, 5216, 1006, 2302, 2941, 8587, 3787, 1997, 2367, 10826, 2362, 1007, 1012, 102, 101, 1999, 2023, 2553, 2057, 2071, 3495, 24134, 23284, 1997, 15920, 23435, 1010, 2404, 2009, 2000, 23435, 2906, 9447, 1998, 2059, 9991, 2009, 2000, 23435, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 14, 25, 63, 77, 80, 109, 150, 182, 211, 244, 271, 285], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51928900", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.bidirectional_dynamic_rnn", "pos": [17, 32]}], [{"sent_id": 2, "name": "tf.contrib.rnn.stack_bidirectional_dynamic_rnn", "pos": [39, 60]}]], "sents": ["As @Taras pointed out, you can use:", "(1) tf.nn.bidirectional_dynamic_rnn()", "(2) tf.contrib.rnn.stack_bidirectional_dynamic_rnn().", "All previous answers only capture (1), so I give some details on (2), in particular since it usually outperforms (1).", "For an intuition about the different connectivities \nsee here.", "Let's say you want to create a stack of 3 BLSTM layers, each with 64 nodes:", "<code>Code Snippet</code>.", "In this case, I use the final states of the stacked biRNN rather than the states at all timesteps (saved in all_states), since I was using an encoding decoding scheme, where the above code was only the encoder."], "sent_idxs": [101, 2004, 1030, 10225, 2015, 4197, 2041, 1010, 2017, 2064, 2224, 1024, 102, 101, 1006, 1015, 1007, 1056, 2546, 1012, 1050, 2078, 1012, 7226, 7442, 7542, 2389, 1035, 8790, 1035, 29300, 2078, 1006, 1007, 102, 101, 1006, 1016, 1007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 9991, 1035, 7226, 7442, 7542, 2389, 1035, 8790, 1035, 29300, 2078, 1006, 1007, 1012, 102, 101, 2035, 3025, 6998, 2069, 5425, 1006, 1015, 1007, 1010, 2061, 1045, 2507, 2070, 4751, 2006, 1006, 1016, 1007, 1010, 1999, 3327, 2144, 2009, 2788, 2041, 4842, 22694, 1006, 1015, 1007, 1012, 102, 101, 2005, 2019, 26406, 2055, 1996, 2367, 7532, 12848, 6447, 2156, 2182, 1012, 102, 101, 2292, 1005, 1055, 2360, 2017, 2215, 2000, 3443, 1037, 9991, 1997, 1017, 1038, 4877, 21246, 9014, 1010, 2169, 2007, 4185, 14164, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1999, 2023, 2553, 1010, 1045, 2224, 1996, 2345, 2163, 1997, 1996, 16934, 12170, 6826, 2078, 2738, 2084, 1996, 2163, 2012, 2035, 2335, 2618, 4523, 1006, 5552, 1999, 2035, 1035, 2163, 1007, 1010, 2144, 1045, 2001, 2478, 2019, 17181, 21933, 4667, 5679, 1010, 2073, 1996, 2682, 3642, 2001, 2069, 1996, 4372, 16044, 2099, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1, 2, 3]}], "na_triple": [], "sent_ends": [0, 13, 35, 64, 97, 111, 135, 149, 204], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52470287", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [29, 35]}], [{"sent_id": 0, "name": "tf.contrib.data", "pos": [29, 37]}], [{"sent_id": 0, "name": "tf.contrib.data.choose_from_datasets", "pos": [29, 45]}]], "sents": ["I would split the Dataset into two streams by using a filter on each label, then merge those streams back, for example by relying on tf.contrib.data.choose_from_datasets.", "An advantage is that you don't loose any sample in the process (as opposed to the example you give).", "A small example with toy data:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2052, 3975, 1996, 2951, 13462, 2046, 2048, 9199, 2011, 2478, 1037, 11307, 2006, 2169, 3830, 1010, 2059, 13590, 2216, 9199, 2067, 1010, 2005, 2742, 2011, 18345, 2006, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 5454, 1035, 2013, 1035, 2951, 13462, 2015, 1012, 102, 101, 2019, 5056, 2003, 2008, 2017, 2123, 1005, 1056, 6065, 2151, 7099, 1999, 1996, 2832, 1006, 2004, 4941, 2000, 1996, 2742, 2017, 2507, 1007, 1012, 102, 101, 1037, 2235, 2742, 2007, 9121, 2951, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 47, 73, 82, 96], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37915182", "vertexSet": [[{"sent_id": 0, "name": "tf.summary", "pos": [5, 9]}, {"sent_id": 0, "name": "tf.summary", "pos": [20, 24]}, {"sent_id": 1, "name": "tf.summary", "pos": [53, 57]}, {"sent_id": 2, "name": "tf.summary", "pos": [79, 83]}], [{"sent_id": 2, "name": "tf.summary.value", "pos": [79, 85]}], [{"sent_id": 0, "name": "tf.summary.filewriter", "pos": [20, 27]}]], "sents": ["You can create a tf.Summary object in your Python program and write it to the same tf.summary.FileWriter object that takes your TensorFlow-produced summaries using the SummaryWriter.add_summary() method.", "The tf.Summary class is a Python protocol buffer wrapper for the Summary protocol buffer.", "Each Summary contains a list of tf.Summary.Value protocol buffers, which each have a tag and a either a \"simple\" (floating-point scalar) value, an image, a histogram, or an audio snippet.", "For example, you can generate a scalar summary from a Python object as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 3443, 1037, 1056, 2546, 1012, 12654, 4874, 1999, 2115, 18750, 2565, 1998, 4339, 2009, 2000, 1996, 2168, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 4874, 2008, 3138, 2115, 23435, 12314, 1011, 2550, 7680, 7849, 3111, 2478, 1996, 12654, 15994, 1012, 5587, 1035, 12654, 1006, 1007, 4118, 1012, 102, 101, 1996, 1056, 2546, 1012, 12654, 2465, 2003, 1037, 18750, 8778, 17698, 10236, 4842, 2005, 1996, 12654, 8778, 17698, 1012, 102, 101, 2169, 12654, 3397, 1037, 2862, 1997, 1056, 2546, 1012, 12654, 1012, 3643, 8778, 17698, 2015, 1010, 2029, 2169, 2031, 1037, 6415, 1998, 1037, 2593, 1037, 1000, 3722, 1000, 1006, 8274, 1011, 2391, 26743, 2099, 1007, 3643, 1010, 2019, 3746, 1010, 1037, 2010, 3406, 13113, 1010, 2030, 2019, 5746, 1055, 3490, 29519, 1012, 102, 101, 2005, 2742, 1010, 2017, 2064, 9699, 1037, 26743, 2099, 12654, 2013, 1037, 18750, 4874, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 51, 72, 126, 145, 159], "sent_pos": [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48101633", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [39, 44]}], [{"sent_id": 2, "name": "tf.estimator", "pos": [79, 85]}], [{"sent_id": 1, "name": "tf.nn.dropout", "pos": [39, 47]}], [{"sent_id": 4, "name": "tf.set_random_seed", "pos": [129, 137]}]], "sents": ["I don't see any way to access the session inside model_fn to get the current value of global_step.", "Even if it was possible, changing the seed for tf.nn.dropout at each step would create a new Graph operation at each step with a different seed, which will make the graph grow bigger and bigger.", "Even without a tf.estimator, I don't know how you can implement this?", "I think what you want is to make sure that you get the same randomness between two runs.", "Setting a graph-level random seed with tf.set_random_seed() or just using a normal seed in the dropout should create a reproducible sequence of masks.", "Here is an example with code:", "<code>Code Snippet</code>.", "The answer here provides more details on how to make the graph randomness reproducible."], "sent_idxs": [101, 1045, 2123, 1005, 1056, 2156, 2151, 2126, 2000, 3229, 1996, 5219, 2503, 2944, 1035, 1042, 2078, 2000, 2131, 1996, 2783, 3643, 1997, 3795, 1035, 3357, 1012, 102, 101, 2130, 2065, 2009, 2001, 2825, 1010, 5278, 1996, 6534, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2012, 2169, 3357, 2052, 3443, 1037, 2047, 10629, 3169, 2012, 2169, 3357, 2007, 1037, 2367, 6534, 1010, 2029, 2097, 2191, 1996, 10629, 4982, 7046, 1998, 7046, 1012, 102, 101, 2130, 2302, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1010, 1045, 2123, 1005, 1056, 2113, 2129, 2017, 2064, 10408, 2023, 1029, 102, 101, 1045, 2228, 2054, 2017, 2215, 2003, 2000, 2191, 2469, 2008, 2017, 2131, 1996, 2168, 6721, 2791, 2090, 2048, 3216, 1012, 102, 101, 4292, 1037, 10629, 1011, 2504, 6721, 6534, 2007, 1056, 2546, 1012, 2275, 1035, 6721, 1035, 6534, 1006, 1007, 2030, 2074, 2478, 1037, 3671, 6534, 1999, 1996, 4530, 5833, 2323, 3443, 1037, 16360, 14127, 21104, 5537, 1997, 15806, 1012, 102, 101, 2182, 2003, 2019, 2742, 2007, 3642, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3437, 2182, 3640, 2062, 4751, 2006, 2129, 2000, 2191, 1996, 10629, 6721, 2791, 16360, 14127, 21104, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 28, 75, 98, 120, 160, 169, 183, 203], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40649185", "vertexSet": [[{"sent_id": 1, "name": "tf.variable", "pos": [34, 38]}, {"sent_id": 11, "name": "tf.variable", "pos": [253, 257]}], [{"sent_id": 2, "name": "tf.placeholder", "pos": [70, 75]}, {"sent_id": 5, "name": "tf.placeholder", "pos": [133, 138]}, {"sent_id": 11, "name": "tf.placeholder", "pos": [258, 263]}], [{"sent_id": 1, "name": "tf.train", "pos": [23, 27]}], [{"sent_id": 7, "name": "tf.gradients", "pos": [186, 191]}], [{"sent_id": 1, "name": "tf.train.optimizer", "pos": [23, 31]}]], "sents": ["As you've noticed, TensorFlow optimizers (i.e.", "subclasses of tf.train.Optimizer) operate on tf.Variable objects because they need to be able to assign new values to those objects, and in TensorFlow only variables support an assign operation.", "If you use a tf.placeholder(), there's nothing to update, because the value of a placeholder is immutable within each step.", "So how do you optimize with respect to a fed-in value?", "I can think of two options:", "Instead of feeding a tf.placeholder(), you could first assign a fed-in value to a variable and then optimize with respect to it:", "<code>Code Snippet</code>.", "You could use the lower-level tf.gradients() function to get the gradient of the loss with respect to a placeholder in a single step.", "You could then use that gradient in Python:", "<code>Code Snippet</code>.", "PS.", "The code in your question, where you define a tf.Variable(tf.placeholder(...), ...) is just defining a variable whose initial value is fed by the placeholder.", "This probably isn't what you want, because the training op that the optimizer creates will only use the value assigned to the variable, and ignore whatever you feed to the placeholder (after the initialization step)."], "sent_idxs": [101, 2004, 2017, 1005, 2310, 4384, 1010, 23435, 12314, 23569, 27605, 16750, 1006, 1045, 1012, 1041, 1012, 102, 101, 4942, 26266, 2229, 1997, 1056, 2546, 1012, 3345, 1012, 23569, 27605, 6290, 1007, 5452, 2006, 1056, 2546, 1012, 8023, 5200, 2138, 2027, 2342, 2000, 2022, 2583, 2000, 23911, 2047, 5300, 2000, 2216, 5200, 1010, 1998, 1999, 23435, 12314, 2069, 10857, 2490, 2019, 23911, 3169, 1012, 102, 101, 2065, 2017, 2224, 1037, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 1010, 2045, 1005, 1055, 2498, 2000, 10651, 1010, 2138, 1996, 3643, 1997, 1037, 2173, 14528, 2003, 10047, 28120, 3085, 2306, 2169, 3357, 1012, 102, 101, 2061, 2129, 2079, 2017, 23569, 27605, 4371, 2007, 4847, 2000, 1037, 7349, 1011, 1999, 3643, 1029, 102, 101, 1045, 2064, 2228, 1997, 2048, 7047, 1024, 102, 101, 2612, 1997, 8521, 1037, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 1010, 2017, 2071, 2034, 23911, 1037, 7349, 1011, 1999, 3643, 2000, 1037, 8023, 1998, 2059, 23569, 27605, 4371, 2007, 4847, 2000, 2009, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2071, 2224, 1996, 2896, 1011, 2504, 1056, 2546, 1012, 17978, 2015, 1006, 1007, 3853, 2000, 2131, 1996, 17978, 1997, 1996, 3279, 2007, 4847, 2000, 1037, 2173, 14528, 1999, 1037, 2309, 3357, 1012, 102, 101, 2017, 2071, 2059, 2224, 2008, 17978, 1999, 18750, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 8827, 1012, 102, 101, 1996, 3642, 1999, 2115, 3160, 1010, 2073, 2017, 9375, 1037, 1056, 2546, 1012, 8023, 1006, 1056, 2546, 1012, 2173, 14528, 1006, 1012, 1012, 1012, 1007, 1010, 1012, 1012, 1012, 1007, 2003, 2074, 12854, 1037, 8023, 3005, 3988, 3643, 2003, 7349, 2011, 1996, 2173, 14528, 1012, 102, 101, 2023, 2763, 3475, 1005, 1056, 2054, 2017, 2215, 1010, 2138, 1996, 2731, 6728, 2008, 1996, 23569, 27605, 6290, 9005, 2097, 2069, 2224, 1996, 3643, 4137, 2000, 1996, 8023, 1010, 1998, 8568, 3649, 2017, 5438, 2000, 1996, 2173, 14528, 1006, 2044, 1996, 3988, 3989, 3357, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 18, 65, 101, 119, 128, 164, 178, 213, 224, 238, 242, 289, 337], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54538449", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [13, 19]}], [{"sent_id": 1, "name": "tf.contrib.tpu", "pos": [13, 22]}], [{"sent_id": 1, "name": "tf.contrib.tpu.keras_to_tpu_model", "pos": [13, 32]}]], "sents": ["Import keras from tensorflow.", "This is because tf.contrib.tpu.keras_to_tpu_model( )' requires a tensorflow version Model, not the keras version.", "For example, use from tensorflow.keras.layers import Dense, Activation instead.", "And so on."], "sent_idxs": [101, 12324, 17710, 8180, 2013, 23435, 12314, 1012, 102, 101, 2023, 2003, 2138, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1056, 14289, 1012, 17710, 8180, 1035, 2000, 1035, 1056, 14289, 1035, 2944, 1006, 1007, 1005, 5942, 1037, 23435, 12314, 2544, 2944, 1010, 2025, 1996, 17710, 8180, 2544, 1012, 102, 101, 2005, 2742, 1010, 2224, 2013, 23435, 12314, 1012, 17710, 8180, 1012, 9014, 12324, 9742, 1010, 13791, 2612, 1012, 102, 101, 1998, 2061, 2006, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 49, 69, 75], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55446572", "vertexSet": [[{"sent_id": 1, "name": "tf.zeros", "pos": [19, 24]}], [{"sent_id": 5, "name": "tf.train", "pos": [153, 157]}], [{"sent_id": 1, "name": "tf.variable", "pos": [14, 18]}, {"sent_id": 1, "name": "tf.variable", "pos": [36, 40]}], [{"sent_id": 1, "name": "tf.random_normal", "pos": [41, 47]}], [{"sent_id": 5, "name": "tf.train.gradientdescentoptimizer", "pos": [153, 165]}]], "sents": ["You have 2 things to take into consideration.", "1) tf.Variable(tf.zeros([784, 500])) change this with tf.Variable(tf.random_normal([784, 500])) As it is better to have random initialization of weights rather than defining them as 0 s from the start.", "By making it 0 initially (meaning everything gets the same value) model will follow the same gradient path and will be unable to learn.", "For the start change every zeros with random_normal.", "There are better ways to firstly define variables but this will give you a good start", "2) your learning rate is too high \ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) change this line to", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2031, 1016, 2477, 2000, 2202, 2046, 9584, 1012, 102, 101, 1015, 1007, 1056, 2546, 1012, 8023, 1006, 1056, 2546, 1012, 5717, 2015, 1006, 1031, 6275, 2549, 1010, 3156, 1033, 1007, 1007, 2689, 2023, 2007, 1056, 2546, 1012, 8023, 1006, 1056, 2546, 1012, 6721, 1035, 3671, 1006, 1031, 6275, 2549, 1010, 3156, 1033, 1007, 1007, 2004, 2009, 2003, 2488, 2000, 2031, 6721, 3988, 3989, 1997, 15871, 2738, 2084, 12854, 2068, 2004, 1014, 1055, 2013, 1996, 2707, 1012, 102, 101, 2011, 2437, 2009, 1014, 3322, 1006, 3574, 2673, 4152, 1996, 2168, 3643, 1007, 2944, 2097, 3582, 1996, 2168, 17978, 4130, 1998, 2097, 2022, 4039, 2000, 4553, 1012, 102, 101, 2005, 1996, 2707, 2689, 2296, 5717, 2015, 2007, 6721, 1035, 3671, 1012, 102, 101, 2045, 2024, 2488, 3971, 2000, 15847, 9375, 10857, 2021, 2023, 2097, 2507, 2017, 1037, 2204, 2707, 102, 101, 1016, 1007, 2115, 4083, 3446, 2003, 2205, 2152, 3345, 1035, 3357, 1027, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1006, 1014, 1012, 1019, 1007, 1012, 18478, 1006, 2892, 1035, 23077, 1007, 2689, 2023, 2240, 2000, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 11, 79, 108, 122, 140, 182, 196], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47889528", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [72, 89]}], [{"sent_id": 1, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [25, 43]}]], "sents": ["You are right by defining areas where each of these losses are applicable:", "binary_crossentropy (and tf.nn.sigmoid_cross_entropy_with_logits under the hood) is for binary multi-label classification (labels are independent)..", "categorical_crossentropy (and tf.nn.softmax_cross_entropy_with_logits under the hood) is for multi-class classification (classes are exclusive)..", "See also the detailed analysis in this question.", "I'm not sure what tutorials you mean, so can't comment whether binary_crossentropy is a good or bad choice for autoencoders.", "As for the naming, it is absolutely correct and reasonable.", "Or do you think sigmoid and softmax names sound better?", "So the only confusion left in your question is the categorical_crossentropy documentation.", "Note that everything that has been stated is correct: the loss supports one-hot representation.", "This function indeed works with any probability distribution for labels (in addition to one-hot vectors) in case of tensorflow backend and it could be included into the doc, but this doesn't look critical to me.", "Moreover, need to check if soft classes are supported in other backends, theano and CNTK.", "Remember that keras tries to be minimalistic and targets for most popular use cases, so I can understand the logic here."], "sent_idxs": [101, 2017, 2024, 2157, 2011, 12854, 2752, 2073, 2169, 1997, 2122, 6409, 2024, 12711, 1024, 102, 101, 12441, 1035, 2892, 4765, 18981, 2100, 1006, 1998, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2104, 1996, 7415, 1007, 2003, 2005, 12441, 4800, 1011, 3830, 5579, 1006, 10873, 2024, 2981, 1007, 1012, 1012, 102, 101, 4937, 27203, 1035, 2892, 4765, 18981, 2100, 1006, 1998, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2104, 1996, 7415, 1007, 2003, 2005, 4800, 1011, 2465, 5579, 1006, 4280, 2024, 7262, 1007, 1012, 1012, 102, 101, 2156, 2036, 1996, 6851, 4106, 1999, 2023, 3160, 1012, 102, 101, 1045, 1005, 1049, 2025, 2469, 2054, 14924, 26340, 2017, 2812, 1010, 2061, 2064, 1005, 1056, 7615, 3251, 12441, 1035, 2892, 4765, 18981, 2100, 2003, 1037, 2204, 2030, 2919, 3601, 2005, 8285, 2368, 16044, 2869, 1012, 102, 101, 2004, 2005, 1996, 10324, 1010, 2009, 2003, 7078, 6149, 1998, 9608, 1012, 102, 101, 2030, 2079, 2017, 2228, 9033, 21693, 9314, 1998, 3730, 17848, 3415, 2614, 2488, 1029, 102, 101, 2061, 1996, 2069, 6724, 2187, 1999, 2115, 3160, 2003, 1996, 4937, 27203, 1035, 2892, 4765, 18981, 2100, 12653, 1012, 102, 101, 3602, 2008, 2673, 2008, 2038, 2042, 3090, 2003, 6149, 1024, 1996, 3279, 6753, 2028, 1011, 2980, 6630, 1012, 102, 101, 2023, 3853, 5262, 2573, 2007, 2151, 9723, 4353, 2005, 10873, 1006, 1999, 2804, 2000, 2028, 1011, 2980, 19019, 1007, 1999, 2553, 1997, 23435, 12314, 2067, 10497, 1998, 2009, 2071, 2022, 2443, 2046, 1996, 9986, 1010, 2021, 2023, 2987, 1005, 1056, 2298, 4187, 2000, 2033, 1012, 102, 101, 9308, 1010, 2342, 2000, 4638, 2065, 3730, 4280, 2024, 3569, 1999, 2060, 2067, 10497, 2015, 1010, 1996, 6761, 1998, 27166, 2102, 2243, 1012, 102, 101, 3342, 2008, 17710, 8180, 5363, 2000, 2022, 10124, 6553, 1998, 7889, 2005, 2087, 2759, 2224, 3572, 1010, 2061, 1045, 2064, 3305, 1996, 7961, 2182, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1, 2]}], "na_triple": [], "sent_ends": [0, 16, 62, 107, 118, 155, 169, 185, 206, 226, 273, 298, 325], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56899286", "vertexSet": [[{"sent_id": 1, "name": "tf.losses", "pos": [56, 60]}], [{"sent_id": 0, "name": "tf.graphkeys", "pos": [13, 19]}], [{"sent_id": 1, "name": "tf.losses.get_regularization_loss", "pos": [56, 67]}]], "sents": ["So do we need to manually get the variable from the collection  tf.GraphKeys.REGULARIZATION_LOSSES and add it to our main loss in order for it to be applied?", "Yes, and no: You need to manually get the regularization losses via tf.losses.get_regularization_loss() (this will already get all regularization losses defined in the collection, no need to search for the variables in it), then you simply add the regularization loss to your model's loss and use that as the loss your optimizer trains on:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2061, 2079, 2057, 2342, 2000, 21118, 2131, 1996, 8023, 2013, 1996, 3074, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3180, 3989, 1035, 6409, 1998, 5587, 2009, 2000, 2256, 2364, 3279, 1999, 2344, 2005, 2009, 2000, 2022, 4162, 1029, 102, 101, 2748, 1010, 1998, 2053, 1024, 2017, 2342, 2000, 21118, 2131, 1996, 3180, 3989, 6409, 3081, 1056, 2546, 1012, 6409, 1012, 2131, 1035, 3180, 3989, 1035, 3279, 1006, 1007, 1006, 2023, 2097, 2525, 2131, 2035, 3180, 3989, 6409, 4225, 1999, 1996, 3074, 1010, 2053, 2342, 2000, 3945, 2005, 1996, 10857, 1999, 2009, 1007, 1010, 2059, 2017, 3432, 5587, 1996, 3180, 3989, 3279, 2000, 2115, 2944, 1005, 1055, 3279, 1998, 2224, 2008, 2004, 1996, 3279, 2115, 23569, 27605, 6290, 4499, 2006, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 40, 122, 136], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49457562", "vertexSet": [[{"sent_id": 5, "name": "tf.train", "pos": [79, 83]}], [{"sent_id": 4, "name": "tf.errors", "pos": [60, 64]}], [{"sent_id": 5, "name": "tf.train.limit_epochs", "pos": [79, 88]}], [{"sent_id": 4, "name": "tf.errors.outofrangeerror", "pos": [60, 70]}]], "sents": ["My question is why would this \"input_fn\" code does the trick?", "If I change the code to this, it will run into an infinite loop.", "Why?", "?", "The documentation states that input_fn is called repeatedly until it returns a tf.errors.OutOfRangeError.", "Adorning your tensor with tf.train.limit_epochs ensures that the error is eventually raised, which signals to KMeans that it should stop training."], "sent_idxs": [101, 2026, 3160, 2003, 2339, 2052, 2023, 1000, 7953, 1035, 1042, 2078, 1000, 3642, 2515, 1996, 7577, 1029, 102, 101, 2065, 1045, 2689, 1996, 3642, 2000, 2023, 1010, 2009, 2097, 2448, 2046, 2019, 10709, 7077, 1012, 102, 101, 2339, 1029, 102, 101, 1029, 102, 101, 1996, 12653, 2163, 2008, 7953, 1035, 1042, 2078, 2003, 2170, 8385, 2127, 2009, 5651, 1037, 1056, 2546, 1012, 10697, 1012, 2041, 11253, 24388, 11510, 29165, 1012, 102, 101, 4748, 9691, 2075, 2115, 23435, 2007, 1056, 2546, 1012, 3345, 1012, 5787, 1035, 25492, 2015, 21312, 2008, 1996, 7561, 2003, 2776, 2992, 1010, 2029, 7755, 2000, 2463, 11219, 2015, 2008, 2009, 2323, 2644, 2731, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 19, 37, 41, 44, 72, 109], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47528535", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [9, 13]}], [{"sent_id": 2, "name": "tf.reset_default_graph", "pos": [81, 89]}], [{"sent_id": 0, "name": "tf.train.gradientdescentoptimizer", "pos": [9, 21]}]], "sents": ["Every time you run train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) you're adding (quite some) operations to your graph, which becomes bigger and bigger with more loops of your program.", "The bigger the graph, the slower the execution.", "Put your model definition in the loops' body and call tf.reset_default_graph() each time you start a new iteration:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2296, 2051, 2017, 2448, 3345, 1035, 3357, 1027, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1006, 1014, 1012, 5890, 1007, 1012, 18478, 1006, 3279, 1007, 2017, 1005, 2128, 5815, 1006, 3243, 2070, 1007, 3136, 2000, 2115, 10629, 1010, 2029, 4150, 7046, 1998, 7046, 2007, 2062, 15932, 1997, 2115, 2565, 1012, 102, 101, 1996, 7046, 1996, 10629, 1010, 1996, 12430, 1996, 7781, 1012, 102, 101, 2404, 2115, 2944, 6210, 1999, 1996, 15932, 1005, 2303, 1998, 2655, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2169, 2051, 2017, 2707, 1037, 2047, 27758, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 57, 69, 100, 114], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46039856", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [7, 13]}], [{"sent_id": 0, "name": "tf.contrib.learn", "pos": [7, 15]}], [{"sent_id": 0, "name": "tf.contrib.learn.dnnclassifier", "pos": [7, 20]}]], "sents": ["You need to pass correct arguments tf.contrib.learn.DNNClassifier, here you didn't passfeature_columns argument.", "For example, you can use real_valued_column as features_columns", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2342, 2000, 3413, 6149, 9918, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 1040, 10695, 26266, 18095, 1010, 2182, 2017, 2134, 1005, 1056, 3413, 7959, 4017, 5397, 1035, 7753, 6685, 1012, 102, 101, 2005, 2742, 1010, 2017, 2064, 2224, 2613, 1035, 11126, 1035, 5930, 2004, 2838, 1035, 7753, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 35, 52, 66], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61775062", "vertexSet": [[{"sent_id": 5, "name": "tf.keras", "pos": [141, 146]}], [{"sent_id": 5, "name": "tf.keras.layers", "pos": [141, 148]}], [{"sent_id": 5, "name": "tf.keras.layers.embedding", "pos": [141, 152]}]], "sents": ["Here is what happens.", "When you first call Tokenizer with num_words and then, call fit_on_texts on it, then, from your text, the top num_words with highest frequency will be taken by it and then, a simple dictionary will be created in the format.", "dict = {'topmost_word': 1,\n        '2nd_topmost_word': 2,\n         ...\n        'num_wordsth_topmost_word: num_words\n        }", "Thus, only words are being converted into integers.", "No other magic happens here.", "That something more about clustering happens in the tf.keras.layers.Embedding where the embedding are learning for each word.", "The embeddings you will get from here will be learned vectors and words of same class will be close here."], "sent_idxs": [101, 2182, 2003, 2054, 6433, 1012, 102, 101, 2043, 2017, 2034, 2655, 19204, 17629, 2007, 16371, 2213, 1035, 2616, 1998, 2059, 1010, 2655, 4906, 1035, 2006, 1035, 6981, 2006, 2009, 1010, 2059, 1010, 2013, 2115, 3793, 1010, 1996, 2327, 16371, 2213, 1035, 2616, 2007, 3284, 6075, 2097, 2022, 2579, 2011, 2009, 1998, 2059, 1010, 1037, 3722, 9206, 2097, 2022, 2580, 1999, 1996, 4289, 1012, 102, 101, 4487, 6593, 1027, 1063, 1005, 2327, 11800, 1035, 2773, 1005, 1024, 1015, 1010, 1005, 3416, 1035, 2327, 11800, 1035, 2773, 1005, 1024, 1016, 1010, 1012, 1012, 1012, 1005, 16371, 2213, 1035, 2616, 2705, 1035, 2327, 11800, 1035, 2773, 1024, 16371, 2213, 1035, 2616, 1065, 102, 101, 2947, 1010, 2069, 2616, 2024, 2108, 4991, 2046, 24028, 1012, 102, 101, 2053, 2060, 3894, 6433, 2182, 1012, 102, 101, 2008, 2242, 2062, 2055, 9324, 2075, 6433, 1999, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 7861, 8270, 4667, 2073, 1996, 7861, 8270, 4667, 2024, 4083, 2005, 2169, 2773, 1012, 102, 101, 1996, 7861, 8270, 4667, 2015, 2017, 2097, 2131, 2013, 2182, 2097, 2022, 4342, 19019, 1998, 2616, 1997, 2168, 2465, 2097, 2022, 2485, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 7, 65, 111, 123, 131, 164, 190], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47522433", "vertexSet": [[{"sent_id": 1, "name": "tf.metrics", "pos": [13, 18]}, {"sent_id": 5, "name": "tf.metrics", "pos": [192, 197]}, {"sent_id": 5, "name": "tf.metrics", "pos": [204, 209]}], [{"sent_id": 1, "name": "tf.metrics.accuracy", "pos": [13, 20]}], [{"sent_id": 5, "name": "tf.metrics.mean_squared_error", "pos": [192, 203]}], [{"sent_id": 5, "name": "tf.metrics.mean_absolute_error", "pos": [204, 215]}]], "sents": ["I think you are learning a regression model.", "The tf.metrics.accuracy is supposed to run for a classification model.", "When your model predicts 1.2 but your target value is 1.15, it does not make sense to use accuracy to measure whether this is a correct prediction.", "accuracy is for classification problems (e.g., mnist), when your model predicts a digit to be '9' and your target image is also '9': this is a correct prediction and you get full credit; Or when your model predicts a digit to be '9' but your target image is '6': this is a wrong prediction and you get no credit.", "For your regression problem, we measure the difference between prediction and target value either by absolute error - |target - prediction| or mean squared error - the one you used in your MSE calculation.", "Thus tf.metrics.mean_squared_error or tf.metrics.mean_absolute_error is the one you should use to measure the prediction error for regression models."], "sent_idxs": [101, 1045, 2228, 2017, 2024, 4083, 1037, 26237, 2944, 1012, 102, 101, 1996, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 2003, 4011, 2000, 2448, 2005, 1037, 5579, 2944, 1012, 102, 101, 2043, 2115, 2944, 16014, 2015, 1015, 1012, 1016, 2021, 2115, 4539, 3643, 2003, 1015, 1012, 2321, 1010, 2009, 2515, 2025, 2191, 3168, 2000, 2224, 10640, 2000, 5468, 3251, 2023, 2003, 1037, 6149, 17547, 1012, 102, 101, 10640, 2003, 2005, 5579, 3471, 1006, 1041, 1012, 1043, 1012, 1010, 24098, 2923, 1007, 1010, 2043, 2115, 2944, 16014, 2015, 1037, 15340, 2000, 2022, 1005, 1023, 1005, 1998, 2115, 4539, 3746, 2003, 2036, 1005, 1023, 1005, 1024, 2023, 2003, 1037, 6149, 17547, 1998, 2017, 2131, 2440, 4923, 1025, 2030, 2043, 2115, 2944, 16014, 2015, 1037, 15340, 2000, 2022, 1005, 1023, 1005, 2021, 2115, 4539, 3746, 2003, 1005, 1020, 1005, 1024, 2023, 2003, 1037, 3308, 17547, 1998, 2017, 2131, 2053, 4923, 1012, 102, 101, 2005, 2115, 26237, 3291, 1010, 2057, 5468, 1996, 4489, 2090, 17547, 1998, 4539, 3643, 2593, 2011, 7619, 7561, 1011, 1064, 4539, 1011, 17547, 1064, 2030, 2812, 19942, 7561, 1011, 1996, 2028, 2017, 2109, 1999, 2115, 5796, 2063, 17208, 1012, 102, 101, 2947, 1056, 2546, 1012, 12046, 2015, 1012, 2812, 1035, 19942, 1035, 7561, 2030, 1056, 2546, 1012, 12046, 2015, 1012, 2812, 1035, 7619, 1035, 7561, 2003, 1996, 2028, 2017, 2323, 2224, 2000, 5468, 1996, 17547, 7561, 2005, 26237, 4275, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 11, 30, 66, 149, 190, 231], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44977196", "vertexSet": [[{"sent_id": 1, "name": "tf.saved_model", "pos": [29, 35]}], [{"sent_id": 0, "name": "tf.tables_initializer", "pos": [9, 16]}], [{"sent_id": 1, "name": "tf.saved_model.main_op", "pos": [29, 39]}], [{"sent_id": 1, "name": "tf.saved_model.main_op.main_op", "pos": [29, 43]}]], "sents": ["I think you would be better off using tf.tables_initializer() as the legacy_init_op.", "tf.saved_model.main_op.main_op() also adds local and global initialization ops in addition to table initialization.", "when you load the saved model and it runs the legacy_init_op, it would reset your variables, which is not what you want."], "sent_idxs": [101, 1045, 2228, 2017, 2052, 2022, 2488, 2125, 2478, 1056, 2546, 1012, 7251, 1035, 3988, 17629, 1006, 1007, 2004, 1996, 8027, 1035, 1999, 4183, 1035, 6728, 1012, 102, 101, 1056, 2546, 1012, 5552, 1035, 2944, 1012, 2364, 1035, 6728, 1012, 2364, 1035, 6728, 1006, 1007, 2036, 9909, 2334, 1998, 3795, 3988, 3989, 23092, 1999, 2804, 2000, 2795, 3988, 3989, 1012, 102, 101, 2043, 2017, 7170, 1996, 5552, 2944, 1998, 2009, 3216, 1996, 8027, 1035, 1999, 4183, 1035, 6728, 1010, 2009, 2052, 25141, 2115, 10857, 1010, 2029, 2003, 2025, 2054, 2017, 2215, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 28, 61, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43602962", "vertexSet": [[{"sent_id": 5, "name": "tf.contrib", "pos": [82, 88]}], [{"sent_id": 0, "name": "tf.graphkeys", "pos": [17, 23]}], [{"sent_id": 5, "name": "tf.contrib.framework", "pos": [82, 90]}], [{"sent_id": 9, "name": "tf.local_variables_initializer", "pos": [171, 180]}], [{"sent_id": 9, "name": "tf.global_variables_initializer", "pos": [159, 168]}], [{"sent_id": 5, "name": "tf.contrib.framework.local_variable", "pos": [82, 94]}]], "sents": ["A local variable in TF is any variable which was created with collections=[tf.GraphKeys.LOCAL_VARIABLES].", "For example:", "<code>Code Snippet</code>.", "LOCAL_VARIABLES: the subset of Variable objects that are local to each\n  machine.", "Usually used for temporarily variables, like counters.", "Note:\n  use tf.contrib.framework.local_variable to add to this collection.", "They are usually not saved/restored to checkpoint and used for temporary or intermediate values.", "For a more detailed answer, take a look here.", "A global variable is mostly every other variable initialized by you.", "In a new version of TF you should use tf.global_variables_initializer(), tf.local_variables_initializer(), because the previous functions were deprecated."], "sent_idxs": [101, 1037, 2334, 8023, 1999, 1056, 2546, 2003, 2151, 8023, 2029, 2001, 2580, 2007, 6407, 1027, 1031, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 2334, 1035, 10857, 1033, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2334, 1035, 10857, 1024, 1996, 16745, 1997, 8023, 5200, 2008, 2024, 2334, 2000, 2169, 3698, 1012, 102, 101, 2788, 2109, 2005, 8184, 10857, 1010, 2066, 24094, 1012, 102, 101, 3602, 1024, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7705, 1012, 2334, 1035, 8023, 2000, 5587, 2000, 2023, 3074, 1012, 102, 101, 2027, 2024, 2788, 2025, 5552, 1013, 5854, 2000, 26520, 1998, 2109, 2005, 5741, 2030, 7783, 5300, 1012, 102, 101, 2005, 1037, 2062, 6851, 3437, 1010, 2202, 1037, 2298, 2182, 1012, 102, 101, 1037, 3795, 8023, 2003, 3262, 2296, 2060, 8023, 3988, 3550, 2011, 2017, 1012, 102, 101, 1999, 1037, 2047, 2544, 1997, 1056, 2546, 2017, 2323, 2224, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1010, 1056, 2546, 1012, 2334, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1010, 2138, 1996, 3025, 4972, 2020, 2139, 28139, 12921, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 30, 35, 49, 67, 78, 101, 120, 133, 148, 193], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49963569", "vertexSet": [[{"sent_id": 1, "name": "tf.data", "pos": [12, 16]}], [{"sent_id": 3, "name": "tf.tensor", "pos": [44, 48]}], [{"sent_id": 6, "name": "tf.estimator", "pos": [105, 111]}], [{"sent_id": 1, "name": "tf.data.dataset", "pos": [12, 19]}], [{"sent_id": 6, "name": "tf.estimator.estimator", "pos": [105, 115]}]], "sents": ["There are two parameters that are causing this:", "tf.data.Dataset.repeat has a count parameter:", "count: (Optional.)", "A tf.int64 scalar tf.Tensor, representing the\n  number of times the dataset should be repeated.", "The default behavior\n  (if count is None or -1) is for the dataset be repeated indefinitely.", "In your case, count is always None, so the dataset is repeated indefinitely.", "tf.estimator.Estimator.evaluate has the steps parameter:", "steps: Number of steps for which to evaluate model.", "If None, evaluates until input_fn raises an end-of-input exception.", "Steps are set for the training, but not for the evaluation, as a result the estimator is running until input_fn raises an end-of-input exception, which, as described above, never happens.", "You should set either of those, I think count=1 is the most reasonable for evaluation."], "sent_idxs": [101, 2045, 2024, 2048, 11709, 2008, 2024, 4786, 2023, 1024, 102, 101, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 9377, 2038, 1037, 4175, 16381, 1024, 102, 101, 4175, 1024, 1006, 11887, 1012, 1007, 102, 101, 1037, 1056, 2546, 1012, 20014, 21084, 26743, 2099, 1056, 2546, 1012, 23435, 1010, 5052, 1996, 2193, 1997, 2335, 1996, 2951, 13462, 2323, 2022, 5567, 1012, 102, 101, 1996, 12398, 5248, 1006, 2065, 4175, 2003, 3904, 2030, 1011, 1015, 1007, 2003, 2005, 1996, 2951, 13462, 2022, 5567, 20733, 1012, 102, 101, 1999, 2115, 2553, 1010, 4175, 2003, 2467, 3904, 1010, 2061, 1996, 2951, 13462, 2003, 5567, 20733, 1012, 102, 101, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1012, 16157, 2038, 1996, 4084, 16381, 1024, 102, 101, 4084, 1024, 2193, 1997, 4084, 2005, 2029, 2000, 16157, 2944, 1012, 102, 101, 2065, 3904, 1010, 16157, 2015, 2127, 7953, 1035, 1042, 2078, 13275, 2019, 2203, 1011, 1997, 1011, 7953, 6453, 1012, 102, 101, 4084, 2024, 2275, 2005, 1996, 2731, 1010, 2021, 2025, 2005, 1996, 9312, 1010, 2004, 1037, 2765, 1996, 9765, 9581, 4263, 2003, 2770, 2127, 7953, 1035, 1042, 2078, 13275, 2019, 2203, 1011, 1997, 1011, 7953, 6453, 1010, 2029, 1010, 2004, 2649, 2682, 1010, 2196, 6433, 1012, 102, 101, 2017, 2323, 2275, 2593, 1997, 2216, 1010, 1045, 2228, 4175, 1027, 1015, 2003, 1996, 2087, 9608, 2005, 9312, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 11, 27, 35, 62, 85, 104, 123, 136, 157, 204, 225], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64133031", "vertexSet": [[{"sent_id": 4, "name": "tf.keras", "pos": [108, 113]}], [{"sent_id": 4, "name": "tf.keras.backend", "pos": [108, 116]}], [{"sent_id": 4, "name": "tf.keras.backend.floatx", "pos": [108, 119]}]], "sents": ["<code>Code Snippet</code>.", "If you look attentively, the interpolation you specify in the case\nof cv2 is cv2.INTER_LINEAR (bilinear interpolation); however, by default,\nimage.load_img() uses an INTER_NEAREST interpolation method.", "img_to_array(img).", "The dtype argument here is: None", "Default to None, in which case the global setting\ntf.keras.backend.floatx() is used (unless you changed it, it defaults\nto \"float32\")", "Therefore, in img_to_array(img) you have an image that consists of float32 values, while the cv2.imread(img) returns a numpy array of uint8 values.", "Ensure you convert to RGB from BGR, as OpenCV loads directly into RGB format.", "You can use image = image[:,:,::-1] or image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB); otherwise you will have the R and B channels reversed resulting in an incorrect comparison..", "Since the preprocessing that you apply is the same in both cases, the only differences are the ones that I mentioned above; adapting those changes should ensure reproducibility.", "There is one observation I would like to make: provided that one uses a library (cv2 in this case) which automatically (and arguably only loads ints) instead of floats, the only correct way is to cast the first prediction array (Keras) to uint8 because by casting the latter to float32, the possible difference in information is lost.", "For example, with cv2 you load to uint8, and by casting instead of 233 you get 233.0.", "However, maybe the initial pixel value was 233,3 but this was lost due to the first conversion."], "sent_idxs": [101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2298, 2012, 6528, 25499, 1010, 1996, 6970, 18155, 3370, 2017, 20648, 1999, 1996, 2553, 1997, 26226, 2475, 2003, 26226, 2475, 1012, 6970, 1035, 7399, 1006, 12170, 4179, 2906, 6970, 18155, 3370, 1007, 1025, 2174, 1010, 2011, 12398, 1010, 3746, 1012, 7170, 1035, 10047, 2290, 1006, 1007, 3594, 2019, 6970, 1035, 7205, 6970, 18155, 3370, 4118, 1012, 102, 101, 10047, 2290, 1035, 2000, 1035, 9140, 1006, 10047, 2290, 1007, 1012, 102, 101, 1996, 26718, 18863, 6685, 2182, 2003, 1024, 3904, 102, 101, 12398, 2000, 3904, 1010, 1999, 2029, 2553, 1996, 3795, 4292, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 14257, 2595, 1006, 1007, 2003, 2109, 1006, 4983, 2017, 2904, 2009, 1010, 2009, 12398, 2015, 2000, 1000, 14257, 16703, 1000, 1007, 102, 101, 3568, 1010, 1999, 10047, 2290, 1035, 2000, 1035, 9140, 1006, 10047, 2290, 1007, 2017, 2031, 2019, 3746, 2008, 3774, 1997, 14257, 16703, 5300, 1010, 2096, 1996, 26226, 2475, 1012, 10047, 16416, 2094, 1006, 10047, 2290, 1007, 5651, 1037, 16371, 8737, 2100, 9140, 1997, 21318, 3372, 2620, 5300, 1012, 102, 101, 5676, 2017, 10463, 2000, 1054, 18259, 2013, 1038, 16523, 1010, 2004, 2330, 2278, 2615, 15665, 3495, 2046, 1054, 18259, 4289, 1012, 102, 101, 2017, 2064, 2224, 3746, 1027, 3746, 1031, 1024, 1010, 1024, 1010, 1024, 1024, 1011, 1015, 1033, 2030, 3746, 1027, 26226, 2475, 1012, 26226, 13535, 12898, 2099, 1006, 3746, 1010, 26226, 2475, 1012, 3609, 1035, 1038, 16523, 2475, 10623, 2497, 1007, 1025, 4728, 2017, 2097, 2031, 1996, 1054, 1998, 1038, 6833, 11674, 4525, 1999, 2019, 16542, 7831, 1012, 1012, 102, 101, 2144, 1996, 17463, 3217, 9623, 7741, 2008, 2017, 6611, 2003, 1996, 2168, 1999, 2119, 3572, 1010, 1996, 2069, 5966, 2024, 1996, 3924, 2008, 1045, 3855, 2682, 1025, 25357, 2216, 3431, 2323, 5676, 16360, 14127, 14194, 13464, 1012, 102, 101, 2045, 2003, 2028, 8089, 1045, 2052, 2066, 2000, 2191, 1024, 3024, 2008, 2028, 3594, 1037, 3075, 1006, 26226, 2475, 1999, 2023, 2553, 1007, 2029, 8073, 1006, 1998, 15835, 2069, 15665, 20014, 2015, 1007, 2612, 1997, 24885, 1010, 1996, 2069, 6149, 2126, 2003, 2000, 3459, 1996, 2034, 17547, 9140, 1006, 17710, 8180, 1007, 2000, 21318, 3372, 2620, 2138, 2011, 9179, 1996, 3732, 2000, 14257, 16703, 1010, 1996, 2825, 4489, 1999, 2592, 2003, 2439, 1012, 102, 101, 2005, 2742, 1010, 2007, 26226, 2475, 2017, 7170, 2000, 21318, 3372, 2620, 1010, 1998, 2011, 9179, 2612, 1997, 22115, 2017, 2131, 22115, 1012, 1014, 1012, 102, 101, 2174, 1010, 2672, 1996, 3988, 22138, 3643, 2001, 22115, 1010, 1017, 2021, 2023, 2001, 2439, 2349, 2000, 1996, 2034, 7584, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 74, 87, 97, 139, 189, 212, 272, 311, 386, 413, 436], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47161981", "vertexSet": [[{"sent_id": 2, "name": "tf.layers", "pos": [74, 78]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [5, 11]}, {"sent_id": 2, "name": "tf.contrib", "pos": [52, 58]}, {"sent_id": 4, "name": "tf.contrib", "pos": [122, 128]}], [{"sent_id": 2, "name": "tf.layers.dense", "pos": [74, 80]}], [{"sent_id": 0, "name": "tf.contrib.layers", "pos": [5, 13]}, {"sent_id": 2, "name": "tf.contrib.layers", "pos": [52, 60]}, {"sent_id": 4, "name": "tf.contrib.layers", "pos": [122, 130]}], [{"sent_id": 4, "name": "tf.contrib.layers.l1_regularizer", "pos": [122, 136]}]], "sents": ["If you are using tf.contrib.layers, the fully_connected function accepts weights_regularizer argument, so your code should look like thus", "<code>Code Snippet</code>.", "That said, tf.contrib.layers has been mostly moved to the core API, so you should be using tf.layers.dense instead with kernel_regularizer argument.", "The code above will regularize the weights in the layer.", "If you want to regularize both weights and the layer output, you can use the same tf.contrib.layers.l1_regularizer or create a different one with different parameters.", "Something like this should work for you:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2065, 2017, 2024, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1010, 1996, 3929, 1035, 4198, 3853, 13385, 15871, 1035, 3180, 17629, 6685, 1010, 2061, 2115, 3642, 2323, 2298, 2066, 2947, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2008, 2056, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 2038, 2042, 3262, 2333, 2000, 1996, 4563, 17928, 1010, 2061, 2017, 2323, 2022, 2478, 1056, 2546, 1012, 9014, 1012, 9742, 2612, 2007, 16293, 1035, 3180, 17629, 6685, 1012, 102, 101, 1996, 3642, 2682, 2097, 3180, 4697, 1996, 15871, 1999, 1996, 6741, 1012, 102, 101, 2065, 2017, 2215, 2000, 3180, 4697, 2119, 15871, 1998, 1996, 6741, 6434, 1010, 2017, 2064, 2224, 1996, 2168, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 1048, 2487, 1035, 3180, 17629, 2030, 3443, 1037, 2367, 2028, 2007, 2367, 11709, 1012, 102, 101, 2242, 2066, 2023, 2323, 2147, 2005, 2017, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 34, 48, 89, 103, 146, 156, 170], "sent_pos": [0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46350278", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [92, 97]}, {"sent_id": 3, "name": "tf.nn", "pos": [159, 164]}, {"sent_id": 3, "name": "tf.nn", "pos": [182, 187]}, {"sent_id": 6, "name": "tf.nn", "pos": [266, 271]}], [{"sent_id": 5, "name": "tf.placeholder", "pos": [234, 239]}], [{"sent_id": 6, "name": "tf.reduce_mean", "pos": [259, 265]}], [{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [92, 109]}], [{"sent_id": 3, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [182, 201]}, {"sent_id": 6, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [266, 285]}]], "sents": ["Most likely your labels are single integer values rather than one-hot vectors, so your labelBatch is a vector of size [50] containing single numbers like \"1\" or \"4\".", "Now, when you reshape them using train_labels = np.reshape(train_labels, (-1, NUM_CLASSES)) \nyou're changing the shape to [10, 5].", "The tf.nn.softmax_cross_entropy_with_logits function expects labels to be \"one-hot\" encodings of the labels (this means that a label of 3 translates into a vector of size 5 with a 1 in position 3 and zeros elsewhere).", "You can achieve this using the tf.nn.one_hot function, but an easier way to do it is instead to use the tf.nn.sparse_softmax_cross_entropy_with_logits function which is designed to work with these single-valued labels.", "To achieve this, you'll need to change these line:", "y_ = tf.placeholder(tf.float32, [None])  # Desired output", "cross_entropy = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))", "And get rid of the train_labels = np.reshape(train_labels, (-1, NUM_CLASSES)) line.", "(By the way, you don't actually need to use placeholders when reading data in this way - you can just directly use the output tensors.)"], "sent_idxs": [101, 2087, 3497, 2115, 10873, 2024, 2309, 16109, 5300, 2738, 2084, 2028, 1011, 2980, 19019, 1010, 2061, 2115, 3830, 14479, 2818, 2003, 1037, 9207, 1997, 2946, 1031, 2753, 1033, 4820, 2309, 3616, 2066, 1000, 1015, 1000, 2030, 1000, 1018, 1000, 1012, 102, 101, 2085, 1010, 2043, 2017, 24501, 3270, 5051, 2068, 2478, 3345, 1035, 10873, 1027, 27937, 1012, 24501, 3270, 5051, 1006, 3345, 1035, 10873, 1010, 1006, 1011, 1015, 1010, 16371, 2213, 1035, 4280, 1007, 1007, 2017, 1005, 2128, 5278, 1996, 4338, 2000, 1031, 2184, 1010, 1019, 1033, 1012, 102, 101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 3853, 24273, 10873, 2000, 2022, 1000, 2028, 1011, 2980, 1000, 17181, 2015, 1997, 1996, 10873, 1006, 2023, 2965, 2008, 1037, 3830, 1997, 1017, 16315, 2046, 1037, 9207, 1997, 2946, 1019, 2007, 1037, 1015, 1999, 2597, 1017, 1998, 5717, 2015, 6974, 1007, 1012, 102, 101, 2017, 2064, 6162, 2023, 2478, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 2028, 1035, 2980, 3853, 1010, 2021, 2019, 6082, 2126, 2000, 2079, 2009, 2003, 2612, 2000, 2224, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 3853, 2029, 2003, 2881, 2000, 2147, 2007, 2122, 2309, 1011, 11126, 10873, 1012, 102, 101, 2000, 6162, 2023, 1010, 2017, 1005, 2222, 2342, 2000, 2689, 2122, 2240, 1024, 102, 101, 1061, 1035, 1027, 1056, 2546, 1012, 2173, 14528, 1006, 1056, 2546, 1012, 14257, 16703, 1010, 1031, 3904, 1033, 1007, 1001, 9059, 6434, 102, 101, 2892, 1035, 23077, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 10873, 1027, 1061, 1035, 1010, 8833, 12762, 1027, 1061, 1035, 9530, 2615, 1007, 1007, 102, 101, 1998, 2131, 9436, 1997, 1996, 3345, 1035, 10873, 1027, 27937, 1012, 24501, 3270, 5051, 1006, 3345, 1035, 10873, 1010, 1006, 1011, 1015, 1010, 16371, 2213, 1035, 4280, 1007, 1007, 2240, 1012, 102, 101, 1006, 2011, 1996, 2126, 1010, 2017, 2123, 1005, 1056, 2941, 2342, 2000, 2224, 2173, 17794, 2043, 3752, 2951, 1999, 2023, 2126, 1011, 2017, 2064, 2074, 3495, 2224, 1996, 6434, 23435, 2015, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 42, 90, 152, 215, 230, 254, 301, 334, 369], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61244193", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [10, 14]}], [{"sent_id": 3, "name": "tf.image", "pos": [96, 100]}], [{"sent_id": 3, "name": "tf.image.random_flip_left_right", "pos": [96, 108]}]], "sents": ["When you apply data augmentation with the tf.data API, it is done on-the-fly, meaning that every example is transformed as implemented in your method.", "Augmenting data this way does not mean that the number of examples in your pipeline changes.", "If you want to use every example n times, simply add dataset = dataset.repeat(count=n).", "You might want to update your code to use tf.image.random_flip_left_right, otherwise the flip is done the same way each time."], "sent_idxs": [101, 2043, 2017, 6611, 2951, 15476, 3672, 3370, 2007, 1996, 1056, 2546, 1012, 2951, 17928, 1010, 2009, 2003, 2589, 2006, 1011, 1996, 1011, 4875, 1010, 3574, 2008, 2296, 2742, 2003, 8590, 2004, 7528, 1999, 2115, 4118, 1012, 102, 101, 15476, 3672, 2075, 2951, 2023, 2126, 2515, 2025, 2812, 2008, 1996, 2193, 1997, 4973, 1999, 2115, 13117, 3431, 1012, 102, 101, 2065, 2017, 2215, 2000, 2224, 2296, 2742, 1050, 2335, 1010, 3432, 5587, 2951, 13462, 1027, 2951, 13462, 1012, 9377, 1006, 4175, 1027, 1050, 1007, 1012, 102, 101, 2017, 2453, 2215, 2000, 10651, 2115, 3642, 2000, 2224, 1056, 2546, 1012, 3746, 1012, 6721, 1035, 11238, 1035, 2187, 1035, 2157, 1010, 4728, 1996, 11238, 2003, 2589, 1996, 2168, 2126, 2169, 2051, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 59, 86, 121], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55827029", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [19, 24]}, {"sent_id": 1, "name": "tf.nn", "pos": [60, 65]}], [{"sent_id": 0, "name": "tf.keras", "pos": [1, 6]}, {"sent_id": 0, "name": "tf.keras", "pos": [33, 38]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [1, 8]}], [{"sent_id": 0, "name": "tf.keras.backend", "pos": [33, 41]}], [{"sent_id": 0, "name": "tf.nn.conv2d_transpose", "pos": [19, 32]}, {"sent_id": 1, "name": "tf.nn.conv2d_transpose", "pos": [60, 73]}], [{"sent_id": 0, "name": "tf.keras.backend.conv2d_transpose", "pos": [33, 49]}]], "sents": ["tf.keras.layers.Conv2DTranpose backends to tf.nn.conv2d_transpose via tf.keras.backend.conv2d_transpose.", "To compute the output_shape argument for tf.nn.conv2d_transpose it utilizes the function deconv_output_length (defined here):", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2475, 11927, 5521, 20688, 2067, 10497, 2015, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 3081, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 1012, 102, 101, 2000, 24134, 1996, 6434, 1035, 4338, 6685, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1035, 9099, 20688, 2009, 21852, 1996, 3853, 21933, 2078, 2615, 1035, 6434, 1035, 3091, 1006, 4225, 2182, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 51, 90, 104], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51250638", "vertexSet": [[{"sent_id": 1, "name": "tf.data", "pos": [27, 31]}], [{"sent_id": 10, "name": "tf.estimator", "pos": [202, 208]}], [{"sent_id": 8, "name": "tf.placeholder", "pos": [164, 169]}], [{"sent_id": 1, "name": "tf.data.dataset", "pos": [27, 34]}], [{"sent_id": 10, "name": "tf.estimator.export", "pos": [202, 210]}], [{"sent_id": 10, "name": "tf.estimator.export.servinginputreceiver", "pos": [202, 217]}]], "sents": ["The general pattern to follow is (see this doc):", "Create an input_fn used in training, typically using tf.data.Dataset.", "The input_fn should call helper functions to do data transformations, like those in your code.", "The output will be a dictionary of feature names to batches of values..", "Define FeatureColumns for the items in the output of your input_fn.", "If necessary, do things like feature crosses, bucketization, etc..", "Instantiate the estimator (e.g.", "DnnRegressor), passing the FeatureColumns to the constructor.", "Create an input_fn specifically for serving, that has one or more tf.Placeholder with None (variable batch size) as the outer dimension.", "Call the same helper functions from (1) to do the transformations.", "Return a tf.estimator.export.ServingInputReceiver with the placeholders as inputs and a dict that should look the same as the dict in (1)..", "Your particular case warrants a few additional details.", "First, you have hardcoded a batch size of 1 into your placeholder and the corresponding code continues that assumption.", "Your placeholder must have shape=[None].", "Unfortunately, your code has been written under the assumption that shape was 1, e.g., split_date_time.values[0] will no longer be valid.", "I've added a helper function to address that in the code below.", "Here's some code that should hopefully work for you:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2236, 5418, 2000, 3582, 2003, 1006, 2156, 2023, 9986, 1007, 1024, 102, 101, 3443, 2019, 7953, 1035, 1042, 2078, 2109, 1999, 2731, 1010, 4050, 2478, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 102, 101, 1996, 7953, 1035, 1042, 2078, 2323, 2655, 2393, 2121, 4972, 2000, 2079, 2951, 21865, 1010, 2066, 2216, 1999, 2115, 3642, 1012, 102, 101, 1996, 6434, 2097, 2022, 1037, 9206, 1997, 3444, 3415, 2000, 14108, 2229, 1997, 5300, 1012, 1012, 102, 101, 9375, 3444, 25778, 2819, 3619, 2005, 1996, 5167, 1999, 1996, 6434, 1997, 2115, 7953, 1035, 1042, 2078, 1012, 102, 101, 2065, 4072, 1010, 2079, 2477, 2066, 3444, 7821, 1010, 13610, 3989, 1010, 4385, 1012, 1012, 102, 101, 7107, 13143, 1996, 9765, 9581, 4263, 1006, 1041, 1012, 1043, 1012, 102, 101, 1040, 10695, 2890, 17603, 24137, 2099, 1007, 1010, 4458, 1996, 3444, 25778, 2819, 3619, 2000, 1996, 9570, 2953, 1012, 102, 101, 3443, 2019, 7953, 1035, 1042, 2078, 4919, 2005, 3529, 1010, 2008, 2038, 2028, 2030, 2062, 1056, 2546, 1012, 2173, 14528, 2007, 3904, 1006, 8023, 14108, 2946, 1007, 2004, 1996, 6058, 9812, 1012, 102, 101, 2655, 1996, 2168, 2393, 2121, 4972, 2013, 1006, 1015, 1007, 2000, 2079, 1996, 21865, 1012, 102, 101, 2709, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9167, 1012, 3529, 2378, 18780, 2890, 3401, 16402, 2007, 1996, 2173, 17794, 2004, 20407, 1998, 1037, 4487, 6593, 2008, 2323, 2298, 1996, 2168, 2004, 1996, 4487, 6593, 1999, 1006, 1015, 1007, 1012, 1012, 102, 101, 2115, 3327, 2553, 10943, 2015, 1037, 2261, 3176, 4751, 1012, 102, 101, 2034, 1010, 2017, 2031, 2524, 16044, 2094, 1037, 14108, 2946, 1997, 1015, 2046, 2115, 2173, 14528, 1998, 1996, 7978, 3642, 4247, 2008, 11213, 1012, 102, 101, 2115, 2173, 14528, 2442, 2031, 4338, 1027, 1031, 3904, 1033, 1012, 102, 101, 6854, 1010, 2115, 3642, 2038, 2042, 2517, 2104, 1996, 11213, 2008, 4338, 2001, 1015, 1010, 1041, 1012, 1043, 1012, 1010, 3975, 1035, 3058, 1035, 2051, 1012, 5300, 1031, 1014, 1033, 2097, 2053, 2936, 2022, 9398, 1012, 102, 101, 1045, 1005, 2310, 2794, 1037, 2393, 2121, 3853, 2000, 4769, 2008, 1999, 1996, 3642, 2917, 1012, 102, 101, 2182, 1005, 1055, 2070, 3642, 2008, 2323, 11504, 2147, 2005, 2017, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 14, 36, 59, 77, 97, 114, 127, 148, 182, 199, 243, 255, 281, 294, 332, 350, 364, 378], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48361515", "vertexSet": [[{"sent_id": 3, "name": "tf.train", "pos": [48, 52]}, {"sent_id": 3, "name": "tf.train", "pos": [63, 67]}, {"sent_id": 4, "name": "tf.train", "pos": [82, 86]}, {"sent_id": 10, "name": "tf.train", "pos": [245, 249]}, {"sent_id": 12, "name": "tf.train", "pos": [286, 290]}], [{"sent_id": 4, "name": "tf.train.adamoptimizer", "pos": [82, 92]}], [{"sent_id": 3, "name": "tf.train.exponential_decay", "pos": [63, 71]}], [{"sent_id": 3, "name": "tf.train.gradientdescentoptimizer", "pos": [48, 60]}, {"sent_id": 10, "name": "tf.train.gradientdescentoptimizer", "pos": [245, 257]}, {"sent_id": 12, "name": "tf.train.gradientdescentoptimizer", "pos": [286, 298]}]], "sents": ["I found the reason for the NaN's error.", "In retrospect, I have to say, it's been in front of my nose the whole time.", "The short version:", "I used the tf.train.GradientDescentOptimizer() with tf.train.exponential_decay() for the optimization.", "Changing it to tf.train.AdamOptimizer() solve my problem.", "The long version:", "So it wasn't the GPU cluster, but the optimization algorithm.", "But I didn't immediately notice that, because if I used only one GPU on the GPU cluster, the total loss values were not infinite but if I used several GPUs, the loss value added up and then went into the infinite range.", "Only when I ran the script on my local machine for a very long time (using a NVIDIA GTX 770) did I get a NaN error.", "That's when I knew it had nothing to do with the NVIDIA Tesla P100.", "This GitHub issue caused me to get more involved with the tf.train.GradientDescentOptimizer().", "Now it looks like that solved my problem.", "The TensorFlow tutorial Convolutional Neural Networks use tf.train.GradientDescentOptimizer() and i change now the code from:", "<code>Code Snippet</code>.", "to:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2179, 1996, 3114, 2005, 1996, 16660, 1005, 1055, 7561, 1012, 102, 101, 1999, 22307, 13102, 22471, 1010, 1045, 2031, 2000, 2360, 1010, 2009, 1005, 1055, 2042, 1999, 2392, 1997, 2026, 4451, 1996, 2878, 2051, 1012, 102, 101, 1996, 2460, 2544, 1024, 102, 101, 1045, 2109, 1996, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1006, 1007, 2007, 1056, 2546, 1012, 3345, 1012, 27258, 1035, 13121, 1006, 1007, 2005, 1996, 20600, 1012, 102, 101, 5278, 2009, 2000, 1056, 2546, 1012, 3345, 1012, 4205, 7361, 3775, 4328, 6290, 1006, 1007, 9611, 2026, 3291, 1012, 102, 101, 1996, 2146, 2544, 1024, 102, 101, 2061, 2009, 2347, 1005, 1056, 1996, 14246, 2226, 9324, 1010, 2021, 1996, 20600, 9896, 1012, 102, 101, 2021, 1045, 2134, 1005, 1056, 3202, 5060, 2008, 1010, 2138, 2065, 1045, 2109, 2069, 2028, 14246, 2226, 2006, 1996, 14246, 2226, 9324, 1010, 1996, 2561, 3279, 5300, 2020, 2025, 10709, 2021, 2065, 1045, 2109, 2195, 14246, 2271, 1010, 1996, 3279, 3643, 2794, 2039, 1998, 2059, 2253, 2046, 1996, 10709, 2846, 1012, 102, 101, 2069, 2043, 1045, 2743, 1996, 5896, 2006, 2026, 2334, 3698, 2005, 1037, 2200, 2146, 2051, 1006, 2478, 1037, 1050, 17258, 2401, 14181, 2595, 29065, 1007, 2106, 1045, 2131, 1037, 16660, 7561, 1012, 102, 101, 2008, 1005, 1055, 2043, 1045, 2354, 2009, 2018, 2498, 2000, 2079, 2007, 1996, 1050, 17258, 2401, 26060, 1052, 18613, 1012, 102, 101, 2023, 21025, 2705, 12083, 3277, 3303, 2033, 2000, 2131, 2062, 2920, 2007, 1996, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1006, 1007, 1012, 102, 101, 2085, 2009, 3504, 2066, 2008, 13332, 2026, 3291, 1012, 102, 101, 1996, 23435, 12314, 14924, 4818, 9530, 6767, 7630, 3508, 2389, 15756, 6125, 2224, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1006, 1007, 1998, 1045, 2689, 2085, 1996, 3642, 2013, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 13, 38, 44, 78, 99, 105, 122, 175, 209, 231, 261, 272, 309, 323, 327, 341], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54428903", "vertexSet": [[{"sent_id": 2, "name": "tf.session", "pos": [105, 109]}, {"sent_id": 3, "name": "tf.session", "pos": [157, 161]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [75, 81]}], [{"sent_id": 2, "name": "tf.contrib.eager", "pos": [75, 83]}], [{"sent_id": 2, "name": "tf.contrib.eager.defun", "pos": [75, 86]}]], "sents": ["Your code, in tensorflow 2.0 will look something like (please note you can already try the nightly build of tensorflow 2.0 (https://pypi.org/project/tf-nightly-2.0-preview/))", "<code>Code Snippet</code>.", "Please note that tf.contrib.eager.defun and  Autograph (available in 1.12 and above), that are the base of @tf.session are still under active development and are experimental, hence implementation is a little bit buggy right now; so if it fails to run or it is slower probably is worth opening an issue on Github.", "In 2.0 @tf.session will merge the pros of both defun and autograd"], "sent_idxs": [101, 2115, 3642, 1010, 1999, 23435, 12314, 1016, 1012, 1014, 2097, 2298, 2242, 2066, 1006, 3531, 3602, 2017, 2064, 2525, 3046, 1996, 22390, 3857, 1997, 23435, 12314, 1016, 1012, 1014, 1006, 16770, 1024, 1013, 1013, 1052, 22571, 2072, 1012, 8917, 1013, 2622, 1013, 1056, 2546, 1011, 22390, 1011, 1016, 1012, 1014, 1011, 19236, 1013, 1007, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3531, 3602, 2008, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9461, 1012, 13366, 4609, 1998, 8285, 14413, 1006, 2800, 1999, 1015, 1012, 2260, 1998, 2682, 1007, 1010, 2008, 2024, 1996, 2918, 1997, 1030, 1056, 2546, 1012, 5219, 2024, 2145, 2104, 3161, 2458, 1998, 2024, 6388, 1010, 6516, 7375, 2003, 1037, 2210, 2978, 11829, 6292, 2157, 2085, 1025, 2061, 2065, 2009, 11896, 2000, 2448, 2030, 2009, 2003, 12430, 2763, 2003, 4276, 3098, 2019, 3277, 2006, 21025, 2705, 12083, 1012, 102, 101, 1999, 1016, 1012, 1014, 1030, 1056, 2546, 1012, 5219, 2097, 13590, 1996, 4013, 2015, 1997, 2119, 13366, 4609, 1998, 8285, 16307, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 57, 71, 151, 174], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39358402", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [5, 9]}, {"sent_id": 1, "name": "tf.train", "pos": [42, 46]}, {"sent_id": 3, "name": "tf.train", "pos": [161, 165]}], [{"sent_id": 1, "name": "tf.train.shuffle_batch", "pos": [42, 50]}], [{"sent_id": 0, "name": "tf.train.string_input_producer", "pos": [5, 15]}, {"sent_id": 3, "name": "tf.train.string_input_producer", "pos": [161, 171]}]], "sents": ["The default behavior for tf.train.string_input_producer(fnames) is to produce an infinite number of copies of the elements in fnames.", "Therefore, since your tf.train.shuffle_batch() capacity is larger than the total number of elements in your input files (5 elements per file * 10 files = 50 elements), and the min_after_dequeue is also larger than the number of elements, the queue will contain at least two full copies of the input data before the first batch is produced.", "As a result, it is likely that some batches will contain duplicate data.", "If you only want to process each example once, you can set an explicit num_epochs=1 when creating the tf.train.string_input_producer().", "For example:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 12398, 5248, 2005, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1042, 18442, 2015, 1007, 2003, 2000, 3965, 2019, 10709, 2193, 1997, 4809, 1997, 1996, 3787, 1999, 1042, 18442, 2015, 1012, 102, 101, 3568, 1010, 2144, 2115, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 3977, 2003, 3469, 2084, 1996, 2561, 2193, 1997, 3787, 1999, 2115, 7953, 6764, 1006, 1019, 3787, 2566, 5371, 1008, 2184, 6764, 1027, 2753, 3787, 1007, 1010, 1998, 1996, 8117, 1035, 2044, 1035, 2139, 4226, 5657, 2003, 2036, 3469, 2084, 1996, 2193, 1997, 3787, 1010, 1996, 24240, 2097, 5383, 2012, 2560, 2048, 2440, 4809, 1997, 1996, 7953, 2951, 2077, 1996, 2034, 14108, 2003, 2550, 1012, 102, 101, 2004, 1037, 2765, 1010, 2009, 2003, 3497, 2008, 2070, 14108, 2229, 2097, 5383, 24473, 2951, 1012, 102, 101, 2065, 2017, 2069, 2215, 2000, 2832, 2169, 2742, 2320, 1010, 2017, 2064, 2275, 2019, 13216, 16371, 2213, 1035, 25492, 2015, 1027, 1015, 2043, 4526, 1996, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 117, 135, 175, 180, 194], "sent_pos": [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56722207", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [88, 93]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [6, 11]}], [{"sent_id": 2, "name": "tf.keras.applications", "pos": [88, 95]}]], "sents": ["As the error indicates, tf.placeholder() which is used as placeholder for feeding data to a tf Session using feed_dict, is incompatible with eager mode.", "This link nicely explains it with an example: https://github.com/tensorflow/tensorflow/issues/18165#issuecomment-377841925", "You can use models from tf.keras.applications for this purpose.", "I've tried with TF2.0 Beta release.", "https://www.tensorflow.org/beta/tutorials/images/transfer_learning#create_the_base_model_from_the_pre-trained_convnets", "<code>Code Snippet</code>.", "True", "ResNeXt models are not available(I had to make some changes like copying resnext.py from keras/applications to tensorflow/python/keras/applications and changes to __init__.py etc.)", "but you can try with the existing models like ResNet50, if they work then you can try porting ResNeXt."], "sent_idxs": [101, 2004, 1996, 7561, 7127, 1010, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 2029, 2003, 2109, 2004, 2173, 14528, 2005, 8521, 2951, 2000, 1037, 1056, 2546, 5219, 2478, 5438, 1035, 4487, 6593, 1010, 2003, 25876, 2007, 9461, 5549, 1012, 102, 101, 2023, 4957, 19957, 7607, 2009, 2007, 2019, 2742, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3314, 1013, 12357, 2629, 1001, 3277, 9006, 3672, 1011, 4261, 2581, 2620, 23632, 2683, 17788, 102, 101, 2017, 2064, 2224, 4275, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 5097, 2005, 2023, 3800, 1012, 102, 101, 1045, 1005, 2310, 2699, 2007, 1056, 2546, 2475, 1012, 1014, 8247, 2713, 1012, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 8247, 1013, 14924, 26340, 1013, 4871, 1013, 4651, 1035, 4083, 1001, 3443, 1035, 1996, 1035, 2918, 1035, 2944, 1035, 2013, 1035, 1996, 1035, 3653, 1011, 4738, 1035, 9530, 16022, 8454, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2995, 102, 101, 24501, 2638, 18413, 4275, 2024, 2025, 2800, 1006, 1045, 2018, 2000, 2191, 2070, 3431, 2066, 24731, 24501, 2638, 18413, 1012, 1052, 2100, 2013, 17710, 8180, 1013, 5097, 2000, 23435, 12314, 1013, 18750, 1013, 17710, 8180, 1013, 5097, 1998, 3431, 2000, 1035, 1035, 1999, 4183, 1035, 1035, 1012, 1052, 2100, 4385, 1012, 1007, 102, 101, 2021, 2017, 2064, 3046, 2007, 1996, 4493, 4275, 2066, 24501, 7159, 12376, 1010, 2065, 2027, 2147, 2059, 2017, 2064, 3046, 3417, 2075, 24501, 2638, 18413, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 40, 82, 100, 115, 158, 172, 175, 229, 257], "sent_pos": [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50616742", "vertexSet": [[{"sent_id": 2, "name": "tf.layers", "pos": [63, 67]}, {"sent_id": 2, "name": "tf.layers", "pos": [73, 77]}], [{"sent_id": 2, "name": "tf.layers.dense", "pos": [73, 79]}], [{"sent_id": 3, "name": "tf.initializers", "pos": [86, 92]}], [{"sent_id": 2, "name": "tf.layers.conv2d", "pos": [63, 72]}], [{"sent_id": 4, "name": "tf.constant_initializer", "pos": [119, 126]}]], "sents": ["In general, you will need to define initializer when you define the model.", "For canned Estimators, you don't have access to their initializer.", "If you define your own model_fn, you can make use of kernel_initializer an bias_initializer for tf.layers.conv2d and tf.layers.dense.", "Tensorflow has a tf.initializers module which allows you to define your own initializer.", "For example, if you have specific values in mind, you can use tf.constant_initializer(init_value) to initialize weights of your choice."], "sent_idxs": [101, 1999, 2236, 1010, 2017, 2097, 2342, 2000, 9375, 3988, 17629, 2043, 2017, 9375, 1996, 2944, 1012, 102, 101, 2005, 27141, 9765, 9581, 6591, 1010, 2017, 2123, 1005, 1056, 2031, 3229, 2000, 2037, 3988, 17629, 1012, 102, 101, 2065, 2017, 9375, 2115, 2219, 2944, 1035, 1042, 2078, 1010, 2017, 2064, 2191, 2224, 1997, 16293, 1035, 3988, 17629, 2019, 13827, 1035, 3988, 17629, 2005, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1998, 1056, 2546, 1012, 9014, 1012, 9742, 1012, 102, 101, 23435, 12314, 2038, 1037, 1056, 2546, 1012, 3988, 17629, 2015, 11336, 2029, 4473, 2017, 2000, 9375, 2115, 2219, 3988, 17629, 1012, 102, 101, 2005, 2742, 1010, 2065, 2017, 2031, 3563, 5300, 1999, 2568, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1006, 1999, 4183, 1035, 3643, 1007, 2000, 3988, 4697, 15871, 1997, 2115, 3601, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 18, 37, 81, 104, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43703735", "vertexSet": [[{"sent_id": 10, "name": "tf.test", "pos": [225, 229]}, {"sent_id": 11, "name": "tf.test", "pos": [246, 250]}], [{"sent_id": 0, "name": "tf.session", "pos": [7, 11]}], [{"sent_id": 0, "name": "tf.configproto", "pos": [16, 24]}], [{"sent_id": 11, "name": "tf.test.gpu_device_name", "pos": [246, 257]}], [{"sent_id": 10, "name": "tf.test.is_gpu_available", "pos": [225, 236]}]], "sents": ["Apart from using sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) which is outlined in other answers as well as in the official TensorFlow documentation, you can try to assign a computation to the gpu and see whether you have an error.", "<code>Code Snippet</code>.", "Here", "\"/cpu:0\": The CPU of your machine..", "\"/gpu:0\": The GPU of your machine, if you have one..", "If you have a gpu and can use it, you will see the result.", "Otherwise you will see an error with a long stacktrace.", "In the end you will have something like this:", "Cannot assign a device to node 'MatMul': Could not satisfy explicit\n  device specification '/device:GPU:0' because no devices matching that\n  specification are registered in this process", "Recently a few helpful functions appeared in TF:", "tf.test.is_gpu_available tells if the gpu is available.", "tf.test.gpu_device_name returns the name of the gpu device.", "You can also check for available devices in the session:", "<code>Code Snippet</code>.", "devices will return you something like", "<code>Code Snippet</code>."], "sent_idxs": [101, 4237, 2013, 2478, 7367, 4757, 1027, 1056, 2546, 1012, 5219, 1006, 9530, 8873, 2290, 1027, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 1006, 8833, 1035, 5080, 1035, 11073, 1027, 2995, 1007, 1007, 2029, 2003, 14801, 1999, 2060, 6998, 2004, 2092, 2004, 1999, 1996, 2880, 23435, 12314, 12653, 1010, 2017, 2064, 3046, 2000, 23911, 1037, 22334, 2000, 1996, 14246, 2226, 1998, 2156, 3251, 2017, 2031, 2019, 7561, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2182, 102, 101, 1000, 1013, 17368, 1024, 1014, 1000, 1024, 1996, 17368, 1997, 2115, 3698, 1012, 1012, 102, 101, 1000, 1013, 14246, 2226, 1024, 1014, 1000, 1024, 1996, 14246, 2226, 1997, 2115, 3698, 1010, 2065, 2017, 2031, 2028, 1012, 1012, 102, 101, 2065, 2017, 2031, 1037, 14246, 2226, 1998, 2064, 2224, 2009, 1010, 2017, 2097, 2156, 1996, 2765, 1012, 102, 101, 4728, 2017, 2097, 2156, 2019, 7561, 2007, 1037, 2146, 9991, 6494, 3401, 1012, 102, 101, 1999, 1996, 2203, 2017, 2097, 2031, 2242, 2066, 2023, 1024, 102, 101, 3685, 23911, 1037, 5080, 2000, 13045, 1005, 13523, 12274, 2140, 1005, 1024, 2071, 2025, 13225, 13216, 5080, 12827, 1005, 1013, 5080, 1024, 14246, 2226, 1024, 1014, 1005, 2138, 2053, 5733, 9844, 2008, 12827, 2024, 5068, 1999, 2023, 2832, 102, 101, 3728, 1037, 2261, 14044, 4972, 2596, 1999, 1056, 2546, 1024, 102, 101, 1056, 2546, 1012, 3231, 1012, 2003, 1035, 14246, 2226, 1035, 2800, 4136, 2065, 1996, 14246, 2226, 2003, 2800, 1012, 102, 101, 1056, 2546, 1012, 3231, 1012, 14246, 2226, 1035, 5080, 1035, 2171, 5651, 1996, 2171, 1997, 1996, 14246, 2226, 5080, 1012, 102, 101, 2017, 2064, 2036, 4638, 2005, 2800, 5733, 1999, 1996, 5219, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5733, 2097, 2709, 2017, 2242, 2066, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 70, 84, 87, 103, 126, 145, 160, 172, 212, 224, 245, 267, 280, 294, 302, 316], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47054931", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [65, 71]}], [{"sent_id": 2, "name": "tf.contrib.layers", "pos": [65, 73]}], [{"sent_id": 2, "name": "tf.contrib.layers.softmax", "pos": [65, 76]}]], "sents": ["[Updated] The new versions of code are identical in the functional sense: you will get the same pred_val on each iteration and, as a result, the same preds.", "But there's a catch in the second snippet and it's important.", "Each consecutive call of tf.contrib.layers.softmax creates a new node in the graph, even though it's used only once.", "If you print the graph definition, you'll see 100 softmax ops, while the first snippet will have just one.", "Another fun side effect: this node will not be present in tensorboard.", "Try to avoid creation of ops while training, because it's a sure way to consume all of your RAM and crash the process.", "In other words, the first snippet is better.", "[Original answer] No, just because you've defined logits_sf node in the graph doesn't mean it will be executed in session.", "Your second code snippet evaluates only logits, which doesn't depend on the softmax op.", "So the results will be different: the first one will produce the probability distribution in each pred_val, while the second one will produce raw logits."], "sent_idxs": [101, 1031, 7172, 1033, 1996, 2047, 4617, 1997, 3642, 2024, 7235, 1999, 1996, 8360, 3168, 1024, 2017, 2097, 2131, 1996, 2168, 3653, 2094, 1035, 11748, 2006, 2169, 27758, 1998, 1010, 2004, 1037, 2765, 1010, 1996, 2168, 3653, 5104, 1012, 102, 101, 2021, 2045, 1005, 1055, 1037, 4608, 1999, 1996, 2117, 1055, 3490, 29519, 1998, 2009, 1005, 1055, 2590, 1012, 102, 101, 2169, 5486, 2655, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 3730, 17848, 9005, 1037, 2047, 13045, 1999, 1996, 10629, 1010, 2130, 2295, 2009, 1005, 1055, 2109, 2069, 2320, 1012, 102, 101, 2065, 2017, 6140, 1996, 10629, 6210, 1010, 2017, 1005, 2222, 2156, 2531, 3730, 17848, 23092, 1010, 2096, 1996, 2034, 1055, 3490, 29519, 2097, 2031, 2074, 2028, 1012, 102, 101, 2178, 4569, 2217, 3466, 1024, 2023, 13045, 2097, 2025, 2022, 2556, 1999, 23435, 6277, 1012, 102, 101, 3046, 2000, 4468, 4325, 1997, 23092, 2096, 2731, 1010, 2138, 2009, 1005, 1055, 1037, 2469, 2126, 2000, 16678, 2035, 1997, 2115, 8223, 1998, 5823, 1996, 2832, 1012, 102, 101, 1999, 2060, 2616, 1010, 1996, 2034, 1055, 3490, 29519, 2003, 2488, 1012, 102, 101, 1031, 2434, 3437, 1033, 2053, 1010, 2074, 2138, 2017, 1005, 2310, 4225, 8833, 12762, 1035, 16420, 13045, 1999, 1996, 10629, 2987, 1005, 1056, 2812, 2009, 2097, 2022, 6472, 1999, 5219, 1012, 102, 101, 2115, 2117, 3642, 1055, 3490, 29519, 16157, 2015, 2069, 8833, 12762, 1010, 2029, 2987, 1005, 1056, 12530, 2006, 1996, 3730, 17848, 6728, 1012, 102, 101, 2061, 1996, 3463, 2097, 2022, 2367, 1024, 1996, 2034, 2028, 2097, 3965, 1996, 9723, 4353, 1999, 2169, 3653, 2094, 1035, 11748, 1010, 2096, 1996, 2117, 2028, 2097, 3965, 6315, 8833, 12762, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 40, 60, 94, 123, 140, 169, 183, 216, 241, 275], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48715382", "vertexSet": [[{"sent_id": 4, "name": "tf.audio", "pos": [133, 137]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [11, 17]}, {"sent_id": 1, "name": "tf.contrib", "pos": [46, 52]}, {"sent_id": 4, "name": "tf.contrib", "pos": [96, 102]}], [{"sent_id": 1, "name": "tf.contrib.ffmpeg", "pos": [11, 21]}, {"sent_id": 1, "name": "tf.contrib.ffmpeg", "pos": [46, 56]}], [{"sent_id": 1, "name": "tf.contrib.ffmpeg.decode_audio", "pos": [11, 26]}], [{"sent_id": 1, "name": "tf.contrib.ffmpeg.encode_audio", "pos": [46, 61]}]], "sents": ["TensorFlow 1.x:.", "The tf.contrib.ffmpeg.decode_audio() op can load audio data (including in WAV format) into a tensor, and the tf.contrib.ffmpeg.encode_audio() can covert it back into audio data.", "<code>Code Snippet</code>.", "TensorFlow 2.x.", "The tf.contrib module has been deprecated, but you are still able to load and save audio files in 16-bit PCM WAV format using eager execution and tf.audio:", "<code>Code Snippet</code>."], "sent_idxs": [101, 23435, 12314, 1015, 1012, 1060, 1024, 1012, 102, 101, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 21461, 8737, 13910, 1012, 21933, 3207, 1035, 5746, 1006, 1007, 6728, 2064, 7170, 5746, 2951, 1006, 2164, 1999, 11333, 2615, 4289, 1007, 2046, 1037, 23435, 1010, 1998, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 21461, 8737, 13910, 1012, 4372, 16044, 1035, 5746, 1006, 1007, 2064, 19813, 2009, 2067, 2046, 5746, 2951, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 23435, 12314, 1016, 1012, 1060, 1012, 102, 101, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 11336, 2038, 2042, 2139, 28139, 12921, 1010, 2021, 2017, 2024, 2145, 2583, 2000, 7170, 1998, 3828, 5746, 6764, 1999, 2385, 1011, 2978, 7473, 2213, 11333, 2615, 4289, 2478, 9461, 7781, 1998, 1056, 2546, 1012, 5746, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 9, 72, 86, 94, 139, 153], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58316577", "vertexSet": [[{"sent_id": 2, "name": "tf.size", "pos": [37, 41]}], [{"sent_id": 2, "name": "tf.equal", "pos": [32, 36]}], [{"sent_id": 4, "name": "tf.count_nonzero", "pos": [69, 77]}]], "sents": ["There are multiple ways to solve this, basically you are trying to identify a null tensor.", "Possible solutions can be:", "is_empty = tf.equal(tf.size(boolean_tensor), 0).", "If not empty it will give false.", "Count non zeros number using tf.count_nonzero(boolean_tensor).", "By simply printing the tensor and checking the vaules."], "sent_idxs": [101, 2045, 2024, 3674, 3971, 2000, 9611, 2023, 1010, 10468, 2017, 2024, 2667, 2000, 6709, 1037, 19701, 23435, 1012, 102, 101, 2825, 7300, 2064, 2022, 1024, 102, 101, 2003, 1035, 4064, 1027, 1056, 2546, 1012, 5020, 1006, 1056, 2546, 1012, 2946, 1006, 22017, 20898, 1035, 23435, 1007, 1010, 1014, 1007, 1012, 102, 101, 2065, 2025, 4064, 2009, 2097, 2507, 6270, 1012, 102, 101, 4175, 2512, 5717, 2015, 2193, 2478, 1056, 2546, 1012, 4175, 1035, 2512, 6290, 2080, 1006, 22017, 20898, 1035, 23435, 1007, 1012, 102, 101, 2011, 3432, 8021, 1996, 23435, 1998, 9361, 1996, 12436, 16308, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 27, 52, 62, 85, 98], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50533226", "vertexSet": [[{"sent_id": 4, "name": "tf.contrib", "pos": [56, 62]}], [{"sent_id": 9, "name": "tf.transpose", "pos": [250, 255]}], [{"sent_id": 4, "name": "tf.contrib.seq2seq", "pos": [56, 68]}], [{"sent_id": 4, "name": "tf.contrib.seq2seq.dynamic_decode", "pos": [56, 73]}]], "sents": ["I wrote followig toy code to explore it a little bit myself.", "<code>Code Snippet</code>.", "The print is as follows:", "<code>Code Snippet</code>.", "The outputs of tf.contrib.seq2seq.dynamic_decode(BeamSearchDecoder) is actually an instance of class FinalBeamSearchDecoderOutput which consists of:", "predicted_ids: Final outputs returned by the beam search after all decoding is finished.", "A tensor of shape [batch_size, num_steps, beam_width] (or [num_steps, batch_size, beam_width] if output_time_major is True).", "Beams are ordered from best to worst.", "beam_search_decoder_output: An instance of BeamSearchDecoderOutput that describes the state of the beam search.", "So need to make sure the final predictions/translations are of shape [beam_width, batch_size, num_steps] by transpose([2, 0, 1]) or tf.transpose(final_predicted_ids) if output_time_major=True."], "sent_idxs": [101, 1045, 2626, 3582, 8004, 9121, 3642, 2000, 8849, 2009, 1037, 2210, 2978, 2870, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 6140, 2003, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 27852, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 8790, 1035, 21933, 3207, 1006, 13110, 14644, 2818, 3207, 16044, 2099, 1007, 2003, 2941, 2019, 6013, 1997, 2465, 2345, 28302, 17310, 11140, 3207, 16044, 22494, 25856, 4904, 2029, 3774, 1997, 1024, 102, 101, 10173, 1035, 8909, 2015, 1024, 2345, 27852, 2513, 2011, 1996, 7504, 3945, 2044, 2035, 21933, 4667, 2003, 2736, 1012, 102, 101, 1037, 23435, 1997, 4338, 1031, 14108, 1035, 2946, 1010, 16371, 2213, 1035, 4084, 1010, 7504, 1035, 9381, 1033, 1006, 2030, 1031, 16371, 2213, 1035, 4084, 1010, 14108, 1035, 2946, 1010, 7504, 1035, 9381, 1033, 2065, 6434, 1035, 2051, 1035, 2350, 2003, 2995, 1007, 1012, 102, 101, 13110, 2024, 3641, 2013, 2190, 2000, 5409, 1012, 102, 101, 7504, 1035, 3945, 1035, 21933, 4063, 1035, 6434, 1024, 2019, 6013, 1997, 13110, 14644, 2818, 3207, 16044, 22494, 25856, 4904, 2008, 5577, 1996, 2110, 1997, 1996, 7504, 3945, 1012, 102, 101, 2061, 2342, 2000, 2191, 2469, 1996, 2345, 20932, 1013, 11913, 2024, 1997, 4338, 1031, 7504, 1035, 9381, 1010, 14108, 1035, 2946, 1010, 16371, 2213, 1035, 4084, 1033, 2011, 9099, 20688, 1006, 1031, 1016, 1010, 1014, 1010, 1015, 1033, 1007, 2030, 1056, 2546, 1012, 9099, 20688, 1006, 2345, 1035, 10173, 1035, 8909, 2015, 1007, 2065, 6434, 1035, 2051, 1035, 2350, 1027, 2995, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 30, 38, 52, 101, 122, 168, 178, 209, 273], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55518654", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib.layers", "pos": [66, 74]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [10, 16]}, {"sent_id": 2, "name": "tf.contrib", "pos": [66, 72]}], [{"sent_id": 2, "name": "tf.layers", "pos": [55, 59]}]], "sents": ["According to an RFC document from August 2018, tf.contrib will be deleted with some of its parts becoming standalone projects (such as tensorflow/probability).", "This not the case of tf.conrib.layers.", "Even tf.layers (which was distilled from tf.contrib.layers) will be no longer supported.", "A detailed description of how to use the Keras API instead is provided in the migration guide."], "sent_idxs": [101, 2429, 2000, 2019, 14645, 6254, 2013, 2257, 2760, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 2097, 2022, 17159, 2007, 2070, 1997, 2049, 3033, 3352, 26609, 3934, 1006, 2107, 2004, 23435, 12314, 1013, 9723, 1007, 1012, 102, 101, 2023, 2025, 1996, 2553, 1997, 1056, 2546, 1012, 9530, 3089, 2497, 1012, 9014, 1012, 102, 101, 2130, 1056, 2546, 1012, 9014, 1006, 2029, 2001, 4487, 16643, 11001, 2013, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1007, 2097, 2022, 2053, 2936, 3569, 1012, 102, 101, 1037, 6851, 6412, 1997, 2129, 2000, 2224, 1996, 17710, 8180, 17928, 2612, 2003, 3024, 1999, 1996, 9230, 5009, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 53, 82, 103], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47608887", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [67, 73]}], [{"sent_id": 5, "name": "tf.variable", "pos": [142, 146]}], [{"sent_id": 3, "name": "tf.get_variable", "pos": [94, 100]}]], "sents": ["As far as I know, yes.", "Looking at the source code for model_variable on github, model_variable wraps variable (defined in the linked module), which itself is a wrapper for get_variable.", "This answer seems to confirm that, and this Google Groups discussion gives insight into why tf.contrib has seemingly duplicate functions, and specifically this function.", "As for which one to use, probably always tf.get_variable unless you are integrating something with slim or something else that specifically calls for it.", "I've never used model_variable myself.", "EDIT: Clarify that variable is defined in the link, not tf.Variable."], "sent_idxs": [101, 2004, 2521, 2004, 1045, 2113, 1010, 2748, 1012, 102, 101, 2559, 2012, 1996, 3120, 3642, 2005, 2944, 1035, 8023, 2006, 21025, 2705, 12083, 1010, 2944, 1035, 8023, 19735, 8023, 1006, 4225, 1999, 1996, 5799, 11336, 1007, 1010, 2029, 2993, 2003, 1037, 10236, 4842, 2005, 2131, 1035, 8023, 1012, 102, 101, 2023, 3437, 3849, 2000, 12210, 2008, 1010, 1998, 2023, 8224, 2967, 6594, 3957, 12369, 2046, 2339, 1056, 2546, 1012, 9530, 18886, 2497, 2038, 9428, 24473, 4972, 1010, 1998, 4919, 2023, 3853, 1012, 102, 101, 2004, 2005, 2029, 2028, 2000, 2224, 1010, 2763, 2467, 1056, 2546, 1012, 2131, 1035, 8023, 4983, 2017, 2024, 22380, 2242, 2007, 11754, 2030, 2242, 2842, 2008, 4919, 4455, 2005, 2009, 1012, 102, 101, 1045, 1005, 2310, 2196, 2109, 2944, 1035, 8023, 2870, 1012, 102, 101, 10086, 1024, 25037, 2008, 8023, 2003, 4225, 1999, 1996, 4957, 1010, 2025, 1056, 2546, 1012, 8023, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 50, 84, 117, 129, 148], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0]}, {"title": "39870693", "vertexSet": [[{"sent_id": 6, "name": "tf.nn", "pos": [124, 129]}], [{"sent_id": 2, "name": "tf.print", "pos": [44, 48]}], [{"sent_id": 6, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [124, 143]}]], "sents": ["I would normally say your learning rate it too high however it looks like you have ruled that out.", "You should check the magnitude of the numbers coming into and out of the layers.", "You can use tf.Print to do so.", "Maybe you are somehow inputting a black image by accident or you can find the layer where the numbers go crazy.", "Also how are you calculating the cross entropy?", "You might want to add a small epsilon inside of the log since it's value will go to infinity as its input approaches zero.", "Or better yet use the tf.nn.sparse_softmax_cross_entropy_with_logits(...) function which takes care of numerical stability for you.", "Since the cost is so high for your crossentropy it sounds like the network is outputting almost all zeros (or values close to zero).", "Since you did not post any code I can not say why.", "I think you may just be zeroing something out in the cost function calculation by accident."], "sent_idxs": [101, 1045, 2052, 5373, 2360, 2115, 4083, 3446, 2009, 2205, 2152, 2174, 2009, 3504, 2066, 2017, 2031, 5451, 2008, 2041, 1012, 102, 101, 2017, 2323, 4638, 1996, 10194, 1997, 1996, 3616, 2746, 2046, 1998, 2041, 1997, 1996, 9014, 1012, 102, 101, 2017, 2064, 2224, 1056, 2546, 1012, 6140, 2000, 2079, 2061, 1012, 102, 101, 2672, 2017, 2024, 5064, 7953, 3436, 1037, 2304, 3746, 2011, 4926, 2030, 2017, 2064, 2424, 1996, 6741, 2073, 1996, 3616, 2175, 4689, 1012, 102, 101, 2036, 2129, 2024, 2017, 20177, 1996, 2892, 23077, 1029, 102, 101, 2017, 2453, 2215, 2000, 5587, 1037, 2235, 28038, 2503, 1997, 1996, 8833, 2144, 2009, 1005, 1055, 3643, 2097, 2175, 2000, 15579, 2004, 2049, 7953, 8107, 5717, 1012, 102, 101, 2030, 2488, 2664, 2224, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1012, 1012, 1012, 1007, 3853, 2029, 3138, 2729, 1997, 15973, 9211, 2005, 2017, 1012, 102, 101, 2144, 1996, 3465, 2003, 2061, 2152, 2005, 2115, 2892, 4765, 18981, 2100, 2009, 4165, 2066, 1996, 2897, 2003, 6434, 3436, 2471, 2035, 5717, 2015, 1006, 2030, 5300, 2485, 2000, 5717, 1007, 1012, 102, 101, 2144, 2017, 2106, 2025, 2695, 2151, 3642, 1045, 2064, 2025, 2360, 2339, 1012, 102, 101, 1045, 2228, 2017, 2089, 2074, 2022, 5717, 2075, 2242, 2041, 1999, 1996, 3465, 3853, 17208, 2011, 4926, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 40, 53, 78, 89, 118, 159, 193, 208, 228], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34711711", "vertexSet": [[{"sent_id": 1, "name": "tf.gather", "pos": [24, 28]}], [{"sent_id": 4, "name": "tf.sparsetensor", "pos": [117, 123]}], [{"sent_id": 4, "name": "tf.sparse_tensor_to_dense", "pos": [126, 136]}]], "sents": ["There are several ways you could achieve this.", "The most direct adaptation of your code would be to use the tf.gather() operation to select rows from an identity matrix, as follows:", "<code>Code Snippet</code>.", "For the case you showed where there are only 3 different letters, the performance probably isn't critical.", "However, if the number of letters (and hence the size of identity_matrix) is much larger\u2014compared to the batch size\u2014you could achieve better memory efficiency by building a tf.SparseTensor and using the tf.sparse_tensor_to_dense() op to build input."], "sent_idxs": [101, 2045, 2024, 2195, 3971, 2017, 2071, 6162, 2023, 1012, 102, 101, 1996, 2087, 3622, 6789, 1997, 2115, 3642, 2052, 2022, 2000, 2224, 1996, 1056, 2546, 1012, 8587, 1006, 1007, 3169, 2000, 7276, 10281, 2013, 2019, 4767, 8185, 1010, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 1996, 2553, 2017, 3662, 2073, 2045, 2024, 2069, 1017, 2367, 4144, 1010, 1996, 2836, 2763, 3475, 1005, 1056, 4187, 1012, 102, 101, 2174, 1010, 2065, 1996, 2193, 1997, 4144, 1006, 1998, 6516, 1996, 2946, 1997, 4767, 1035, 8185, 1007, 2003, 2172, 3469, 1517, 4102, 2000, 1996, 14108, 2946, 1517, 2017, 2071, 6162, 2488, 3638, 8122, 2011, 2311, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 1998, 2478, 1996, 1056, 2546, 1012, 20288, 1035, 23435, 1035, 2000, 1035, 9742, 1006, 1007, 6728, 2000, 3857, 7953, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 11, 43, 57, 80, 144], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42708799", "vertexSet": [[{"sent_id": 0, "name": "tf.expand_dims", "pos": [4, 11]}], [{"sent_id": 2, "name": "tf.reshape", "pos": [39, 45]}]], "sents": ["You can use tf.expand_dims() to add a new dimension.", "<code>Code Snippet</code>.", "You can also use tf.reshape() for this, but would recommend you to use expand_dims, as this will also carry some values to new dimension if new shape can be satisfied.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 1006, 1007, 2000, 5587, 1037, 2047, 9812, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2036, 2224, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 2005, 2023, 1010, 2021, 2052, 16755, 2017, 2000, 2224, 7818, 1035, 11737, 2015, 1010, 2004, 2023, 2097, 2036, 4287, 2070, 5300, 2000, 2047, 9812, 2065, 2047, 4338, 2064, 2022, 8510, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2]}], "na_triple": [], "sent_ends": [0, 20, 34, 79, 93], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41578733", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [73, 77]}, {"sent_id": 2, "name": "tf.train", "pos": [100, 104]}], [{"sent_id": 0, "name": "tf.graph", "pos": [40, 44]}, {"sent_id": 1, "name": "tf.graph", "pos": [53, 57]}], [{"sent_id": 0, "name": "tf.session", "pos": [7, 11]}, {"sent_id": 5, "name": "tf.session", "pos": [192, 196]}], [{"sent_id": 2, "name": "tf.train.import_meta_graph", "pos": [73, 83]}, {"sent_id": 2, "name": "tf.train.import_meta_graph", "pos": [100, 110]}]], "sents": ["TL;DR: The tf.Session is closed between the two calls to test() in your code, but you are running into a problem because the two sessions are sharing the same tf.Graph.", "Create each session with a fresh tf.Graph to avoid the problem.", "In particular, the nodes created when you call tf.train.import_meta_graph() in the call to test(False) remain in the graph when subsequently call tf.train.import_meta_graph() in the call to test(True).", "This means that each of the two calls to session.graph.get_tensor_by_name('dummy_graph/a:0') will return the same node (which was created when you first called test()).", "There are a few ways to avoid this problem.", "The simplest is to create the tf.Session with its own graph:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2140, 1025, 2852, 1024, 1996, 1056, 2546, 1012, 5219, 2003, 2701, 2090, 1996, 2048, 4455, 2000, 3231, 1006, 1007, 1999, 2115, 3642, 1010, 2021, 2017, 2024, 2770, 2046, 1037, 3291, 2138, 1996, 2048, 6521, 2024, 6631, 1996, 2168, 1056, 2546, 1012, 10629, 1012, 102, 101, 3443, 2169, 5219, 2007, 1037, 4840, 1056, 2546, 1012, 10629, 2000, 4468, 1996, 3291, 1012, 102, 101, 1999, 3327, 1010, 1996, 14164, 2580, 2043, 2017, 2655, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 1999, 1996, 2655, 2000, 3231, 1006, 6270, 1007, 3961, 1999, 1996, 10629, 2043, 3525, 2655, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 1999, 1996, 2655, 2000, 3231, 1006, 2995, 1007, 1012, 102, 101, 2023, 2965, 2008, 2169, 1997, 1996, 2048, 4455, 2000, 5219, 1012, 10629, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 1005, 24369, 1035, 10629, 1013, 1037, 1024, 1014, 1005, 1007, 2097, 2709, 1996, 2168, 13045, 1006, 2029, 2001, 2580, 2043, 2017, 2034, 2170, 3231, 1006, 1007, 1007, 1012, 102, 101, 2045, 2024, 1037, 2261, 3971, 2000, 4468, 2023, 3291, 1012, 102, 101, 1996, 21304, 2003, 2000, 3443, 1996, 1056, 2546, 1012, 5219, 2007, 2049, 2219, 10629, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 46, 63, 122, 173, 185, 202, 216], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49506723", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [1, 5]}, {"sent_id": 4, "name": "tf.train", "pos": [70, 74]}, {"sent_id": 5, "name": "tf.train", "pos": [97, 101]}], [{"sent_id": 0, "name": "tf.train.batch", "pos": [1, 7]}], [{"sent_id": 5, "name": "tf.train.add_queue_runner", "pos": [97, 107]}], [{"sent_id": 4, "name": "tf.train.start_queue_runners", "pos": [70, 80]}]], "sents": ["tf.train.batch creates own queue runners:", "This function is implemented using a queue.", "A QueueRunner for the queue is added to the current Graph's QUEUE_RUNNER collection.", "They also need to be started.", "TensoFlow has a function that starts all queue runners collected in the graph: tf.train.start_queue_runners.", "Also it makes sense to add your queue runner to the corresponding collection using tf.train.add_queue_runner.", "This way start_queue_runners will also start your queue runner."], "sent_idxs": [101, 1056, 2546, 1012, 3345, 1012, 14108, 9005, 2219, 24240, 7190, 1024, 102, 101, 2023, 3853, 2003, 7528, 2478, 1037, 24240, 1012, 102, 101, 1037, 24240, 23195, 2005, 1996, 24240, 2003, 2794, 2000, 1996, 2783, 10629, 1005, 1055, 24240, 1035, 5479, 3074, 1012, 102, 101, 2027, 2036, 2342, 2000, 2022, 2318, 1012, 102, 101, 15295, 11253, 8261, 2038, 1037, 3853, 2008, 4627, 2035, 24240, 7190, 5067, 1999, 1996, 10629, 1024, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1012, 102, 101, 2036, 2009, 3084, 3168, 2000, 5587, 2115, 24240, 5479, 2000, 1996, 7978, 3074, 2478, 1056, 2546, 1012, 3345, 1012, 5587, 1035, 24240, 1035, 5479, 1012, 102, 101, 2023, 2126, 2707, 1035, 24240, 1035, 7190, 2097, 2036, 2707, 2115, 24240, 5479, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 13, 23, 44, 53, 82, 109, 125], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58169044", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [15, 21]}, {"sent_id": 5, "name": "tf.contrib", "pos": [100, 106]}], [{"sent_id": 1, "name": "tf.contrib.image", "pos": [15, 23]}, {"sent_id": 5, "name": "tf.contrib.image", "pos": [100, 108]}], [{"sent_id": 1, "name": "tf.contrib.image.dense_image_warp", "pos": [15, 29]}, {"sent_id": 5, "name": "tf.contrib.image.dense_image_warp", "pos": [100, 114]}]], "sents": ["After some research, here is a solution.", "it uses the tf.contrib.image.dense_image_warp function and is not really pretty, but still, it works :", "This first function computes the optical flow needed to perform the homography :", "<code>Code Snippet</code>.", "Then, it used to warp the original image to the distorted image.", "There is one trick : due to how the tf.contrib.image.dense_image_warp function works, you need to pass the inverse of the homography matrix to find the correct optical flow to use.", "<code>Code Snippet</code>.", "I still hope to find a better answer (one which does not have to compute a whole tensor of flow), therefore, I leave the question unanswered for now."], "sent_idxs": [101, 2044, 2070, 2470, 1010, 2182, 2003, 1037, 5576, 1012, 102, 101, 2009, 3594, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 9742, 1035, 3746, 1035, 24136, 3853, 1998, 2003, 2025, 2428, 3492, 1010, 2021, 2145, 1010, 2009, 2573, 1024, 102, 101, 2023, 2034, 3853, 24134, 2015, 1996, 9380, 4834, 2734, 2000, 4685, 1996, 24004, 12565, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1010, 2009, 2109, 2000, 24136, 1996, 2434, 3746, 2000, 1996, 19112, 3746, 1012, 102, 101, 2045, 2003, 2028, 7577, 1024, 2349, 2000, 2129, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 9742, 1035, 3746, 1035, 24136, 3853, 2573, 1010, 2017, 2342, 2000, 3413, 1996, 19262, 1997, 1996, 24004, 12565, 8185, 2000, 2424, 1996, 6149, 9380, 4834, 2000, 2224, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 2145, 3246, 2000, 2424, 1037, 2488, 3437, 1006, 2028, 2029, 2515, 2025, 2031, 2000, 24134, 1037, 2878, 23435, 1997, 4834, 1007, 1010, 3568, 1010, 1045, 2681, 1996, 3160, 14477, 3619, 13777, 2098, 2005, 2085, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 11, 43, 60, 74, 90, 138, 152, 190], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44135179", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [2, 6]}], [{"sent_id": 2, "name": "tf.truncated_normal", "pos": [82, 88]}], [{"sent_id": 2, "name": "tf.initialize_all_variables", "pos": [58, 67]}]], "sents": ["The tf.Variable() Op is using the \"initial\" variable as its initial value.", "If you look at the help for Variable, you will see the first parameter in the _ init _ method is \"initial_value\".", "Your code calls \"tf.initialize_all_variables()\" only once which calls the initilaize op the \"tf.truncated_normal\" which creates the [2,3] matrix to initialize output to the same value.", "Your code then prints 2 copies of that variable.", "If you would like to re-init the variable, you need to explicitly state that like this:", "<code>Code Snippet</code>.", "This might not be the functionality you are looking for as this has the side effect or re-initializing all variables (trainging weights, etc).", "If you are looking to get just the random data set, call the initial op directly.", "Also note, since you don't have any variables or other Ops that require initialization you don't need that Op to execute to prepare the graph.", "<code>Code Snippet</code>.", "You can directly mix the \"initial\" op with math operators as well.", "So if you are looking for random variable at each sess.run() don't use a variable but use the initial Op directly."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 8023, 1006, 1007, 6728, 2003, 2478, 1996, 1000, 3988, 1000, 8023, 2004, 2049, 3988, 3643, 1012, 102, 101, 2065, 2017, 2298, 2012, 1996, 2393, 2005, 8023, 1010, 2017, 2097, 2156, 1996, 2034, 16381, 1999, 1996, 1035, 1999, 4183, 1035, 4118, 2003, 1000, 3988, 1035, 3643, 1000, 1012, 102, 101, 2115, 3642, 4455, 1000, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 1000, 2069, 2320, 2029, 4455, 1996, 1999, 25090, 19771, 4371, 6728, 1996, 1000, 1056, 2546, 1012, 25449, 1035, 3671, 1000, 2029, 9005, 1996, 1031, 1016, 1010, 1017, 1033, 8185, 2000, 3988, 4697, 6434, 2000, 1996, 2168, 3643, 1012, 102, 101, 2115, 3642, 2059, 11204, 1016, 4809, 1997, 2008, 8023, 1012, 102, 101, 2065, 2017, 2052, 2066, 2000, 2128, 1011, 1999, 4183, 1996, 8023, 1010, 2017, 2342, 2000, 12045, 2110, 2008, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2453, 2025, 2022, 1996, 15380, 2017, 2024, 2559, 2005, 2004, 2023, 2038, 1996, 2217, 3466, 2030, 2128, 1011, 3988, 6026, 2035, 10857, 1006, 3345, 4726, 15871, 1010, 4385, 1007, 1012, 102, 101, 2065, 2017, 2024, 2559, 2000, 2131, 2074, 1996, 6721, 2951, 2275, 1010, 2655, 1996, 3988, 6728, 3495, 1012, 102, 101, 2036, 3602, 1010, 2144, 2017, 2123, 1005, 1056, 2031, 2151, 10857, 2030, 2060, 23092, 2008, 5478, 3988, 3989, 2017, 2123, 1005, 1056, 2342, 2008, 6728, 2000, 15389, 2000, 7374, 1996, 10629, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 3495, 4666, 1996, 1000, 3988, 1000, 6728, 2007, 8785, 9224, 2004, 2092, 1012, 102, 101, 2061, 2065, 2017, 2024, 2559, 2005, 6721, 8023, 2012, 2169, 7367, 4757, 1012, 2448, 1006, 1007, 2123, 1005, 1056, 2224, 1037, 8023, 2021, 2224, 1996, 3988, 6728, 3495, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 53, 108, 120, 143, 157, 190, 210, 244, 258, 275, 306], "sent_pos": [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62713066", "vertexSet": [[{"sent_id": 4, "name": "tf.function", "pos": [119, 123]}, {"sent_id": 5, "name": "tf.function", "pos": [163, 167]}, {"sent_id": 5, "name": "tf.function", "pos": [193, 197]}, {"sent_id": 6, "name": "tf.function", "pos": [268, 272]}, {"sent_id": 8, "name": "tf.function", "pos": [307, 311]}, {"sent_id": 8, "name": "tf.function", "pos": [316, 320]}], [{"sent_id": 9, "name": "tf.py_function", "pos": [344, 351]}], [{"sent_id": 9, "name": "tf.numpy_function", "pos": [335, 343]}]], "sents": ["Yes, \"tracing\" means to run a Python function and \"record\" its TensorFlow operations in a graph.", "Note the traced code may not exactly correspond to the written Python code, if Autograph has performed some transformation.", "Tracing is ideally only done once, the first time the function is called, so subsequent calls can directly use the traced graph and save the Python code execution.", "As you say, though, future calls may require retracing the function depending on the given arguments, as explained in the link you posted.", "You can call a @tf.function from a function that works in eager mode, in which case, yes, it will sort of \"mix\" both modes.", "But if you call an unnanotated function from a @tf.function, then its code will also be traced - that is, you cannot temporarily go back to eager/Python mode from within a @tf.function.", "That is the reason why, at some point, there was the suggestion that you only needed to annotate higher-level functions, because the lower-level ones would be \"graphed\" too anyway - although it's not so clear-cut when one should or should not annotate a function, see Should I use @tf.function for all functions?", "and this GitHub discussion.", "EDIT: When I say \"you cannot temporarily go back to eager/Python mode from within a @tf.function\", I mean @tf.function cannot go out of \"traced\" mode.", "Of course, using tf.numpy_function or tf.py_function you can have a traced function that uses eager/Python mode, which will be encapsulated in an operation as part of the traced graph."], "sent_idxs": [101, 2748, 1010, 1000, 16907, 1000, 2965, 2000, 2448, 1037, 18750, 3853, 1998, 1000, 2501, 1000, 2049, 23435, 12314, 3136, 1999, 1037, 10629, 1012, 102, 101, 3602, 1996, 9551, 3642, 2089, 2025, 3599, 17254, 2000, 1996, 2517, 18750, 3642, 1010, 2065, 8285, 14413, 2038, 2864, 2070, 8651, 1012, 102, 101, 16907, 2003, 28946, 2069, 2589, 2320, 1010, 1996, 2034, 2051, 1996, 3853, 2003, 2170, 1010, 2061, 4745, 4455, 2064, 3495, 2224, 1996, 9551, 10629, 1998, 3828, 1996, 18750, 3642, 7781, 1012, 102, 101, 2004, 2017, 2360, 1010, 2295, 1010, 2925, 4455, 2089, 5478, 2128, 6494, 6129, 1996, 3853, 5834, 2006, 1996, 2445, 9918, 1010, 2004, 4541, 1999, 1996, 4957, 2017, 6866, 1012, 102, 101, 2017, 2064, 2655, 1037, 1030, 1056, 2546, 1012, 3853, 2013, 1037, 3853, 2008, 2573, 1999, 9461, 5549, 1010, 1999, 2029, 2553, 1010, 2748, 1010, 2009, 2097, 4066, 1997, 1000, 4666, 1000, 2119, 11583, 1012, 102, 101, 2021, 2065, 2017, 2655, 2019, 4895, 7229, 17287, 3064, 3853, 2013, 1037, 1030, 1056, 2546, 1012, 3853, 1010, 2059, 2049, 3642, 2097, 2036, 2022, 9551, 1011, 2008, 2003, 1010, 2017, 3685, 8184, 2175, 2067, 2000, 9461, 1013, 18750, 5549, 2013, 2306, 1037, 1030, 1056, 2546, 1012, 3853, 1012, 102, 101, 2008, 2003, 1996, 3114, 2339, 1010, 2012, 2070, 2391, 1010, 2045, 2001, 1996, 10293, 2008, 2017, 2069, 2734, 2000, 5754, 17287, 2618, 3020, 1011, 2504, 4972, 1010, 2138, 1996, 2896, 1011, 2504, 3924, 2052, 2022, 1000, 10629, 2098, 1000, 2205, 4312, 1011, 2348, 2009, 1005, 1055, 2025, 2061, 3154, 1011, 3013, 2043, 2028, 2323, 2030, 2323, 2025, 5754, 17287, 2618, 1037, 3853, 1010, 2156, 2323, 1045, 2224, 1030, 1056, 2546, 1012, 3853, 2005, 2035, 4972, 1029, 102, 101, 1998, 2023, 21025, 2705, 12083, 6594, 1012, 102, 101, 10086, 1024, 2043, 1045, 2360, 1000, 2017, 3685, 8184, 2175, 2067, 2000, 9461, 1013, 18750, 5549, 2013, 2306, 1037, 1030, 1056, 2546, 1012, 3853, 1000, 1010, 1045, 2812, 1030, 1056, 2546, 1012, 3853, 3685, 2175, 2041, 1997, 1000, 9551, 1000, 5549, 1012, 102, 101, 1997, 2607, 1010, 2478, 1056, 2546, 1012, 16371, 8737, 2100, 1035, 3853, 2030, 1056, 2546, 1012, 1052, 2100, 1035, 3853, 2017, 2064, 2031, 1037, 9551, 3853, 2008, 3594, 9461, 1013, 18750, 5549, 1010, 2029, 2097, 2022, 4372, 17695, 23722, 4383, 1999, 2019, 3169, 2004, 2112, 1997, 1996, 9551, 10629, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 25, 49, 82, 113, 149, 199, 277, 286, 330, 382], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58772596", "vertexSet": [[{"sent_id": 3, "name": "tf.keras", "pos": [99, 104]}], [{"sent_id": 3, "name": "tf.keras.callbacks", "pos": [99, 107]}], [{"sent_id": 3, "name": "tf.keras.callbacks.tensorboard", "pos": [99, 110]}]], "sents": ["I got similar error, and tried different learning rates, batch sizes, loss functions, and model architectures without any luck.", "But then I noticed that I can train my model just fine if I'm not using tensorboard callback.", "Looks like \"Nan in summary histogram\" refers to saving model weights histogram, which somehow makes those Nans explicit.", "Turning off histograms in tensorboard callback solved the issue for me:\ntf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)"], "sent_idxs": [101, 1045, 2288, 2714, 7561, 1010, 1998, 2699, 2367, 4083, 6165, 1010, 14108, 10826, 1010, 3279, 4972, 1010, 1998, 2944, 4294, 2015, 2302, 2151, 6735, 1012, 102, 101, 2021, 2059, 1045, 4384, 2008, 1045, 2064, 3345, 2026, 2944, 2074, 2986, 2065, 1045, 1005, 1049, 2025, 2478, 23435, 6277, 2655, 5963, 1012, 102, 101, 3504, 2066, 1000, 16660, 1999, 12654, 2010, 3406, 13113, 1000, 5218, 2000, 7494, 2944, 15871, 2010, 3406, 13113, 1010, 2029, 5064, 3084, 2216, 16660, 2015, 13216, 1012, 102, 101, 3810, 2125, 2010, 3406, 13113, 2015, 1999, 23435, 6277, 2655, 5963, 13332, 1996, 3277, 2005, 2033, 1024, 1056, 2546, 1012, 17710, 8180, 1012, 2655, 12221, 1012, 23435, 6277, 1006, 8833, 1035, 16101, 1027, 8833, 1035, 16101, 1010, 2010, 3406, 13113, 1035, 10424, 2063, 4160, 1027, 1014, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 52, 81, 130], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61480022", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [20, 26]}], [{"sent_id": 0, "name": "tf.contrib.factorization", "pos": [20, 29]}], [{"sent_id": 0, "name": "tf.contrib.factorization.kmeans", "pos": [20, 33]}]], "sents": ["The old KMeans API was refactored to follow the Estimator API and the tf.contrib.factorization.KMeans was deprecated even before moving to Tensorflow 2.0.", "The \"estimator\" way of doing k-means is the same in both 1.15 and 2.x.", "If you are looking for more \"friendly\" way, to my knowledge you are left with e.g.", "scikit learn.", "Estimators are not going away anytime soon, so familiarising with them isn't a bad idea (though there's a learning curve)."], "sent_idxs": [101, 1996, 2214, 2463, 11219, 2015, 17928, 2001, 25416, 18908, 19574, 2000, 3582, 1996, 9765, 9581, 4263, 17928, 1998, 1996, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 5387, 3989, 1012, 2463, 11219, 2015, 2001, 2139, 28139, 12921, 2130, 2077, 3048, 2000, 23435, 12314, 1016, 1012, 1014, 1012, 102, 101, 1996, 1000, 9765, 9581, 4263, 1000, 2126, 1997, 2725, 1047, 1011, 2965, 2003, 1996, 2168, 1999, 2119, 1015, 1012, 2321, 1998, 1016, 1012, 1060, 1012, 102, 101, 2065, 2017, 2024, 2559, 2005, 2062, 1000, 5379, 1000, 2126, 1010, 2000, 2026, 3716, 2017, 2024, 2187, 2007, 1041, 1012, 1043, 1012, 102, 101, 16596, 23615, 4553, 1012, 102, 101, 9765, 9581, 6591, 2024, 2025, 2183, 2185, 15933, 2574, 1010, 2061, 5220, 9355, 2007, 2068, 3475, 1005, 1056, 1037, 2919, 2801, 1006, 2295, 2045, 1005, 1055, 1037, 4083, 7774, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 48, 75, 99, 105, 138], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35559980", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [18, 23]}], [{"sent_id": 4, "name": "tf.train", "pos": [123, 127]}], [{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [18, 28]}], [{"sent_id": 2, "name": "tf.expand_dims", "pos": [80, 87]}], [{"sent_id": 4, "name": "tf.train.batch", "pos": [123, 129]}]], "sents": ["The inputs() function returns a tensor of shape 180 x 180 x 3, but tf.nn.conv2d() expects a 4-D tensor of shape batch_size x height x width x num_channels.", "As etarion suggests, you can make this work by reshaping the image tensor (e.g.", "using image = tf.expand_dims(image, 0)).", "However, if you are training a neural network you will probably want to batch the inputs.", "One way to do this is using tf.train.batch():", "<code>Code Snippet</code>.", "...then use image_batch or label_batch where you've used image and label respectively."], "sent_idxs": [101, 1996, 20407, 1006, 1007, 3853, 5651, 1037, 23435, 1997, 4338, 8380, 1060, 8380, 1060, 1017, 1010, 2021, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1006, 1007, 24273, 1037, 1018, 1011, 1040, 23435, 1997, 4338, 14108, 1035, 2946, 1060, 4578, 1060, 9381, 1060, 16371, 2213, 1035, 6833, 1012, 102, 101, 2004, 27859, 14772, 6083, 1010, 2017, 2064, 2191, 2023, 2147, 2011, 24501, 3270, 4691, 1996, 3746, 23435, 1006, 1041, 1012, 1043, 1012, 102, 101, 2478, 3746, 1027, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 1006, 3746, 1010, 1014, 1007, 1007, 1012, 102, 101, 2174, 1010, 2065, 2017, 2024, 2731, 1037, 15756, 2897, 2017, 2097, 2763, 2215, 2000, 14108, 1996, 20407, 1012, 102, 101, 2028, 2126, 2000, 2079, 2023, 2003, 2478, 1056, 2546, 1012, 3345, 1012, 14108, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2059, 2224, 3746, 1035, 14108, 2030, 3830, 1035, 14108, 2073, 2017, 1005, 2310, 2109, 3746, 1998, 3830, 4414, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 52, 76, 95, 115, 133, 147, 171], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37949814", "vertexSet": [[{"sent_id": 3, "name": "tf.group", "pos": [70, 74]}], [{"sent_id": 2, "name": "tf.tensor", "pos": [40, 44]}], [{"sent_id": 2, "name": "tf.identity", "pos": [45, 49]}]], "sents": ["As far as I know, you cannot rename a Tensor once created.", "However, you can use additional \"no-op\" operations (like you said):", "for a tf.Tensor: tf.identity(input_tensor, name='your_new_name')", "for an operation: tf.group(input_operation, name='your_new_name')", "After that, you can call the input_tensor with:", "<code>Code Snippet</code>.", "Or the input_operation with:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 2521, 2004, 1045, 2113, 1010, 2017, 3685, 14916, 14074, 1037, 23435, 2320, 2580, 1012, 102, 101, 2174, 1010, 2017, 2064, 2224, 3176, 1000, 2053, 1011, 6728, 1000, 3136, 1006, 2066, 2017, 2056, 1007, 1024, 102, 101, 2005, 1037, 1056, 2546, 1012, 23435, 1024, 1056, 2546, 1012, 4767, 1006, 7953, 1035, 23435, 1010, 2171, 1027, 1005, 2115, 1035, 2047, 1035, 2171, 1005, 1007, 102, 101, 2005, 2019, 3169, 1024, 1056, 2546, 1012, 2177, 1006, 7953, 1035, 3169, 1010, 2171, 1027, 1005, 2115, 1035, 2047, 1035, 2171, 1005, 1007, 102, 101, 2044, 2008, 1010, 2017, 2064, 2655, 1996, 7953, 1035, 23435, 2007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 1996, 7953, 1035, 3169, 2007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 17, 37, 65, 90, 104, 118, 127, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50664668", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [1, 6]}], [{"sent_id": 1, "name": "tf.add", "pos": [34, 38]}], [{"sent_id": 0, "name": "tf.nn.bias_add", "pos": [1, 10]}]], "sents": ["tf.nn.bias_add(value, bias) requires A 1-D Tensor with size matching the last dimension of value.", "so just tf.add will do:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 13827, 1035, 5587, 1006, 3643, 1010, 13827, 1007, 5942, 1037, 1015, 1011, 1040, 23435, 2007, 2946, 9844, 1996, 2197, 9812, 1997, 3643, 1012, 102, 101, 2061, 2074, 1056, 2546, 1012, 5587, 2097, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 42, 56], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44518275", "vertexSet": [[{"sent_id": 5, "name": "tf.nn", "pos": [101, 106]}, {"sent_id": 10, "name": "tf.nn", "pos": [192, 197]}, {"sent_id": 10, "name": "tf.nn", "pos": [201, 206]}], [{"sent_id": 1, "name": "tf.losses", "pos": [16, 20]}], [{"sent_id": 5, "name": "tf.nn.softmax", "pos": [101, 109]}, {"sent_id": 10, "name": "tf.nn.softmax", "pos": [192, 200]}, {"sent_id": 10, "name": "tf.nn.softmax", "pos": [201, 209]}], [{"sent_id": 1, "name": "tf.losses.log_loss", "pos": [16, 24]}], [{"sent_id": 5, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [101, 118]}, {"sent_id": 10, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [201, 218]}]], "sents": ["I had the same problem.", "After looking up the source code of tf.losses.log_loss, its key lines show wat is going on:", "<code>Code Snippet</code>.", "It is binary log-loss (i.e.", "every class is considered non-exclusive) rather than multi-class log-loss.", "As I worked with probabilities (rather than logits), I couldn't use tf.nn.softmax_cross_entropy_with_logits (though, I could have applied logarithm).", "My solution was to implement log-loss by hand:", "<code>Code Snippet</code>.", "See also:", "https://github.com/tensorflow/tensorflow/issues/2462.", "difference between tensorflow tf.nn.softmax and tf.nn.softmax_cross_entropy_with_logits."], "sent_idxs": [101, 1045, 2018, 1996, 2168, 3291, 1012, 102, 101, 2044, 2559, 2039, 1996, 3120, 3642, 1997, 1056, 2546, 1012, 6409, 1012, 8833, 1035, 3279, 1010, 2049, 3145, 3210, 2265, 28194, 2003, 2183, 2006, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2009, 2003, 12441, 8833, 1011, 3279, 1006, 1045, 1012, 1041, 1012, 102, 101, 2296, 2465, 2003, 2641, 2512, 1011, 7262, 1007, 2738, 2084, 4800, 1011, 2465, 8833, 1011, 3279, 1012, 102, 101, 2004, 1045, 2499, 2007, 4013, 3676, 14680, 1006, 2738, 2084, 8833, 12762, 1007, 1010, 1045, 2481, 1005, 1056, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 2295, 1010, 1045, 2071, 2031, 4162, 8833, 8486, 2705, 2213, 1007, 1012, 102, 101, 2026, 5576, 2001, 2000, 10408, 8833, 1011, 3279, 2011, 2192, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2156, 2036, 1024, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3314, 1013, 22376, 2475, 1012, 102, 101, 4489, 2090, 23435, 12314, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1998, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 8, 35, 49, 62, 81, 132, 145, 159, 164, 187, 220], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0]}, {"title": "55346597", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [13, 18]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [13, 20]}], [{"sent_id": 0, "name": "tf.keras.layers.inputlayer", "pos": [13, 23]}]], "sents": ["Ok, I was able to sort this issue by using the tf.keras.layers.InputLayer(dtype='string', input_shape=(1,)) at the start of the sequential model.", "This idea is introduced here: https://gist.github.com/colinmorris/9183206284b4fe3179809098e809d009", "Here is the changed model:", "<code>Code Snippet</code>.", "Full colab notebook: https://colab.research.google.com/drive/1SvGOEtCYHJkpBVAOU0qRtwR8IPE_b2Lw"], "sent_idxs": [101, 7929, 1010, 1045, 2001, 2583, 2000, 4066, 2023, 3277, 2011, 2478, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 7953, 24314, 1006, 26718, 18863, 1027, 1005, 5164, 1005, 1010, 7953, 1035, 4338, 1027, 1006, 1015, 1010, 1007, 1007, 2012, 1996, 2707, 1997, 1996, 25582, 2944, 1012, 102, 101, 2023, 2801, 2003, 3107, 2182, 1024, 16770, 1024, 1013, 1013, 21025, 3367, 1012, 21025, 2705, 12083, 1012, 4012, 1013, 6972, 5302, 18752, 2015, 1013, 6205, 2620, 16703, 2692, 2575, 22407, 2549, 2497, 2549, 7959, 21486, 2581, 2683, 17914, 21057, 2683, 2620, 2063, 17914, 2683, 2094, 8889, 2683, 102, 101, 2182, 2003, 1996, 2904, 2944, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2440, 15270, 2497, 14960, 1024, 16770, 1024, 1013, 1013, 15270, 2497, 1012, 2470, 1012, 8224, 1012, 4012, 1013, 3298, 1013, 1015, 2015, 2615, 3995, 3388, 5666, 2232, 15992, 2361, 2497, 24682, 2226, 2692, 4160, 5339, 13088, 2620, 15457, 1035, 1038, 2475, 2140, 2860, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 49, 98, 106, 120, 165], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38486938", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [22, 27]}], [{"sent_id": 4, "name": "tf.one_hot", "pos": [107, 113]}], [{"sent_id": 1, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [22, 39]}]], "sents": ["The typical loss function used for comparing two probability distributions is called cross entropy.", "TensorFlow has the tf.nn.softmax_cross_entropy_with_logits function which implements that loss.", "In your case, you can simply do :", "<code>Code Snippet</code>.", "But if you really want to convert [0.3, 0.3, 0.4] to a one-hot representation for a different purpose, you can use the tf.one_hot function as follows :", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 5171, 3279, 3853, 2109, 2005, 13599, 2048, 9723, 20611, 2003, 2170, 2892, 23077, 1012, 102, 101, 23435, 12314, 2038, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 3853, 2029, 22164, 2008, 3279, 1012, 102, 101, 1999, 2115, 2553, 1010, 2017, 2064, 3432, 2079, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2021, 2065, 2017, 2428, 2215, 2000, 10463, 1031, 1014, 1012, 1017, 1010, 1014, 1012, 1017, 1010, 1014, 1012, 1018, 1033, 2000, 1037, 2028, 1011, 2980, 6630, 2005, 1037, 2367, 3800, 1010, 2017, 2064, 2224, 1996, 1056, 2546, 1012, 2028, 1035, 2980, 3853, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 17, 46, 57, 71, 118, 132], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43972347", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [20, 26]}], [{"sent_id": 0, "name": "tf.estimator", "pos": [33, 39]}], [{"sent_id": 0, "name": "tf.contrib.learn", "pos": [20, 28]}], [{"sent_id": 0, "name": "tf.estimator.estimator", "pos": [33, 43]}], [{"sent_id": 0, "name": "tf.contrib.learn.estimator", "pos": [20, 32]}]], "sents": ["Attached is a final solution for integrating an external reader into the high-level TF api (tf.contrib.learn.Estimator / tf.estimator.Estimator).", "Please note:", "the architecture and \"logic\" is not important.", "it's a stupid simple net..", "the external reader outputs a dictionary of numpy matrices..", "the input_fn is using this reader..", "In order to verify that the reader \"pulls new values\", I both\n\n\nsave the recent value to self.status (should be > 1.0)\nsave a summary, to be viewed in tensorboard.", ".", "save the recent value to self.status (should be > 1.0).", "save a summary, to be viewed in tensorboard..", "Code example is in gist, and below.", "<code>Code Snippet</code>."], "sent_idxs": [101, 4987, 2003, 1037, 2345, 5576, 2005, 22380, 2019, 6327, 8068, 2046, 1996, 2152, 1011, 2504, 1056, 2546, 17928, 1006, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 9765, 9581, 4263, 1013, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 4263, 1007, 1012, 102, 101, 3531, 3602, 1024, 102, 101, 1996, 4294, 1998, 1000, 7961, 1000, 2003, 2025, 2590, 1012, 102, 101, 2009, 1005, 1055, 1037, 5236, 3722, 5658, 1012, 1012, 102, 101, 1996, 6327, 8068, 27852, 1037, 9206, 1997, 16371, 8737, 2100, 21520, 1012, 1012, 102, 101, 1996, 7953, 1035, 1042, 2078, 2003, 2478, 2023, 8068, 1012, 1012, 102, 101, 1999, 2344, 2000, 20410, 2008, 1996, 8068, 1000, 8005, 2047, 5300, 1000, 1010, 1045, 2119, 3828, 1996, 3522, 3643, 2000, 2969, 1012, 3570, 1006, 2323, 2022, 1028, 1015, 1012, 1014, 1007, 3828, 1037, 12654, 1010, 2000, 2022, 7021, 1999, 23435, 6277, 1012, 102, 101, 1012, 102, 101, 3828, 1996, 3522, 3643, 2000, 2969, 1012, 3570, 1006, 2323, 2022, 1028, 1015, 1012, 1014, 1007, 1012, 102, 101, 3828, 1037, 12654, 1010, 2000, 2022, 7021, 1999, 23435, 6277, 1012, 1012, 102, 101, 3642, 2742, 2003, 1999, 21025, 3367, 1010, 1998, 2917, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 46, 51, 63, 74, 89, 102, 146, 149, 168, 182, 194, 208], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61041738", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [31, 36]}], [{"sent_id": 2, "name": "tf.keras.preprocessing", "pos": [31, 41]}], [{"sent_id": 2, "name": "tf.keras.preprocessing.image", "pos": [31, 43]}], [{"sent_id": 2, "name": "tf.keras.preprocessing.image.save_img", "pos": [31, 48]}]], "sents": ["Staying within the TF-realm is usually more advisable.", "In TF 2.0 you can therefore also simply use", "tf.keras.preprocessing.image.save_img('path/to/file/file.png',adv_x[0])", "It uses PIL in the background and gives you all its flexibility, but no additional import or conversion of the tensor is needed."], "sent_idxs": [101, 6595, 2306, 1996, 1056, 2546, 1011, 8391, 2003, 2788, 2062, 4748, 11365, 3085, 1012, 102, 101, 1999, 1056, 2546, 1016, 1012, 1014, 2017, 2064, 3568, 2036, 3432, 2224, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 17463, 3217, 9623, 7741, 1012, 3746, 1012, 3828, 1035, 10047, 2290, 1006, 1005, 4130, 1013, 2000, 1013, 5371, 1013, 5371, 1012, 1052, 3070, 1005, 1010, 4748, 2615, 1035, 1060, 1031, 1014, 1033, 1007, 102, 101, 2009, 3594, 14255, 2140, 1999, 1996, 4281, 1998, 3957, 2017, 2035, 2049, 16991, 1010, 2021, 2053, 3176, 12324, 2030, 7584, 1997, 1996, 23435, 2003, 2734, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 30, 71, 99], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46651930", "vertexSet": [[{"sent_id": 2, "name": "tf.data", "pos": [54, 58]}], [{"sent_id": 2, "name": "tf.contrib", "pos": [45, 51]}], [{"sent_id": 2, "name": "tf.contrib.data", "pos": [45, 53]}]], "sents": ["There is an estimator your can base your code on that uses batch_sequences_with_states.", "It is called StateSavingRNNEstimator.", "Unless you are using the new tf.contrib.data / tf.data API, it should be enough to get you started."], "sent_idxs": [101, 2045, 2003, 2019, 9765, 9581, 4263, 2115, 2064, 2918, 2115, 3642, 2006, 2008, 3594, 14108, 1035, 10071, 1035, 2007, 1035, 2163, 1012, 102, 101, 2009, 2003, 2170, 2163, 18891, 3070, 6826, 5267, 3775, 18900, 2953, 1012, 102, 101, 4983, 2017, 2024, 2478, 1996, 2047, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1013, 1056, 2546, 1012, 2951, 17928, 1010, 2009, 2323, 2022, 2438, 2000, 2131, 2017, 2318, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 38, 70], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52815622", "vertexSet": [[{"sent_id": 0, "name": "tf.cast", "pos": [21, 25]}], [{"sent_id": 5, "name": "tf.where", "pos": [90, 94]}], [{"sent_id": 0, "name": "tf.equal", "pos": [6, 10]}], [{"sent_id": 5, "name": "tf.scatter_nd", "pos": [95, 103]}]], "sents": ["You can just compare with tf.equal and then convert the boolean result to a number with tf.cast:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "EDIT:", "The above solves the simpler first question, but I think what you need to solve your problem is something like the following, using tf.where and tf.scatter_nd:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "EDIT:", "About your latest example, I have put together a snippet for what I think you are trying to achieve:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2074, 12826, 2007, 1056, 2546, 1012, 5020, 1998, 2059, 10463, 1996, 22017, 20898, 2765, 2000, 1037, 2193, 2007, 1056, 2546, 1012, 3459, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10086, 1024, 102, 101, 1996, 2682, 9611, 2015, 1996, 16325, 2034, 3160, 1010, 2021, 1045, 2228, 2054, 2017, 2342, 2000, 9611, 2115, 3291, 2003, 2242, 2066, 1996, 2206, 1010, 2478, 1056, 2546, 1012, 2073, 1998, 1056, 2546, 1012, 8040, 20097, 1035, 1050, 2094, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 10086, 1024, 102, 101, 2055, 2115, 6745, 2742, 1010, 1045, 2031, 2404, 2362, 1037, 1055, 3490, 29519, 2005, 2054, 1045, 2228, 2017, 2024, 2667, 2000, 6162, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 27, 41, 45, 59, 63, 105, 119, 123, 137, 141, 166, 180, 184, 198], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41351418", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib.learn.linearclassifier", "pos": [26, 38]}], [{"sent_id": 2, "name": "tf.contrib.learn.linearregressor", "pos": [48, 62]}]], "sents": ["In that tutorial they are using logistic regression, that is a linear binary classifier.", "They use the class tf.contrib.learn.LinearClassifier as their model.", "If you use class tf.contrib.learn.LinearRegressor then you can do linear regression instead of classification.", "In that webpage you have tutorials for other models.", "If you want to create a neural network you have different tutorial in the left menu, for example:", "https://www.tensorflow.org/tutorials/mnist/beginners/", "In this repository you have python notebooks with the full code of many different neural networks:", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity"], "sent_idxs": [101, 1999, 2008, 14924, 4818, 2027, 2024, 2478, 8833, 6553, 26237, 1010, 2008, 2003, 1037, 7399, 12441, 2465, 18095, 1012, 102, 101, 2027, 2224, 1996, 2465, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7399, 26266, 18095, 2004, 2037, 2944, 1012, 102, 101, 2065, 2017, 2224, 2465, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7399, 2890, 17603, 24137, 2099, 2059, 2017, 2064, 2079, 7399, 26237, 2612, 1997, 5579, 1012, 102, 101, 1999, 2008, 4773, 13704, 2017, 2031, 14924, 26340, 2005, 2060, 4275, 1012, 102, 101, 2065, 2017, 2215, 2000, 3443, 1037, 15756, 2897, 2017, 2031, 2367, 14924, 4818, 1999, 1996, 2187, 12183, 1010, 2005, 2742, 1024, 102, 101, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 14924, 26340, 1013, 24098, 2923, 1013, 4088, 16912, 1013, 102, 101, 1999, 2023, 22409, 2017, 2031, 18750, 14960, 2015, 2007, 1996, 2440, 3642, 1997, 2116, 2367, 15756, 6125, 1024, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3392, 1013, 3040, 1013, 23435, 12314, 1013, 4973, 1013, 20904, 6305, 3012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2]}], "na_triple": [], "sent_ends": [0, 21, 43, 73, 87, 110, 132, 152, 182], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47051723", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [73, 79]}], [{"sent_id": 1, "name": "tf.placeholder", "pos": [50, 55]}], [{"sent_id": 0, "name": "tf.convert_to_tensor", "pos": [12, 20]}], [{"sent_id": 2, "name": "tf.contrib.predictor", "pos": [73, 82]}]], "sents": ["Finally, i found this is caused by call too many tf.convert_to_tensor , each time calling that function will generate a new node in tensorflow graph, which needs some memory.", "To solve this problem, just use tf.placeholder to feed data.", "Also, tensorflow v1.3 add a new method tf.contrib.predictor to do this.", "Read more in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/predictor"], "sent_idxs": [101, 2633, 1010, 1045, 2179, 2023, 2003, 3303, 2011, 2655, 2205, 2116, 1056, 2546, 1012, 10463, 1035, 2000, 1035, 23435, 1010, 2169, 2051, 4214, 2008, 3853, 2097, 9699, 1037, 2047, 13045, 1999, 23435, 12314, 10629, 1010, 2029, 3791, 2070, 3638, 1012, 102, 101, 2000, 9611, 2023, 3291, 1010, 2074, 2224, 1056, 2546, 1012, 2173, 14528, 2000, 5438, 2951, 1012, 102, 101, 2036, 1010, 23435, 12314, 1058, 2487, 1012, 1017, 5587, 1037, 2047, 4118, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 16014, 2953, 2000, 2079, 2023, 1012, 102, 101, 3191, 2062, 1999, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3392, 1013, 3040, 1013, 23435, 12314, 1013, 9530, 18886, 2497, 1013, 16014, 2953, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 42, 60, 87, 121], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53483547", "vertexSet": [[{"sent_id": 0, "name": "tf.while_loop", "pos": [6, 12]}], [{"sent_id": 2, "name": "tf.reduce_any", "pos": [94, 100]}], [{"sent_id": 2, "name": "tf.reduce_all", "pos": [81, 87]}]], "sents": ["The first argument of the  tf.while_loop() should return scalar (the tensor of rank 0 is, actually, a scalar - that's what the error message is about).", "In your example you probably want to make the condition return true in case if all the numbers in the a1 tensor are less than 6.14.", "This can be achieved by tf.reduce_all() (logical AND) and tf.reduce_any() (logical OR).", "That snippet has worked for me:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2034, 6685, 1997, 1996, 1056, 2546, 1012, 2096, 1035, 7077, 1006, 1007, 2323, 2709, 26743, 2099, 1006, 1996, 23435, 1997, 4635, 1014, 2003, 1010, 2941, 1010, 1037, 26743, 2099, 1011, 2008, 1005, 1055, 2054, 1996, 7561, 4471, 2003, 2055, 1007, 1012, 102, 101, 1999, 2115, 2742, 2017, 2763, 2215, 2000, 2191, 1996, 4650, 2709, 2995, 1999, 2553, 2065, 2035, 1996, 3616, 1999, 1996, 17350, 23435, 2024, 2625, 2084, 1020, 1012, 2403, 1012, 102, 101, 2023, 2064, 2022, 4719, 2011, 1056, 2546, 1012, 5547, 1035, 2035, 1006, 1007, 1006, 11177, 1998, 1007, 1998, 1056, 2546, 1012, 5547, 1035, 2151, 1006, 1007, 1006, 11177, 2030, 1007, 1012, 102, 101, 2008, 1055, 3490, 29519, 2038, 2499, 2005, 2033, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 44, 75, 108, 119, 133], "sent_pos": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59816366", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [16, 22]}, {"sent_id": 0, "name": "tf.contrib", "pos": [29, 35]}], [{"sent_id": 0, "name": "tf.contrib.image", "pos": [29, 37]}], [{"sent_id": 0, "name": "tf.contrib.image.rotate", "pos": [29, 39]}]], "sents": ["For those who are still looking here, starting in TensorFlow 2.x tf.contrib is deprecated, and so tf.contrib.image.rotate will no longer work.", "Instead you can use tfa.image.rotate from the tensorflow_addons library.", "This function rotates an image counterclockwise and takes an angle in radians as an input.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2005, 2216, 2040, 2024, 2145, 2559, 2182, 1010, 3225, 1999, 23435, 12314, 1016, 1012, 1060, 1056, 2546, 1012, 9530, 18886, 2497, 2003, 2139, 28139, 12921, 1010, 1998, 2061, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 24357, 2097, 2053, 2936, 2147, 1012, 102, 101, 2612, 2017, 2064, 2224, 1056, 7011, 1012, 3746, 1012, 24357, 2013, 1996, 23435, 12314, 1035, 5587, 5644, 3075, 1012, 102, 101, 2023, 3853, 24357, 2015, 2019, 3746, 4675, 20464, 7432, 14244, 1998, 3138, 2019, 6466, 1999, 10958, 11692, 2015, 2004, 2019, 7953, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 45, 66, 90, 104], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59239728", "vertexSet": [[{"sent_id": 0, "name": "tf.debugging.enable_check_numerics", "pos": [51, 65]}, {"sent_id": 1, "name": "tf.debugging.enable_check_numerics", "pos": [70, 84]}], [{"sent_id": 1, "name": "tf.add_check_numerics_ops", "pos": [98, 110]}], [{"sent_id": 5, "name": "tf.function", "pos": [216, 220]}]], "sents": ["Since tensorflow 2.1.0rc0, which is soon to be released as the final 2.1.0, there is a new API specifically designed to help users find out the root cause of such numerical issues liek this, namely tf.debugging.enable_check_numerics().", "tf.debugging.enable_check_numerics() is the successor to the old API in TF1 called tf.add_check_numerics_ops(), which is not supported in TF2.", "To use the new API in TF2, you just need to add the line to a place in your code before your model is built and run.", "As a simplistic example.", "<code>Code Snippet</code>.", "During the execution (i.e., the darknet(img)) call, the program will error out as soon as any ops (whether executed eagerly or inside a tf.function graph) outputs any infinity or NaN in their floating-type tensor outputs.", "The error message will tell you", "The name of the op.", "The dtype and shape of the output tensor.", "Whether -infinity, +infinity, NaN, or any combination of them exist in the output tensor.", "The stack trace (line of code) that created the op originally, to help you locate the source of the problem.", "See example screenshot here.."], "sent_idxs": [101, 2144, 23435, 12314, 1016, 1012, 1015, 1012, 1014, 11890, 2692, 1010, 2029, 2003, 2574, 2000, 2022, 2207, 2004, 1996, 2345, 1016, 1012, 1015, 1012, 1014, 1010, 2045, 2003, 1037, 2047, 17928, 4919, 2881, 2000, 2393, 5198, 2424, 2041, 1996, 7117, 3426, 1997, 2107, 15973, 3314, 4682, 2243, 2023, 1010, 8419, 1056, 2546, 1012, 2139, 8569, 12588, 1012, 9585, 1035, 4638, 1035, 16371, 25531, 2015, 1006, 1007, 1012, 102, 101, 1056, 2546, 1012, 2139, 8569, 12588, 1012, 9585, 1035, 4638, 1035, 16371, 25531, 2015, 1006, 1007, 2003, 1996, 6332, 2000, 1996, 2214, 17928, 1999, 1056, 2546, 2487, 2170, 1056, 2546, 1012, 5587, 1035, 4638, 1035, 16371, 25531, 2015, 1035, 23092, 1006, 1007, 1010, 2029, 2003, 2025, 3569, 1999, 1056, 2546, 2475, 1012, 102, 101, 2000, 2224, 1996, 2047, 17928, 1999, 1056, 2546, 2475, 1010, 2017, 2074, 2342, 2000, 5587, 1996, 2240, 2000, 1037, 2173, 1999, 2115, 3642, 2077, 2115, 2944, 2003, 2328, 1998, 2448, 1012, 102, 101, 2004, 1037, 21934, 24759, 6553, 2742, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2076, 1996, 7781, 1006, 1045, 1012, 1041, 1012, 1010, 1996, 2601, 7159, 1006, 10047, 2290, 1007, 1007, 2655, 1010, 1996, 2565, 2097, 7561, 2041, 2004, 2574, 2004, 2151, 23092, 1006, 3251, 6472, 17858, 2030, 2503, 1037, 1056, 2546, 1012, 3853, 10629, 1007, 27852, 2151, 15579, 2030, 16660, 1999, 2037, 8274, 1011, 2828, 23435, 27852, 1012, 102, 101, 1996, 7561, 4471, 2097, 2425, 2017, 102, 101, 1996, 2171, 1997, 1996, 6728, 1012, 102, 101, 1996, 26718, 18863, 1998, 4338, 1997, 1996, 6434, 23435, 1012, 102, 101, 3251, 1011, 15579, 1010, 1009, 15579, 1010, 16660, 1010, 2030, 2151, 5257, 1997, 2068, 4839, 1999, 1996, 6434, 23435, 1012, 102, 101, 1996, 9991, 7637, 1006, 2240, 1997, 3642, 1007, 2008, 2580, 1996, 6728, 2761, 1010, 2000, 2393, 2017, 12453, 1996, 3120, 1997, 1996, 3291, 1012, 102, 101, 2156, 2742, 12117, 12326, 2182, 1012, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 69, 123, 156, 165, 179, 236, 244, 252, 264, 286, 312, 321], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52339760", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator", "pos": [34, 40]}, {"sent_id": 0, "name": "tf.estimator", "pos": [57, 63]}], [{"sent_id": 0, "name": "tf.estimator.export", "pos": [34, 42]}], [{"sent_id": 0, "name": "tf.estimator.estimatorspec", "pos": [57, 69]}], [{"sent_id": 0, "name": "tf.estimator.export.predictoutput", "pos": [34, 46]}]], "sents": ["I solved this by changing the / for a _ when creating keys for the predictions dictionary that will be fed to export_outputs = {_DEFAULT_SERVING_KEY tf.estimator.export.PredictOutput(predictions)} for the export_outputs of the tf.estimator.EstimatorSpec.", "So the final key creator function stayed like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 13332, 2023, 2011, 5278, 1996, 1013, 2005, 1037, 1035, 2043, 4526, 6309, 2005, 1996, 20932, 9206, 2008, 2097, 2022, 7349, 2000, 9167, 1035, 27852, 1027, 1063, 1035, 12398, 1035, 3529, 1035, 3145, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9167, 1012, 16014, 5833, 18780, 1006, 20932, 1007, 1065, 2005, 1996, 9167, 1035, 27852, 1997, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 6591, 5051, 2278, 1012, 102, 101, 2061, 1996, 2345, 3145, 8543, 3853, 4370, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 71, 83, 97], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49550420", "vertexSet": [[{"sent_id": 3, "name": "tf.pad", "pos": [76, 80]}], [{"sent_id": 3, "name": "tf.tile", "pos": [81, 85]}], [{"sent_id": 1, "name": "tf.shape", "pos": [39, 43]}]], "sents": ["Tensor's get_shape function returns inferred shape, and for dynamic dimensions inferred shape is None.", "There is a function in TensorFlow that returns dynamic shape: tf.shape.", "This function returns tensor that will evaluate to actual value of a shape.", "Also there are two functions that you may find useful for your case: tf.pad and tf.tile."], "sent_idxs": [101, 23435, 1005, 1055, 2131, 1035, 4338, 3853, 5651, 1999, 7512, 5596, 4338, 1010, 1998, 2005, 8790, 9646, 1999, 7512, 5596, 4338, 2003, 3904, 1012, 102, 101, 2045, 2003, 1037, 3853, 1999, 23435, 12314, 2008, 5651, 8790, 4338, 1024, 1056, 2546, 1012, 4338, 1012, 102, 101, 2023, 3853, 5651, 23435, 2008, 2097, 16157, 2000, 5025, 3643, 1997, 1037, 4338, 1012, 102, 101, 2036, 2045, 2024, 2048, 4972, 2008, 2017, 2089, 2424, 6179, 2005, 2115, 2553, 1024, 1056, 2546, 1012, 11687, 1998, 1056, 2546, 1012, 14090, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 45, 61, 87], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 0, 0]}, {"title": "38025292", "vertexSet": [[{"sent_id": 2, "name": "tf.tensor", "pos": [95, 99]}], [{"sent_id": 0, "name": "tf.sparsetensor", "pos": [21, 27]}, {"sent_id": 0, "name": "tf.sparsetensor", "pos": [31, 37]}, {"sent_id": 1, "name": "tf.sparsetensor", "pos": [60, 66]}, {"sent_id": 3, "name": "tf.sparsetensor", "pos": [129, 135]}, {"sent_id": 3, "name": "tf.sparsetensor", "pos": [142, 148]}, {"sent_id": 3, "name": "tf.sparsetensor", "pos": [156, 162]}], [{"sent_id": 0, "name": "tf.sparsetensorvalue", "pos": [21, 29]}, {"sent_id": 3, "name": "tf.sparsetensorvalue", "pos": [142, 150]}]], "sents": ["TL;DR: For the return type of create_sparse_vec(), use tf.SparseTensorValue instead of tf.SparseTensor.", "The problem here comes from the return type of create_sparse_vec(), which is tf.SparseTensor, and is not understood as a feed value in the call to sess.run().", "When you feed a (dense) tf.Tensor, the expected value type is a NumPy array (or certain objects that can be converted to an array).", "When you feed a tf.SparseTensor, the expected value type is a tf.SparseTensorValue, which is similar to a tf.SparseTensor but its indices, values, and shape properties are NumPy arrays (or certain objects that can be converted to arrays, like the lists in your example.", "The following code should work:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2140, 1025, 2852, 1024, 2005, 1996, 2709, 2828, 1997, 3443, 1035, 20288, 1035, 2310, 2278, 1006, 1007, 1010, 2224, 1056, 2546, 1012, 20288, 25808, 2953, 10175, 5657, 2612, 1997, 1056, 2546, 1012, 20288, 25808, 2953, 1012, 102, 101, 1996, 3291, 2182, 3310, 2013, 1996, 2709, 2828, 1997, 3443, 1035, 20288, 1035, 2310, 2278, 1006, 1007, 1010, 2029, 2003, 1056, 2546, 1012, 20288, 25808, 2953, 1010, 1998, 2003, 2025, 5319, 2004, 1037, 5438, 3643, 1999, 1996, 2655, 2000, 7367, 4757, 1012, 2448, 1006, 1007, 1012, 102, 101, 2043, 2017, 5438, 1037, 1006, 9742, 1007, 1056, 2546, 1012, 23435, 1010, 1996, 3517, 3643, 2828, 2003, 1037, 16371, 8737, 2100, 9140, 1006, 2030, 3056, 5200, 2008, 2064, 2022, 4991, 2000, 2019, 9140, 1007, 1012, 102, 101, 2043, 2017, 5438, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 1010, 1996, 3517, 3643, 2828, 2003, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 10175, 5657, 1010, 2029, 2003, 2714, 2000, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 2021, 2049, 29299, 1010, 5300, 1010, 1998, 4338, 5144, 2024, 16371, 8737, 2100, 27448, 1006, 2030, 3056, 5200, 2008, 2064, 2022, 4991, 2000, 27448, 1010, 2066, 1996, 7201, 1999, 2115, 2742, 1012, 102, 101, 1996, 2206, 3642, 2323, 2147, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 39, 87, 124, 195, 203, 217], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61197999", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [18, 23]}], [{"sent_id": 1, "name": "tf.keras.utils", "pos": [18, 26]}], [{"sent_id": 1, "name": "tf.keras.utils.get_file", "pos": [18, 30]}]], "sents": ["Tensorflow takes the file wherever you specify.", "You can find the documentation of  tf.keras.utils.get_file() function.", "In the example you specified URL in Google Cloud Storage, that's why Tensorflow goes to GCS.", "You can change it to a local path."], "sent_idxs": [101, 23435, 12314, 3138, 1996, 5371, 11210, 2017, 20648, 1012, 102, 101, 2017, 2064, 2424, 1996, 12653, 1997, 1056, 2546, 1012, 17710, 8180, 1012, 21183, 12146, 1012, 2131, 1035, 5371, 1006, 1007, 3853, 1012, 102, 101, 1999, 1996, 2742, 2017, 9675, 24471, 2140, 1999, 8224, 6112, 5527, 1010, 2008, 1005, 1055, 2339, 23435, 12314, 3632, 2000, 1043, 6169, 1012, 102, 101, 2017, 2064, 2689, 2009, 2000, 1037, 2334, 4130, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 11, 35, 60, 71], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44404530", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [45, 50]}, {"sent_id": 2, "name": "tf.nn", "pos": [68, 73]}], [{"sent_id": 0, "name": "tf.layers", "pos": [33, 37]}, {"sent_id": 2, "name": "tf.layers", "pos": [91, 95]}, {"sent_id": 3, "name": "tf.layers", "pos": [125, 129]}], [{"sent_id": 0, "name": "tf.nn.dropout", "pos": [45, 53]}, {"sent_id": 2, "name": "tf.nn.dropout", "pos": [68, 76]}], [{"sent_id": 0, "name": "tf.layers.dropout", "pos": [33, 40]}, {"sent_id": 2, "name": "tf.layers.dropout", "pos": [91, 98]}, {"sent_id": 3, "name": "tf.layers.dropout", "pos": [125, 132]}]], "sents": ["A quick glance through \ntensorflow/python/layers/core.py and tensorflow/python/ops/nn_ops.py\nreveals that tf.layers.dropout is a wrapper for tf.nn.dropout.", "The only differences in the two functions are:", "The tf.nn.dropout has parameter keep_prob: \"Probability that each element is kept\" tf.layers.dropout has parameter rate: \"The dropout rate\" Thus, keep_prob = 1 - rate as defined here.", "The tf.layers.dropout has training parameter: \"Whether to return the output in training mode (apply dropout) or in inference mode (return the input untouched).", "\"."], "sent_idxs": [101, 1037, 4248, 6054, 2083, 23435, 12314, 1013, 18750, 1013, 9014, 1013, 4563, 1012, 1052, 2100, 1998, 23435, 12314, 1013, 18750, 1013, 23092, 1013, 1050, 2078, 1035, 23092, 1012, 1052, 2100, 7657, 2008, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 2003, 1037, 10236, 4842, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1012, 102, 101, 1996, 2069, 5966, 1999, 1996, 2048, 4972, 2024, 1024, 102, 101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2038, 16381, 2562, 1035, 4013, 2497, 1024, 1000, 9723, 2008, 2169, 5783, 2003, 2921, 1000, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 2038, 16381, 3446, 1024, 1000, 1996, 4530, 5833, 3446, 1000, 2947, 1010, 2562, 1035, 4013, 2497, 1027, 1015, 1011, 3446, 2004, 4225, 2182, 1012, 102, 101, 1996, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 2038, 2731, 16381, 1024, 1000, 3251, 2000, 2709, 1996, 6434, 1999, 2731, 5549, 1006, 6611, 4530, 5833, 1007, 2030, 1999, 28937, 5549, 1006, 2709, 1996, 7953, 22154, 1007, 1012, 102, 101, 1000, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 55, 66, 123, 162, 166], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61550151", "vertexSet": [[{"sent_id": 4, "name": "tf.keras", "pos": [79, 84]}, {"sent_id": 10, "name": "tf.keras", "pos": [238, 243]}], [{"sent_id": 4, "name": "tf.keras.layers", "pos": [79, 86]}], [{"sent_id": 10, "name": "tf.keras.callbacks", "pos": [238, 246]}], [{"sent_id": 4, "name": "tf.keras.layers.dense", "pos": [79, 88]}], [{"sent_id": 10, "name": "tf.keras.callbacks.history", "pos": [238, 248]}]], "sents": ["The first problem is with the LSTM input_shape.", "input_shape = (20,85,1).", "From the doc: https://keras.io/layers/recurrent/", "LSTM layer expects 3D tensor with shape (batch_size, timesteps, input_dim).", "model.add(tf.keras.layers.Dense(nb_classes, activation='softmax')) - this suggets you're doing a multi-class classification.", "So, you need your y_train and y_test have to be one-hot-encoded.", "That means they must have dimension (number_of_samples, 3), where 3 denotes number of classes.", "You need to apply tensorflow.keras.utils.to_categorical to them.", "<code>Code Snippet</code>.", "ref: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical", "tf.keras.callbacks.History() - this callback is automatically applied to every Keras model.", "The History object gets returned by the fit method of models.", "ref: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History"], "sent_idxs": [101, 1996, 2034, 3291, 2003, 2007, 1996, 1048, 3367, 2213, 7953, 1035, 4338, 1012, 102, 101, 7953, 1035, 4338, 1027, 1006, 2322, 1010, 5594, 1010, 1015, 1007, 1012, 102, 101, 2013, 1996, 9986, 1024, 16770, 1024, 1013, 1013, 17710, 8180, 1012, 22834, 1013, 9014, 1013, 28667, 29264, 1013, 102, 101, 1048, 3367, 2213, 6741, 24273, 7605, 23435, 2007, 4338, 1006, 14108, 1035, 2946, 1010, 2335, 2618, 4523, 1010, 7953, 1035, 11737, 1007, 1012, 102, 101, 2944, 1012, 5587, 1006, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 1050, 2497, 1035, 4280, 1010, 13791, 1027, 1005, 3730, 17848, 1005, 1007, 1007, 1011, 2023, 10514, 13871, 8454, 2017, 1005, 2128, 2725, 1037, 4800, 1011, 2465, 5579, 1012, 102, 101, 2061, 1010, 2017, 2342, 2115, 1061, 1035, 3345, 1998, 1061, 1035, 3231, 2031, 2000, 2022, 2028, 1011, 2980, 1011, 12359, 1012, 102, 101, 2008, 2965, 2027, 2442, 2031, 9812, 1006, 2193, 1035, 1997, 1035, 8168, 1010, 1017, 1007, 1010, 2073, 1017, 14796, 2193, 1997, 4280, 1012, 102, 101, 2017, 2342, 2000, 6611, 23435, 12314, 1012, 17710, 8180, 1012, 21183, 12146, 1012, 2000, 1035, 4937, 27203, 2000, 2068, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 25416, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 17710, 8180, 1013, 21183, 12146, 1013, 2000, 1035, 4937, 27203, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 2655, 12221, 1012, 2381, 1006, 1007, 1011, 2023, 2655, 5963, 2003, 8073, 4162, 2000, 2296, 17710, 8180, 2944, 1012, 102, 101, 1996, 2381, 4874, 4152, 2513, 2011, 1996, 4906, 4118, 1997, 4275, 1012, 102, 101, 25416, 1024, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 17928, 1035, 9986, 2015, 1013, 18750, 1013, 1056, 2546, 1013, 17710, 8180, 1013, 2655, 12221, 1013, 2381, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 15, 29, 49, 74, 118, 141, 166, 188, 202, 237, 264, 278, 310], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43595505", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [24, 28]}, {"sent_id": 0, "name": "tf.train", "pos": [37, 41]}, {"sent_id": 0, "name": "tf.train", "pos": [53, 57]}], [{"sent_id": 0, "name": "tf.train.input_producer", "pos": [53, 61]}], [{"sent_id": 0, "name": "tf.train.slice_input_producer", "pos": [37, 47]}], [{"sent_id": 0, "name": "tf.train.string_input_producer", "pos": [24, 34]}]], "sents": ["A queue-based TensorFlow input pipeline typically begins with some kind of \"input producer\" stage, such as a tf.train.string_input_producer(), tf.train.slice_input_producer(), or the generic tf.train.input_producer().", "These input producers each take an optional num_epochs argument, and are backed by a \"queue runner\" thread that repeatedly enqueues the input num_epochs times into the input pipeline.", "Passing num_epochs=n to one of these input producer functions will cause the input pipeline to receive n copies of the input data.", "You can also repeat the input infinitely, by passing num_epochs=None."], "sent_idxs": [101, 1037, 24240, 1011, 2241, 23435, 12314, 7953, 13117, 4050, 4269, 2007, 2070, 2785, 1997, 1000, 7953, 3135, 1000, 2754, 1010, 2107, 2004, 1037, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1007, 1010, 1056, 2546, 1012, 3345, 1012, 14704, 1035, 7953, 1035, 3135, 1006, 1007, 1010, 2030, 1996, 12391, 1056, 2546, 1012, 3345, 1012, 7953, 1035, 3135, 1006, 1007, 1012, 102, 101, 2122, 7953, 6443, 2169, 2202, 2019, 11887, 16371, 2213, 1035, 25492, 2015, 6685, 1010, 1998, 2024, 6153, 2011, 1037, 1000, 24240, 5479, 1000, 11689, 2008, 8385, 4372, 4226, 15808, 1996, 7953, 16371, 2213, 1035, 25492, 2015, 2335, 2046, 1996, 7953, 13117, 1012, 102, 101, 4458, 16371, 2213, 1035, 25492, 2015, 1027, 1050, 2000, 2028, 1997, 2122, 7953, 3135, 4972, 2097, 3426, 1996, 7953, 13117, 2000, 4374, 1050, 4809, 1997, 1996, 7953, 2951, 1012, 102, 101, 2017, 2064, 2036, 9377, 1996, 7953, 25773, 1010, 2011, 4458, 16371, 2213, 1035, 25492, 2015, 1027, 3904, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 65, 109, 140, 160], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35729020", "vertexSet": [[{"sent_id": 5, "name": "tf.train", "pos": [71, 75]}, {"sent_id": 7, "name": "tf.train", "pos": [167, 171]}, {"sent_id": 8, "name": "tf.train", "pos": [185, 189]}], [{"sent_id": 5, "name": "tf.randomshufflequeue", "pos": [85, 93]}], [{"sent_id": 5, "name": "tf.train.shuffle_batch", "pos": [71, 79]}], [{"sent_id": 7, "name": "tf.train.start_queue_runners", "pos": [167, 177]}, {"sent_id": 8, "name": "tf.train.start_queue_runners", "pos": [185, 195]}]], "sents": ["It looks like the problem arises because the statement", "<code>Code Snippet</code>.", "...executes before any queue runners have been created.", "If you move this line after images = get_batch(), your program should work.", "What is the problem here?", "The tf.train.shuffle_batch() function internally uses a tf.RandomShuffleQueue to produce a randomized batch.", "Currently, the only way to get elements into that queue is to run a step that invokes the q.enqueue() op.", "To make this easier, TensorFlow has a notion of \"queue runners\" that are implicitly collected as you build your graph, and then started with a call to tf.train.start_queue_runners().", "However, calling tf.train.start_queue_runners() only starts the queue runners that have been defined at that point in time, so it must come after the code that creates the queue runners."], "sent_idxs": [101, 2009, 3504, 2066, 1996, 3291, 18653, 2138, 1996, 4861, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 15389, 2015, 2077, 2151, 24240, 7190, 2031, 2042, 2580, 1012, 102, 101, 2065, 2017, 2693, 2023, 2240, 2044, 4871, 1027, 2131, 1035, 14108, 1006, 1007, 1010, 2115, 2565, 2323, 2147, 1012, 102, 101, 2054, 2003, 1996, 3291, 2182, 1029, 102, 101, 1996, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 3853, 16058, 3594, 1037, 1056, 2546, 1012, 6721, 14235, 18142, 4226, 5657, 2000, 3965, 1037, 6721, 3550, 14108, 1012, 102, 101, 2747, 1010, 1996, 2069, 2126, 2000, 2131, 3787, 2046, 2008, 24240, 2003, 2000, 2448, 1037, 3357, 2008, 1999, 6767, 9681, 1996, 1053, 1012, 4372, 4226, 5657, 1006, 1007, 6728, 1012, 102, 101, 2000, 2191, 2023, 6082, 1010, 23435, 12314, 2038, 1037, 9366, 1997, 1000, 24240, 7190, 1000, 2008, 2024, 24655, 2135, 5067, 2004, 2017, 3857, 2115, 10629, 1010, 1998, 2059, 2318, 2007, 1037, 2655, 2000, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 1007, 1012, 102, 101, 2174, 1010, 4214, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 1007, 2069, 4627, 1996, 24240, 7190, 2008, 2031, 2042, 4225, 2012, 2008, 2391, 1999, 2051, 1010, 2061, 2009, 2442, 2272, 2044, 1996, 3642, 2008, 9005, 1996, 24240, 7190, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 11, 25, 40, 61, 69, 101, 133, 181, 226], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54492280", "vertexSet": [[{"sent_id": 0, "name": "tf.add", "pos": [35, 39]}], [{"sent_id": 0, "name": "tf.layers", "pos": [8, 12]}], [{"sent_id": 0, "name": "tf.matmul", "pos": [28, 34]}]], "sents": ["There is absolutely no difference between using tf.layers and defining your own layers by creating W and b matricies and then doing tf.matmul and tf.add.", "For example, the first snippet:", "<code>Code Snippet</code>.", "evaluates to the same values as the second snippet:", "<code>Code Snippet</code>.", "Besides, you can monitor both approaches.", "Everything that is defined within the graph creation can be monitored in tensorboard."], "sent_idxs": [101, 2045, 2003, 7078, 2053, 4489, 2090, 2478, 1056, 2546, 1012, 9014, 1998, 12854, 2115, 2219, 9014, 2011, 4526, 1059, 1998, 1038, 13523, 7277, 3111, 1998, 2059, 2725, 1056, 2546, 1012, 13523, 12274, 2140, 1998, 1056, 2546, 1012, 5587, 1012, 102, 101, 2005, 2742, 1010, 1996, 2034, 1055, 3490, 29519, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 16157, 2015, 2000, 1996, 2168, 5300, 2004, 1996, 2117, 1055, 3490, 29519, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 4661, 1010, 2017, 2064, 8080, 2119, 8107, 1012, 102, 101, 2673, 2008, 2003, 4225, 2306, 1996, 10629, 4325, 2064, 2022, 17785, 1999, 23435, 6277, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 41, 52, 66, 81, 95, 105, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57959532", "vertexSet": [[{"sent_id": 5, "name": "tf.variable", "pos": [103, 107]}], [{"sent_id": 4, "name": "tf.constant", "pos": [50, 54]}], [{"sent_id": 5, "name": "tf.placeholder", "pos": [95, 100]}]], "sents": ["numpy solution:", "<code>Code Snippet</code>.", "gives", "<code>Code Snippet</code>.", "you will not be able to do this for a 'tf.constant()', as it is a constant variable and does not support having its values changed.", "If you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.Variable.", "However these require predefined dimensions, and cannot have their sizes changed as desired in your question."], "sent_idxs": [101, 16371, 8737, 2100, 5576, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3957, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2097, 2025, 2022, 2583, 2000, 2079, 2023, 2005, 1037, 1005, 1056, 2546, 1012, 5377, 1006, 1007, 1005, 1010, 2004, 2009, 2003, 1037, 5377, 8023, 1998, 2515, 2025, 2490, 2383, 2049, 5300, 2904, 1012, 102, 101, 2065, 2017, 2215, 2000, 2689, 5300, 2306, 23435, 12314, 2951, 5090, 2009, 2003, 2190, 2000, 2593, 3413, 5300, 2000, 1037, 1056, 2546, 1012, 2173, 14528, 2030, 2224, 1037, 1056, 2546, 1012, 8023, 1012, 102, 101, 2174, 2122, 5478, 3653, 3207, 23460, 2094, 9646, 1010, 1998, 3685, 2031, 2037, 10826, 2904, 2004, 9059, 1999, 2115, 3160, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [4, 5, 6]}, {"r": "S1", "h": 0, "t": 1, "evidence": [4, 5, 6]}, {"r": "S1", "h": 1, "t": 2, "evidence": [4, 5, 6]}, {"r": "S1", "h": 2, "t": 1, "evidence": [4, 5, 6]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 7, 21, 24, 38, 74, 109, 132], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53617271", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [9, 14]}], [{"sent_id": 0, "name": "tf.losses", "pos": [34, 38]}], [{"sent_id": 0, "name": "tf.reduce_mean", "pos": [2, 8]}], [{"sent_id": 0, "name": "tf.losses.sigmoid_cross_entropy", "pos": [34, 46]}], [{"sent_id": 0, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [9, 27]}]], "sents": ["Both tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(...)) and tf.losses.sigmoid_cross_entropy(...) (with default arguments) are computing the same thing.", "The problem is in your tests where you use == to compare two floating-point numbers.", "Instead, use np.isclose method to check whether two floating-point numbers are equal or not:", "<code>Code Snippet</code>.", "And:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2119, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1012, 1012, 1012, 1007, 1007, 1998, 1056, 2546, 1012, 6409, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1006, 1012, 1012, 1012, 1007, 1006, 2007, 12398, 9918, 1007, 2024, 9798, 1996, 2168, 2518, 1012, 102, 101, 1996, 3291, 2003, 1999, 2115, 5852, 2073, 2017, 2224, 1027, 1027, 2000, 12826, 2048, 8274, 1011, 2391, 3616, 1012, 102, 101, 2612, 1010, 2224, 27937, 1012, 2003, 20464, 9232, 4118, 2000, 4638, 3251, 2048, 8274, 1011, 2391, 3616, 2024, 5020, 2030, 2025, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 63, 84, 108, 122, 126, 140], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51732374", "vertexSet": [[{"sent_id": 1, "name": "tf.name_scope", "pos": [15, 21]}], [{"sent_id": 1, "name": "tf.get_variable", "pos": [27, 33]}], [{"sent_id": 0, "name": "tf.variable", "pos": [5, 9]}, {"sent_id": 2, "name": "tf.variable", "pos": [56, 60]}], [{"sent_id": 0, "name": "tf.variable_scope", "pos": [5, 11]}], [{"sent_id": 1, "name": "tf.layers", "pos": [40, 44]}]], "sents": ["You can try using tf.variable_scope instead.", "tf.name_scope is ignored by variables created via tf.get_variable() which is usually used by tf.layers functions.", "This is in contrast to variables created via tf.Variable.", "See this question for an (albeit somewhat outdated) explanation of the differences."], "sent_idxs": [101, 2017, 2064, 3046, 2478, 1056, 2546, 1012, 8023, 1035, 9531, 2612, 1012, 102, 101, 1056, 2546, 1012, 2171, 1035, 9531, 2003, 6439, 2011, 10857, 2580, 3081, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 2029, 2003, 2788, 2109, 2011, 1056, 2546, 1012, 9014, 4972, 1012, 102, 101, 2023, 2003, 1999, 5688, 2000, 10857, 2580, 3081, 1056, 2546, 1012, 8023, 1012, 102, 101, 2156, 2023, 3160, 2005, 2019, 1006, 12167, 5399, 25963, 1007, 7526, 1997, 1996, 5966, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 2, "evidence": [1, 2]}, {"r": "S1", "h": 2, "t": 1, "evidence": [1, 2]}, {"r": "S1", "h": 3, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 3, "evidence": [0, 1]}], "na_triple": [[0, 1], [0, 2], [0, 4], [1, 0], [1, 3], [1, 4], [2, 0], [2, 3], [2, 4], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 14, 47, 62, 79], "sent_pos": [0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49446327", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [32, 37]}, {"sent_id": 2, "name": "tf.nn", "pos": [43, 48]}], [{"sent_id": 2, "name": "tf.nn.raw_rnn", "pos": [43, 53]}], [{"sent_id": 3, "name": "tf.while_loop", "pos": [64, 70]}], [{"sent_id": 2, "name": "tf.nn.dynamic_rnn", "pos": [32, 42]}]], "sents": ["There are some classes in tensorflow to create networks with a swap_memory argument.", "e.g.", "for RNN you can use tf.nn.dynamic_rnn or tf.nn.raw_rnn", "There is also a more generic looping class tf.while_loop with this argument.", "But there is imho no general option for using memory swapping.", "Just take a look at tensorflow.org and use its search function.", "You can find relevant classes using swap_memory"], "sent_idxs": [101, 2045, 2024, 2070, 4280, 1999, 23435, 12314, 2000, 3443, 6125, 2007, 1037, 19948, 1035, 3638, 6685, 1012, 102, 101, 1041, 1012, 1043, 1012, 102, 101, 2005, 29300, 2078, 2017, 2064, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 2030, 1056, 2546, 1012, 1050, 2078, 1012, 6315, 1035, 29300, 2078, 102, 101, 2045, 2003, 2036, 1037, 2062, 12391, 7077, 2075, 2465, 1056, 2546, 1012, 2096, 1035, 7077, 2007, 2023, 6685, 1012, 102, 101, 2021, 2045, 2003, 10047, 6806, 2053, 2236, 5724, 2005, 2478, 3638, 19948, 4691, 1012, 102, 101, 2074, 2202, 1037, 2298, 2012, 23435, 12314, 1012, 8917, 1998, 2224, 2049, 3945, 3853, 1012, 102, 101, 2017, 2064, 2424, 7882, 4280, 2478, 19948, 1035, 3638, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 19, 25, 54, 75, 91, 108, 119], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55442167", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [4, 9]}, {"sent_id": 0, "name": "tf.keras", "pos": [19, 24]}], [{"sent_id": 2, "name": "tf.graphkeys", "pos": [58, 64]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [4, 11]}, {"sent_id": 0, "name": "tf.keras.layers", "pos": [19, 26]}], [{"sent_id": 0, "name": "tf.keras.layers.layer", "pos": [19, 28]}], [{"sent_id": 0, "name": "tf.keras.layers.batchnormalization", "pos": [4, 16]}]], "sents": ["This is because tf.keras.layers.BatchNormalization inherits from tf.keras.layers.Layer.", "Keras API handle update ops as part of its fit and evaluate loops.", "This in turn means that it won't update tf.GraphKeys.UPDATE_OPS collection without it.", "So in order to make it work, you need to update it manually", "<code>Code Snippet</code>.", "This creates separate class instance", "<code>Code Snippet</code>.", "And this updates needed collection.", "Also take a look https://github.com/tensorflow/tensorflow/issues/25525"], "sent_idxs": [101, 2023, 2003, 2138, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 14108, 12131, 9067, 3989, 22490, 2015, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 6741, 1012, 102, 101, 17710, 8180, 17928, 5047, 10651, 23092, 2004, 2112, 1997, 2049, 4906, 1998, 16157, 15932, 1012, 102, 101, 2023, 1999, 2735, 2965, 2008, 2009, 2180, 1005, 1056, 10651, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 3074, 2302, 2009, 1012, 102, 101, 2061, 1999, 2344, 2000, 2191, 2009, 2147, 1010, 2017, 2342, 2000, 10651, 2009, 21118, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 9005, 3584, 2465, 6013, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2023, 14409, 2734, 3074, 1012, 102, 101, 2036, 2202, 1037, 2298, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3314, 1013, 20637, 17788, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 30, 47, 73, 89, 103, 110, 124, 132, 158], "sent_pos": [0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46844995", "vertexSet": [[{"sent_id": 0, "name": "tf.identity", "pos": [17, 21]}, {"sent_id": 1, "name": "tf.identity", "pos": [41, 45]}], [{"sent_id": 0, "name": "tf.name_scope", "pos": [7, 13]}], [{"sent_id": 1, "name": "tf.get_default_graph", "pos": [69, 77]}, {"sent_id": 1, "name": "tf.get_default_graph", "pos": [97, 105]}]], "sents": ["It seems it works also without tf.name_scope only with z = tf.identity(z, name=\"z_name\").", "If you run additionally z = tf.identity(z, name=\"z_name_new\") then you can access the same tensor using both names: tf.get_default_graph().get_tensor_by_name(\"z_name:0\") or tf.get_default_graph().get_tensor_by_name(\"z_name_new:0\")"], "sent_idxs": [101, 2009, 3849, 2009, 2573, 2036, 2302, 1056, 2546, 1012, 2171, 1035, 9531, 2069, 2007, 1062, 1027, 1056, 2546, 1012, 4767, 1006, 1062, 1010, 2171, 1027, 1000, 1062, 1035, 2171, 1000, 1007, 1012, 102, 101, 2065, 2017, 2448, 5678, 1062, 1027, 1056, 2546, 1012, 4767, 1006, 1062, 1010, 2171, 1027, 1000, 1062, 1035, 2171, 1035, 2047, 1000, 1007, 2059, 2017, 2064, 3229, 1996, 2168, 23435, 2478, 2119, 3415, 1024, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 1000, 1062, 1035, 2171, 1024, 1014, 1000, 1007, 2030, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 1000, 1062, 1035, 2171, 1035, 2047, 1024, 1014, 1000, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 34, 127], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47257999", "vertexSet": [[{"sent_id": 0, "name": "tf.constant", "pos": [9, 13]}, {"sent_id": 1, "name": "tf.constant", "pos": [32, 36]}], [{"sent_id": 1, "name": "tf.placeholder", "pos": [47, 52]}, {"sent_id": 1, "name": "tf.placeholder", "pos": [68, 73]}], [{"sent_id": 1, "name": "tf.placeholder_with_default", "pos": [68, 77]}]], "sents": ["You defined keep_prob as a tf.constant, but then trying to feed the value into it.", "Replace keep_prob = tf.constant(1.0) with keep_prob = tf.placeholder(tf.float32,[]) or keep_prob = tf.placeholder_with_default(1.0,[])"], "sent_idxs": [101, 2017, 4225, 2562, 1035, 4013, 2497, 2004, 1037, 1056, 2546, 1012, 5377, 1010, 2021, 2059, 2667, 2000, 5438, 1996, 3643, 2046, 2009, 1012, 102, 101, 5672, 2562, 1035, 4013, 2497, 1027, 1056, 2546, 1012, 5377, 1006, 1015, 1012, 1014, 1007, 2007, 2562, 1035, 4013, 2497, 1027, 1056, 2546, 1012, 2173, 14528, 1006, 1056, 2546, 1012, 14257, 16703, 1010, 1031, 1033, 1007, 2030, 2562, 1035, 4013, 2497, 1027, 1056, 2546, 1012, 2173, 14528, 1035, 2007, 1035, 12398, 1006, 1015, 1012, 1014, 1010, 1031, 1033, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 25, 86], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47428080", "vertexSet": [[{"sent_id": 1, "name": "tf.layers", "pos": [13, 17]}, {"sent_id": 3, "name": "tf.layers", "pos": [93, 97]}], [{"sent_id": 1, "name": "tf.layers.dense", "pos": [13, 19]}, {"sent_id": 3, "name": "tf.layers.dense", "pos": [93, 99]}], [{"sent_id": 3, "name": "tf.get_variable", "pos": [73, 79]}]], "sents": ["I came across this problem and just solved it.", "tf.layers.dense 's name is not necessary to be the same with the kernel's name's prefix.", "My tensor is \"dense_2/xxx\" but it's kernel is \"dense_1/kernel:0\".", "To ensure that tf.get_variable works, you'd better set the name=xxx in the tf.layers.dense function to make two names owning same prefix.", "It works as the demo below:", "<code>Code Snippet</code>.", "By the way, my tf version is 1.3."], "sent_idxs": [101, 1045, 2234, 2408, 2023, 3291, 1998, 2074, 13332, 2009, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 9742, 1005, 1055, 2171, 2003, 2025, 4072, 2000, 2022, 1996, 2168, 2007, 1996, 16293, 1005, 1055, 2171, 1005, 1055, 17576, 1012, 102, 101, 2026, 23435, 2003, 1000, 9742, 1035, 1016, 1013, 22038, 2595, 1000, 2021, 2009, 1005, 1055, 16293, 2003, 1000, 9742, 1035, 1015, 1013, 16293, 1024, 1014, 1000, 1012, 102, 101, 2000, 5676, 2008, 1056, 2546, 1012, 2131, 1035, 8023, 2573, 1010, 2017, 1005, 1040, 2488, 2275, 1996, 2171, 1027, 22038, 2595, 1999, 1996, 1056, 2546, 1012, 9014, 1012, 9742, 3853, 2000, 2191, 2048, 3415, 19273, 2168, 17576, 1012, 102, 101, 2009, 2573, 2004, 1996, 9703, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2011, 1996, 2126, 1010, 2026, 1056, 2546, 2544, 2003, 1015, 1012, 1017, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 40, 69, 109, 118, 132, 147], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34771937", "vertexSet": [[{"sent_id": 11, "name": "tf.graph", "pos": [242, 246]}], [{"sent_id": 8, "name": "tf.train", "pos": [180, 184]}], [{"sent_id": 8, "name": "tf.train.saver", "pos": [180, 187]}], [{"sent_id": 6, "name": "tf.reset_default_graph", "pos": [128, 136]}]], "sents": ["It looks like you are using Jupyter to build your model.", "One possible issue, when constructing a tf.Saver with the default arguments is that it will use the (auto-generated) names for the variables as the keys in your checkpoint.", "Since in Jupyter its easy to re-execute code cells multiple times, you might be ending up with multiple copies of the variable nodes in the session that you save.", "See my answer to this question for an explanation of what can go wrong.", "There are a few possible solutions.", "Here are the easiest:", "Call tf.reset_default_graph() before you build your model (and the Saver).", "This will ensure that the variables get the names you intended, but it will invalidate previously-created graphs.", "Use explicit arguments to tf.train.Saver() to specify the persistent names for the variables.", "For your example this shouldn't be too hard (though it becomes unwieldy for larger models):", "<code>Code Snippet</code>.", "Create a new tf.Graph() and make it the default each time you create the model.", "This can be tricky in Jupyter, since it forces you to put all of the model building code in one cell, but it works well for scripts:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2009, 3504, 2066, 2017, 2024, 2478, 18414, 7685, 3334, 2000, 3857, 2115, 2944, 1012, 102, 101, 2028, 2825, 3277, 1010, 2043, 15696, 1037, 1056, 2546, 1012, 3828, 2099, 2007, 1996, 12398, 9918, 2003, 2008, 2009, 2097, 2224, 1996, 1006, 8285, 1011, 7013, 1007, 3415, 2005, 1996, 10857, 2004, 1996, 6309, 1999, 2115, 26520, 1012, 102, 101, 2144, 1999, 18414, 7685, 3334, 2049, 3733, 2000, 2128, 1011, 15389, 3642, 4442, 3674, 2335, 1010, 2017, 2453, 2022, 4566, 2039, 2007, 3674, 4809, 1997, 1996, 8023, 14164, 1999, 1996, 5219, 2008, 2017, 3828, 1012, 102, 101, 2156, 2026, 3437, 2000, 2023, 3160, 2005, 2019, 7526, 1997, 2054, 2064, 2175, 3308, 1012, 102, 101, 2045, 2024, 1037, 2261, 2825, 7300, 1012, 102, 101, 2182, 2024, 1996, 25551, 1024, 102, 101, 2655, 1056, 2546, 1012, 25141, 1035, 12398, 1035, 10629, 1006, 1007, 2077, 2017, 3857, 2115, 2944, 1006, 1998, 1996, 3828, 2099, 1007, 1012, 102, 101, 2023, 2097, 5676, 2008, 1996, 10857, 2131, 1996, 3415, 2017, 3832, 1010, 2021, 2009, 2097, 19528, 3686, 3130, 1011, 2580, 19287, 1012, 102, 101, 2224, 13216, 9918, 2000, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 1006, 1007, 2000, 20648, 1996, 14516, 3415, 2005, 1996, 10857, 1012, 102, 101, 2005, 2115, 2742, 2023, 5807, 1005, 1056, 2022, 2205, 2524, 1006, 2295, 2009, 4150, 4895, 9148, 14273, 2100, 2005, 3469, 4275, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3443, 1037, 2047, 1056, 2546, 1012, 10629, 1006, 1007, 1998, 2191, 2009, 1996, 12398, 2169, 2051, 2017, 3443, 1996, 2944, 1012, 102, 101, 2023, 2064, 2022, 24026, 1999, 18414, 7685, 3334, 1010, 2144, 2009, 2749, 2017, 2000, 2404, 2035, 1997, 1996, 2944, 2311, 3642, 1999, 2028, 3526, 1010, 2021, 2009, 2573, 2092, 2005, 14546, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 56, 93, 110, 119, 126, 151, 175, 199, 224, 238, 261, 295, 309], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46065066", "vertexSet": [[{"sent_id": 5, "name": "tf.graph", "pos": [115, 119]}], [{"sent_id": 4, "name": "tf.summary", "pos": [79, 83]}, {"sent_id": 4, "name": "tf.summary", "pos": [87, 91]}, {"sent_id": 5, "name": "tf.summary", "pos": [110, 114]}, {"sent_id": 5, "name": "tf.summary", "pos": [130, 134]}, {"sent_id": 6, "name": "tf.summary", "pos": [167, 171]}], [{"sent_id": 4, "name": "tf.summary.scalar", "pos": [79, 86]}, {"sent_id": 4, "name": "tf.summary.scalar", "pos": [87, 94]}], [{"sent_id": 5, "name": "tf.summary.merge_all", "pos": [130, 138]}], [{"sent_id": 6, "name": "tf.summary.filewriter", "pos": [167, 174]}]], "sents": ["When calling DNNClassifier.train, it accepts hooks parameter, you can create a SummarySaverHook and add it to hooks.", "Update.", "When add a metric (accuracy for example) into TensorBoard, you should flow several steps:", "Define a Tensor which calculate the accuracy: acc_op = ...;", "Add the Tensor into tf.summary.scalar: tf.summary.scalar('acc', acc_op);", "There can be multiple tf.summary in tf.Graph, so we define a merge_summary_op = tf.summary.merge_all() to get an op to merge all the metric Tensors.", "Add the merge_summary_op into a summary_writer = tf.summary.FileWriter();", "Add the summary_writer into a SummarySaverHook or call the summary_writer by your own code."], "sent_idxs": [101, 2043, 4214, 1040, 10695, 26266, 18095, 1012, 3345, 1010, 2009, 13385, 18008, 16381, 1010, 2017, 2064, 3443, 1037, 12654, 3736, 6299, 6806, 6559, 1998, 5587, 2009, 2000, 18008, 1012, 102, 101, 10651, 1012, 102, 101, 2043, 5587, 1037, 12046, 1006, 10640, 2005, 2742, 1007, 2046, 23435, 6277, 1010, 2017, 2323, 4834, 2195, 4084, 1024, 102, 101, 9375, 1037, 23435, 2029, 18422, 1996, 10640, 1024, 16222, 1035, 6728, 1027, 1012, 1012, 1012, 1025, 102, 101, 5587, 1996, 23435, 2046, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1024, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1006, 1005, 16222, 1005, 1010, 16222, 1035, 6728, 1007, 1025, 102, 101, 2045, 2064, 2022, 3674, 1056, 2546, 1012, 12654, 1999, 1056, 2546, 1012, 10629, 1010, 2061, 2057, 9375, 1037, 13590, 1035, 12654, 1035, 6728, 1027, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 1006, 1007, 2000, 2131, 2019, 6728, 2000, 13590, 2035, 1996, 12046, 23435, 2015, 1012, 102, 101, 5587, 1996, 13590, 1035, 12654, 1035, 6728, 2046, 1037, 12654, 1035, 3213, 1027, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1006, 1007, 1025, 102, 101, 5587, 1996, 12654, 1035, 3213, 2046, 1037, 12654, 3736, 6299, 6806, 6559, 2030, 2655, 1996, 12654, 1035, 3213, 2011, 2115, 2219, 3642, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 31, 35, 56, 74, 105, 153, 178, 203], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "33702428", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [3, 7]}, {"sent_id": 4, "name": "tf.train", "pos": [114, 118]}], [{"sent_id": 5, "name": "tf.constant", "pos": [142, 146]}], [{"sent_id": 0, "name": "tf.train.gradientdescentoptimizer", "pos": [3, 15]}, {"sent_id": 4, "name": "tf.train.gradientdescentoptimizer", "pos": [114, 126]}]], "sents": ["Currently the tf.train.GradientDescentOptimizer class only supports training on 32-bit floating-point variables and loss values.", "However, it looks like the kernel is implemented for double-precision values, so it should be possible to train in your scenario.", "A quick workaround would be to define a subclass that supports tf.float64 values as well:", "<code>Code Snippet</code>.", "...and then use DoubleGDOptimizer in place of tf.train.GradientDescentOptimizer.", "EDIT: You'll need to pass in the learning rate as tf.constant(learning_rate, tf.float64) to make this work.", "(N.B.", "This isn't a supported interface and it may be subject to change in future, but the team is aware of the desire for optimizing double-precision floats, and intends to provide a built-in solution.)"], "sent_idxs": [101, 2747, 1996, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 2465, 2069, 6753, 2731, 2006, 3590, 1011, 2978, 8274, 1011, 2391, 10857, 1998, 3279, 5300, 1012, 102, 101, 2174, 1010, 2009, 3504, 2066, 1996, 16293, 2003, 7528, 2005, 3313, 1011, 11718, 5300, 1010, 2061, 2009, 2323, 2022, 2825, 2000, 3345, 1999, 2115, 11967, 1012, 102, 101, 1037, 4248, 2147, 24490, 2052, 2022, 2000, 9375, 1037, 4942, 26266, 2008, 6753, 1056, 2546, 1012, 14257, 21084, 5300, 2004, 2092, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 1998, 2059, 2224, 3313, 2290, 3527, 13876, 27605, 6290, 1999, 2173, 1997, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1012, 102, 101, 10086, 1024, 2017, 1005, 2222, 2342, 2000, 3413, 1999, 1996, 4083, 3446, 2004, 1056, 2546, 1012, 5377, 1006, 4083, 1035, 3446, 1010, 1056, 2546, 1012, 14257, 21084, 1007, 2000, 2191, 2023, 2147, 1012, 102, 101, 1006, 1050, 1012, 1038, 1012, 102, 101, 2023, 3475, 1005, 1056, 1037, 3569, 8278, 1998, 2009, 2089, 2022, 3395, 2000, 2689, 1999, 2925, 1010, 2021, 1996, 2136, 2003, 5204, 1997, 1996, 4792, 2005, 23569, 27605, 6774, 3313, 1011, 11718, 24885, 1010, 1998, 18754, 2000, 3073, 1037, 2328, 1011, 1999, 5576, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 32, 60, 84, 98, 128, 163, 170, 217], "sent_pos": [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35662013", "vertexSet": [[{"sent_id": 0, "name": "tf.tensor", "pos": [32, 36]}, {"sent_id": 1, "name": "tf.tensor", "pos": [52, 56]}, {"sent_id": 2, "name": "tf.tensor", "pos": [79, 83]}, {"sent_id": 3, "name": "tf.tensor", "pos": [104, 108]}], [{"sent_id": 9, "name": "tf.variable", "pos": [276, 280]}], [{"sent_id": 0, "name": "tf.constant", "pos": [2, 6]}, {"sent_id": 3, "name": "tf.constant", "pos": [117, 121]}, {"sent_id": 7, "name": "tf.constant", "pos": [216, 220]}], [{"sent_id": 2, "name": "tf.random_normal", "pos": [68, 74]}], [{"sent_id": 4, "name": "tf.interactivesession", "pos": [140, 146]}]], "sents": ["The tf.constant() op takes a numpy array (or something implicitly convertible to a numpy array), and returns a tf.Tensor whose value is the same as that array.", "It does not accept a tf.Tensor as its argument.", "On the other hand, the tf.random_normal() op returns a tf.Tensor whose value is generated randomly according to the given distribution each time it runs.", "Since it returns a tf.Tensor, it cannot be used as the argument to tf.constant().", "This explains the TypeError (which is unrelated to the use of tf.InteractiveSession, since it occurs when you build the graph).", "I'm assuming you want your graph to include a tensor that (i) is randomly generated on its first use, and (ii) constant thereafter.", "There are two ways to do this:", "Use NumPy to generate the random value and put it in a tf.constant(), as you did in your question:", "<code>Code Snippet</code>.", "(Potentially faster, as it can use the GPU to generate the random numbers) Use TensorFlow to generate the random value and put it in a tf.Variable:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 5377, 1006, 1007, 6728, 3138, 1037, 16371, 8737, 2100, 9140, 1006, 2030, 2242, 24655, 2135, 22840, 2000, 1037, 16371, 8737, 2100, 9140, 1007, 1010, 1998, 5651, 1037, 1056, 2546, 1012, 23435, 3005, 3643, 2003, 1996, 2168, 2004, 2008, 9140, 1012, 102, 101, 2009, 2515, 2025, 5138, 1037, 1056, 2546, 1012, 23435, 2004, 2049, 6685, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 1996, 1056, 2546, 1012, 6721, 1035, 3671, 1006, 1007, 6728, 5651, 1037, 1056, 2546, 1012, 23435, 3005, 3643, 2003, 7013, 18154, 2429, 2000, 1996, 2445, 4353, 2169, 2051, 2009, 3216, 1012, 102, 101, 2144, 2009, 5651, 1037, 1056, 2546, 1012, 23435, 1010, 2009, 3685, 2022, 2109, 2004, 1996, 6685, 2000, 1056, 2546, 1012, 5377, 1006, 1007, 1012, 102, 101, 2023, 7607, 1996, 2828, 2121, 29165, 1006, 2029, 2003, 15142, 2000, 1996, 2224, 1997, 1056, 2546, 1012, 9123, 8583, 10992, 1010, 2144, 2009, 5158, 2043, 2017, 3857, 1996, 10629, 1007, 1012, 102, 101, 1045, 1005, 1049, 10262, 2017, 2215, 2115, 10629, 2000, 2421, 1037, 23435, 2008, 1006, 1045, 1007, 2003, 18154, 7013, 2006, 2049, 2034, 2224, 1010, 1998, 1006, 2462, 1007, 5377, 6920, 1012, 102, 101, 2045, 2024, 2048, 3971, 2000, 2079, 2023, 1024, 102, 101, 2224, 16371, 8737, 2100, 2000, 9699, 1996, 6721, 3643, 1998, 2404, 2009, 1999, 1037, 1056, 2546, 1012, 5377, 1006, 1007, 1010, 2004, 2017, 2106, 1999, 2115, 3160, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1006, 9280, 5514, 1010, 2004, 2009, 2064, 2224, 1996, 14246, 2226, 2000, 9699, 1996, 6721, 3616, 1007, 2224, 23435, 12314, 2000, 9699, 1996, 6721, 3643, 1998, 2404, 2009, 1999, 1037, 1056, 2546, 1012, 8023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 46, 61, 99, 125, 158, 191, 201, 231, 245, 282, 296], "sent_pos": [0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59350211", "vertexSet": [[{"sent_id": 4, "name": "tf.less", "pos": [190, 194]}], [{"sent_id": 1, "name": "tf.graph", "pos": [78, 82]}], [{"sent_id": 2, "name": "tf.variable", "pos": [114, 118]}], [{"sent_id": 0, "name": "tf.function", "pos": [15, 19]}, {"sent_id": 1, "name": "tf.function", "pos": [54, 58]}, {"sent_id": 1, "name": "tf.function", "pos": [66, 70]}, {"sent_id": 3, "name": "tf.function", "pos": [131, 135]}, {"sent_id": 3, "name": "tf.function", "pos": [157, 161]}], [{"sent_id": 4, "name": "tf.while_loop", "pos": [216, 222]}]], "sents": ["The code in your first snippet (the one without the @tf.function) takes advantage of TensorFlow 2's eager execution to manipulate a numpy array (i.e., your outer iteration object) directly.", "With @tf.function, this doesn't work because @tf.function tries to compile your code into a tf.Graph, which cannot operate on a numpy array directly (it can only process tensorflow tensors).", "To get around this issue, use a tf.Variable and keep assigning value into its slices.", "With @tf.function, what you are trying to do is actually achievable with simpler code, by taking advantage of @tf.function's automatic Python-to-graph transformation feature (known as AutoGraph).", "You just write a normal Python while loop (using tf.less() in lieu of the < operator), and the while loop will be compiled by AutoGraph into a tf.while_loop under the hood.", "The code looks something like:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3642, 1999, 2115, 2034, 1055, 3490, 29519, 1006, 1996, 2028, 2302, 1996, 1030, 1056, 2546, 1012, 3853, 1007, 3138, 5056, 1997, 23435, 12314, 1016, 1005, 1055, 9461, 7781, 2000, 17708, 1037, 16371, 8737, 2100, 9140, 1006, 1045, 1012, 1041, 1012, 1010, 2115, 6058, 27758, 4874, 1007, 3495, 1012, 102, 101, 2007, 1030, 1056, 2546, 1012, 3853, 1010, 2023, 2987, 1005, 1056, 2147, 2138, 1030, 1056, 2546, 1012, 3853, 5363, 2000, 4012, 22090, 2115, 3642, 2046, 1037, 1056, 2546, 1012, 10629, 1010, 2029, 3685, 5452, 2006, 1037, 16371, 8737, 2100, 9140, 3495, 1006, 2009, 2064, 2069, 2832, 23435, 12314, 23435, 2015, 1007, 1012, 102, 101, 2000, 2131, 2105, 2023, 3277, 1010, 2224, 1037, 1056, 2546, 1012, 8023, 1998, 2562, 23911, 2075, 3643, 2046, 2049, 25609, 1012, 102, 101, 2007, 1030, 1056, 2546, 1012, 3853, 1010, 2054, 2017, 2024, 2667, 2000, 2079, 2003, 2941, 9353, 4048, 13331, 3468, 2007, 16325, 3642, 1010, 2011, 2635, 5056, 1997, 1030, 1056, 2546, 1012, 3853, 1005, 1055, 6882, 18750, 1011, 2000, 1011, 10629, 8651, 3444, 1006, 2124, 2004, 8285, 14413, 1007, 1012, 102, 101, 2017, 2074, 4339, 1037, 3671, 18750, 2096, 7077, 1006, 2478, 1056, 2546, 1012, 2625, 1006, 1007, 1999, 22470, 1997, 1996, 1026, 6872, 1007, 1010, 1998, 1996, 2096, 7077, 2097, 2022, 9227, 2011, 8285, 14413, 2046, 1037, 1056, 2546, 1012, 2096, 1035, 7077, 2104, 1996, 7415, 1012, 102, 101, 1996, 3642, 3504, 2242, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 51, 105, 128, 179, 227, 235, 249], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44085088", "vertexSet": [[{"sent_id": 1, "name": "tf.train", "pos": [28, 32]}], [{"sent_id": 1, "name": "tf.train.shuffle_batch", "pos": [28, 36]}], [{"sent_id": 1, "name": "tf.fixedlengthrecordreader", "pos": [16, 27]}]], "sents": ["cifar10 example doesn't use placeholder.", "It uses tf.FixedLengthRecordReader and tf.train.shuffle_batch.", "Generated input image batch is directly passed to CNN without placeholder.", "Please refer tensorflow's official tutorial and it's python code:", "https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py", "Also, at the test time, you can link the pipeline of the test data with different number of batch from the train data.", "Please refer the evaluation code:\nhttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_eval.py"], "sent_idxs": [101, 25022, 14971, 10790, 2742, 2987, 1005, 1056, 2224, 2173, 14528, 1012, 102, 101, 2009, 3594, 1056, 2546, 1012, 4964, 7770, 13512, 28362, 27108, 16200, 9648, 2099, 1998, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1012, 102, 101, 7013, 7953, 3746, 14108, 2003, 3495, 2979, 2000, 13229, 2302, 2173, 14528, 1012, 102, 101, 3531, 6523, 23435, 12314, 1005, 1055, 2880, 14924, 4818, 1998, 2009, 1005, 1055, 18750, 3642, 1024, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 4275, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 14924, 26340, 1013, 3746, 1013, 25022, 14971, 10790, 1013, 25022, 14971, 10790, 1035, 3345, 1012, 1052, 2100, 102, 101, 2036, 1010, 2012, 1996, 3231, 2051, 1010, 2017, 2064, 4957, 1996, 13117, 1997, 1996, 3231, 2951, 2007, 2367, 2193, 1997, 14108, 2013, 1996, 3345, 2951, 1012, 102, 101, 3531, 6523, 1996, 9312, 3642, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 4275, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 14924, 26340, 1013, 3746, 1013, 25022, 14971, 10790, 1013, 25022, 14971, 10790, 1035, 9345, 2140, 1012, 1052, 2100, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 13, 38, 53, 71, 111, 139, 186], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "53223967", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [63, 69]}], [{"sent_id": 2, "name": "tf.estimator", "pos": [77, 83]}, {"sent_id": 3, "name": "tf.estimator", "pos": [91, 97]}], [{"sent_id": 2, "name": "tf.contrib.estimator", "pos": [63, 73]}]], "sents": ["The error you're seeing is about a mismatch between the signatures in the SavedModel: the estimator does not write the one you are trying to serve.", "The saved_model_cli should help you inspect it.", "Also, your code uses the long-obsolete tf.contrib.estimator and mixes it with tf.estimator.", "I highly recommend switching to tf.estimator throughout.", "This seems unrelated to the use of TensorFlow Hub.", "By the time your estimator exports a model, all module contents have been inlined into it."], "sent_idxs": [101, 1996, 7561, 2017, 1005, 2128, 3773, 2003, 2055, 1037, 28616, 18900, 2818, 2090, 1996, 16442, 1999, 1996, 5552, 5302, 9247, 1024, 1996, 9765, 9581, 4263, 2515, 2025, 4339, 1996, 2028, 2017, 2024, 2667, 2000, 3710, 1012, 102, 101, 1996, 5552, 1035, 2944, 1035, 18856, 2072, 2323, 2393, 2017, 22459, 2009, 1012, 102, 101, 2036, 1010, 2115, 3642, 3594, 1996, 2146, 1011, 15832, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9765, 9581, 4263, 1998, 21109, 2009, 2007, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 102, 101, 1045, 3811, 16755, 11991, 2000, 1056, 2546, 1012, 9765, 9581, 4263, 2802, 1012, 102, 101, 2023, 3849, 15142, 2000, 1996, 2224, 1997, 23435, 12314, 9594, 1012, 102, 101, 2011, 1996, 2051, 2115, 9765, 9581, 4263, 14338, 1037, 2944, 1010, 2035, 11336, 8417, 2031, 2042, 23881, 2094, 2046, 2009, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 53, 85, 100, 113, 136], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61566373", "vertexSet": [[{"sent_id": 1, "name": "tf.compat", "pos": [26, 32]}], [{"sent_id": 1, "name": "tf.compat.v1", "pos": [26, 35]}], [{"sent_id": 1, "name": "tf.compat.v1.disable_eager_execution", "pos": [26, 42]}]], "sents": ["The TensorFlow 2.0 has enabled eager execution by default.", "At the starting of algorithm, you need to use tf.compat.v1.disable_eager_execution() to disable eager execution.", "<code>Code Snippet</code>.", "The output gives:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 23435, 12314, 1016, 1012, 1014, 2038, 9124, 9461, 7781, 2011, 12398, 1012, 102, 101, 2012, 1996, 3225, 1997, 9896, 1010, 2017, 2342, 2000, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 4487, 19150, 1035, 9461, 1035, 7781, 1006, 1007, 2000, 4487, 19150, 9461, 7781, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 6434, 3957, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 51, 65, 71, 85], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56358348", "vertexSet": [[{"sent_id": 4, "name": "tf.mod", "pos": [110, 114]}], [{"sent_id": 4, "name": "tf.math", "pos": [124, 128]}], [{"sent_id": 4, "name": "tf.math.mod", "pos": [124, 130]}]], "sents": ["Tensorflow 2.0 alpha is the Tensorflow 2.0 alpha version released on the Tensorflow dev summit 2019 day.", "Its API is \"stable\" (that means, it doesn't change every day), although it is explicitly an alpha version, hence something can't work as you expect.", "The nightly builds, instead, are builds created daily with the changes occurring on the master branch of the Tensorflow repo.", "The API changes often (e.g.", "2 days ago the symbol tf.mod was a thing, now it is present only in tf.math.mod) and is more \"unstable\" then the alpha itself.", "In the coming months... who knows.", "The first release candidate was planned for \"spring\" 2019 - but spring is a flexible concept (quote from Martin Wicke - \"Spring is a sort of flexible concept\")."], "sent_idxs": [101, 23435, 12314, 1016, 1012, 1014, 6541, 2003, 1996, 23435, 12314, 1016, 1012, 1014, 6541, 2544, 2207, 2006, 1996, 23435, 12314, 16475, 6465, 10476, 2154, 1012, 102, 101, 2049, 17928, 2003, 1000, 6540, 1000, 1006, 2008, 2965, 1010, 2009, 2987, 1005, 1056, 2689, 2296, 2154, 1007, 1010, 2348, 2009, 2003, 12045, 2019, 6541, 2544, 1010, 6516, 2242, 2064, 1005, 1056, 2147, 2004, 2017, 5987, 1012, 102, 101, 1996, 22390, 16473, 1010, 2612, 1010, 2024, 16473, 2580, 3679, 2007, 1996, 3431, 10066, 2006, 1996, 3040, 3589, 1997, 1996, 23435, 12314, 16360, 2080, 1012, 102, 101, 1996, 17928, 3431, 2411, 1006, 1041, 1012, 1043, 1012, 102, 101, 1016, 2420, 3283, 1996, 6454, 1056, 2546, 1012, 16913, 2001, 1037, 2518, 1010, 2085, 2009, 2003, 2556, 2069, 1999, 1056, 2546, 1012, 8785, 1012, 16913, 1007, 1998, 2003, 2062, 1000, 14480, 1000, 2059, 1996, 6541, 2993, 1012, 102, 101, 1999, 1996, 2746, 2706, 1012, 1012, 1012, 2040, 4282, 1012, 102, 101, 1996, 2034, 2713, 4018, 2001, 3740, 2005, 1000, 3500, 1000, 10476, 1011, 2021, 3500, 2003, 1037, 12379, 4145, 1006, 14686, 2013, 3235, 15536, 19869, 1011, 1000, 3500, 2003, 1037, 4066, 1997, 12379, 4145, 1000, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 66, 93, 104, 143, 155, 193], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57578583", "vertexSet": [[{"sent_id": 0, "name": "tf.where", "pos": [16, 20]}], [{"sent_id": 2, "name": "tf.scatter_nd", "pos": [71, 79]}], [{"sent_id": 2, "name": "tf.boolean_mask", "pos": [53, 60]}]], "sents": ["The simplest thing would be to evaluate the data on both models and the use tf.where to select the final output.", "<code>Code Snippet</code>.", "If you really want to avoid that, you can use tf.boolean_mask to split the data and then recombine it with tf.scatter_nd.", "This is one possible way.", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 21304, 2518, 2052, 2022, 2000, 16157, 1996, 2951, 2006, 2119, 4275, 1998, 1996, 2224, 1056, 2546, 1012, 2073, 2000, 7276, 1996, 2345, 6434, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2428, 2215, 2000, 4468, 2008, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 22017, 20898, 1035, 7308, 2000, 3975, 1996, 2951, 1998, 2059, 28667, 5358, 16765, 2009, 2007, 1056, 2546, 1012, 8040, 20097, 1035, 1050, 2094, 1012, 102, 101, 2023, 2003, 2028, 2825, 2126, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 41, 81, 89, 103], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43369980", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [15, 20]}, {"sent_id": 5, "name": "tf.nn", "pos": [101, 106]}], [{"sent_id": 1, "name": "tf.nn.softmax", "pos": [15, 23]}, {"sent_id": 5, "name": "tf.nn.softmax", "pos": [101, 109]}], [{"sent_id": 1, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [15, 32]}]], "sents": ["<code>Code Snippet</code>.", "tf.nn.softmax_cross_entropy_with_logits wants unscaled logits.", "From the doc:", "WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.", "Do not call this op with the output of softmax, as it will produce incorrect results.", "this means that the line y =  tf.nn.softmax(model) is wrong.", "Instead, you want to pass unscaled logits to that function, thus:", "<code>Code Snippet</code>.", "Moreover, once you fix this problem, if the network doesn't work, try to lower the learning rate from 0.01 to something about 1e-3 or 1e-4.", "(I tell you this because 1e-2 usually is an \"high\" learning rate)"], "sent_idxs": [101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 4122, 4895, 15782, 3709, 8833, 12762, 1012, 102, 101, 2013, 1996, 9986, 1024, 102, 101, 5432, 1024, 2023, 6728, 24273, 4895, 15782, 3709, 8833, 12762, 1010, 2144, 2009, 10438, 1037, 3730, 17848, 2006, 8833, 12762, 16058, 2005, 8122, 1012, 102, 101, 2079, 2025, 2655, 2023, 6728, 2007, 1996, 6434, 1997, 3730, 17848, 1010, 2004, 2009, 2097, 3965, 16542, 3463, 1012, 102, 101, 2023, 2965, 2008, 1996, 2240, 1061, 1027, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 2944, 1007, 2003, 3308, 1012, 102, 101, 2612, 1010, 2017, 2215, 2000, 3413, 4895, 15782, 3709, 8833, 12762, 2000, 2008, 3853, 1010, 2947, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 9308, 1010, 2320, 2017, 8081, 2023, 3291, 1010, 2065, 1996, 2897, 2987, 1005, 1056, 2147, 1010, 3046, 2000, 2896, 1996, 4083, 3446, 2013, 1014, 1012, 5890, 2000, 2242, 2055, 1015, 2063, 1011, 1017, 2030, 1015, 2063, 1011, 1018, 1012, 102, 101, 1006, 1045, 2425, 2017, 2023, 2138, 1015, 2063, 1011, 1016, 2788, 2003, 2019, 1000, 2152, 1000, 4083, 3446, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 40, 46, 72, 93, 116, 135, 149, 190, 211], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62598671", "vertexSet": [[{"sent_id": 0, "name": "tf.compat", "pos": [3, 9]}, {"sent_id": 0, "name": "tf.compat", "pos": [16, 22]}], [{"sent_id": 0, "name": "tf.compat.v1", "pos": [3, 12]}, {"sent_id": 0, "name": "tf.compat.v1", "pos": [16, 25]}], [{"sent_id": 0, "name": "tf.compat.v1.placeholder", "pos": [3, 15]}], [{"sent_id": 0, "name": "tf.compat.v1.reset_default_graph", "pos": [16, 31]}]], "sents": ["Try using tf.compat.v1.placeholder and tf.compat.v1.reset_default_graph.", "If you have any more attribute errors, you can look at the tensorflow documentation."], "sent_idxs": [101, 3046, 2478, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 2173, 14528, 1998, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 25141, 1035, 12398, 1035, 10629, 1012, 102, 101, 2065, 2017, 2031, 2151, 2062, 17961, 10697, 1010, 2017, 2064, 2298, 2012, 1996, 23435, 12314, 12653, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 33, 52], "sent_pos": [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55063849", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [10, 15]}], [{"sent_id": 0, "name": "tf.nn.pool", "pos": [10, 17]}], [{"sent_id": 4, "name": "tf.unsorted_segment_max", "pos": [58, 68]}]], "sents": ["I think you can achieve what you want with tf.nn.pool:", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "If you really want to us tf.unsorted_segment_max, you can do it as you suggest in your own answer.", "Here is an equivalent formulation that avoids transposing and includes the final reshaping:", "<code>Code Snippet</code>.", "Both methods should work fine in a neural network in terms of back-propagation.", "EDIT: In terms of performance, pooling seems to be more scalable than the segmented sum (as one would expect):", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2228, 2017, 2064, 6162, 2054, 2017, 2215, 2007, 1056, 2546, 1012, 1050, 2078, 1012, 4770, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2428, 2215, 2000, 2149, 1056, 2546, 1012, 4895, 21748, 3064, 1035, 6903, 1035, 4098, 1010, 2017, 2064, 2079, 2009, 2004, 2017, 6592, 1999, 2115, 2219, 3437, 1012, 102, 101, 2182, 2003, 2019, 5662, 20219, 2008, 26777, 9099, 6873, 7741, 1998, 2950, 1996, 2345, 24501, 3270, 4691, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2119, 4725, 2323, 2147, 2986, 1999, 1037, 15756, 2897, 1999, 3408, 1997, 2067, 1011, 20594, 1012, 102, 101, 10086, 1024, 1999, 3408, 1997, 2836, 1010, 4770, 2075, 3849, 2000, 2022, 2062, 26743, 3468, 2084, 1996, 6903, 2098, 7680, 1006, 2004, 2028, 2052, 5987, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 33, 37, 51, 82, 102, 116, 134, 163, 177], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48648242", "vertexSet": [[{"sent_id": 3, "name": "tf.image", "pos": [68, 72]}, {"sent_id": 3, "name": "tf.image", "pos": [79, 83]}, {"sent_id": 3, "name": "tf.image", "pos": [93, 97]}], [{"sent_id": 3, "name": "tf.image.decode_png", "pos": [79, 89]}], [{"sent_id": 3, "name": "tf.image.decode_jpeg", "pos": [68, 78]}], [{"sent_id": 3, "name": "tf.image.decode_image", "pos": [93, 102]}]], "sents": ["This was answered by this issue.", "Here's a sample snippet to resize tensor image keeping aspext ratio:", "<code>Code Snippet</code>.", "Problem of having a tensor of shape=<unknown> is solved by using type speific decoders like tf.image.decode_jpeg or tf.image.decode_png, as opposed to tf.image.decode_image"], "sent_idxs": [101, 2023, 2001, 4660, 2011, 2023, 3277, 1012, 102, 101, 2182, 1005, 1055, 1037, 7099, 1055, 3490, 29519, 2000, 24501, 4697, 23435, 3746, 4363, 2004, 5051, 18413, 6463, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3291, 1997, 2383, 1037, 23435, 1997, 4338, 1027, 1026, 4242, 1028, 2003, 13332, 2011, 2478, 2828, 11867, 7416, 8873, 2278, 21933, 13375, 2066, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 2030, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 1052, 3070, 1010, 2004, 4941, 2000, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 3746, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 9, 30, 44, 103], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0]}, {"title": "44640126", "vertexSet": [[{"sent_id": 7, "name": "tf.train", "pos": [143, 147]}], [{"sent_id": 7, "name": "tf.train.import_meta_graph", "pos": [143, 153]}], [{"sent_id": 7, "name": "tf.initialize_all_variables", "pos": [166, 175]}]], "sents": ["I had the same issue and I just figured out what was wrong, at least in my code.", "In the end, I used the wrong file name in saver.restore().", "This function must be given the file name without the file extension, just like the saver.save() function:", "<code>Code Snippet</code>.", "instead of", "<code>Code Snippet</code>.", "With this I do exactly what you wish to do: starting from scratch, stopping, then picking up again.", "I don't need to initialize a second saver from a meta file using the tf.train.import_meta_graph() function, and I don't need to explicitly state tf.initialize_all_variables() after initializing the optimizer.", "My complete model restore looks like this:", "<code>Code Snippet</code>.", "I think in protocol V1 you still had to add the .ckpt to the file name, and for import_meta_graph() you still need to add the .meta, which might cause some confusion among users.", "Maybe this should be pointed out more explicitly in the documentation."], "sent_idxs": [101, 1045, 2018, 1996, 2168, 3277, 1998, 1045, 2074, 6618, 2041, 2054, 2001, 3308, 1010, 2012, 2560, 1999, 2026, 3642, 1012, 102, 101, 1999, 1996, 2203, 1010, 1045, 2109, 1996, 3308, 5371, 2171, 1999, 3828, 2099, 1012, 9239, 1006, 1007, 1012, 102, 101, 2023, 3853, 2442, 2022, 2445, 1996, 5371, 2171, 2302, 1996, 5371, 5331, 1010, 2074, 2066, 1996, 3828, 2099, 1012, 3828, 1006, 1007, 3853, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2612, 1997, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2007, 2023, 1045, 2079, 3599, 2054, 2017, 4299, 2000, 2079, 1024, 3225, 2013, 11969, 1010, 7458, 1010, 2059, 8130, 2039, 2153, 1012, 102, 101, 1045, 2123, 1005, 1056, 2342, 2000, 3988, 4697, 1037, 2117, 3828, 2099, 2013, 1037, 18804, 5371, 2478, 1996, 1056, 2546, 1012, 3345, 1012, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 3853, 1010, 1998, 1045, 2123, 1005, 1056, 2342, 2000, 12045, 2110, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 2044, 3988, 6026, 1996, 23569, 27605, 6290, 1012, 102, 101, 2026, 3143, 2944, 9239, 3504, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 2228, 1999, 8778, 1058, 2487, 2017, 2145, 2018, 2000, 5587, 1996, 1012, 23616, 13876, 2000, 1996, 5371, 2171, 1010, 1998, 2005, 12324, 1035, 18804, 1035, 10629, 1006, 1007, 2017, 2145, 2342, 2000, 5587, 1996, 1012, 18804, 1010, 2029, 2453, 3426, 2070, 6724, 2426, 5198, 1012, 102, 101, 2672, 2023, 2323, 2022, 4197, 2041, 2062, 12045, 1999, 1996, 12653, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 42, 68, 82, 86, 100, 124, 186, 196, 210, 258, 272], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "64092221", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [27, 32]}], [{"sent_id": 1, "name": "tf.keras.preprocessing", "pos": [27, 37]}], [{"sent_id": 1, "name": "tf.keras.preprocessing.timeseries_dataset_from_array", "pos": [27, 47]}]], "sents": ["I think it's introduced in tensorflow 2.3.0, so you need to update your tensorflow.", "tf.keras.preprocessing.timeseries_dataset_from_array"], "sent_idxs": [101, 1045, 2228, 2009, 1005, 1055, 3107, 1999, 23435, 12314, 1016, 1012, 1017, 1012, 1014, 1010, 2061, 2017, 2342, 2000, 10651, 2115, 23435, 12314, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 17463, 3217, 9623, 7741, 1012, 2335, 28077, 1035, 2951, 13462, 1035, 2013, 1035, 9140, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 48], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]}, {"title": "64032101", "vertexSet": [[{"sent_id": 8, "name": "tf.test", "pos": [123, 127]}], [{"sent_id": 9, "name": "tf.keras", "pos": [169, 174]}, {"sent_id": 9, "name": "tf.keras", "pos": [179, 184]}], [{"sent_id": 9, "name": "tf.keras.backend", "pos": [179, 187]}], [{"sent_id": 8, "name": "tf.test.is_built_with_cuda", "pos": [123, 136]}]], "sents": ["From your code you are trying to understand, Is Tensorflow installed and using GPU ?", "In Tensorflow 2.x, the same can be known in the following way", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "The following will also return the name of your GPU devices.", "<code>Code Snippet</code>.", "If a non-GPU version of the package is installed, the function would also return False.", "Use tf.test.is_built_with_cuda to validate if TensorFlow was build with CUDA support.", "Note: From TF 2.3, onwards Keras are integrated with Tensorflow as tf.keras, please use this module tf.keras.backend"], "sent_idxs": [101, 2013, 2115, 3642, 2017, 2024, 2667, 2000, 3305, 1010, 2003, 23435, 12314, 5361, 1998, 2478, 14246, 2226, 1029, 102, 101, 1999, 23435, 12314, 1016, 1012, 1060, 1010, 1996, 2168, 2064, 2022, 2124, 1999, 1996, 2206, 2126, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2206, 2097, 2036, 2709, 1996, 2171, 1997, 2115, 14246, 2226, 5733, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 1037, 2512, 1011, 14246, 2226, 2544, 1997, 1996, 7427, 2003, 5361, 1010, 1996, 3853, 2052, 2036, 2709, 6270, 1012, 102, 101, 2224, 1056, 2546, 1012, 3231, 1012, 2003, 1035, 2328, 1035, 2007, 1035, 12731, 2850, 2000, 9398, 3686, 2065, 23435, 12314, 2001, 3857, 2007, 12731, 2850, 2490, 1012, 102, 101, 3602, 1024, 2013, 1056, 2546, 1016, 1012, 1017, 1010, 9921, 17710, 8180, 2024, 6377, 2007, 23435, 12314, 2004, 1056, 2546, 1012, 17710, 8180, 1010, 3531, 2224, 2023, 11336, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 20, 38, 52, 56, 70, 85, 99, 121, 150, 188], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0]}, {"title": "63185786", "vertexSet": [[{"sent_id": 5, "name": "tf.keras", "pos": [75, 80]}], [{"sent_id": 5, "name": "tf.keras.layers", "pos": [75, 82]}], [{"sent_id": 5, "name": "tf.keras.layers.conv2d", "pos": [75, 87]}]], "sents": ["<code>Code Snippet</code>.", "They should be 4D not 3D in shape.", "You need to give the detail of batch also.", "(batch_size, w,h,c) <---- 4D", "You are missing batch_size", "32,(3,3) from tf.keras.layers.Conv2D(32,(3,3), padding = 'same')(input_image)", "You have 32 filters.", "So the channel depth will be 32.", "But since you have used the padding='same' so your output will have the same dimension as input.", "Only differ in depth."], "sent_idxs": [101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2027, 2323, 2022, 1018, 2094, 2025, 7605, 1999, 4338, 1012, 102, 101, 2017, 2342, 2000, 2507, 1996, 6987, 1997, 14108, 2036, 1012, 102, 101, 1006, 14108, 1035, 2946, 1010, 1059, 1010, 1044, 1010, 1039, 1007, 1026, 1011, 1011, 1011, 1011, 1018, 2094, 102, 101, 2017, 2024, 4394, 14108, 1035, 2946, 102, 101, 3590, 1010, 1006, 1017, 1010, 1017, 1007, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1006, 3590, 1010, 1006, 1017, 1010, 1017, 1007, 1010, 11687, 4667, 1027, 1005, 2168, 1005, 1007, 1006, 7953, 1035, 3746, 1007, 102, 101, 2017, 2031, 3590, 17736, 1012, 102, 101, 2061, 1996, 3149, 5995, 2097, 2022, 3590, 1012, 102, 101, 2021, 2144, 2017, 2031, 2109, 1996, 11687, 4667, 1027, 1005, 2168, 1005, 2061, 2115, 6434, 2097, 2031, 1996, 2168, 9812, 2004, 7953, 1012, 102, 101, 2069, 11234, 1999, 5995, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 14, 26, 38, 58, 66, 109, 116, 126, 151, 158], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51329871", "vertexSet": [[{"sent_id": 2, "name": "tf.gfile.gfile", "pos": [52, 62]}, {"sent_id": 3, "name": "tf.gfile.gfile", "pos": [84, 94]}], [{"sent_id": 0, "name": "tf.gfile.open", "pos": [1, 9]}, {"sent_id": 2, "name": "tf.gfile.open", "pos": [38, 46]}, {"sent_id": 3, "name": "tf.gfile.open", "pos": [68, 76]}], [{"sent_id": 8, "name": "tf.data", "pos": [185, 189]}], [{"sent_id": 3, "name": "tf.tensor", "pos": [113, 117]}], [{"sent_id": 3, "name": "tf.placeholder", "pos": [123, 128]}]], "sents": ["tf.gfile.Open isn't a TensorFlow operation.", "In other words, it does not add operations to the graph to open the file.", "tf.gfile.Open is an alias for the class tf.gfile.GFile.", "So the line tf.gfile.Open(<foo>) is invoking tf.gfile.GFile.__init__ which expects the first argument to be a Python string, not tf.Tensor of strings (which is what tf.placeholder(tf.string) returns).", "You have a few options here:", "Feed the contents of the file.", "<code>Code Snippet</code>.", "Open and read the file in the graph.", "(Using the tf.data classes to setup \"input processing\")", "<code>Code Snippet</code>.", "Use eager execution.", "(See the Research and Experimentation section of the TensorFlow getting started guide)", "Which may help reduce some of the confusion between what's in the graph and what's happening in Python.", "<code>Code Snippet</code>.", "Hope that helps!"], "sent_idxs": [101, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 2330, 3475, 1005, 1056, 1037, 23435, 12314, 3169, 1012, 102, 101, 1999, 2060, 2616, 1010, 2009, 2515, 2025, 5587, 3136, 2000, 1996, 10629, 2000, 2330, 1996, 5371, 1012, 102, 101, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 2330, 2003, 2019, 14593, 2005, 1996, 2465, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 1043, 8873, 2571, 1012, 102, 101, 2061, 1996, 2240, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 2330, 1006, 1026, 29379, 1028, 1007, 2003, 1999, 22776, 1056, 2546, 1012, 1043, 8873, 2571, 1012, 1043, 8873, 2571, 1012, 1035, 1035, 1999, 4183, 1035, 1035, 2029, 24273, 1996, 2034, 6685, 2000, 2022, 1037, 18750, 5164, 1010, 2025, 1056, 2546, 1012, 23435, 1997, 7817, 1006, 2029, 2003, 2054, 1056, 2546, 1012, 2173, 14528, 1006, 1056, 2546, 1012, 5164, 1007, 5651, 1007, 1012, 102, 101, 2017, 2031, 1037, 2261, 7047, 2182, 1024, 102, 101, 5438, 1996, 8417, 1997, 1996, 5371, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2330, 1998, 3191, 1996, 5371, 1999, 1996, 10629, 1012, 102, 101, 1006, 2478, 1996, 1056, 2546, 1012, 2951, 4280, 2000, 16437, 1000, 7953, 6364, 1000, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2224, 9461, 7781, 1012, 102, 101, 1006, 2156, 1996, 2470, 1998, 21470, 2930, 1997, 1996, 23435, 12314, 2893, 2318, 5009, 1007, 102, 101, 2029, 2089, 2393, 5547, 2070, 1997, 1996, 6724, 2090, 2054, 1005, 1055, 1999, 1996, 10629, 1998, 2054, 1005, 1055, 6230, 1999, 18750, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3246, 2008, 7126, 999, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [2, 3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [2, 3]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 18, 37, 64, 138, 147, 156, 170, 181, 198, 212, 218, 235, 260, 274, 280], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44466444", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [9, 15]}], [{"sent_id": 0, "name": "tf.contrib.learn", "pos": [9, 17]}], [{"sent_id": 0, "name": "tf.contrib.learn.runconfig", "pos": [9, 22]}]], "sents": ["You need to add config=tf.contrib.learn.RunConfig( save_checkpoints_secs=save_checkpoints_secs) in your model definition.", "The save_checkpoints_secs can be changed to save_checkpoints_steps, but not both."], "sent_idxs": [101, 2017, 2342, 2000, 5587, 9530, 8873, 2290, 1027, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 2448, 8663, 8873, 2290, 1006, 3828, 1035, 26520, 2015, 1035, 10819, 2015, 1027, 3828, 1035, 26520, 2015, 1035, 10819, 2015, 1007, 1999, 2115, 2944, 6210, 1012, 102, 101, 1996, 3828, 1035, 26520, 2015, 1035, 10819, 2015, 2064, 2022, 2904, 2000, 3828, 1035, 26520, 2015, 1035, 4084, 1010, 2021, 2025, 2119, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 45, 70], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44903756", "vertexSet": [[{"sent_id": 17, "name": "tf.graph", "pos": [296, 300]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [12, 16]}], [{"sent_id": 3, "name": "tf.matmul", "pos": [57, 63]}], [{"sent_id": 7, "name": "tf.session", "pos": [179, 183]}], [{"sent_id": 2, "name": "tf.variable", "pos": [38, 42]}], [{"sent_id": 2, "name": "tf.constant", "pos": [43, 47]}], [{"sent_id": 0, "name": "tf.operation", "pos": [17, 21]}], [{"sent_id": 4, "name": "tf.get_default_graph", "pos": [111, 119]}]], "sents": ["The TensorFlow Graph is an object which contains your various tf.Tensor and tf.Operation.", "When you create these tensors (e.g.", "using tf.Variable or tf.constant) or operations (e.g.", "tf.matmul), they will be added to the default graph (look at the graph member of these object to get the graph they belong to).", "If you haven't specified anything, it will be the graph you get when calling the tf.get_default_graph method.", "But you could also work with multiple graphes using a context manager:", "<code>Code Snippet</code>.", "Suppose you created several graphes in your code, you then need to put the graph you and to run as an argument of the tf.Session method to specify TensorFlow which one to run.", "In Code A, you", "work with the default graph,.", "try to import the meta graph into it (which fails because it already contains some of the nodes) and,.", "would restore the model into it, .", "while in Code B, you", "create a fresh new graph,.", "import the meta graph into it (which succeeds because it's an empty graph) and.", "restore it..", "Useful link:.", "tf.Graph API", "Edit:.", "This piece of code makes the Code A work (I reset the default graph to a fresh one, and I removed the predict name_scope).", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 23435, 12314, 10629, 2003, 2019, 4874, 2029, 3397, 2115, 2536, 1056, 2546, 1012, 23435, 1998, 1056, 2546, 1012, 3169, 1012, 102, 101, 2043, 2017, 3443, 2122, 23435, 2015, 1006, 1041, 1012, 1043, 1012, 102, 101, 2478, 1056, 2546, 1012, 8023, 2030, 1056, 2546, 1012, 5377, 1007, 2030, 3136, 1006, 1041, 1012, 1043, 1012, 102, 101, 1056, 2546, 1012, 13523, 12274, 2140, 1007, 1010, 2027, 2097, 2022, 2794, 2000, 1996, 12398, 10629, 1006, 2298, 2012, 1996, 10629, 2266, 1997, 2122, 4874, 2000, 2131, 1996, 10629, 2027, 7141, 2000, 1007, 1012, 102, 101, 2065, 2017, 4033, 1005, 1056, 9675, 2505, 1010, 2009, 2097, 2022, 1996, 10629, 2017, 2131, 2043, 4214, 1996, 1056, 2546, 1012, 2131, 1035, 12398, 1035, 10629, 4118, 1012, 102, 101, 2021, 2017, 2071, 2036, 2147, 2007, 3674, 10629, 2229, 2478, 1037, 6123, 3208, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6814, 2017, 2580, 2195, 10629, 2229, 1999, 2115, 3642, 1010, 2017, 2059, 2342, 2000, 2404, 1996, 10629, 2017, 1998, 2000, 2448, 2004, 2019, 6685, 1997, 1996, 1056, 2546, 1012, 5219, 4118, 2000, 20648, 23435, 12314, 2029, 2028, 2000, 2448, 1012, 102, 101, 1999, 3642, 1037, 1010, 2017, 102, 101, 2147, 2007, 1996, 12398, 10629, 1010, 1012, 102, 101, 3046, 2000, 12324, 1996, 18804, 10629, 2046, 2009, 1006, 2029, 11896, 2138, 2009, 2525, 3397, 2070, 1997, 1996, 14164, 1007, 1998, 1010, 1012, 102, 101, 2052, 9239, 1996, 2944, 2046, 2009, 1010, 1012, 102, 101, 2096, 1999, 3642, 1038, 1010, 2017, 102, 101, 3443, 1037, 4840, 2047, 10629, 1010, 1012, 102, 101, 12324, 1996, 18804, 10629, 2046, 2009, 1006, 2029, 21645, 2138, 2009, 1005, 1055, 2019, 4064, 10629, 1007, 1998, 1012, 102, 101, 9239, 2009, 1012, 1012, 102, 101, 6179, 4957, 1024, 1012, 102, 101, 1056, 2546, 1012, 10629, 17928, 102, 101, 10086, 1024, 1012, 102, 101, 2023, 3538, 1997, 3642, 3084, 1996, 3642, 1037, 2147, 1006, 1045, 25141, 1996, 12398, 10629, 2000, 1037, 4840, 2028, 1010, 1998, 1045, 3718, 1996, 16014, 2171, 1035, 9531, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 23, 36, 56, 92, 122, 138, 152, 194, 201, 210, 235, 245, 253, 262, 283, 289, 295, 302, 307, 339, 353], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49528836", "vertexSet": [[{"sent_id": 6, "name": "tf.train", "pos": [170, 174]}, {"sent_id": 7, "name": "tf.train", "pos": [225, 229]}], [{"sent_id": 0, "name": "tf.graphkeys", "pos": [17, 23]}, {"sent_id": 0, "name": "tf.graphkeys", "pos": [28, 34]}], [{"sent_id": 6, "name": "tf.train.saver", "pos": [170, 177]}, {"sent_id": 7, "name": "tf.train.saver", "pos": [225, 232]}]], "sents": ["You're right, the saver does get its variables from the union of tf.GraphKeys.GLOBAL_VARIABLES and tf.GraphKeys.SAVEABLE_OBJECTS at its construction time (cf.", "Saver._build implementation or the quote below):", "<code>Code Snippet</code>.", "where _all_saveable_objects is defined in python/ops/variables.py file", "<code>Code Snippet</code>.", "There is no way to add variables to a saver apart from creating a new saver (cf https://github.com/tensorflow/tensorflow/issues/2489#issuecomment-221282483):", "When you create a tf.train.Saver with no arguments, it will implicitly use the current set of variables at the time of Saver construction when it saves and restores.", "If you add a new variable [...], you have to create a new tf.train.Saver to save it."], "sent_idxs": [101, 2017, 1005, 2128, 2157, 1010, 1996, 3828, 2099, 2515, 2131, 2049, 10857, 2013, 1996, 2586, 1997, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3795, 1035, 10857, 1998, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3828, 3085, 1035, 5200, 2012, 2049, 2810, 2051, 1006, 12935, 1012, 102, 101, 3828, 2099, 1012, 1035, 3857, 7375, 2030, 1996, 14686, 2917, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2073, 1035, 2035, 1035, 3828, 3085, 1035, 5200, 2003, 4225, 1999, 18750, 1013, 23092, 1013, 10857, 1012, 1052, 2100, 5371, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2045, 2003, 2053, 2126, 2000, 5587, 10857, 2000, 1037, 3828, 2099, 4237, 2013, 4526, 1037, 2047, 3828, 2099, 1006, 12935, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 3314, 1013, 24568, 2683, 1001, 3277, 9006, 3672, 1011, 19594, 22407, 18827, 2620, 2509, 1007, 1024, 102, 101, 2043, 2017, 3443, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2007, 2053, 9918, 1010, 2009, 2097, 24655, 2135, 2224, 1996, 2783, 2275, 1997, 10857, 2012, 1996, 2051, 1997, 3828, 2099, 2810, 2043, 2009, 13169, 1998, 9239, 2015, 1012, 102, 101, 2065, 2017, 5587, 1037, 2047, 8023, 1031, 1012, 1012, 1012, 1033, 1010, 2017, 2031, 2000, 3443, 1037, 2047, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2000, 3828, 2009, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 47, 61, 75, 97, 111, 165, 206, 237], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0]}, {"title": "47809165", "vertexSet": [[{"sent_id": 8, "name": "tf.nn.dropout", "pos": [160, 168]}, {"sent_id": 9, "name": "tf.nn.dropout", "pos": [188, 196]}], [{"sent_id": 8, "name": "tf.layers.dropout", "pos": [171, 178]}], [{"sent_id": 0, "name": "tf.estimator", "pos": [5, 11]}, {"sent_id": 3, "name": "tf.estimator", "pos": [84, 90]}], [{"sent_id": 0, "name": "tf.estimator.modekeys", "pos": [5, 15]}, {"sent_id": 3, "name": "tf.estimator.modekeys", "pos": [84, 94]}]], "sents": ["You can use the tf.estimator.ModeKeys to detect whether you are calling your estimator in TRAIN mode or in EVAL mode.", "When constructing your model function for your Estimator (as depicted in this tutorial), the function have to respect a skeleton :", "<code>Code Snippet</code>.", "The mode parameter is a tf.estimator.ModeKeys, so, in your model function, you can simply test against the mode parameter to detect whether you are in training or in evaluation.", "(or in prediction).", "A simple example :", "<code>Code Snippet</code>.", "On the side :", "Instead of using tf.nn.dropout, consider using tf.layers.dropout.", "It's a wrapper over tf.nn.dropout, but it comes with a boolean to activate or deactivate the dropout.", "(You can set the boolean the same way that the prob in my example)."], "sent_idxs": [101, 2017, 2064, 2224, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 5549, 14839, 2015, 2000, 11487, 3251, 2017, 2024, 4214, 2115, 9765, 9581, 4263, 1999, 3345, 5549, 2030, 1999, 9345, 2140, 5549, 1012, 102, 101, 2043, 15696, 2115, 2944, 3853, 2005, 2115, 9765, 9581, 4263, 1006, 2004, 8212, 1999, 2023, 14924, 4818, 1007, 1010, 1996, 3853, 2031, 2000, 4847, 1037, 13526, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 5549, 16381, 2003, 1037, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 5549, 14839, 2015, 1010, 2061, 1010, 1999, 2115, 2944, 3853, 1010, 2017, 2064, 3432, 3231, 2114, 1996, 5549, 16381, 2000, 11487, 3251, 2017, 2024, 1999, 2731, 2030, 1999, 9312, 1012, 102, 101, 1006, 2030, 1999, 17547, 1007, 1012, 102, 101, 1037, 3722, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2006, 1996, 2217, 1024, 102, 101, 2612, 1997, 2478, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1010, 5136, 2478, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 1012, 102, 101, 2009, 1005, 1055, 1037, 10236, 4842, 2058, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1010, 2021, 2009, 3310, 2007, 1037, 22017, 20898, 2000, 20544, 2030, 26709, 6593, 21466, 1996, 4530, 5833, 1012, 102, 101, 1006, 2017, 2064, 2275, 1996, 22017, 20898, 1996, 2168, 2126, 2008, 1996, 4013, 2497, 1999, 2026, 2742, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [8, 9, 10]}, {"r": "S1", "h": 1, "t": 0, "evidence": [8, 9, 10]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 35, 64, 78, 122, 130, 136, 150, 156, 180, 215, 236], "sent_pos": [0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34988069", "vertexSet": [[{"sent_id": 4, "name": "tf.argmax", "pos": [85, 91]}], [{"sent_id": 4, "name": "tf.gather", "pos": [134, 138]}], [{"sent_id": 4, "name": "tf.reshape", "pos": [111, 117]}], [{"sent_id": 0, "name": "tf.reduce_max", "pos": [2, 8]}]], "sents": ["The tf.reduce_max() operator provides exactly this functionality.", "By default it computes the global maximum of the given tensor, but you can specify a list of reduction_indices, which has the same meaning as axis in NumPy.", "To complete your example:", "<code>Code Snippet</code>.", "If you compute the argmax using tf.argmax(), you could obtain the the values from a different tensor y by flattening y using tf.reshape(), converting the argmax indices into vector indices as follows, and using tf.gather() to extract the appropriate values:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 5547, 1035, 4098, 1006, 1007, 6872, 3640, 3599, 2023, 15380, 1012, 102, 101, 2011, 12398, 2009, 24134, 2015, 1996, 3795, 4555, 1997, 1996, 2445, 23435, 1010, 2021, 2017, 2064, 20648, 1037, 2862, 1997, 7312, 1035, 29299, 1010, 2029, 2038, 1996, 2168, 3574, 2004, 8123, 1999, 16371, 8737, 2100, 1012, 102, 101, 2000, 3143, 2115, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 24134, 1996, 12098, 21693, 8528, 2478, 1056, 2546, 1012, 12098, 21693, 8528, 1006, 1007, 1010, 2017, 2071, 6855, 1996, 1996, 5300, 2013, 1037, 2367, 23435, 1061, 2011, 4257, 6528, 2075, 1061, 2478, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 1010, 16401, 1996, 12098, 21693, 8528, 29299, 2046, 9207, 29299, 2004, 4076, 1010, 1998, 2478, 1056, 2546, 1012, 8587, 1006, 1007, 2000, 14817, 1996, 6413, 5300, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 17, 55, 62, 76, 147, 161], "sent_pos": [0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45722714", "vertexSet": [[{"sent_id": 2, "name": "tf.session", "pos": [63, 67]}], [{"sent_id": 4, "name": "tf.contrib", "pos": [125, 131]}], [{"sent_id": 4, "name": "tf.contrib.learn", "pos": [125, 133]}], [{"sent_id": 4, "name": "tf.contrib.learn.runconfig", "pos": [125, 138]}]], "sents": ["Checkpoints happen with a certain frequency.", "If a new checkpoint has not occurred by the time a new evaluation is scheduled to occur, you'll get the message \"Skipping evaluation due to same checkpoint...\".", "This is because evaluation needs to work off of frozen weights in a separate tf.Session to avoid having weights change during evaluation, and the only way to communicate these weights between sessions is with a checkpoint.", "So if you want to evaluate more often and you are getting that message, increase your checkpoint frequency.", "You can do this by adding a flag that populates tf.contrib.learn.RunConfig#save_checkpoints_steps."], "sent_idxs": [101, 26520, 2015, 4148, 2007, 1037, 3056, 6075, 1012, 102, 101, 2065, 1037, 2047, 26520, 2038, 2025, 4158, 2011, 1996, 2051, 1037, 2047, 9312, 2003, 5115, 2000, 5258, 1010, 2017, 1005, 2222, 2131, 1996, 4471, 1000, 25978, 9312, 2349, 2000, 2168, 26520, 1012, 1012, 1012, 1000, 1012, 102, 101, 2023, 2003, 2138, 9312, 3791, 2000, 2147, 2125, 1997, 7708, 15871, 1999, 1037, 3584, 1056, 2546, 1012, 5219, 2000, 4468, 2383, 15871, 2689, 2076, 9312, 1010, 1998, 1996, 2069, 2126, 2000, 10639, 2122, 15871, 2090, 6521, 2003, 2007, 1037, 26520, 1012, 102, 101, 2061, 2065, 2017, 2215, 2000, 16157, 2062, 2411, 1998, 2017, 2024, 2893, 2008, 4471, 1010, 3623, 2115, 26520, 6075, 1012, 102, 101, 2017, 2064, 2079, 2023, 2011, 5815, 1037, 5210, 2008, 3769, 18969, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 2448, 8663, 8873, 2290, 1001, 3828, 1035, 26520, 2015, 1035, 4084, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 10, 48, 91, 113, 147], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57562774", "vertexSet": [[{"sent_id": 2, "name": "tf.compat", "pos": [60, 66]}], [{"sent_id": 1, "name": "tf.session", "pos": [24, 28]}], [{"sent_id": 2, "name": "tf.compat.v1", "pos": [60, 69]}], [{"sent_id": 2, "name": "tf.compat.v1.session", "pos": [60, 71]}]], "sents": ["You code is indeed correct.", "The warning that you get indicates that as from Tensorflow 2.0, tf.Session() won't exist in the API.", "Therefore, if you want you code to be compatible with Tensorflow 2.0, you should use tf.compat.v1.Session instead.", "So, just change this line:", "<code>Code Snippet</code>.", "To:", "<code>Code Snippet</code>.", "Then, even if you update Tensorflow from 1.xx to 2.xx, your code would execute in the same way.", "As for the code in Tensorflow 2.0:", "<code>Code Snippet</code>.", "it is fine if you run it in Tensorflow 2.0.", "If you want to run the same code, without installing Tensorflow 2.0, you can do the following:", "<code>Code Snippet</code>.", "The reason for this is because the default way of executing Tensorflow operations starting from Tensorflow 2.0 is in eager mode.", "The way of activating eager mode in Tensorflow 1.xx, is to enable it right after the import of Tensorflow, as I am doing it in the example above."], "sent_idxs": [101, 2017, 3642, 2003, 5262, 6149, 1012, 102, 101, 1996, 5432, 2008, 2017, 2131, 7127, 2008, 2004, 2013, 23435, 12314, 1016, 1012, 1014, 1010, 1056, 2546, 1012, 5219, 1006, 1007, 2180, 1005, 1056, 4839, 1999, 1996, 17928, 1012, 102, 101, 3568, 1010, 2065, 2017, 2215, 2017, 3642, 2000, 2022, 11892, 2007, 23435, 12314, 1016, 1012, 1014, 1010, 2017, 2323, 2224, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 5219, 2612, 1012, 102, 101, 2061, 1010, 2074, 2689, 2023, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1010, 2130, 2065, 2017, 10651, 23435, 12314, 2013, 1015, 1012, 22038, 2000, 1016, 1012, 22038, 1010, 2115, 3642, 2052, 15389, 1999, 1996, 2168, 2126, 1012, 102, 101, 2004, 2005, 1996, 3642, 1999, 23435, 12314, 1016, 1012, 1014, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2009, 2003, 2986, 2065, 2017, 2448, 2009, 1999, 23435, 12314, 1016, 1012, 1014, 1012, 102, 101, 2065, 2017, 2215, 2000, 2448, 1996, 2168, 3642, 1010, 2302, 23658, 23435, 12314, 1016, 1012, 1014, 1010, 2017, 2064, 2079, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3114, 2005, 2023, 2003, 2138, 1996, 12398, 2126, 1997, 23448, 23435, 12314, 3136, 3225, 2013, 23435, 12314, 1016, 1012, 1014, 2003, 1999, 9461, 5549, 1012, 102, 101, 1996, 2126, 1997, 2552, 17441, 9461, 5549, 1999, 23435, 12314, 1015, 1012, 22038, 1010, 2003, 2000, 9585, 2009, 2157, 2044, 1996, 12324, 1997, 23435, 12314, 1010, 2004, 1045, 2572, 2725, 2009, 1999, 1996, 2742, 2682, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 8, 39, 74, 83, 97, 101, 115, 143, 156, 170, 186, 211, 225, 253, 291], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36854491", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [54, 58]}, {"sent_id": 5, "name": "tf.train", "pos": [125, 129]}], [{"sent_id": 10, "name": "tf.placeholder", "pos": [241, 246]}], [{"sent_id": 2, "name": "tf.train.adadeltaoptimizer", "pos": [54, 65]}, {"sent_id": 5, "name": "tf.train.adadeltaoptimizer", "pos": [125, 136]}], [{"sent_id": 0, "name": "tf.initialize_all_variables", "pos": [6, 15]}]], "sents": ["The problem here is that tf.initialize_all_variables() is a misleading name.", "It really means \"return an operation that initializes all variables that have already been created (in the default graph)\".", "When you call tf.train.AdadeltaOptimizer(...).minimize(), TensorFlow creates additional variables, which are not covered by the init op that you created earlier.", "Moving the line:", "<code>Code Snippet</code>.", "...after the construction of the tf.train.AdadeltaOptimizer should make your program work.", "N.B.", "Your program rebuilds the entire network, apart from the variables, on each training step.", "This is likely to be very inefficient, and the Adadelta algorithm will not adapt as expected because its state is recreated on each step.", "I would strongly recommend moving the code from the definition of batch_xs to the creation of the optimizer outside of the two nested for loops.", "You should define tf.placeholder() ops for the batch_xs and batch_ys inputs, and use the feed_dict argument to sess.run() to pass in the values returned by mnist.train.next_batch()."], "sent_idxs": [101, 1996, 3291, 2182, 2003, 2008, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 2003, 1037, 22369, 2171, 1012, 102, 101, 2009, 2428, 2965, 1000, 2709, 2019, 3169, 2008, 3988, 10057, 2035, 10857, 2008, 2031, 2525, 2042, 2580, 1006, 1999, 1996, 12398, 10629, 1007, 1000, 1012, 102, 101, 2043, 2017, 2655, 1056, 2546, 1012, 3345, 1012, 15262, 9247, 28555, 13876, 27605, 6290, 1006, 1012, 1012, 1012, 1007, 1012, 18478, 1006, 1007, 1010, 23435, 12314, 9005, 3176, 10857, 1010, 2029, 2024, 2025, 3139, 2011, 1996, 1999, 4183, 6728, 2008, 2017, 2580, 3041, 1012, 102, 101, 3048, 1996, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2044, 1996, 2810, 1997, 1996, 1056, 2546, 1012, 3345, 1012, 15262, 9247, 28555, 13876, 27605, 6290, 2323, 2191, 2115, 2565, 2147, 1012, 102, 101, 1050, 1012, 1038, 1012, 102, 101, 2115, 2565, 14591, 2015, 1996, 2972, 2897, 1010, 4237, 2013, 1996, 10857, 1010, 2006, 2169, 2731, 3357, 1012, 102, 101, 2023, 2003, 3497, 2000, 2022, 2200, 1999, 12879, 8873, 23402, 3372, 1010, 1998, 1996, 15262, 9247, 2696, 9896, 2097, 2025, 15581, 2004, 3517, 2138, 2049, 2110, 2003, 29414, 2006, 2169, 3357, 1012, 102, 101, 1045, 2052, 6118, 16755, 3048, 1996, 3642, 2013, 1996, 6210, 1997, 14108, 1035, 1060, 2015, 2000, 1996, 4325, 1997, 1996, 23569, 27605, 6290, 2648, 1997, 1996, 2048, 9089, 2098, 2005, 15932, 1012, 102, 101, 2017, 2323, 9375, 1056, 2546, 1012, 2173, 14528, 1006, 1007, 23092, 2005, 1996, 14108, 1035, 1060, 2015, 1998, 14108, 1035, 1061, 2015, 20407, 1010, 1998, 2224, 1996, 5438, 1035, 4487, 6593, 6685, 2000, 7367, 4757, 1012, 2448, 1006, 1007, 2000, 3413, 1999, 1996, 5300, 2513, 2011, 24098, 2923, 1012, 3345, 1012, 2279, 1035, 14108, 1006, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 23, 50, 96, 102, 116, 143, 149, 169, 203, 237, 296], "sent_pos": [0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42525985", "vertexSet": [[{"sent_id": 2, "name": "tf.tanh", "pos": [118, 123]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [6, 12]}, {"sent_id": 0, "name": "tf.contrib", "pos": [21, 27]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [6, 15]}, {"sent_id": 0, "name": "tf.contrib.rnn", "pos": [21, 30]}], [{"sent_id": 0, "name": "tf.contrib.rnn.lstmcell", "pos": [6, 20]}], [{"sent_id": 0, "name": "tf.contrib.rnn.basiclstmcell", "pos": [21, 36]}]], "sents": ["When you instantiate both tf.contrib.rnn.LSTMcell and tf.contrib.rnn.BasicLSTMCell you can pass the activation function as the activation parameter.", "If you look at the linked documentation, you'll see, for example, that the constructor's signature for BasicLSTMCell is", "__init__(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=tf.tanh)"], "sent_idxs": [101, 2043, 2017, 7107, 13143, 2119, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 1048, 3367, 12458, 5349, 1998, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 3937, 4877, 21246, 29109, 2140, 2017, 2064, 3413, 1996, 13791, 3853, 2004, 1996, 13791, 16381, 1012, 102, 101, 2065, 2017, 2298, 2012, 1996, 5799, 12653, 1010, 2017, 1005, 2222, 2156, 1010, 2005, 2742, 1010, 2008, 1996, 9570, 2953, 1005, 1055, 8085, 2005, 3937, 4877, 21246, 29109, 2140, 2003, 102, 101, 1035, 1035, 1999, 4183, 1035, 1035, 1006, 16371, 2213, 1035, 3197, 1010, 5293, 1035, 13827, 1027, 1015, 1012, 1014, 1010, 7953, 1035, 2946, 1027, 3904, 1010, 2110, 1035, 2003, 1035, 10722, 10814, 1027, 2995, 1010, 13791, 1027, 1056, 2546, 1012, 9092, 2232, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 48, 80, 125], "sent_pos": [0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]}, {"title": "55693825", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator", "pos": [9, 15]}, {"sent_id": 1, "name": "tf.estimator", "pos": [44, 50]}], [{"sent_id": 0, "name": "tf.estimator.export", "pos": [9, 17]}, {"sent_id": 1, "name": "tf.estimator.export", "pos": [44, 52]}], [{"sent_id": 1, "name": "tf.estimator.export.build_raw_serving_input_receiver_fn", "pos": [44, 65]}], [{"sent_id": 0, "name": "tf.estimator.export.build_parsing_serving_input_receiver_fn", "pos": [9, 31]}]], "sents": ["ParseExample is used in the tf.estimator.export.build_parsing_serving_input_receiver_fn method.", "If you want to avoid it you should use tf.estimator.export.build_raw_serving_input_receiver_fn.", "Keep in mind that when you want to predict on the resulting SavedModel you should set the signature_def_key=\"predict\".", "So it will look like this \npredict_fn = predictor.from_saved_model(export_dir='tmp/...', signature_def_key=\"predict\")"], "sent_idxs": [101, 11968, 19763, 18684, 23344, 2003, 2109, 1999, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9167, 1012, 3857, 1035, 11968, 7741, 1035, 3529, 1035, 7953, 1035, 8393, 1035, 1042, 2078, 4118, 1012, 102, 101, 2065, 2017, 2215, 2000, 4468, 2009, 2017, 2323, 2224, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9167, 1012, 3857, 1035, 6315, 1035, 3529, 1035, 7953, 1035, 8393, 1035, 1042, 2078, 1012, 102, 101, 2562, 1999, 2568, 2008, 2043, 2017, 2215, 2000, 16014, 2006, 1996, 4525, 5552, 5302, 9247, 2017, 2323, 2275, 1996, 8085, 1035, 13366, 1035, 3145, 1027, 1000, 16014, 1000, 1012, 102, 101, 2061, 2009, 2097, 2298, 2066, 2023, 16014, 1035, 1042, 2078, 1027, 16014, 2953, 1012, 2013, 1035, 5552, 1035, 2944, 1006, 9167, 1035, 16101, 1027, 1005, 1056, 8737, 1013, 1012, 1012, 1012, 1005, 1010, 8085, 1035, 13366, 1035, 3145, 1027, 1000, 16014, 1000, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 34, 67, 98, 143], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41001210", "vertexSet": [[{"sent_id": 1, "name": "tf.square", "pos": [31, 35]}], [{"sent_id": 1, "name": "tf.transpose", "pos": [36, 41]}], [{"sent_id": 1, "name": "tf.reduce_mean", "pos": [24, 30]}]], "sents": ["Following modified code works.", "The main problem was loss function, which should be loss = 0.5 * tf.reduce_mean(tf.square(tf.transpose(logits) - tfTrainY))", "<code>Code Snippet</code>.", "Output with modified code."], "sent_idxs": [101, 2206, 6310, 3642, 2573, 1012, 102, 101, 1996, 2364, 3291, 2001, 3279, 3853, 1010, 2029, 2323, 2022, 3279, 1027, 1014, 1012, 1019, 1008, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1056, 2546, 1012, 2675, 1006, 1056, 2546, 1012, 9099, 20688, 1006, 8833, 12762, 1007, 1011, 1056, 6199, 21166, 2100, 1007, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 2007, 6310, 3642, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 7, 53, 67, 74], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56802190", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [3, 9]}, {"sent_id": 4, "name": "tf.contrib", "pos": [88, 94]}], [{"sent_id": 0, "name": "tf.contrib.rnn", "pos": [3, 12]}, {"sent_id": 4, "name": "tf.contrib.rnn", "pos": [88, 97]}], [{"sent_id": 0, "name": "tf.contrib.rnn.lstmcell", "pos": [3, 17]}, {"sent_id": 4, "name": "tf.contrib.rnn.lstmcell", "pos": [88, 102]}]], "sents": ["Note that tf.contrib.rnn.LSTMCell is an example of a callable class.", "That is a class that can be called like a function.", "The line you are struggling with does exactly that.", "It calls cell with the parameters in brackets.", "If you want to see what this does you can inspect the __call__ method on the class definition for tf.contrib.rnn.LSTMCell"], "sent_idxs": [101, 3602, 2008, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 1048, 3367, 12458, 5349, 2003, 2019, 2742, 1997, 1037, 2655, 3085, 2465, 1012, 102, 101, 2008, 2003, 1037, 2465, 2008, 2064, 2022, 2170, 2066, 1037, 3853, 1012, 102, 101, 1996, 2240, 2017, 2024, 8084, 2007, 2515, 3599, 2008, 1012, 102, 101, 2009, 4455, 3526, 2007, 1996, 11709, 1999, 19719, 1012, 102, 101, 2065, 2017, 2215, 2000, 2156, 2054, 2023, 2515, 2017, 2064, 22459, 1996, 1035, 1035, 2655, 1035, 1035, 4118, 2006, 1996, 2465, 6210, 2005, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 1048, 3367, 12458, 5349, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 41, 53, 64, 103], "sent_pos": [0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0]}, {"title": "49955623", "vertexSet": [[{"sent_id": 3, "name": "tf.tuple", "pos": [112, 117]}], [{"sent_id": 3, "name": "tf.group", "pos": [107, 111]}], [{"sent_id": 3, "name": "tf.control_dependencies", "pos": [118, 125]}]], "sents": ["The method 1 is the correct one because you're defining only once the gradient graph (for computing the backpropagation).", "In this way, you use a single loss function with a single graph, for doing a single update of the same parameter (the update takes into account both terms of the loss).", "The second method, instead, defines 2 different graphs for computing the gradient, and is wrong.", "When you execute the training op, you're executing in parallel (because you used tf.group / tf.tuple / tf.control_dependencies) the computation of the training operations.", "The operations will compute two different losses and two different set of updated variables.", "When the moment of updating the variables comes, you have a problem:\nwhich update operation executes first, the one defined by the first graph or the other?", "And in any case, you're discarding one computation, because one will overwrite the other.", "There's no synchronization in the update and there's no relation in the computed losses."], "sent_idxs": [101, 1996, 4118, 1015, 2003, 1996, 6149, 2028, 2138, 2017, 1005, 2128, 12854, 2069, 2320, 1996, 17978, 10629, 1006, 2005, 9798, 1996, 2067, 21572, 4502, 12540, 1007, 1012, 102, 101, 1999, 2023, 2126, 1010, 2017, 2224, 1037, 2309, 3279, 3853, 2007, 1037, 2309, 10629, 1010, 2005, 2725, 1037, 2309, 10651, 1997, 1996, 2168, 16381, 1006, 1996, 10651, 3138, 2046, 4070, 2119, 3408, 1997, 1996, 3279, 1007, 1012, 102, 101, 1996, 2117, 4118, 1010, 2612, 1010, 11859, 1016, 2367, 19287, 2005, 9798, 1996, 17978, 1010, 1998, 2003, 3308, 1012, 102, 101, 2043, 2017, 15389, 1996, 2731, 6728, 1010, 2017, 1005, 2128, 23448, 1999, 5903, 1006, 2138, 2017, 2109, 1056, 2546, 1012, 2177, 1013, 1056, 2546, 1012, 10722, 10814, 1013, 1056, 2546, 1012, 2491, 1035, 12530, 15266, 1007, 1996, 22334, 1997, 1996, 2731, 3136, 1012, 102, 101, 1996, 3136, 2097, 24134, 2048, 2367, 6409, 1998, 2048, 2367, 2275, 1997, 7172, 10857, 1012, 102, 101, 2043, 1996, 2617, 1997, 2039, 16616, 1996, 10857, 3310, 1010, 2017, 2031, 1037, 3291, 1024, 2029, 10651, 3169, 15389, 2015, 2034, 1010, 1996, 2028, 4225, 2011, 1996, 2034, 10629, 2030, 1996, 2060, 1029, 102, 101, 1998, 1999, 2151, 2553, 1010, 2017, 1005, 2128, 5860, 29154, 2028, 22334, 1010, 2138, 2028, 2097, 2058, 26373, 1996, 2060, 1012, 102, 101, 2045, 1005, 1055, 2053, 26351, 8093, 10698, 9276, 1999, 1996, 10651, 1998, 2045, 1005, 1055, 2053, 7189, 1999, 1996, 24806, 6409, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 68, 89, 134, 151, 186, 209, 233], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47916956", "vertexSet": [[{"sent_id": 5, "name": "tf.train", "pos": [160, 164]}], [{"sent_id": 6, "name": "tf.summary", "pos": [182, 186]}, {"sent_id": 7, "name": "tf.summary", "pos": [203, 207]}], [{"sent_id": 0, "name": "tf.graphkeys", "pos": [7, 13]}, {"sent_id": 2, "name": "tf.graphkeys", "pos": [60, 66]}, {"sent_id": 9, "name": "tf.graphkeys", "pos": [274, 280]}], [{"sent_id": 9, "name": "tf.get_collection", "pos": [267, 273]}, {"sent_id": 13, "name": "tf.get_collection", "pos": [358, 364]}], [{"sent_id": 4, "name": "tf.global_variables", "pos": [97, 103]}], [{"sent_id": 7, "name": "tf.summary.merge_all", "pos": [203, 211]}], [{"sent_id": 12, "name": "tf.add_to_collection", "pos": [321, 329]}], [{"sent_id": 4, "name": "tf.variables_initializer", "pos": [117, 124]}]], "sents": ["As far as I know, tf.GraphKeys is a collection of collections of keys for variables and ops in the graph.", "The usage (just as common python dictionaries) is to retrieve variables and ops.", "Given that said, here are some subsets of tf.GraphKeys I came across:", "GLOBAL_VARIABLES and LOCAL_VARIABLES contain all variables of the graph, which need to be initialized before training.", "tf.global_variables() returns the global variables in a list and can be used with tf.variables_initializer for initialization..", "Variables created with option trainable=True will be added to TRAINABLE_VARIABLES and will be fetched and updated by any optimizer under tf.train during training..", "SUMMARIES contains keys for all summaries added by tf.summary (scalar, image, histogram, text, etc).", "tf.summary.merge_all gathers all such keys and returns an op to be run and written to file so that you can visualize them on tensorboard.", ".", "Custom functions to update some variables can be added to UPDATE_OPS and separately run at each iteration using sess.run(tf.get_collection(tf.GraphKeys.UPDATE_OPS)).", "In this case, these variables are set trainable=False to avoid being updated by gradient descent.", ".", "You may create your own collections using tf.add_to_collection(some_name, var_or_op) and retrieve the variable or op later.", "You may retrieve specific variables or ops using tf.get_collection() and tweak the scope.", "."], "sent_idxs": [101, 2004, 2521, 2004, 1045, 2113, 1010, 1056, 2546, 1012, 10629, 14839, 2015, 2003, 1037, 3074, 1997, 6407, 1997, 6309, 2005, 10857, 1998, 23092, 1999, 1996, 10629, 1012, 102, 101, 1996, 8192, 1006, 2074, 2004, 2691, 18750, 4487, 7542, 12086, 1007, 2003, 2000, 12850, 10857, 1998, 23092, 1012, 102, 101, 2445, 2008, 2056, 1010, 2182, 2024, 2070, 16745, 2015, 1997, 1056, 2546, 1012, 10629, 14839, 2015, 1045, 2234, 2408, 1024, 102, 101, 3795, 1035, 10857, 1998, 2334, 1035, 10857, 5383, 2035, 10857, 1997, 1996, 10629, 1010, 2029, 2342, 2000, 2022, 3988, 3550, 2077, 2731, 1012, 102, 101, 1056, 2546, 1012, 3795, 1035, 10857, 1006, 1007, 5651, 1996, 3795, 10857, 1999, 1037, 2862, 1998, 2064, 2022, 2109, 2007, 1056, 2546, 1012, 10857, 1035, 3988, 17629, 2005, 3988, 3989, 1012, 1012, 102, 101, 10857, 2580, 2007, 5724, 3345, 3085, 1027, 2995, 2097, 2022, 2794, 2000, 3345, 3085, 1035, 10857, 1998, 2097, 2022, 18584, 2098, 1998, 7172, 2011, 2151, 23569, 27605, 6290, 2104, 1056, 2546, 1012, 3345, 2076, 2731, 1012, 1012, 102, 101, 7680, 7849, 3111, 3397, 6309, 2005, 2035, 7680, 7849, 3111, 2794, 2011, 1056, 2546, 1012, 12654, 1006, 26743, 2099, 1010, 3746, 1010, 2010, 3406, 13113, 1010, 3793, 1010, 4385, 1007, 1012, 102, 101, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 29438, 2035, 2107, 6309, 1998, 5651, 2019, 6728, 2000, 2022, 2448, 1998, 2517, 2000, 5371, 2061, 2008, 2017, 2064, 5107, 4697, 2068, 2006, 23435, 6277, 1012, 102, 101, 1012, 102, 101, 7661, 4972, 2000, 10651, 2070, 10857, 2064, 2022, 2794, 2000, 10651, 1035, 23092, 1998, 10329, 2448, 2012, 2169, 27758, 2478, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 1007, 1007, 1012, 102, 101, 1999, 2023, 2553, 1010, 2122, 10857, 2024, 2275, 3345, 3085, 1027, 6270, 2000, 4468, 2108, 7172, 2011, 17978, 6934, 1012, 102, 101, 1012, 102, 101, 2017, 2089, 3443, 2115, 2219, 6407, 2478, 1056, 2546, 1012, 5587, 1035, 2000, 1035, 3074, 1006, 2070, 1035, 2171, 1010, 13075, 1035, 2030, 1035, 6728, 1007, 1998, 12850, 1996, 8023, 2030, 6728, 2101, 1012, 102, 101, 2017, 2089, 12850, 3563, 10857, 2030, 23092, 2478, 1056, 2546, 1012, 2131, 1035, 3074, 1006, 1007, 1998, 1056, 8545, 4817, 1996, 9531, 1012, 102, 101, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 29, 49, 71, 96, 130, 169, 202, 238, 241, 288, 310, 313, 349, 374, 377], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44680140", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [25, 31]}], [{"sent_id": 1, "name": "tf.contrib.learn", "pos": [25, 33]}], [{"sent_id": 1, "name": "tf.contrib.learn.linearregressor", "pos": [25, 39]}]], "sents": ["It looks like the API that you were working with is depreciated.", "If you use a more modern tf.contrib.learn.LinearRegressor (I think >= 1.0), you are supposed to specify the input_fn, which basically produces the inputs and labels.", "I think in your example, that would be as simple as changing your run function to:", "<code>Code Snippet</code>.", "and then defining an input function called my_input_fn.", "From the docs, this input function takes the form:", "<code>Code Snippet</code>.", "I think the documentation can get you the rest of the way.", "It is difficult from here for me to say how you should proceed without seeing your data."], "sent_idxs": [101, 2009, 3504, 2066, 1996, 17928, 2008, 2017, 2020, 2551, 2007, 2003, 2139, 28139, 7405, 3064, 1012, 102, 101, 2065, 2017, 2224, 1037, 2062, 2715, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 7399, 2890, 17603, 24137, 2099, 1006, 1045, 2228, 1028, 1027, 1015, 1012, 1014, 1007, 1010, 2017, 2024, 4011, 2000, 20648, 1996, 7953, 1035, 1042, 2078, 1010, 2029, 10468, 7137, 1996, 20407, 1998, 10873, 1012, 102, 101, 1045, 2228, 1999, 2115, 2742, 1010, 2008, 2052, 2022, 2004, 3722, 2004, 5278, 2115, 2448, 3853, 2000, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2059, 12854, 2019, 7953, 3853, 2170, 2026, 1035, 7953, 1035, 1042, 2078, 1012, 102, 101, 2013, 1996, 9986, 2015, 1010, 2023, 7953, 3853, 3138, 1996, 2433, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 2228, 1996, 12653, 2064, 2131, 2017, 1996, 2717, 1997, 1996, 2126, 1012, 102, 101, 2009, 2003, 3697, 2013, 2182, 2005, 2033, 2000, 2360, 2129, 2017, 2323, 10838, 2302, 3773, 2115, 2951, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 18, 69, 89, 103, 119, 133, 147, 162, 182], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58014072", "vertexSet": [[{"sent_id": 0, "name": "tf.linalg", "pos": [8, 14]}], [{"sent_id": 0, "name": "tf.transpose", "pos": [33, 38]}], [{"sent_id": 0, "name": "tf.linalg.matmul", "pos": [8, 18]}]], "sents": ["transpose_b=True in tf.linalg.matmul transposes only the two last axes of the second given tensor, while tf.transpose, without more arguments, reverses the dimensions completely.", "The equivalent would be:", "<code>Code Snippet</code>."], "sent_idxs": [101, 9099, 20688, 1035, 1038, 1027, 2995, 1999, 1056, 2546, 1012, 27022, 2140, 2290, 1012, 13523, 12274, 2140, 9099, 20688, 2015, 2069, 1996, 2048, 2197, 19589, 1997, 1996, 2117, 2445, 23435, 1010, 2096, 1056, 2546, 1012, 9099, 20688, 1010, 2302, 2062, 9918, 1010, 7901, 2015, 1996, 9646, 3294, 1012, 102, 101, 1996, 5662, 2052, 2022, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 50, 57, 71], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35871863", "vertexSet": [[{"sent_id": 5, "name": "tf.slice", "pos": [221, 225]}], [{"sent_id": 0, "name": "tf.image", "pos": [2, 6]}, {"sent_id": 4, "name": "tf.image", "pos": [136, 140]}, {"sent_id": 5, "name": "tf.image", "pos": [201, 205]}], [{"sent_id": 0, "name": "tf.image.resize_images", "pos": [2, 11]}], [{"sent_id": 4, "name": "tf.image.crop_to_bounding_box", "pos": [136, 149]}, {"sent_id": 5, "name": "tf.image.crop_to_bounding_box", "pos": [201, 214]}]], "sents": ["The tf.image.resize_images() op does set the image shape, on this line of the implementation.", "(This was added in TensorFlow 0.7.)", "However, if either of the new_height or new_width arguments is a dynamic value, then TensorFlow cannot infer a single shape for that dimension, and so uses None for that dimension.", "I notice in your code that the new height and width values are called random_scale: if a new random value is drawn on each step, then the shape will have None for the height and width dimensions.", "Note that in this case, the tf.image.crop_to_bounding_box() op will not work because\u2014as the error message indicates\u2014the current implementation requires that the shape of the input be fully defined.", "As I noted in a recent answer, the best workaround might be to use the lower level ops from which tf.image.crop_to_bounding_box() is implemented (in particular tf.slice() with computed indices)."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 1006, 1007, 6728, 2515, 2275, 1996, 3746, 4338, 1010, 2006, 2023, 2240, 1997, 1996, 7375, 1012, 102, 101, 1006, 2023, 2001, 2794, 1999, 23435, 12314, 1014, 1012, 1021, 1012, 1007, 102, 101, 2174, 1010, 2065, 2593, 1997, 1996, 2047, 1035, 4578, 2030, 2047, 1035, 9381, 9918, 2003, 1037, 8790, 3643, 1010, 2059, 23435, 12314, 3685, 1999, 7512, 1037, 2309, 4338, 2005, 2008, 9812, 1010, 1998, 2061, 3594, 3904, 2005, 2008, 9812, 1012, 102, 101, 1045, 5060, 1999, 2115, 3642, 2008, 1996, 2047, 4578, 1998, 9381, 5300, 2024, 2170, 6721, 1035, 4094, 1024, 2065, 1037, 2047, 6721, 3643, 2003, 4567, 2006, 2169, 3357, 1010, 2059, 1996, 4338, 2097, 2031, 3904, 2005, 1996, 4578, 1998, 9381, 9646, 1012, 102, 101, 3602, 2008, 1999, 2023, 2553, 1010, 1996, 1056, 2546, 1012, 3746, 1012, 10416, 1035, 2000, 1035, 5391, 2075, 1035, 3482, 1006, 1007, 6728, 2097, 2025, 2147, 2138, 1517, 2004, 1996, 7561, 4471, 7127, 1517, 1996, 2783, 7375, 5942, 2008, 1996, 4338, 1997, 1996, 7953, 2022, 3929, 4225, 1012, 102, 101, 2004, 1045, 3264, 1999, 1037, 3522, 3437, 1010, 1996, 2190, 2147, 24490, 2453, 2022, 2000, 2224, 1996, 2896, 2504, 23092, 2013, 2029, 1056, 2546, 1012, 3746, 1012, 10416, 1035, 2000, 1035, 5391, 2075, 1035, 3482, 1006, 1007, 2003, 7528, 1006, 1999, 3327, 1056, 2546, 1012, 14704, 1006, 1007, 2007, 24806, 29299, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 28, 42, 84, 128, 178, 233], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56091622", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [26, 30]}, {"sent_id": 1, "name": "tf.data", "pos": [40, 44]}], [{"sent_id": 1, "name": "tf.tensor", "pos": [74, 78]}], [{"sent_id": 12, "name": "tf.function", "pos": [285, 289]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [26, 33]}, {"sent_id": 1, "name": "tf.data.dataset", "pos": [40, 47]}]], "sents": ["You can't use the .numpy method on a tensor, if this tensor is going to be used in a tf.data.Dataset.map call.", "The tf.data.Dataset object under the hood works by creating a static graph: this means that you can't use .numpy() because the tf.Tensor object when in a static-graph context do not have this attribute.", "Therefore, the line input_image = random_noise(image.numpy()) should be input_image = random_noise(image).", "But the code is likely to fail again since random_noise calls get_noise from the model.utils package.", "If the get_noise function is written using Tensorflow, then everything will work.", "Otherwise, it won't work.", "The solution?", "Write the code using only the Tensorflow primitives.", "For instance, if your function get_noise just creates random noise with the shee of your input image, you can define it like:", "<code>Code Snippet</code>.", "using only the Tensorflow primitives, and it will work.", "Hope this overview helps!", "P.S: you could be interested in having a look at the articles \"Analyzing tf.function to discover AutoGraph strengths and subtleties\" - they cover this aspect (perhaps part 3 is the one related to your scenario): part 1 part 2 part 3"], "sent_idxs": [101, 2017, 2064, 1005, 1056, 2224, 1996, 1012, 16371, 8737, 2100, 4118, 2006, 1037, 23435, 1010, 2065, 2023, 23435, 2003, 2183, 2000, 2022, 2109, 1999, 1037, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 4949, 2655, 1012, 102, 101, 1996, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 4874, 2104, 1996, 7415, 2573, 2011, 4526, 1037, 10763, 10629, 1024, 2023, 2965, 2008, 2017, 2064, 1005, 1056, 2224, 1012, 16371, 8737, 2100, 1006, 1007, 2138, 1996, 1056, 2546, 1012, 23435, 4874, 2043, 1999, 1037, 10763, 1011, 10629, 6123, 2079, 2025, 2031, 2023, 17961, 1012, 102, 101, 3568, 1010, 1996, 2240, 7953, 1035, 3746, 1027, 6721, 1035, 5005, 1006, 3746, 1012, 16371, 8737, 2100, 1006, 1007, 1007, 2323, 2022, 7953, 1035, 3746, 1027, 6721, 1035, 5005, 1006, 3746, 1007, 1012, 102, 101, 2021, 1996, 3642, 2003, 3497, 2000, 8246, 2153, 2144, 6721, 1035, 5005, 4455, 2131, 1035, 5005, 2013, 1996, 2944, 1012, 21183, 12146, 7427, 1012, 102, 101, 2065, 1996, 2131, 1035, 5005, 3853, 2003, 2517, 2478, 23435, 12314, 1010, 2059, 2673, 2097, 2147, 1012, 102, 101, 4728, 1010, 2009, 2180, 1005, 1056, 2147, 1012, 102, 101, 1996, 5576, 1029, 102, 101, 4339, 1996, 3642, 2478, 2069, 1996, 23435, 12314, 10968, 2015, 1012, 102, 101, 2005, 6013, 1010, 2065, 2115, 3853, 2131, 1035, 5005, 2074, 9005, 6721, 5005, 2007, 1996, 2016, 2063, 1997, 2115, 7953, 3746, 1010, 2017, 2064, 9375, 2009, 2066, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2478, 2069, 1996, 23435, 12314, 10968, 2015, 1010, 1998, 2009, 2097, 2147, 1012, 102, 101, 3246, 2023, 19184, 7126, 999, 102, 101, 1052, 1012, 1055, 1024, 2017, 2071, 2022, 4699, 1999, 2383, 1037, 2298, 2012, 1996, 4790, 1000, 20253, 1056, 2546, 1012, 3853, 2000, 7523, 8285, 14413, 20828, 1998, 11259, 7368, 1000, 1011, 2027, 3104, 2023, 7814, 1006, 3383, 2112, 1017, 2003, 1996, 2028, 3141, 2000, 2115, 11967, 1007, 1024, 2112, 1015, 2112, 1016, 2112, 1017, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 38, 93, 128, 154, 173, 183, 188, 201, 231, 245, 260, 267, 323], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48058050", "vertexSet": [[{"sent_id": 3, "name": "tf.image", "pos": [65, 69]}], [{"sent_id": 4, "name": "tf.map_fn", "pos": [92, 99]}], [{"sent_id": 3, "name": "tf.image.decode_jpeg", "pos": [65, 75]}]], "sents": ["Depending on how your model is designed, you can just feed an array of images to pl.", "The first dimension of your outputs then corresponds to the index of your image in the batch.", "Many tensor ops have an implementation for multiple examples in a batch.", "There are some exceptions though, for example tf.image.decode_jpeg.", "In this case, you will have to rewrite your network, using tf.map_fn, for example."], "sent_idxs": [101, 5834, 2006, 2129, 2115, 2944, 2003, 2881, 1010, 2017, 2064, 2074, 5438, 2019, 9140, 1997, 4871, 2000, 20228, 1012, 102, 101, 1996, 2034, 9812, 1997, 2115, 27852, 2059, 14788, 2000, 1996, 5950, 1997, 2115, 3746, 1999, 1996, 14108, 1012, 102, 101, 2116, 23435, 23092, 2031, 2019, 7375, 2005, 3674, 4973, 1999, 1037, 14108, 1012, 102, 101, 2045, 2024, 2070, 11790, 2295, 1010, 2005, 2742, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 1012, 102, 101, 1999, 2023, 2553, 1010, 2017, 2097, 2031, 2000, 2128, 26373, 2115, 2897, 1010, 2478, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1010, 2005, 2742, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 41, 56, 77, 104], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0]}, {"title": "62939494", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [8, 13]}], [{"sent_id": 0, "name": "tf.keras.layers", "pos": [8, 15]}], [{"sent_id": 0, "name": "tf.keras.layers.upsampling2d", "pos": [8, 21]}]], "sents": ["Your other option would be to use tf.keras.layers.UpSampling2D for your purpose, but that doesn't learn a kernel to upsample (it uses bilinear upsampling).", "So, your approach is correct.", "But, you have used kernel_size as 3x3.", "It should be 2x2 and if you are not satisfied with the results, you should increase the number of filters from [32, 256]."], "sent_idxs": [101, 2115, 2060, 5724, 2052, 2022, 2000, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 11139, 16613, 2989, 2475, 2094, 2005, 2115, 3800, 1010, 2021, 2008, 2987, 1005, 1056, 4553, 1037, 16293, 2000, 11139, 16613, 2571, 1006, 2009, 3594, 12170, 4179, 2906, 11139, 16613, 2989, 1007, 1012, 102, 101, 2061, 1010, 2115, 3921, 2003, 6149, 1012, 102, 101, 2021, 1010, 2017, 2031, 2109, 16293, 1035, 2946, 2004, 1017, 2595, 2509, 1012, 102, 101, 2009, 2323, 2022, 1016, 2595, 2475, 1998, 2065, 2017, 2024, 2025, 8510, 2007, 1996, 3463, 1010, 2017, 2323, 3623, 1996, 2193, 1997, 17736, 2013, 1031, 3590, 1010, 17273, 1033, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 49, 58, 73, 105], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55536414", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [50, 55]}, {"sent_id": 3, "name": "tf.keras", "pos": [88, 93]}], [{"sent_id": 0, "name": "tf.layers", "pos": [5, 9]}, {"sent_id": 2, "name": "tf.layers", "pos": [43, 47]}], [{"sent_id": 5, "name": "tf.session", "pos": [165, 169]}], [{"sent_id": 2, "name": "tf.keras.backend", "pos": [50, 58]}, {"sent_id": 3, "name": "tf.keras.backend", "pos": [88, 96]}], [{"sent_id": 4, "name": "tf.interactivesession", "pos": [141, 147]}], [{"sent_id": 2, "name": "tf.keras.backend.get_session", "pos": [50, 62]}, {"sent_id": 3, "name": "tf.keras.backend.get_session", "pos": [88, 100]}]], "sents": ["The problem is that tf.layers is not using uses is not using your session sess.", "This in turn results in different initializations for the weights, hence the two different values.", "tf.layers ends up using tf.keras.backend.get_session() to retrieve the session used for initialization and retrieval of weights (node.get_weights()).", "tf.keras.backend.get_session() tries to use the default session if there is one, and if there is not then it creates its own session.", "In this case, sess is not configured as default session (only tf.InteractiveSession gets automatically configured as default session on construction).", "The simplest fix is to use tf.Session in the recommended way, as a context manager:", "<code>Code Snippet</code>.", "This will set sess as default session, and also it will make sure its resources are freed when the function is finished (which was another issue in your code).", "If for whatever reason you want to use some session as default but do not want to close it with the context manager, you can just use as_default():", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3291, 2003, 2008, 1056, 2546, 1012, 9014, 2003, 2025, 2478, 3594, 2003, 2025, 2478, 2115, 5219, 7367, 4757, 1012, 102, 101, 2023, 1999, 2735, 3463, 1999, 2367, 3988, 22318, 2005, 1996, 15871, 1010, 6516, 1996, 2048, 2367, 5300, 1012, 102, 101, 1056, 2546, 1012, 9014, 4515, 2039, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 2131, 1035, 5219, 1006, 1007, 2000, 12850, 1996, 5219, 2109, 2005, 3988, 3989, 1998, 26384, 1997, 15871, 1006, 13045, 1012, 2131, 1035, 15871, 1006, 1007, 1007, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 2131, 1035, 5219, 1006, 1007, 5363, 2000, 2224, 1996, 12398, 5219, 2065, 2045, 2003, 2028, 1010, 1998, 2065, 2045, 2003, 2025, 2059, 2009, 9005, 2049, 2219, 5219, 1012, 102, 101, 1999, 2023, 2553, 1010, 7367, 4757, 2003, 2025, 26928, 2004, 12398, 5219, 1006, 2069, 1056, 2546, 1012, 9123, 8583, 10992, 4152, 8073, 26928, 2004, 12398, 5219, 2006, 2810, 1007, 1012, 102, 101, 1996, 21304, 8081, 2003, 2000, 2224, 1056, 2546, 1012, 5219, 1999, 1996, 6749, 2126, 1010, 2004, 1037, 6123, 3208, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2023, 2097, 2275, 7367, 4757, 2004, 12398, 5219, 1010, 1998, 2036, 2009, 2097, 2191, 2469, 2049, 4219, 2024, 10650, 2043, 1996, 3853, 2003, 2736, 1006, 2029, 2001, 2178, 3277, 1999, 2115, 3642, 1007, 1012, 102, 101, 2065, 2005, 3649, 3114, 2017, 2215, 2000, 2224, 2070, 5219, 2004, 12398, 2021, 2079, 2025, 2215, 2000, 2485, 2009, 2007, 1996, 6123, 3208, 1010, 2017, 2064, 2074, 2224, 2004, 1035, 12398, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 22, 42, 87, 126, 158, 180, 194, 230, 266, 280], "sent_pos": [0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45020759", "vertexSet": [[{"sent_id": 4, "name": "tf.metrics", "pos": [114, 119]}], [{"sent_id": 4, "name": "tf.metrics.accuracy", "pos": [114, 121]}], [{"sent_id": 2, "name": "tf.initialize_all_variables", "pos": [53, 62]}], [{"sent_id": 6, "name": "tf.local_variables_initializer", "pos": [177, 186]}]], "sents": ["TL;DR: Add the following line at the beginning of your session:", "<code>Code Snippet</code>.", "The confusion arises from the name of the (as frankyjuang points out) deprecated tf.initialize_all_variables() function.", "This function was deprecated in part because it is misnamed: it doesn't actually initialize all variables, and instead it only initializes global (not local) variables.", "According to the documentation for the tf.metrics.accuracy() function (emphasis added):", "The accuracy function creates two local variables, total and count that are used to compute the frequency with which predictions matches labels.", "Therefore you need to add an explicit initialization step for the local variables, which can be done using tf.local_variables_initializer(), as suggested above."], "sent_idxs": [101, 1056, 2140, 1025, 2852, 1024, 5587, 1996, 2206, 2240, 2012, 1996, 2927, 1997, 2115, 5219, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 6724, 18653, 2013, 1996, 2171, 1997, 1996, 1006, 2004, 3581, 2100, 9103, 5654, 2685, 2041, 1007, 2139, 28139, 12921, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 3853, 1012, 102, 101, 2023, 3853, 2001, 2139, 28139, 12921, 1999, 2112, 2138, 2009, 2003, 28616, 18442, 2094, 1024, 2009, 2987, 1005, 1056, 2941, 3988, 4697, 2035, 10857, 1010, 1998, 2612, 2009, 2069, 3988, 10057, 3795, 1006, 2025, 2334, 1007, 10857, 1012, 102, 101, 2429, 2000, 1996, 12653, 2005, 1996, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 1006, 1007, 3853, 1006, 7902, 2794, 1007, 1024, 102, 101, 1996, 10640, 3853, 9005, 2048, 2334, 10857, 1010, 2561, 1998, 4175, 2008, 2024, 2109, 2000, 24134, 1996, 6075, 2007, 2029, 20932, 3503, 10873, 1012, 102, 101, 3568, 2017, 2342, 2000, 5587, 2019, 13216, 3988, 3989, 3357, 2005, 1996, 2334, 10857, 1010, 2029, 2064, 2022, 2589, 2478, 1056, 2546, 1012, 2334, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1010, 2004, 4081, 2682, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 18, 32, 67, 107, 130, 156, 194], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56823828", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.model.build", "pos": [6, 15]}], [{"sent_id": 1, "name": "tf.keras.model.fit", "pos": [37, 46]}, {"sent_id": 1, "name": "tf.keras.model.fit", "pos": [47, 56]}], [{"sent_id": 1, "name": "tf.keras.model.fit.call", "pos": [47, 58]}]], "sents": ["You need to call the tf.keras.Model.build method before you try to save a subclassed model weights.", "An alternative to this would be calling tf.keras.Model.fit or tf.keras.Model.fit.call on some inputs before you try to save your model weights.", "This same applies to load weights into a newly created instance of your subclassed model.", "you need to call one of the above-mentioned methods before you try to load your weights.", "Here is an example showing both saving and loading weights for a subclassed model", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2342, 2000, 2655, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1012, 3857, 4118, 2077, 2017, 3046, 2000, 3828, 1037, 4942, 26266, 2098, 2944, 15871, 1012, 102, 101, 2019, 4522, 2000, 2023, 2052, 2022, 4214, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1012, 4906, 2030, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1012, 4906, 1012, 2655, 2006, 2070, 20407, 2077, 2017, 3046, 2000, 3828, 2115, 2944, 15871, 1012, 102, 101, 2023, 2168, 12033, 2000, 7170, 15871, 2046, 1037, 4397, 2580, 6013, 1997, 2115, 4942, 26266, 2098, 2944, 1012, 102, 101, 2017, 2342, 2000, 2655, 2028, 1997, 1996, 2682, 1011, 3855, 4725, 2077, 2017, 3046, 2000, 7170, 2115, 15871, 1012, 102, 101, 2182, 2003, 2019, 2742, 4760, 2119, 7494, 1998, 10578, 15871, 2005, 1037, 4942, 26266, 2098, 2944, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 0, "evidence": [0, 1]}], "na_triple": [[1, 2], [2, 1]], "sent_ends": [0, 29, 71, 91, 112, 130, 144], "sent_pos": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47315820", "vertexSet": [[{"sent_id": 9, "name": "tf.nn", "pos": [202, 207]}], [{"sent_id": 3, "name": "tf.train", "pos": [53, 57]}], [{"sent_id": 3, "name": "tf.train.slice_input_producer", "pos": [53, 63]}], [{"sent_id": 9, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [202, 219]}]], "sents": ["There are several things you need to consider to preserve the ordering of images and labels.", "let's say we need a function that gives us images and labels.", "<code>Code Snippet</code>.", "Here,tf.train.slice_input_producer([_img_names,_img_class,index],shuffle=False) is an interesting thing to look at where if you put shuffle=True it will shuffle all three arrays in coordination.", "Second thing is, num_preprocess_threads.", "As long as you are using single threads for dequeue operation, batches will come out in a deterministic way.", "But more than one threads will shuffle the arrays randomly.", "for example for image 0001.jpg if True label is 1 you might get 2 or 4.", "Once its dequeue it is in tensor form.", "tf.nn.softmax_cross_entropy_with_logits shouldn't have problem with such tensors."], "sent_idxs": [101, 2045, 2024, 2195, 2477, 2017, 2342, 2000, 5136, 2000, 7969, 1996, 13063, 1997, 4871, 1998, 10873, 1012, 102, 101, 2292, 1005, 1055, 2360, 2057, 2342, 1037, 3853, 2008, 3957, 2149, 4871, 1998, 10873, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2182, 1010, 1056, 2546, 1012, 3345, 1012, 14704, 1035, 7953, 1035, 3135, 1006, 1031, 1035, 10047, 2290, 1035, 3415, 1010, 1035, 10047, 2290, 1035, 2465, 1010, 5950, 1033, 1010, 23046, 1027, 6270, 1007, 2003, 2019, 5875, 2518, 2000, 2298, 2012, 2073, 2065, 2017, 2404, 23046, 1027, 2995, 2009, 2097, 23046, 2035, 2093, 27448, 1999, 12016, 1012, 102, 101, 2117, 2518, 2003, 1010, 16371, 2213, 1035, 17463, 3217, 9623, 2015, 1035, 16457, 1012, 102, 101, 2004, 2146, 2004, 2017, 2024, 2478, 2309, 16457, 2005, 2139, 4226, 5657, 3169, 1010, 14108, 2229, 2097, 2272, 2041, 1999, 1037, 28283, 25300, 10074, 2126, 1012, 102, 101, 2021, 2062, 2084, 2028, 16457, 2097, 23046, 1996, 27448, 18154, 1012, 102, 101, 2005, 2742, 2005, 3746, 2199, 2487, 1012, 16545, 2290, 2065, 2995, 3830, 2003, 1015, 2017, 2453, 2131, 1016, 2030, 1018, 1012, 102, 101, 2320, 2049, 2139, 4226, 5657, 2009, 2003, 1999, 23435, 2433, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 5807, 1005, 1056, 2031, 3291, 2007, 2107, 23435, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 19, 36, 50, 108, 124, 152, 165, 188, 201, 230], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46424669", "vertexSet": [[{"sent_id": 4, "name": "tf.placeholder", "pos": [148, 153]}], [{"sent_id": 2, "name": "tf.feature_column", "pos": [75, 81]}], [{"sent_id": 2, "name": "tf.feature_column.indicator_column", "pos": [75, 85]}]], "sents": ["Based on what I can glean from the documentation on feature columns, it seems they are used to convert some sort of input data feature into continuous variables that can be used by a regression or neural network model.", "For instance, in regression, if we have a categorical variable, it is common to convert this to a set of dummy variables first.", "tf.feature_column.indicator_column could be used to make this conversion for us.", "Then we could just feed categorical data in our feed dict, and the conversion to dummy variables would happen internally.", "In the case of a numeric_column, there is no such conversion needed, so the class basically just acts like a tf.placeholder."], "sent_idxs": [101, 2241, 2006, 2054, 1045, 2064, 1043, 20898, 2013, 1996, 12653, 2006, 3444, 7753, 1010, 2009, 3849, 2027, 2024, 2109, 2000, 10463, 2070, 4066, 1997, 7953, 2951, 3444, 2046, 7142, 10857, 2008, 2064, 2022, 2109, 2011, 1037, 26237, 2030, 15756, 2897, 2944, 1012, 102, 101, 2005, 6013, 1010, 1999, 26237, 1010, 2065, 2057, 2031, 1037, 4937, 27203, 8023, 1010, 2009, 2003, 2691, 2000, 10463, 2023, 2000, 1037, 2275, 1997, 24369, 10857, 2034, 1012, 102, 101, 1056, 2546, 1012, 3444, 1035, 5930, 1012, 17245, 1035, 5930, 2071, 2022, 2109, 2000, 2191, 2023, 7584, 2005, 2149, 1012, 102, 101, 2059, 2057, 2071, 2074, 5438, 4937, 27203, 2951, 1999, 2256, 5438, 4487, 6593, 1010, 1998, 1996, 7584, 2000, 24369, 10857, 2052, 4148, 16058, 1012, 102, 101, 1999, 1996, 2553, 1997, 1037, 16371, 25531, 1035, 5930, 1010, 2045, 2003, 2053, 2107, 7584, 2734, 1010, 2061, 1996, 2465, 10468, 2074, 4490, 2066, 1037, 1056, 2546, 1012, 2173, 14528, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 44, 74, 96, 122, 155], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]}, {"title": "43782711", "vertexSet": [[{"sent_id": 4, "name": "tf.layers", "pos": [114, 118]}], [{"sent_id": 5, "name": "tf.graphkeys", "pos": [150, 156]}], [{"sent_id": 4, "name": "tf.layers.batch_normalization", "pos": [114, 123]}]], "sents": ["I am not sure if this will solve your problem, the documentation for BatchNorm is not quite easy-to-use/informative, so here is a short recap on how to use simple BatchNorm:", "First of all, you define your BatchNorm layer.", "If you want to use it after an affine/fully-connected layer, you do this (just an example, order can be different/as you desire):", "<code>Code Snippet</code>.", "The function tf.layers.batch_normalization calls variable-initializers.", "These are internal-variables and need a special scope to be called, which is in the tf.GraphKeys.UPDATE_OPS.", "As such, you must call your optimizer function as follows (after all layers have been defined!", "):", "<code>Code Snippet</code>.", "You can read more about it here.", "I know it's a little late to answer your question, but it might help other people coming across BatchNorm problems in tensorflow!", ":)"], "sent_idxs": [101, 1045, 2572, 2025, 2469, 2065, 2023, 2097, 9611, 2115, 3291, 1010, 1996, 12653, 2005, 14108, 12131, 2213, 2003, 2025, 3243, 3733, 1011, 2000, 1011, 2224, 1013, 12367, 8082, 1010, 2061, 2182, 2003, 1037, 2460, 28667, 9331, 2006, 2129, 2000, 2224, 3722, 14108, 12131, 2213, 1024, 102, 101, 2034, 1997, 2035, 1010, 2017, 9375, 2115, 14108, 12131, 2213, 6741, 1012, 102, 101, 2065, 2017, 2215, 2000, 2224, 2009, 2044, 2019, 21358, 23460, 1013, 3929, 1011, 4198, 6741, 1010, 2017, 2079, 2023, 1006, 2074, 2019, 2742, 1010, 2344, 2064, 2022, 2367, 1013, 2004, 2017, 4792, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3853, 1056, 2546, 1012, 9014, 1012, 14108, 1035, 3671, 3989, 4455, 8023, 1011, 3988, 17629, 2015, 1012, 102, 101, 2122, 2024, 4722, 1011, 10857, 1998, 2342, 1037, 2569, 9531, 2000, 2022, 2170, 1010, 2029, 2003, 1999, 1996, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 10651, 1035, 23092, 1012, 102, 101, 2004, 2107, 1010, 2017, 2442, 2655, 2115, 23569, 27605, 6290, 3853, 2004, 4076, 1006, 2044, 2035, 9014, 2031, 2042, 4225, 999, 102, 101, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 3191, 2062, 2055, 2009, 2182, 1012, 102, 101, 1045, 2113, 2009, 1005, 1055, 1037, 2210, 2397, 2000, 3437, 2115, 3160, 1010, 2021, 2009, 2453, 2393, 2060, 2111, 2746, 2408, 14108, 12131, 2213, 3471, 1999, 23435, 12314, 999, 102, 101, 1024, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 47, 61, 97, 111, 131, 162, 185, 189, 203, 213, 244, 248], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59308551", "vertexSet": [[{"sent_id": 5, "name": "tf.cond", "pos": [104, 109]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [12, 16]}], [{"sent_id": 5, "name": "tf.function", "pos": [114, 118]}]], "sents": ["As the error message explain, you try to use a tf.Tensor as a Python bool.", "This happens generally where condition are expected like in:", "<code>Code Snippet</code>.", "The part layer.output in self.keras_model.losses should evaluate to a tensor that Python try to use as a bool to check the if condition.", "This is allowed in eager execution only.", "You must either convert the if construct with tf.cond, or rely on @tf.function to make the job for you.", "Without more code, it is hard to help you more..."], "sent_idxs": [101, 2004, 1996, 7561, 4471, 4863, 1010, 2017, 3046, 2000, 2224, 1037, 1056, 2546, 1012, 23435, 2004, 1037, 18750, 22017, 2140, 1012, 102, 101, 2023, 6433, 3227, 2073, 4650, 2024, 3517, 2066, 1999, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2112, 6741, 1012, 6434, 1999, 2969, 1012, 17710, 8180, 1035, 2944, 1012, 6409, 2323, 16157, 2000, 1037, 23435, 2008, 18750, 3046, 2000, 2224, 2004, 1037, 22017, 2140, 2000, 4638, 1996, 2065, 4650, 1012, 102, 101, 2023, 2003, 3039, 1999, 9461, 7781, 2069, 1012, 102, 101, 2017, 2442, 2593, 10463, 1996, 2065, 9570, 2007, 1056, 2546, 1012, 9530, 2094, 1010, 2030, 11160, 2006, 1030, 1056, 2546, 1012, 3853, 2000, 2191, 1996, 3105, 2005, 2017, 1012, 102, 101, 2302, 2062, 3642, 1010, 2009, 2003, 2524, 2000, 2393, 2017, 2062, 1012, 1012, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 23, 35, 49, 85, 95, 126, 142], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56618036", "vertexSet": [[{"sent_id": 1, "name": "tf.strings", "pos": [51, 55]}, {"sent_id": 1, "name": "tf.strings", "pos": [63, 67]}], [{"sent_id": 1, "name": "tf.strings.split", "pos": [63, 69]}], [{"sent_id": 1, "name": "tf.strings.regex_replace", "pos": [51, 60]}]], "sents": ["Slightly tricky because TensorFlow (at least to my knowledge) doesn't have a regex split function.", "If there is a character that you can be sure your input strings won't contain you could do a slightly messy workaround using tf.strings.regex_replace() and tf.strings.split().", "We first use regex_replace in order to replace the match with our special character then use split to split on the special character", "For example, if we could be sure our input strings would never contain the char | then we could proceed as follows:", "<code>Code Snippet</code>.", "so that, split(\"http://www.bbc.co.uk\") say, gives us:", "<code>Code Snippet</code>."], "sent_idxs": [101, 3621, 24026, 2138, 23435, 12314, 1006, 2012, 2560, 2000, 2026, 3716, 1007, 2987, 1005, 1056, 2031, 1037, 19723, 10288, 3975, 3853, 1012, 102, 101, 2065, 2045, 2003, 1037, 2839, 2008, 2017, 2064, 2022, 2469, 2115, 7953, 7817, 2180, 1005, 1056, 5383, 2017, 2071, 2079, 1037, 3621, 18307, 2147, 24490, 2478, 1056, 2546, 1012, 7817, 1012, 19723, 10288, 1035, 5672, 1006, 1007, 1998, 1056, 2546, 1012, 7817, 1012, 3975, 1006, 1007, 1012, 102, 101, 2057, 2034, 2224, 19723, 10288, 1035, 5672, 1999, 2344, 2000, 5672, 1996, 2674, 2007, 2256, 2569, 2839, 2059, 2224, 3975, 2000, 3975, 2006, 1996, 2569, 2839, 102, 101, 2005, 2742, 1010, 2065, 2057, 2071, 2022, 2469, 2256, 7953, 7817, 2052, 2196, 5383, 1996, 25869, 1064, 2059, 2057, 2071, 10838, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2061, 2008, 1010, 3975, 1006, 1000, 8299, 1024, 1013, 1013, 7479, 1012, 4035, 1012, 2522, 1012, 2866, 1000, 1007, 2360, 1010, 3957, 2149, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 73, 101, 127, 141, 167, 181], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58533519", "vertexSet": [[{"sent_id": 0, "name": "tf.nn.conv2d", "pos": [1, 11]}], [{"sent_id": 0, "name": "tf.keras.layers.conv2d", "pos": [15, 27]}]], "sents": ["tf.nn.conv2d is functional api and tf.keras.layers.Conv2D is layer-class api.", "You should use the latter one.", "It's quite as similar as the relationship between torch.nn.functional.conv2d and torch.nn.Conv2D.", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 2003, 8360, 17928, 1998, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2003, 6741, 1011, 2465, 17928, 1012, 102, 101, 2017, 2323, 2224, 1996, 3732, 2028, 1012, 102, 101, 2009, 1005, 1055, 3243, 2004, 2714, 2004, 1996, 3276, 2090, 12723, 1012, 1050, 2078, 1012, 8360, 1012, 9530, 2615, 2475, 2094, 1998, 12723, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2]}], "na_triple": [], "sent_ends": [0, 34, 43, 77, 91], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47442005", "vertexSet": [[{"sent_id": 2, "name": "tf.nn", "pos": [89, 94]}], [{"sent_id": 3, "name": "tf.device", "pos": [142, 146]}], [{"sent_id": 3, "name": "tf.reduce_mean", "pos": [128, 134]}], [{"sent_id": 2, "name": "tf.nn.sampled_softmax_loss", "pos": [89, 101]}]], "sents": ["The error is raised because AdagradOptimizer does not have a GPU kernel for its sparse apply operation; a sparse apply is triggered because differentiating through the embedding lookup results in a sparse gradient.", "GradientDescentOptimizer and AdamOptimizer do support sparse apply operations.", "If you were to switch to one of these optimizers, you would unfortunately see another error: tf.nn.sampled_softmax_loss appears to create an op that does not have a GPU kernel.", "To get around that, you could wrap the loss = tf.reduce_mean(... line with a with tf.device('/cpu:0'): context, though doing so would introduce cpu-gpu communication overhead."], "sent_idxs": [101, 1996, 7561, 2003, 2992, 2138, 15262, 16307, 7361, 3775, 4328, 6290, 2515, 2025, 2031, 1037, 14246, 2226, 16293, 2005, 2049, 20288, 6611, 3169, 1025, 1037, 20288, 6611, 2003, 13330, 2138, 2367, 15370, 2083, 1996, 7861, 8270, 4667, 2298, 6279, 3463, 1999, 1037, 20288, 17978, 1012, 102, 101, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1998, 4205, 7361, 3775, 4328, 6290, 2079, 2490, 20288, 6611, 3136, 1012, 102, 101, 2065, 2017, 2020, 2000, 6942, 2000, 2028, 1997, 2122, 23569, 27605, 16750, 1010, 2017, 2052, 6854, 2156, 2178, 7561, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 18925, 1035, 3730, 17848, 1035, 3279, 3544, 2000, 3443, 2019, 6728, 2008, 2515, 2025, 2031, 1037, 14246, 2226, 16293, 1012, 102, 101, 2000, 2131, 2105, 2008, 1010, 2017, 2071, 10236, 1996, 3279, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1012, 1012, 1012, 2240, 2007, 1037, 2007, 1056, 2546, 1012, 5080, 1006, 1005, 1013, 17368, 1024, 1014, 1005, 1007, 1024, 6123, 1010, 2295, 2725, 2061, 2052, 8970, 17368, 1011, 14246, 2226, 4807, 8964, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 47, 68, 116, 170], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57776691", "vertexSet": [[{"sent_id": 1, "name": "tf.train.example", "pos": [31, 37]}], [{"sent_id": 3, "name": "tf.io.parse_example", "pos": [85, 94]}], [{"sent_id": 7, "name": "tf.data", "pos": [191, 195]}], [{"sent_id": 7, "name": "tf.data.dataset", "pos": [191, 198]}]], "sents": ["It looks like your serialized_tf_example placeholder has shape=[] which is a single example.", "You should pass a single tf.train.Example, serialized as a string:", "<code>Code Snippet</code>.", "If you want to feed a batch of examples instead of a single serialized example, you need to use shape=[None] and tf.io.parse_example instead.", "Additionally, the example that you feed should have features defined on it that you reference in your features dictionary (e.g.", "var1) so that they can be properly parsed.", "A serving function specifies how your model should receive its' input at prediction time - when you export your trained TensorFlow graph / model as a SavedModel.", "Instead of having an active TF session like in training where you normally use placeholder or tf.data.Dataset and requires stateful python / TF runtime execution you want your SavedModel to be able to be serialized and written to disk so that you can deploy with TF Serving or run it on a mobile device, etc.", "So you export this SavedModel - the serving input is the piece inside that defines how to parse the requests it gets sent and then connect that into your model The way you probably want to send them is as tf.train.Examples protocol buffers (serialized as strings so that can be sent in an RPC).", "And then you parse them into a feature dict so your estimator can understand them in the same way it understood your training data."], "sent_idxs": [101, 2009, 3504, 2066, 2115, 27289, 1035, 1056, 2546, 1035, 2742, 2173, 14528, 2038, 4338, 1027, 1031, 1033, 2029, 2003, 1037, 2309, 2742, 1012, 102, 101, 2017, 2323, 3413, 1037, 2309, 1056, 2546, 1012, 3345, 1012, 2742, 1010, 27289, 2004, 1037, 5164, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 2000, 5438, 1037, 14108, 1997, 4973, 2612, 1997, 1037, 2309, 27289, 2742, 1010, 2017, 2342, 2000, 2224, 4338, 1027, 1031, 3904, 1033, 1998, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 2742, 2612, 1012, 102, 101, 5678, 1010, 1996, 2742, 2008, 2017, 5438, 2323, 2031, 2838, 4225, 2006, 2009, 2008, 2017, 4431, 1999, 2115, 2838, 9206, 1006, 1041, 1012, 1043, 1012, 102, 101, 13075, 2487, 1007, 2061, 2008, 2027, 2064, 2022, 7919, 11968, 6924, 1012, 102, 101, 1037, 3529, 3853, 27171, 2129, 2115, 2944, 2323, 4374, 2049, 1005, 7953, 2012, 17547, 2051, 1011, 2043, 2017, 9167, 2115, 4738, 23435, 12314, 10629, 1013, 2944, 2004, 1037, 5552, 5302, 9247, 1012, 102, 101, 2612, 1997, 2383, 2019, 3161, 1056, 2546, 5219, 2066, 1999, 2731, 2073, 2017, 5373, 2224, 2173, 14528, 2030, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1998, 5942, 2110, 3993, 18750, 1013, 1056, 2546, 2448, 7292, 7781, 2017, 2215, 2115, 5552, 5302, 9247, 2000, 2022, 2583, 2000, 2022, 27289, 1998, 2517, 2000, 9785, 2061, 2008, 2017, 2064, 21296, 2007, 1056, 2546, 3529, 2030, 2448, 2009, 2006, 1037, 4684, 5080, 1010, 4385, 1012, 102, 101, 2061, 2017, 9167, 2023, 5552, 5302, 9247, 1011, 1996, 3529, 7953, 2003, 1996, 3538, 2503, 2008, 11859, 2129, 2000, 11968, 3366, 1996, 11186, 2009, 4152, 2741, 1998, 2059, 7532, 2008, 2046, 2115, 2944, 1996, 2126, 2017, 2763, 2215, 2000, 4604, 2068, 2003, 2004, 1056, 2546, 1012, 3345, 1012, 4973, 8778, 17698, 2015, 1006, 27289, 2004, 7817, 2061, 2008, 2064, 2022, 2741, 1999, 2019, 1054, 15042, 1007, 1012, 102, 101, 1998, 2059, 2017, 11968, 3366, 2068, 2046, 1037, 3444, 4487, 6593, 2061, 2115, 9765, 9581, 4263, 2064, 3305, 2068, 1999, 1996, 2168, 2126, 2009, 5319, 2115, 2731, 2951, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 3]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 25, 44, 58, 97, 124, 138, 172, 245, 314, 345], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39436168", "vertexSet": [[{"sent_id": 1, "name": "tf.contrib", "pos": [48, 54]}], [{"sent_id": 1, "name": "tf.contrib.metrics", "pos": [48, 57]}], [{"sent_id": 1, "name": "tf.initialize_local_variables", "pos": [32, 41]}], [{"sent_id": 1, "name": "tf.contrib.metrics.streaming_auc", "pos": [48, 62]}]], "sents": ["I've found the same issue on github.", "At the moment, it seems that you also need to run sess.run(tf.initialize_local_variables()) in order to make tf.contrib.metrics.streaming_auc() work.", "They're working on it.", "Here you have an example demonstrating how you can solve this issue:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 1005, 2310, 2179, 1996, 2168, 3277, 2006, 21025, 2705, 12083, 1012, 102, 101, 2012, 1996, 2617, 1010, 2009, 3849, 2008, 2017, 2036, 2342, 2000, 2448, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 3988, 4697, 1035, 2334, 1035, 10857, 1006, 1007, 1007, 1999, 2344, 2000, 2191, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 12046, 2015, 1012, 11058, 1035, 8740, 2278, 1006, 1007, 2147, 1012, 102, 101, 2027, 1005, 2128, 2551, 2006, 2009, 1012, 102, 101, 2182, 2017, 2031, 2019, 2742, 14313, 2129, 2017, 2064, 9611, 2023, 3277, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 14, 67, 76, 91, 105], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63366062", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [29, 34]}], [{"sent_id": 1, "name": "tf.keras.callbacks", "pos": [29, 37]}], [{"sent_id": 1, "name": "tf.keras.callbacks.callback", "pos": [29, 40]}]], "sents": ["To do this, you will need to create a custom callback so you have access to batch related methods.", "When you inherit from tf.keras.callbacks.Callback, you can override on_train_batch_end and set the learning rate on each batch.", "If you want to do it every N steps, then you can just add a counter property and increment it every time on_train_batch_end is called.", "Then, only set the learning rate if self.counter % N == 0.", "Some boilerplate code could look like this.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2000, 2079, 2023, 1010, 2017, 2097, 2342, 2000, 3443, 1037, 7661, 2655, 5963, 2061, 2017, 2031, 3229, 2000, 14108, 3141, 4725, 1012, 102, 101, 2043, 2017, 22490, 2013, 1056, 2546, 1012, 17710, 8180, 1012, 2655, 12221, 1012, 2655, 5963, 1010, 2017, 2064, 2058, 15637, 2006, 1035, 3345, 1035, 14108, 1035, 2203, 1998, 2275, 1996, 4083, 3446, 2006, 2169, 14108, 1012, 102, 101, 2065, 2017, 2215, 2000, 2079, 2009, 2296, 1050, 4084, 1010, 2059, 2017, 2064, 2074, 5587, 1037, 4675, 3200, 1998, 4297, 28578, 4765, 2009, 2296, 2051, 2006, 1035, 3345, 1035, 14108, 1035, 2203, 2003, 2170, 1012, 102, 101, 2059, 1010, 2069, 2275, 1996, 4083, 3446, 2065, 2969, 1012, 4675, 1003, 1050, 1027, 1027, 1014, 1012, 102, 101, 2070, 15635, 15725, 3642, 2071, 2298, 2066, 2023, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 62, 99, 118, 129, 143], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45206574", "vertexSet": [[{"sent_id": 2, "name": "tf.data", "pos": [57, 61]}, {"sent_id": 2, "name": "tf.data", "pos": [67, 71]}], [{"sent_id": 8, "name": "tf.py_func", "pos": [211, 219]}], [{"sent_id": 2, "name": "tf.data.dataset", "pos": [57, 64]}, {"sent_id": 2, "name": "tf.data.dataset", "pos": [67, 74]}]], "sents": ["You have a large numpy array that lies on the host memory.", "You want to be able to process it in parallel on the CPU and send batches to the device.", "Since TF 1.4, the best way to do it is to use tf.data.Dataset, and particularly tf.data.Dataset.from_tensor_slices.", "However, as the documentation points out, you should probably not provide your numpy arrays as arguments to this function, because it will end up being copied to device memory.", "What you should do instead is to use placeholders.", "The example given in the doc is pretty self-explanatory:", "<code>Code Snippet</code>.", "Further preprocessing or data augmentation can be applied to the slices using the .map method.", "To make sure that those operations happen concurrently, make sure to use tensorflow operations only and avoid wrapping python operations with tf.py_func."], "sent_idxs": [101, 2017, 2031, 1037, 2312, 16371, 8737, 2100, 9140, 2008, 3658, 2006, 1996, 3677, 3638, 1012, 102, 101, 2017, 2215, 2000, 2022, 2583, 2000, 2832, 2009, 1999, 5903, 2006, 1996, 17368, 1998, 4604, 14108, 2229, 2000, 1996, 5080, 1012, 102, 101, 2144, 1056, 2546, 1015, 1012, 1018, 1010, 1996, 2190, 2126, 2000, 2079, 2009, 2003, 2000, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1010, 1998, 3391, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 23435, 1035, 25609, 1012, 102, 101, 2174, 1010, 2004, 1996, 12653, 2685, 2041, 1010, 2017, 2323, 2763, 2025, 3073, 2115, 16371, 8737, 2100, 27448, 2004, 9918, 2000, 2023, 3853, 1010, 2138, 2009, 2097, 2203, 2039, 2108, 15826, 2000, 5080, 3638, 1012, 102, 101, 2054, 2017, 2323, 2079, 2612, 2003, 2000, 2224, 2173, 17794, 1012, 102, 101, 1996, 2742, 2445, 1999, 1996, 9986, 2003, 3492, 2969, 1011, 4654, 24759, 5162, 7062, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2582, 17463, 3217, 9623, 7741, 2030, 2951, 15476, 3672, 3370, 2064, 2022, 4162, 2000, 1996, 25609, 2478, 1996, 1012, 4949, 4118, 1012, 102, 101, 2000, 2191, 2469, 2008, 2216, 3136, 4148, 15442, 1010, 2191, 2469, 2000, 2224, 23435, 12314, 3136, 2069, 1998, 4468, 12252, 18750, 3136, 2007, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 17, 40, 82, 119, 132, 149, 163, 187, 221], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0]}, {"title": "38062223", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [4, 10]}, {"sent_id": 2, "name": "tf.contrib", "pos": [60, 66]}, {"sent_id": 4, "name": "tf.contrib", "pos": [110, 116]}], [{"sent_id": 4, "name": "tf.contrib.layers", "pos": [110, 118]}], [{"sent_id": 4, "name": "tf.contrib.layers.sparse_column_with_hash_bucket", "pos": [110, 128]}]], "sents": ["In general, tf.contrib contains contributed code.", "It is meant to contain features and contributions that eventually should get merged into core TensorFlow, but whose interfaces may still change, or which require some testing to see whether they can find broader acceptance.", "The code in tf.contrib isn't supported by the Tensorflow team.", "It is included in the hope that it is helpful, but it might change or be removed at any time; there are no guarantees.", "The source of tf.contrib.layers.sparse_column_with_hash_bucket can be found at", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py#L365"], "sent_idxs": [101, 1999, 2236, 1010, 1056, 2546, 1012, 9530, 18886, 2497, 3397, 5201, 3642, 1012, 102, 101, 2009, 2003, 3214, 2000, 5383, 2838, 1998, 5857, 2008, 2776, 2323, 2131, 5314, 2046, 4563, 23435, 12314, 1010, 2021, 3005, 19706, 2089, 2145, 2689, 1010, 2030, 2029, 5478, 2070, 5604, 2000, 2156, 3251, 2027, 2064, 2424, 12368, 9920, 1012, 102, 101, 1996, 3642, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 3475, 1005, 1056, 3569, 2011, 1996, 23435, 12314, 2136, 1012, 102, 101, 2009, 2003, 2443, 1999, 1996, 3246, 2008, 2009, 2003, 14044, 1010, 2021, 2009, 2453, 2689, 2030, 2022, 3718, 2012, 2151, 2051, 1025, 2045, 2024, 2053, 21586, 1012, 102, 101, 1996, 3120, 1997, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 20288, 1035, 5930, 1035, 2007, 1035, 23325, 1035, 13610, 2064, 2022, 2179, 2012, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 3040, 1013, 23435, 12314, 1013, 9530, 18886, 2497, 1013, 9014, 1013, 18750, 1013, 9014, 1013, 3444, 1035, 5930, 1012, 1052, 2100, 1001, 1048, 21619, 2629, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 15, 56, 77, 106, 133, 180], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47271906", "vertexSet": [[{"sent_id": 0, "name": "tf.global_variables", "pos": [2, 8]}, {"sent_id": 0, "name": "tf.global_variables", "pos": [17, 23]}, {"sent_id": 2, "name": "tf.global_variables", "pos": [60, 66]}, {"sent_id": 2, "name": "tf.global_variables", "pos": [85, 91]}], [{"sent_id": 2, "name": "tf.variables_initializer", "pos": [77, 84]}], [{"sent_id": 0, "name": "tf.global_variables_initializer", "pos": [2, 11]}, {"sent_id": 2, "name": "tf.global_variables_initializer", "pos": [60, 69]}]], "sents": ["The tf.global_variables_initializer just initializes all variables that tf.global_variables() would list.", "This actually makes much sense in a distributed environment where the graph might be located in different computing nodes in a cluster.", "In such a case, tf.global_variables_initializer() which is just an alias for tf.variables_initializer(tf.global_variables()) would initialize all the variables in all the computing nodes, where the graph is placed."], "sent_idxs": [101, 1996, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 2074, 3988, 10057, 2035, 10857, 2008, 1056, 2546, 1012, 3795, 1035, 10857, 1006, 1007, 2052, 2862, 1012, 102, 101, 2023, 2941, 3084, 2172, 3168, 1999, 1037, 5500, 4044, 2073, 1996, 10629, 2453, 2022, 2284, 1999, 2367, 9798, 14164, 1999, 1037, 9324, 1012, 102, 101, 1999, 2107, 1037, 2553, 1010, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 2029, 2003, 2074, 2019, 14593, 2005, 1056, 2546, 1012, 10857, 1035, 3988, 17629, 1006, 1056, 2546, 1012, 3795, 1035, 10857, 1006, 1007, 1007, 2052, 3988, 4697, 2035, 1996, 10857, 1999, 2035, 1996, 9798, 14164, 1010, 2073, 1996, 10629, 2003, 2872, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 29, 54, 113], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51586823", "vertexSet": [[{"sent_id": 2, "name": "tf.boolean_mask", "pos": [47, 54]}], [{"sent_id": 2, "name": "tf.gather", "pos": [61, 65]}], [{"sent_id": 2, "name": "tf.where", "pos": [56, 60]}], [{"sent_id": 0, "name": "tf.not_equal", "pos": [6, 12]}]], "sents": ["I think you should use tf.not_equal to perform elementwise comparison on the tensor.", "<code>Code Snippet</code>.", "You can also shorten this a bit and use tf.boolean_mask instead of tf.where and tf.gather:", "<code>Code Snippet</code>.", "Note the difference in the shape of the outputs."], "sent_idxs": [101, 1045, 2228, 2017, 2323, 2224, 1056, 2546, 1012, 2025, 1035, 5020, 2000, 4685, 5783, 14244, 7831, 2006, 1996, 23435, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 2064, 2036, 2460, 2368, 2023, 1037, 2978, 1998, 2224, 1056, 2546, 1012, 22017, 20898, 1035, 7308, 2612, 1997, 1056, 2546, 1012, 2073, 1998, 1056, 2546, 1012, 8587, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 1996, 4489, 1999, 1996, 4338, 1997, 1996, 27852, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [2]}, {"r": "S1", "h": 1, "t": 0, "evidence": [2]}, {"r": "S1", "h": 0, "t": 2, "evidence": [2]}, {"r": "S1", "h": 2, "t": 0, "evidence": [2]}], "na_triple": [[0, 3], [1, 2], [1, 3], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 22, 36, 67, 81, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 3, 3, 3, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34725458", "vertexSet": [[{"sent_id": 7, "name": "tf.fill", "pos": [210, 214]}], [{"sent_id": 7, "name": "tf.shape", "pos": [191, 195]}], [{"sent_id": 7, "name": "tf.stack", "pos": [198, 202]}], [{"sent_id": 0, "name": "tf.zeros_like", "pos": [20, 27]}, {"sent_id": 4, "name": "tf.zeros_like", "pos": [120, 127]}]], "sents": ["The recommended way to make a zero tensor with the same shape as another tensor is to use the tf.zeros_like() op:", "<code>Code Snippet</code>.", "The resulting tensor y appears to have the shape [None, None] according to Tensor.get_shape(), but at runtime it will expand to the  same shape as x:", "<code>Code Snippet</code>.", "The [None, None] static shape is returned because shape inference hasn't been specialized for tf.zeros_like().", "I've filed a GitHub issue for that and it should be fixed soon.", "EDIT: In your comment, you asked how to deal with the case where the zero tensor had a shape based on, but different from the original tensor.", "This is also possible, using tf.shape() and tf.stack() to build the dimensions, and tf.fill() to produce the zero tensor:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 6749, 2126, 2000, 2191, 1037, 5717, 23435, 2007, 1996, 2168, 4338, 2004, 2178, 23435, 2003, 2000, 2224, 1996, 1056, 2546, 1012, 5717, 2015, 1035, 2066, 1006, 1007, 6728, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 4525, 23435, 1061, 3544, 2000, 2031, 1996, 4338, 1031, 3904, 1010, 3904, 1033, 2429, 2000, 23435, 1012, 2131, 1035, 4338, 1006, 1007, 1010, 2021, 2012, 2448, 7292, 2009, 2097, 7818, 2000, 1996, 2168, 4338, 2004, 1060, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 1031, 3904, 1010, 3904, 1033, 10763, 4338, 2003, 2513, 2138, 4338, 28937, 8440, 1005, 1056, 2042, 7772, 2005, 1056, 2546, 1012, 5717, 2015, 1035, 2066, 1006, 1007, 1012, 102, 101, 1045, 1005, 2310, 6406, 1037, 21025, 2705, 12083, 3277, 2005, 2008, 1998, 2009, 2323, 2022, 4964, 2574, 1012, 102, 101, 10086, 1024, 1999, 2115, 7615, 1010, 2017, 2356, 2129, 2000, 3066, 2007, 1996, 2553, 2073, 1996, 5717, 23435, 2018, 1037, 4338, 2241, 2006, 1010, 2021, 2367, 2013, 1996, 2434, 23435, 1012, 102, 101, 2023, 2003, 2036, 2825, 1010, 2478, 1056, 2546, 1012, 4338, 1006, 1007, 1998, 1056, 2546, 1012, 9991, 1006, 1007, 2000, 3857, 1996, 9646, 1010, 1998, 1056, 2546, 1012, 6039, 1006, 1007, 2000, 3965, 1996, 5717, 23435, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 32, 46, 86, 100, 131, 151, 184, 223, 237], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56936811", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [27, 32]}, {"sent_id": 1, "name": "tf.nn", "pos": [52, 57]}, {"sent_id": 4, "name": "tf.nn", "pos": [153, 158]}, {"sent_id": 6, "name": "tf.nn", "pos": [202, 207]}], [{"sent_id": 6, "name": "tf.nn.sigmoid", "pos": [202, 211]}], [{"sent_id": 0, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [27, 44]}, {"sent_id": 1, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [52, 69]}, {"sent_id": 4, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [153, 170]}]], "sents": ["The model is not converging, and the problem seems to be that you are doing a sigmoid activation directly followed by tf.nn.softmax_cross_entropy_with_logits.", "In the documentation for the tf.nn.softmax_cross_entropy_with_logits it says:", "WARNING: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.", "Do not call this op with the output of softmax, as it will produce incorrect results.", "Hence no softmax, sigmoid, relu, tanh or any other activations should be done on the output of the previous layer before passed to tf.nn.softmax_cross_entropy_with_logits.", "For more in depth description of when to use sigmoid or softmax output activation, see here.", "Therfore by replacing return tf.nn.sigmoid(lr) with just return lr in the logistic_regression function, the model is converging.", "Below is a working example of your code with the above fix.", "I also changed the variable name epochs to n_batches as your training loop actually goes through 1000 batches not 1000 epochs (i also bumped it up to 10000 as there was sign of more iterations needed).", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2944, 2003, 2025, 9530, 6299, 4726, 1010, 1998, 1996, 3291, 3849, 2000, 2022, 2008, 2017, 2024, 2725, 1037, 9033, 21693, 9314, 13791, 3495, 2628, 2011, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1012, 102, 101, 1999, 1996, 12653, 2005, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 2009, 2758, 1024, 102, 101, 5432, 1024, 2023, 6728, 24273, 4895, 15782, 3709, 8833, 12762, 1010, 2144, 2009, 10438, 1037, 3730, 17848, 2006, 8833, 12762, 16058, 2005, 8122, 1012, 102, 101, 2079, 2025, 2655, 2023, 6728, 2007, 1996, 6434, 1997, 3730, 17848, 1010, 2004, 2009, 2097, 3965, 16542, 3463, 1012, 102, 101, 6516, 2053, 3730, 17848, 1010, 9033, 21693, 9314, 1010, 2128, 7630, 1010, 9092, 2232, 2030, 2151, 2060, 13791, 2015, 2323, 2022, 2589, 2006, 1996, 6434, 1997, 1996, 3025, 6741, 2077, 2979, 2000, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1012, 102, 101, 2005, 2062, 1999, 5995, 6412, 1997, 2043, 2000, 2224, 9033, 21693, 9314, 2030, 3730, 17848, 6434, 13791, 1010, 2156, 2182, 1012, 102, 101, 1996, 12881, 5686, 2011, 6419, 2709, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1006, 1048, 2099, 1007, 2007, 2074, 2709, 1048, 2099, 1999, 1996, 8833, 6553, 1035, 26237, 3853, 1010, 1996, 2944, 2003, 9530, 6299, 4726, 1012, 102, 101, 2917, 2003, 1037, 2551, 2742, 1997, 2115, 3642, 2007, 1996, 2682, 8081, 1012, 102, 101, 1045, 2036, 2904, 1996, 8023, 2171, 25492, 2015, 2000, 1050, 1035, 14108, 2229, 2004, 2115, 2731, 7077, 2941, 3632, 2083, 6694, 14108, 2229, 2025, 6694, 25492, 2015, 1006, 1045, 2036, 19030, 2009, 2039, 2000, 6694, 2692, 2004, 2045, 2001, 3696, 1997, 2062, 27758, 2015, 2734, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 46, 73, 99, 120, 172, 195, 236, 251, 300, 314], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "38832082", "vertexSet": [[{"sent_id": 0, "name": "tf.graph", "pos": [4, 8]}], [{"sent_id": 2, "name": "tf.train", "pos": [68, 72]}], [{"sent_id": 2, "name": "tf.train.write_graph", "pos": [68, 76]}]], "sents": ["You can use tf.Graph.as_graph_def() and then Protobuf's SerializeToString() like so:", "<code>Code Snippet</code>.", "If you just want to write the file and do not care about the encoding you can also use tf.train.write_graph()", "<code>Code Snippet</code>.", "Note: Tested on TF 0.10, not sure about earlier versions."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 10629, 1012, 2004, 1035, 10629, 1035, 13366, 1006, 1007, 1998, 2059, 15053, 8569, 2546, 1005, 1055, 7642, 4697, 13122, 18886, 3070, 1006, 1007, 2066, 2061, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2074, 2215, 2000, 4339, 1996, 5371, 1998, 2079, 2025, 2729, 2055, 1996, 17181, 2017, 2064, 2036, 2224, 1056, 2546, 1012, 3345, 1012, 4339, 1035, 10629, 1006, 1007, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 1024, 7718, 2006, 1056, 2546, 1014, 1012, 2184, 1010, 2025, 2469, 2055, 3041, 4617, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 34, 48, 79, 93, 111], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54035916", "vertexSet": [[{"sent_id": 0, "name": "tf.image", "pos": [5, 9]}, {"sent_id": 4, "name": "tf.image", "pos": [82, 86]}], [{"sent_id": 0, "name": "tf.image.crop_and_resize", "pos": [5, 16]}], [{"sent_id": 4, "name": "tf.image.crop_to_bounding_box", "pos": [82, 95]}]], "sents": ["Try to use :\ntf.image.crop_and_resize(image,boxes,box_ind,crop_size,method='bilinear',extrapolation_value=0,name=None)", "For example:", "<code>Code Snippet</code>.", "Worked for me!", "or if you want to use tf.image.crop_to_bounding_box(...) try this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 3046, 2000, 2224, 1024, 1056, 2546, 1012, 3746, 1012, 10416, 1035, 1998, 1035, 24501, 4697, 1006, 3746, 1010, 8378, 1010, 3482, 1035, 27427, 1010, 10416, 1035, 2946, 1010, 4118, 1027, 1005, 12170, 4179, 2906, 1005, 1010, 4469, 18155, 3370, 1035, 3643, 1027, 1014, 1010, 2171, 1027, 3904, 1007, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2499, 2005, 2033, 999, 102, 101, 2030, 2065, 2017, 2215, 2000, 2224, 1056, 2546, 1012, 3746, 1012, 10416, 1035, 2000, 1035, 5391, 2075, 1035, 3482, 1006, 1012, 1012, 1012, 1007, 3046, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 50, 55, 69, 75, 104, 118], "sent_pos": [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59876894", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [49, 54]}], [{"sent_id": 2, "name": "tf.keras.layers", "pos": [49, 56]}], [{"sent_id": 2, "name": "tf.keras.layers.dense", "pos": [49, 58]}]], "sents": ["Answer 1: Yes, these predictions are from model after compiling but before training it.", "Answer 2: Yes, they are random weights, for example, in Dense layer they are initialised using glorot_uniform .", "tf.keras.layers.Dense", "Answer 3: The model we saved above had a bias initialised using np.log([pos/neg]), it is mentioned here.", "So, in zero_bias_history, they initialised bias with zeroes using model.layers[-1].bias.assign([0.0]), and in careful_bias_history they just loaded the saved model which already had the initialised bias."], "sent_idxs": [101, 3437, 1015, 1024, 2748, 1010, 2122, 20932, 2024, 2013, 2944, 2044, 21953, 2021, 2077, 2731, 2009, 1012, 102, 101, 3437, 1016, 1024, 2748, 1010, 2027, 2024, 6721, 15871, 1010, 2005, 2742, 1010, 1999, 9742, 6741, 2027, 2024, 3988, 5084, 2478, 1043, 10626, 4140, 1035, 6375, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 102, 101, 3437, 1017, 1024, 1996, 2944, 2057, 5552, 2682, 2018, 1037, 13827, 3988, 5084, 2478, 27937, 1012, 8833, 1006, 1031, 13433, 2015, 1013, 11265, 2290, 1033, 1007, 1010, 2009, 2003, 3855, 2182, 1012, 102, 101, 2061, 1010, 1999, 5717, 1035, 13827, 1035, 2381, 1010, 2027, 3988, 5084, 13827, 2007, 5717, 2229, 2478, 2944, 1012, 9014, 1031, 1011, 1015, 1033, 1012, 13827, 1012, 23911, 1006, 1031, 1014, 1012, 1014, 1033, 1007, 1010, 1998, 1999, 6176, 1035, 13827, 1035, 2381, 2027, 2074, 8209, 1996, 5552, 2944, 2029, 2525, 2018, 1996, 3988, 5084, 13827, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 48, 59, 93, 152], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44419834", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [9, 15]}], [{"sent_id": 0, "name": "tf.contrib.layers", "pos": [9, 17]}], [{"sent_id": 0, "name": "tf.contrib.layers.scale_gradient", "pos": [9, 21]}]], "sents": ["Here's a simple example of using tf.contrib.layers.scale_gradient to do elementwise multiplication of gradients.", "In the forward pass it's just an identity op, and in the backward pass it multiplies gradients by its second argument.", "<code>Code Snippet</code>.", "Prints:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2182, 1005, 1055, 1037, 3722, 2742, 1997, 2478, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 4094, 1035, 17978, 2000, 2079, 5783, 14244, 24856, 1997, 17978, 2015, 1012, 102, 101, 1999, 1996, 2830, 3413, 2009, 1005, 1055, 2074, 2019, 4767, 6728, 1010, 1998, 1999, 1996, 8848, 3413, 2009, 4800, 24759, 3111, 17978, 2015, 2011, 2049, 2117, 6685, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 11204, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 31, 61, 75, 79, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37149500", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [11, 16]}], [{"sent_id": 0, "name": "tf.abs", "pos": [20, 24]}, {"sent_id": 0, "name": "tf.abs", "pos": [33, 37]}], [{"sent_id": 0, "name": "tf.nn.relu", "pos": [11, 19]}]], "sents": ["Instead of using the comparison operator, you could use\ntf.nn.relu(tf.abs(y_predicted-y_)-tf.abs(y_))\nto get a differentiable cost function.", "The relu operation compute max(0, x) and thus will give a continuous version of what you were doing.", "The main difference will be that you will be more penalized for being farther away and thus you don't have the binary property (but this is usually what you aim for in regression).", "This should yield better result than the squared loss."], "sent_idxs": [101, 2612, 1997, 2478, 1996, 7831, 6872, 1010, 2017, 2071, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 2128, 7630, 1006, 1056, 2546, 1012, 14689, 1006, 1061, 1035, 10173, 1011, 1061, 1035, 1007, 1011, 1056, 2546, 1012, 14689, 1006, 1061, 1035, 1007, 1007, 2000, 2131, 1037, 2367, 19210, 3465, 3853, 1012, 102, 101, 1996, 2128, 7630, 3169, 24134, 4098, 1006, 1014, 1010, 1060, 1007, 1998, 2947, 2097, 2507, 1037, 7142, 2544, 1997, 2054, 2017, 2020, 2725, 1012, 102, 101, 1996, 2364, 4489, 2097, 2022, 2008, 2017, 2097, 2022, 2062, 18476, 3550, 2005, 2108, 8736, 2185, 1998, 2947, 2017, 2123, 1005, 1056, 2031, 1996, 12441, 3200, 1006, 2021, 2023, 2003, 2788, 2054, 2017, 6614, 2005, 1999, 26237, 1007, 1012, 102, 101, 2023, 2323, 10750, 2488, 2765, 2084, 1996, 19942, 3279, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 51, 77, 118, 130], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36275218", "vertexSet": [[{"sent_id": 3, "name": "tf.nn", "pos": [104, 109]}, {"sent_id": 3, "name": "tf.nn", "pos": [118, 123]}], [{"sent_id": 3, "name": "tf.nn.top_k", "pos": [118, 127]}], [{"sent_id": 3, "name": "tf.nn.softmax", "pos": [104, 112]}]], "sents": ["You can reuse your trained network for evaluation, using TensorFlow's feed mechanism.", "Typically you will have a tensor called input that contains a batch of images and serves as the input to the first layer of your model.", "Let's also assume that you have a tensor called logits that is the output of the final layer of your network (before it is passed to the loss function).", "You can get the top-five most probable classes by passing the logits to a tf.nn.softmax() op followed by a tf.nn.top_k() op.", "The resulting program will look like the following:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2128, 8557, 2115, 4738, 2897, 2005, 9312, 1010, 2478, 23435, 12314, 1005, 1055, 5438, 7337, 1012, 102, 101, 4050, 2017, 2097, 2031, 1037, 23435, 2170, 7953, 2008, 3397, 1037, 14108, 1997, 4871, 1998, 4240, 2004, 1996, 7953, 2000, 1996, 2034, 6741, 1997, 2115, 2944, 1012, 102, 101, 2292, 1005, 1055, 2036, 7868, 2008, 2017, 2031, 1037, 23435, 2170, 8833, 12762, 2008, 2003, 1996, 6434, 1997, 1996, 2345, 6741, 1997, 2115, 2897, 1006, 2077, 2009, 2003, 2979, 2000, 1996, 3279, 3853, 1007, 1012, 102, 101, 2017, 2064, 2131, 1996, 2327, 1011, 2274, 2087, 15596, 4280, 2011, 4458, 1996, 8833, 12762, 2000, 1037, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1007, 6728, 2628, 2011, 1037, 1056, 2546, 1012, 1050, 2078, 1012, 2327, 1035, 1047, 1006, 1007, 6728, 1012, 102, 101, 1996, 4525, 2565, 2097, 2298, 2066, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 49, 86, 132, 143, 157], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47723110", "vertexSet": [[{"sent_id": 0, "name": "tf.summary.merge_all", "pos": [13, 21]}], [{"sent_id": 0, "name": "tf.summary.merge", "pos": [13, 19]}, {"sent_id": 0, "name": "tf.summary.merge", "pos": [40, 46]}]], "sents": ["Instead of merging all summaries by merged_summary = tf.summary.merge_all(), you can merge the ops that you wanted like merged_summary_group1 = tf.summary.merge([op1, op2, ...]).", "After that, replacing all merged_summary in sess.run with merged_summary_group1."], "sent_idxs": [101, 2612, 1997, 16468, 2035, 7680, 7849, 3111, 2011, 5314, 1035, 12654, 1027, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 1006, 1007, 1010, 2017, 2064, 13590, 1996, 23092, 2008, 2017, 2359, 2066, 5314, 1035, 12654, 1035, 2177, 2487, 1027, 1056, 2546, 1012, 12654, 1012, 13590, 1006, 1031, 6728, 2487, 1010, 6728, 2475, 1010, 1012, 1012, 1012, 1033, 1007, 1012, 102, 101, 2044, 2008, 1010, 6419, 2035, 5314, 1035, 12654, 1999, 7367, 4757, 1012, 2448, 2007, 5314, 1035, 12654, 1035, 2177, 2487, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0]}], "na_triple": [], "sent_ends": [0, 61, 84], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56833142", "vertexSet": [[{"sent_id": 1, "name": "tf.concat", "pos": [49, 54]}], [{"sent_id": 1, "name": "tf.reshape", "pos": [55, 61]}], [{"sent_id": 1, "name": "tf.transpose", "pos": [63, 68]}, {"sent_id": 7, "name": "tf.transpose", "pos": [160, 165]}]], "sents": ["It seems to me that your example array actually has the shape (batch_size, T, C*4) rather than (batch_size, T*4, C).", "Anyway, you can get what you need with tf.concat, tf.reshape, and tf.transpose.", "A simpler example in 2d is as follows:", "<code>Code Snippet</code>.", "You concatenate A and B to get a matrix of shape (2,6).", "Then you reshape it which interleaves the rows.", "To do this in 3d, the dimension which is multiplied by 4 needs to be the last one.", "So you may need to use tf.transpose, interleave using concat and reshape, then transpose again to reorder the dimensions."], "sent_idxs": [101, 2009, 3849, 2000, 2033, 2008, 2115, 2742, 9140, 2941, 2038, 1996, 4338, 1006, 14108, 1035, 2946, 1010, 1056, 1010, 1039, 1008, 1018, 1007, 2738, 2084, 1006, 14108, 1035, 2946, 1010, 1056, 1008, 1018, 1010, 1039, 1007, 1012, 102, 101, 4312, 1010, 2017, 2064, 2131, 2054, 2017, 2342, 2007, 1056, 2546, 1012, 9530, 11266, 1010, 1056, 2546, 1012, 24501, 3270, 5051, 1010, 1998, 1056, 2546, 1012, 9099, 20688, 1012, 102, 101, 1037, 16325, 2742, 1999, 14134, 2003, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2017, 9530, 16280, 12556, 1037, 1998, 1038, 2000, 2131, 1037, 8185, 1997, 4338, 1006, 1016, 1010, 1020, 1007, 1012, 102, 101, 2059, 2017, 24501, 3270, 5051, 2009, 2029, 6970, 19738, 6961, 1996, 10281, 1012, 102, 101, 2000, 2079, 2023, 1999, 7605, 1010, 1996, 9812, 2029, 2003, 28608, 2011, 1018, 3791, 2000, 2022, 1996, 2197, 2028, 1012, 102, 101, 2061, 2017, 2089, 2342, 2000, 2224, 1056, 2546, 1012, 9099, 20688, 1010, 6970, 19738, 3726, 2478, 9530, 11266, 1998, 24501, 3270, 5051, 1010, 2059, 9099, 20688, 2153, 2000, 2128, 8551, 2121, 1996, 9646, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 39, 70, 81, 95, 116, 131, 153, 189], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49594327", "vertexSet": [[{"sent_id": 1, "name": "tf.io", "pos": [11, 15]}, {"sent_id": 1, "name": "tf.io", "pos": [28, 32]}, {"sent_id": 2, "name": "tf.io", "pos": [55, 59]}], [{"sent_id": 1, "name": "tf.io.fixedlenfeature", "pos": [11, 21]}], [{"sent_id": 1, "name": "tf.io.fixedlensequencefeature", "pos": [28, 41]}]], "sents": ["I found the problem.", "Instead of using tf.io.FixedLenFeature for parsing an array, use tf.io.FixedLenSequenceFeature\n(for TensorFlow 1, use tf.", "instead of tf.io.)"], "sent_idxs": [101, 1045, 2179, 1996, 3291, 1012, 102, 101, 2612, 1997, 2478, 1056, 2546, 1012, 22834, 1012, 4964, 7770, 7959, 4017, 5397, 2005, 11968, 7741, 2019, 9140, 1010, 2224, 1056, 2546, 1012, 22834, 1012, 4964, 7770, 3366, 4226, 5897, 7959, 4017, 5397, 1006, 2005, 23435, 12314, 1015, 1010, 2224, 1056, 2546, 1012, 102, 101, 2612, 1997, 1056, 2546, 1012, 22834, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 7, 52, 62], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]}, {"title": "41478373", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [7, 11]}, {"sent_id": 2, "name": "tf.train", "pos": [118, 122]}], [{"sent_id": 1, "name": "tf.session", "pos": [73, 77]}], [{"sent_id": 0, "name": "tf.configproto", "pos": [19, 27]}], [{"sent_id": 2, "name": "tf.train.server", "pos": [118, 124]}], [{"sent_id": 0, "name": "tf.train.monitoredtrainingsession", "pos": [7, 17]}]], "sents": ["The config argument to tf.train.MonitoredTrainingSession takes a tf.ConfigProto protocol buffer message.", "It looks like you should actually pass your argument (\"grpc://localhost:2222\") as the master argument, which takes the same values as the target argument to the tf.Session initializer: e.g. \"\"", "means \"in-process runtime\", and \"grpc://localhost:2222\" means \"the gRPC-based tf.train.Server listening on localhost:2222."], "sent_idxs": [101, 1996, 9530, 8873, 2290, 6685, 2000, 1056, 2546, 1012, 3345, 1012, 17785, 23654, 8613, 7971, 3258, 3138, 1037, 1056, 2546, 1012, 9530, 8873, 21600, 21709, 2080, 8778, 17698, 4471, 1012, 102, 101, 2009, 3504, 2066, 2017, 2323, 2941, 3413, 2115, 6685, 1006, 1000, 24665, 15042, 1024, 1013, 1013, 2334, 15006, 2102, 1024, 19015, 2475, 1000, 1007, 2004, 1996, 3040, 6685, 1010, 2029, 3138, 1996, 2168, 5300, 2004, 1996, 4539, 6685, 2000, 1996, 1056, 2546, 1012, 5219, 3988, 17629, 1024, 1041, 1012, 1043, 1012, 1000, 1000, 102, 101, 2965, 1000, 1999, 1011, 2832, 2448, 7292, 1000, 1010, 1998, 1000, 24665, 15042, 1024, 1013, 1013, 2334, 15006, 2102, 1024, 19015, 2475, 1000, 2965, 1000, 1996, 24665, 15042, 1011, 2241, 1056, 2546, 1012, 3345, 1012, 8241, 5962, 2006, 2334, 15006, 2102, 1024, 19015, 2475, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 32, 87, 134], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54906367", "vertexSet": [[{"sent_id": 2, "name": "tf.scan", "pos": [26, 30]}], [{"sent_id": 2, "name": "tf.while_loop", "pos": [35, 41]}], [{"sent_id": 0, "name": "tf.tensorarray", "pos": [2, 8]}, {"sent_id": 2, "name": "tf.tensorarray", "pos": [42, 48]}]], "sents": ["Use tf.TensorArray:", "<code>Code Snippet</code>.", "or tf.scan (which is implemented using tf.while_loop and tf.TensorArray):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2224, 1056, 2546, 1012, 23435, 2906, 9447, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 1056, 2546, 1012, 13594, 1006, 2029, 2003, 7528, 2478, 1056, 2546, 1012, 2096, 1035, 7077, 1998, 1056, 2546, 1012, 23435, 2906, 9447, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 10, 24, 51, 65], "sent_pos": [0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55831614", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [54, 59]}, {"sent_id": 2, "name": "tf.nn", "pos": [90, 95]}], [{"sent_id": 1, "name": "tf.keras", "pos": [70, 75]}, {"sent_id": 3, "name": "tf.keras", "pos": [142, 147]}, {"sent_id": 3, "name": "tf.keras", "pos": [155, 160]}], [{"sent_id": 1, "name": "tf.nn.rnn_cell", "pos": [54, 64]}, {"sent_id": 2, "name": "tf.nn.rnn_cell", "pos": [90, 100]}], [{"sent_id": 1, "name": "tf.keras.layers", "pos": [70, 77]}, {"sent_id": 3, "name": "tf.keras.layers", "pos": [142, 149]}, {"sent_id": 3, "name": "tf.keras.layers", "pos": [155, 162]}], [{"sent_id": 1, "name": "tf.nn.rnn_cell.grucell", "pos": [54, 68]}], [{"sent_id": 1, "name": "tf.keras.layers.grucell", "pos": [70, 81]}], [{"sent_id": 3, "name": "tf.keras.layers.lstmcell", "pos": [142, 154]}], [{"sent_id": 3, "name": "tf.keras.layers.simplernncell", "pos": [155, 167]}]], "sents": ["While searching for the documentation for these classes to add links, I noticed something that may be tripping you up: there are (currently, just before the official TF 2.0 release) two GRUCell implementations in TensorFlow!", "There is a tf.nn.rnn_cell.GRUCell and a tf.keras.layers.GRUCell.", "It looks like the one from tf.nn.rnn_cell is deprecated, and the Keras one is the one you should use.", "From what I can tell, the GRUCell has the same __call__() method signature as tf.keras.layers.LSTMCell and tf.keras.layers.SimpleRNNCell, and they all inherit from Layer.", "The RNN documentation gives some requirements on what the __call__() method of the objects you pass to its cell argument must do, but my guess is that all three of these should meet those requirements.", "You should be able to just use the same RNN framework and pass it a list of GRUCell objects instead of LSTMCell or SimpleRNNCell.", "I can't test this right now, so I'm not sure if you pass a list of GRUCell objects or just GRU objects into RNN, but I think one of those should work."], "sent_idxs": [101, 2096, 6575, 2005, 1996, 12653, 2005, 2122, 4280, 2000, 5587, 6971, 1010, 1045, 4384, 2242, 2008, 2089, 2022, 4440, 4691, 2017, 2039, 1024, 2045, 2024, 1006, 2747, 1010, 2074, 2077, 1996, 2880, 1056, 2546, 1016, 1012, 1014, 2713, 1007, 2048, 24665, 18796, 3363, 24977, 1999, 23435, 12314, 999, 102, 101, 2045, 2003, 1037, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 24665, 18796, 3363, 1998, 1037, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 24665, 18796, 3363, 1012, 102, 101, 2009, 3504, 2066, 1996, 2028, 2013, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 2003, 2139, 28139, 12921, 1010, 1998, 1996, 17710, 8180, 2028, 2003, 1996, 2028, 2017, 2323, 2224, 1012, 102, 101, 2013, 2054, 1045, 2064, 2425, 1010, 1996, 24665, 18796, 3363, 2038, 1996, 2168, 1035, 1035, 2655, 1035, 1035, 1006, 1007, 4118, 8085, 2004, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 1048, 3367, 12458, 5349, 1998, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 16325, 10695, 29109, 2140, 1010, 1998, 2027, 2035, 22490, 2013, 6741, 1012, 102, 101, 1996, 29300, 2078, 12653, 3957, 2070, 5918, 2006, 2054, 1996, 1035, 1035, 2655, 1035, 1035, 1006, 1007, 4118, 1997, 1996, 5200, 2017, 3413, 2000, 2049, 3526, 6685, 2442, 2079, 1010, 2021, 2026, 3984, 2003, 2008, 2035, 2093, 1997, 2122, 2323, 3113, 2216, 5918, 1012, 102, 101, 2017, 2323, 2022, 2583, 2000, 2074, 2224, 1996, 2168, 29300, 2078, 7705, 1998, 3413, 2009, 1037, 2862, 1997, 24665, 18796, 3363, 5200, 2612, 1997, 1048, 3367, 12458, 5349, 2030, 16325, 10695, 29109, 2140, 1012, 102, 101, 1045, 2064, 1005, 1056, 3231, 2023, 2157, 2085, 1010, 2061, 1045, 1005, 1049, 2025, 2469, 2065, 2017, 3413, 1037, 2862, 1997, 24665, 18796, 3363, 5200, 2030, 2074, 24665, 2226, 5200, 2046, 29300, 2078, 1010, 2021, 1045, 2228, 2028, 1997, 2216, 2323, 2147, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 50, 83, 118, 176, 222, 258, 303], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47976802", "vertexSet": [[{"sent_id": 0, "name": "tf.image", "pos": [17, 21]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [71, 76]}], [{"sent_id": 0, "name": "tf.image.resize_images", "pos": [17, 26]}]], "sents": ["Your resized_image variable is a tensor as you have initialized it to tf.image.resize_images(image, [272, 272])  ... your feed has to be a numpy array whose shape has to match the tensor x that you have defined in the code \nFor example if in your case x = tf.placeholder(tf.float32, (None, 272, 272, 3))\n and then you have to give it bacth of images which has to be of shape (bacth_number, 272, 272, 3)", "I would suggest you following code for reading the image ... take this code as an example", "<code>Code Snippet</code>.", "and when running it in the session:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2115, 24501, 3550, 1035, 3746, 8023, 2003, 1037, 23435, 2004, 2017, 2031, 3988, 3550, 2009, 2000, 1056, 2546, 1012, 3746, 1012, 24501, 4697, 1035, 4871, 1006, 3746, 1010, 1031, 24231, 1010, 24231, 1033, 1007, 1012, 1012, 1012, 2115, 5438, 2038, 2000, 2022, 1037, 16371, 8737, 2100, 9140, 3005, 4338, 2038, 2000, 2674, 1996, 23435, 1060, 2008, 2017, 2031, 4225, 1999, 1996, 3642, 2005, 2742, 2065, 1999, 2115, 2553, 1060, 1027, 1056, 2546, 1012, 2173, 14528, 1006, 1056, 2546, 1012, 14257, 16703, 1010, 1006, 3904, 1010, 24231, 1010, 24231, 1010, 1017, 1007, 1007, 1998, 2059, 2017, 2031, 2000, 2507, 2009, 8670, 6593, 2232, 1997, 4871, 2029, 2038, 2000, 2022, 1997, 4338, 1006, 8670, 6593, 2232, 1035, 2193, 1010, 24231, 1010, 24231, 1010, 1017, 1007, 102, 101, 1045, 2052, 6592, 2017, 2206, 3642, 2005, 3752, 1996, 3746, 1012, 1012, 1012, 2202, 2023, 3642, 2004, 2019, 2742, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1998, 2043, 2770, 2009, 1999, 1996, 5219, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 125, 146, 160, 170, 184], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51749250", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [28, 32]}, {"sent_id": 1, "name": "tf.data", "pos": [64, 68]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [9, 15]}], [{"sent_id": 0, "name": "tf.contrib.data", "pos": [9, 17]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [28, 35]}, {"sent_id": 1, "name": "tf.data.dataset", "pos": [64, 71]}], [{"sent_id": 0, "name": "tf.contrib.data.parallel_interleave", "pos": [9, 23]}]], "sents": ["The return value of the function passed to tf.contrib.data.parallel_interleave() must be a tf.data.Dataset.", "Therefore you can solve this by attaching the filename tensor to each element of the TFRecordDataset, using tf.data.Dataset.zip() as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2709, 3643, 1997, 1996, 3853, 2979, 2000, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2951, 1012, 5903, 1035, 6970, 19738, 3726, 1006, 1007, 2442, 2022, 1037, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 102, 101, 3568, 2017, 2064, 9611, 2023, 2011, 22476, 2075, 1996, 5371, 18442, 23435, 2000, 2169, 5783, 1997, 1996, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1010, 2478, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 14101, 1006, 1007, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 37, 79, 93], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49138172", "vertexSet": [[{"sent_id": 8, "name": "tf.nn", "pos": [190, 195]}], [{"sent_id": 4, "name": "tf.losses", "pos": [103, 107]}], [{"sent_id": 1, "name": "tf.contrib", "pos": [11, 17]}, {"sent_id": 8, "name": "tf.contrib", "pos": [211, 217]}], [{"sent_id": 1, "name": "tf.contrib.seq2seq", "pos": [11, 23]}, {"sent_id": 8, "name": "tf.contrib.seq2seq", "pos": [211, 223]}], [{"sent_id": 4, "name": "tf.losses.softmax_cross_entropy", "pos": [103, 114]}], [{"sent_id": 1, "name": "tf.contrib.seq2seq.sequence_loss", "pos": [11, 27]}, {"sent_id": 8, "name": "tf.contrib.seq2seq.sequence_loss", "pos": [211, 227]}], [{"sent_id": 8, "name": "tf.nn.sparse_softmax_cross_entropy_with_logits", "pos": [190, 209]}]], "sents": ["This can't be done efficiently.", "tf.contrib.seq2seq.sequence_loss is designed to work with very large vocabularies, hence it's expecting a loss function from sparse family (see this question for details).", "The main difference is that labels use ordinal encoding instead of one-hot, because the latter takes too much memory.", "Actual one-hot encoding is never computed.", "label_smoothing parameter of tf.losses.softmax_cross_entropy on the other hand is an option to manipulate the one-hot encoding.", "Here's what it does:", "<code>Code Snippet</code>.", "As you can see, to compute this tensor, onehot_labels must be stored explicitly, which is exactly what sparse functions try to avoid.", "That's why neither tf.nn.sparse_softmax_cross_entropy_with_logits, nor tf.contrib.seq2seq.sequence_loss provide a similar parameter.", "Of course, you can do the conversion yourself, but this defeats the whole optimization."], "sent_idxs": [101, 2023, 2064, 1005, 1056, 2022, 2589, 18228, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 5537, 1035, 3279, 2003, 2881, 2000, 2147, 2007, 2200, 2312, 29536, 3540, 28808, 3111, 1010, 6516, 2009, 1005, 1055, 8074, 1037, 3279, 3853, 2013, 20288, 2155, 1006, 2156, 2023, 3160, 2005, 4751, 1007, 1012, 102, 101, 1996, 2364, 4489, 2003, 2008, 10873, 2224, 2030, 18979, 2140, 17181, 2612, 1997, 2028, 1011, 2980, 1010, 2138, 1996, 3732, 3138, 2205, 2172, 3638, 1012, 102, 101, 5025, 2028, 1011, 2980, 17181, 2003, 2196, 24806, 1012, 102, 101, 3830, 1035, 27045, 16381, 1997, 1056, 2546, 1012, 6409, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 2006, 1996, 2060, 2192, 2003, 2019, 5724, 2000, 17708, 1996, 2028, 1011, 2980, 17181, 1012, 102, 101, 2182, 1005, 1055, 2054, 2009, 2515, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2004, 2017, 2064, 2156, 1010, 2000, 24134, 2023, 23435, 1010, 2028, 12326, 1035, 10873, 2442, 2022, 8250, 12045, 1010, 2029, 2003, 3599, 2054, 20288, 4972, 3046, 2000, 4468, 1012, 102, 101, 2008, 1005, 1055, 2339, 4445, 1056, 2546, 1012, 1050, 2078, 1012, 20288, 1035, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1010, 4496, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7367, 4160, 2475, 3366, 4160, 1012, 5537, 1035, 3279, 3073, 1037, 2714, 16381, 1012, 102, 101, 1997, 2607, 1010, 2017, 2064, 2079, 1996, 7584, 4426, 1010, 2021, 2023, 14222, 1996, 2878, 20600, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 10, 59, 86, 97, 130, 139, 153, 184, 233, 252], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47917849", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [19, 23]}], [{"sent_id": 1, "name": "tf.errors", "pos": [42, 46]}], [{"sent_id": 1, "name": "tf.errors.outofrangeerror", "pos": [42, 52]}], [{"sent_id": 0, "name": "tf.train.monitoredtrainingsession", "pos": [19, 29]}]], "sents": ["If your tensorflow version is 1.3+, I recommend the high-level API tf.train.MonitoredTrainingSession.", "The sess created by this API can automatically detect tf.errors.OutOfRangeError with sess.should_stop().", "For most of training situations, you need to shuffle data and get a batch each step, I have added these in the following code.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2065, 2115, 23435, 12314, 2544, 2003, 1015, 1012, 1017, 1009, 1010, 1045, 16755, 1996, 2152, 1011, 2504, 17928, 1056, 2546, 1012, 3345, 1012, 17785, 23654, 8613, 7971, 3258, 1012, 102, 101, 1996, 7367, 4757, 2580, 2011, 2023, 17928, 2064, 8073, 11487, 1056, 2546, 1012, 10697, 1012, 2041, 11253, 24388, 11510, 29165, 2007, 7367, 4757, 1012, 2323, 1035, 2644, 1006, 1007, 1012, 102, 101, 2005, 2087, 1997, 2731, 8146, 1010, 2017, 2342, 2000, 23046, 2951, 1998, 2131, 1037, 14108, 2169, 3357, 1010, 1045, 2031, 2794, 2122, 1999, 1996, 2206, 3642, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 31, 63, 92, 106], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49177382", "vertexSet": [[{"sent_id": 1, "name": "tf.nn", "pos": [16, 21]}, {"sent_id": 2, "name": "tf.nn", "pos": [45, 50]}], [{"sent_id": 2, "name": "tf.layers", "pos": [32, 36]}], [{"sent_id": 1, "name": "tf.nn.dropout", "pos": [16, 24]}, {"sent_id": 2, "name": "tf.nn.dropout", "pos": [45, 53]}], [{"sent_id": 2, "name": "tf.layers.dropout", "pos": [32, 39]}], [{"sent_id": 6, "name": "tf.set_random_seed", "pos": [125, 133]}]], "sents": ["There are two primary ways to perform dropout in tensorflow:", "tf.nn.dropout (low-level).", "tf.layers.dropout (high-level, uses tf.nn.dropout under the hood).", "Both functions accept a seed parameter that is used to generate the random mask.", "By default, seed=None, which means random seed, i.e.", "non-deterministic.", "In order to make the result deterministic, you either set the seed on per-op level or call tf.set_random_seed (sets the the graph-level random seed) or, better, both.", "Example:", "<code>Code Snippet</code>.", "Caveat: in general, there are other sources in randomness in the training scripts, so you have to set also pure python seed (random.seed) and numpy seed (numpy.random.seed)."], "sent_idxs": [101, 2045, 2024, 2048, 3078, 3971, 2000, 4685, 4530, 5833, 1999, 23435, 12314, 1024, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 1006, 2659, 1011, 2504, 1007, 1012, 102, 101, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 1006, 2152, 1011, 2504, 1010, 3594, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 2104, 1996, 7415, 1007, 1012, 102, 101, 2119, 4972, 5138, 1037, 6534, 16381, 2008, 2003, 2109, 2000, 9699, 1996, 6721, 7308, 1012, 102, 101, 2011, 12398, 1010, 6534, 1027, 3904, 1010, 2029, 2965, 6721, 6534, 1010, 1045, 1012, 1041, 1012, 102, 101, 2512, 1011, 28283, 25300, 10074, 1012, 102, 101, 1999, 2344, 2000, 2191, 1996, 2765, 28283, 25300, 10074, 1010, 2017, 2593, 2275, 1996, 6534, 2006, 2566, 1011, 6728, 2504, 2030, 2655, 1056, 2546, 1012, 2275, 1035, 6721, 1035, 6534, 1006, 4520, 1996, 1996, 10629, 1011, 2504, 6721, 6534, 1007, 2030, 1010, 2488, 1010, 2119, 1012, 102, 101, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 5430, 4017, 1024, 1999, 2236, 1010, 2045, 2024, 2060, 4216, 1999, 6721, 2791, 1999, 1996, 2731, 14546, 1010, 2061, 2017, 2031, 2000, 2275, 2036, 5760, 18750, 6534, 1006, 6721, 1012, 6534, 1007, 1998, 16371, 8737, 2100, 6534, 1006, 16371, 8737, 2100, 1012, 6721, 1012, 6534, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 15, 31, 59, 76, 94, 102, 150, 154, 168, 217], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45308609", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.conv2d", "pos": [26, 36]}, {"sent_id": 6, "name": "tf.nn.conv2d", "pos": [83, 93]}], [{"sent_id": 4, "name": "tf.layers.conv2d", "pos": [54, 63]}, {"sent_id": 6, "name": "tf.layers.conv2d", "pos": [125, 134]}]], "sents": ["As GBY mentioned, they use the same implementation.", "There is a slight difference in the parameters.", "For tf.nn.conv2d:", "<code>Code Snippet</code>.", "For tf.layers.conv2d:", "<code>Code Snippet</code>.", "I would use tf.nn.conv2d when loading a pretrained model (example code: https://github.com/ry/tensorflow-vgg16), and tf.layers.conv2d for a model trained from scratch."], "sent_idxs": [101, 2004, 16351, 2100, 3855, 1010, 2027, 2224, 1996, 2168, 7375, 1012, 102, 101, 2045, 2003, 1037, 7263, 4489, 1999, 1996, 11709, 1012, 102, 101, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 2052, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2475, 2094, 2043, 10578, 1037, 3653, 23654, 2098, 2944, 1006, 2742, 3642, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 29431, 1013, 23435, 12314, 1011, 1058, 13871, 16048, 1007, 1010, 1998, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2475, 2094, 2005, 1037, 2944, 4738, 2013, 11969, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [6]}, {"r": "S1", "h": 1, "t": 0, "evidence": [6]}], "na_triple": [], "sent_ends": [0, 13, 24, 38, 52, 65, 79, 142], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44746203", "vertexSet": [[{"sent_id": 1, "name": "tf.assign", "pos": [64, 68]}], [{"sent_id": 5, "name": "tf.variables_initializer", "pos": [207, 214]}], [{"sent_id": 2, "name": "tf.global_variables_initializer", "pos": [90, 99]}, {"sent_id": 4, "name": "tf.global_variables_initializer", "pos": [188, 197]}]], "sents": ["In TensorFlow the differences between constants and variables are that when you declare some constant, its value can't be changed in the future (also the initialization should be with a value, not with operation).", "Nevertheless, when you declare a Variable, you can change its value in the future with tf.assign() method (and the initialization can be achieved with a value or operation).", "The function tf.global_variables_initializer() initialises all variables in your code with the value passed as parameter, but it works in async mode, so doesn't work properly when dependencies exists between variables.", "Your first code (#1) works properly because there is no dependencies on variable initialization and the constant is constructed with a value.", "The second code (#2) doesn't work because of the async behavior of tf.global_variables_initializer().", "You can fix it using tf.variables_initializer() as follows:", "<code>Code Snippet</code>.", "The third code (#3) doesn't work properly because you are trying to initialize a constant with an operation, that isn't possible.", "To solve it, an appropriate strategy is (#1).", "Regarding to your last question.", "You need to run (a) session.run(model) when there are variables in your calculation graph (b) print(session.run(y))."], "sent_idxs": [101, 1999, 23435, 12314, 1996, 5966, 2090, 5377, 2015, 1998, 10857, 2024, 2008, 2043, 2017, 13520, 2070, 5377, 1010, 2049, 3643, 2064, 1005, 1056, 2022, 2904, 1999, 1996, 2925, 1006, 2036, 1996, 3988, 3989, 2323, 2022, 2007, 1037, 3643, 1010, 2025, 2007, 3169, 1007, 1012, 102, 101, 6600, 1010, 2043, 2017, 13520, 1037, 8023, 1010, 2017, 2064, 2689, 2049, 3643, 1999, 1996, 2925, 2007, 1056, 2546, 1012, 23911, 1006, 1007, 4118, 1006, 1998, 1996, 3988, 3989, 2064, 2022, 4719, 2007, 1037, 3643, 2030, 3169, 1007, 1012, 102, 101, 1996, 3853, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 3988, 13087, 2035, 10857, 1999, 2115, 3642, 2007, 1996, 3643, 2979, 2004, 16381, 1010, 2021, 2009, 2573, 1999, 2004, 6038, 2278, 5549, 1010, 2061, 2987, 1005, 1056, 2147, 7919, 2043, 12530, 15266, 6526, 2090, 10857, 1012, 102, 101, 2115, 2034, 3642, 1006, 1001, 1015, 1007, 2573, 7919, 2138, 2045, 2003, 2053, 12530, 15266, 2006, 8023, 3988, 3989, 1998, 1996, 5377, 2003, 3833, 2007, 1037, 3643, 1012, 102, 101, 1996, 2117, 3642, 1006, 1001, 1016, 1007, 2987, 1005, 1056, 2147, 2138, 1997, 1996, 2004, 6038, 2278, 5248, 1997, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1012, 102, 101, 2017, 2064, 8081, 2009, 2478, 1056, 2546, 1012, 10857, 1035, 3988, 17629, 1006, 1007, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 2353, 3642, 1006, 1001, 1017, 1007, 2987, 1005, 1056, 2147, 7919, 2138, 2017, 2024, 2667, 2000, 3988, 4697, 1037, 5377, 2007, 2019, 3169, 1010, 2008, 3475, 1005, 1056, 2825, 1012, 102, 101, 2000, 9611, 2009, 1010, 2019, 6413, 5656, 2003, 1006, 1001, 1015, 1007, 1012, 102, 101, 4953, 2000, 2115, 2197, 3160, 1012, 102, 101, 2017, 2342, 2000, 2448, 1006, 1037, 1007, 5219, 1012, 2448, 1006, 2944, 1007, 2043, 2045, 2024, 10857, 1999, 2115, 17208, 10629, 1006, 1038, 1007, 6140, 1006, 5219, 1012, 2448, 1006, 1061, 1007, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 46, 87, 138, 168, 201, 220, 234, 267, 282, 290, 326], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52267911", "vertexSet": [[{"sent_id": 3, "name": "tf.contrib", "pos": [47, 53]}, {"sent_id": 3, "name": "tf.contrib", "pos": [60, 66]}, {"sent_id": 4, "name": "tf.contrib", "pos": [74, 80]}, {"sent_id": 4, "name": "tf.contrib", "pos": [98, 104]}], [{"sent_id": 3, "name": "tf.contrib.slim", "pos": [47, 55]}, {"sent_id": 4, "name": "tf.contrib.slim", "pos": [74, 82]}], [{"sent_id": 3, "name": "tf.contrib.layers", "pos": [60, 68]}, {"sent_id": 4, "name": "tf.contrib.layers", "pos": [98, 106]}]], "sents": ["There is no difference.", "<code>Code Snippet</code>.", "The reason they both exist is likely historical, and to support backwards compatibility - i.e.", "it probably existed in tf.contrib.slim, then was moved to tf.contrib.layers.", "Removing it from tf.contrib.slim would have broken existing models however, so I imagine the code has been ported to tf.contrib.layers and there's a line in slim somewhere that creates an alias - something like", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2003, 2053, 4489, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 3114, 2027, 2119, 4839, 2003, 3497, 3439, 1010, 1998, 2000, 2490, 11043, 21778, 1011, 1045, 1012, 1041, 1012, 102, 101, 2009, 2763, 5839, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 11754, 1010, 2059, 2001, 2333, 2000, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 102, 101, 9268, 2009, 2013, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 11754, 2052, 2031, 3714, 4493, 4275, 2174, 1010, 2061, 1045, 5674, 1996, 3642, 2038, 2042, 27650, 2000, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1998, 2045, 1005, 1055, 1037, 2240, 1999, 11754, 4873, 2008, 9005, 2019, 14593, 1011, 2242, 2066, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 7, 21, 42, 70, 123, 137], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54740065", "vertexSet": [[{"sent_id": 6, "name": "tf.data", "pos": [125, 129]}], [{"sent_id": 1, "name": "tf.split", "pos": [37, 41]}], [{"sent_id": 6, "name": "tf.data.dataset", "pos": [125, 132]}]], "sents": ["I know three ways of feeding data on multi-gpu model.", "if all your inputs are of same shape, you may build placeholder x on CPU, then use tf.split to split x into xs.", "Then on each tower of GPU, get xs[i] as your input..", "<code>Code Snippet</code>.", "if your inputs have different shape, you need to build placeholder x on every GPU with a scope..", "<code>Code Snippet</code>.", "use tf.data.Dataset to feed data.", "google official cifar10_multi_gpu_train.py use Queue, which is similar with this way.."], "sent_idxs": [101, 1045, 2113, 2093, 3971, 1997, 8521, 2951, 2006, 4800, 1011, 14246, 2226, 2944, 1012, 102, 101, 2065, 2035, 2115, 20407, 2024, 1997, 2168, 4338, 1010, 2017, 2089, 3857, 2173, 14528, 1060, 2006, 17368, 1010, 2059, 2224, 1056, 2546, 1012, 3975, 2000, 3975, 1060, 2046, 1060, 2015, 1012, 102, 101, 2059, 2006, 2169, 3578, 1997, 14246, 2226, 1010, 2131, 1060, 2015, 1031, 1045, 1033, 2004, 2115, 7953, 1012, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2115, 20407, 2031, 2367, 4338, 1010, 2017, 2342, 2000, 3857, 2173, 14528, 1060, 2006, 2296, 14246, 2226, 2007, 1037, 9531, 1012, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 2000, 5438, 2951, 1012, 102, 101, 8224, 2880, 25022, 14971, 10790, 1035, 4800, 1035, 14246, 2226, 1035, 3345, 1012, 1052, 2100, 2224, 24240, 1010, 2029, 2003, 2714, 2007, 2023, 2126, 1012, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 16, 49, 70, 84, 109, 123, 137, 165], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "61896836", "vertexSet": [[{"sent_id": 4, "name": "tf.compat.v1.session", "pos": [80, 91]}], [{"sent_id": 4, "name": "tf.session", "pos": [98, 102]}]], "sents": ["Eager Execution is enabled by default in TF 2.0, so you need to call .numpy() on the Tensor object.", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>.", "If Eager Execution is disabled, you can build a graph and then run it through tf.compat.v1.Session in TF 2.x and tf.Session in TF 1.x", "<code>Code Snippet</code>.", "Output:", "<code>Code Snippet</code>."], "sent_idxs": [101, 9461, 7781, 2003, 9124, 2011, 12398, 1999, 1056, 2546, 1016, 1012, 1014, 1010, 2061, 2017, 2342, 2000, 2655, 1012, 16371, 8737, 2100, 1006, 1007, 2006, 1996, 23435, 4874, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 9461, 7781, 2003, 9776, 1010, 2017, 2064, 3857, 1037, 10629, 1998, 2059, 2448, 2009, 2083, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 5219, 1999, 1056, 2546, 1016, 1012, 1060, 1998, 1056, 2546, 1012, 5219, 1999, 1056, 2546, 1015, 1012, 1060, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 6434, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 4]}], "na_triple": [], "sent_ends": [0, 31, 45, 49, 63, 109, 123, 127, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "40596147", "vertexSet": [[{"sent_id": 1, "name": "tf.abs", "pos": [37, 41]}], [{"sent_id": 1, "name": "tf.less", "pos": [47, 51]}, {"sent_id": 1, "name": "tf.less", "pos": [52, 56]}], [{"sent_id": 1, "name": "tf.less_equal", "pos": [52, 58]}], [{"sent_id": 2, "name": "tf.reduce_all", "pos": [72, 78]}, {"sent_id": 3, "name": "tf.reduce_all", "pos": [85, 91]}], [{"sent_id": 4, "name": "tf.reduce_any", "pos": [96, 102]}]], "sents": ["Unfortunately, there are no ops that do exactly the same thing for allclose or isclose, but you can have workarounds.", "isclose: combine tf.abs, tf.sub, tf.less or tf.less_equal.", "allclose: based on isclose, use tf.reduce_all in addition", "all: use tf.reduce_all", "any: use tf.reduce_any"], "sent_idxs": [101, 6854, 1010, 2045, 2024, 2053, 23092, 2008, 2079, 3599, 1996, 2168, 2518, 2005, 2035, 20464, 9232, 2030, 2003, 20464, 9232, 1010, 2021, 2017, 2064, 2031, 2147, 24490, 2015, 1012, 102, 101, 2003, 20464, 9232, 1024, 11506, 1056, 2546, 1012, 14689, 1010, 1056, 2546, 1012, 4942, 1010, 1056, 2546, 1012, 2625, 2030, 1056, 2546, 1012, 2625, 1035, 5020, 1012, 102, 101, 2035, 20464, 9232, 1024, 2241, 2006, 2003, 20464, 9232, 1010, 2224, 1056, 2546, 1012, 5547, 1035, 2035, 1999, 2804, 102, 101, 2035, 1024, 2224, 1056, 2546, 1012, 5547, 1035, 2035, 102, 101, 2151, 1024, 2224, 1056, 2546, 1012, 5547, 1035, 2151, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 31, 60, 81, 92, 103], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0]}, {"title": "57479139", "vertexSet": [[{"sent_id": 1, "name": "tf.gather", "pos": [28, 32]}, {"sent_id": 3, "name": "tf.gather", "pos": [85, 89]}], [{"sent_id": 1, "name": "tf.indexedslices", "pos": [13, 19]}], [{"sent_id": 3, "name": "tf.custom_gradient", "pos": [100, 106]}]], "sents": ["Your gradient seems to be fine.", "TensorFlow uses tf.IndexedSlices to represent sparse gradients in some cases like tf.gather, but you can easily convert it to a regular tensor like this (the example is in graph mode but the function would be the same in eager mode):", "<code>Code Snippet</code>.", "If you want to force tf.gather to produce regular tensors, you can wrap it with tf.custom_gradient like this:", "<code>Code Snippet</code>.", "Note this assumes axis=0 and one-dimensional indices, otherwise it would still be possible to do the same but it would require a bit more of work."], "sent_idxs": [101, 2115, 17978, 3849, 2000, 2022, 2986, 1012, 102, 101, 23435, 12314, 3594, 1056, 2546, 1012, 25331, 14540, 23522, 2000, 5050, 20288, 17978, 2015, 1999, 2070, 3572, 2066, 1056, 2546, 1012, 8587, 1010, 2021, 2017, 2064, 4089, 10463, 2009, 2000, 1037, 3180, 23435, 2066, 2023, 1006, 1996, 2742, 2003, 1999, 10629, 5549, 2021, 1996, 3853, 2052, 2022, 1996, 2168, 1999, 9461, 5549, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 2000, 2486, 1056, 2546, 1012, 8587, 2000, 3965, 3180, 23435, 2015, 1010, 2017, 2064, 10236, 2009, 2007, 1056, 2546, 1012, 7661, 1035, 17978, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2023, 15980, 8123, 1027, 1014, 1998, 2028, 1011, 8789, 29299, 1010, 4728, 2009, 2052, 2145, 2022, 2825, 2000, 2079, 1996, 2168, 2021, 2009, 2052, 5478, 1037, 2978, 2062, 1997, 2147, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 65, 79, 110, 124, 158], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47793706", "vertexSet": [[{"sent_id": 0, "name": "tf.graph", "pos": [14, 18]}], [{"sent_id": 7, "name": "tf.contrib", "pos": [175, 181]}], [{"sent_id": 0, "name": "tf.session", "pos": [20, 24]}, {"sent_id": 0, "name": "tf.session", "pos": [36, 40]}, {"sent_id": 5, "name": "tf.session", "pos": [156, 160]}], [{"sent_id": 3, "name": "tf.constant", "pos": [101, 105]}], [{"sent_id": 5, "name": "tf.placeholder", "pos": [144, 149]}], [{"sent_id": 4, "name": "tf.convert_to_tensor", "pos": [127, 135]}], [{"sent_id": 7, "name": "tf.contrib.factorization", "pos": [175, 184]}], [{"sent_id": 7, "name": "tf.contrib.factorization.kmeansclustering", "pos": [175, 190]}]], "sents": ["The KMeansClustering Estimator API builds its own tf.Graph and manage tf.Session by itself, so you don't need to run a tf.Session to feed values (that is done by input_fn), that's why the ValueError arise.", "The correct usage of KMeansClustering Estimator is just:", "<code>Code Snippet</code>.", "where X is a tf.constant input tensor that holds the values (e.g.", "define X as np.array and than use tf.convert_to_tensor).", "Here X is not a tf.placeholder that needs to be feed at a tf.Session run.", "Update for TensorFlow 1.4:", "Use tf.contrib.factorization.KMeansClustering API to find cluster centers:", "<code>Code Snippet</code>.", "To predict centers for given features just use:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2463, 11219, 11020, 7393, 17989, 9765, 9581, 4263, 17928, 16473, 2049, 2219, 1056, 2546, 1012, 10629, 1998, 6133, 1056, 2546, 1012, 5219, 2011, 2993, 1010, 2061, 2017, 2123, 1005, 1056, 2342, 2000, 2448, 1037, 1056, 2546, 1012, 5219, 2000, 5438, 5300, 1006, 2008, 2003, 2589, 2011, 7953, 1035, 1042, 2078, 1007, 1010, 2008, 1005, 1055, 2339, 1996, 3643, 2121, 29165, 13368, 1012, 102, 101, 1996, 6149, 8192, 1997, 2463, 11219, 11020, 7393, 17989, 9765, 9581, 4263, 2003, 2074, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2073, 1060, 2003, 1037, 1056, 2546, 1012, 5377, 7953, 23435, 2008, 4324, 1996, 5300, 1006, 1041, 1012, 1043, 1012, 102, 101, 9375, 1060, 2004, 27937, 1012, 9140, 1998, 2084, 2224, 1056, 2546, 1012, 10463, 1035, 2000, 1035, 23435, 1007, 1012, 102, 101, 2182, 1060, 2003, 2025, 1037, 1056, 2546, 1012, 2173, 14528, 2008, 3791, 2000, 2022, 5438, 2012, 1037, 1056, 2546, 1012, 5219, 2448, 1012, 102, 101, 10651, 2005, 23435, 12314, 1015, 1012, 1018, 1024, 102, 101, 2224, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 5387, 3989, 1012, 2463, 11219, 11020, 7393, 17989, 17928, 2000, 2424, 9324, 6401, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 16014, 6401, 2005, 2445, 2838, 2074, 2224, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6]], "sent_ends": [0, 65, 82, 96, 117, 138, 163, 173, 197, 211, 222, 236], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49477712", "vertexSet": [[{"sent_id": 8, "name": "tf.nn", "pos": [229, 234]}, {"sent_id": 9, "name": "tf.nn", "pos": [267, 272]}], [{"sent_id": 1, "name": "tf.layers", "pos": [20, 24]}, {"sent_id": 8, "name": "tf.layers", "pos": [246, 250]}, {"sent_id": 10, "name": "tf.layers", "pos": [313, 317]}], [{"sent_id": 8, "name": "tf.nn.conv1d", "pos": [229, 239]}, {"sent_id": 9, "name": "tf.nn.conv1d", "pos": [267, 277]}], [{"sent_id": 1, "name": "tf.layers.conv1d", "pos": [20, 29]}, {"sent_id": 8, "name": "tf.layers.conv1d", "pos": [246, 255]}]], "sents": ["I was wondering the same thing and your question motivated me to look into this.", "Calling tf.layers.conv1d actually results in a long stream of convolution-related classes/methods being called (ops using ops using ops...).", "To be precise (skip this if not interested):", "layers.conv1d uses layers._Conv (base class for all conv layers)..", "layers._Conv calls nn_ops.Convolution as its _convolution_op..", "nn_ops.Convolution uses _WithSpaceToBatch as its conv_op..", "_WithSpaceToBatch gets _NonAtrousConvolution as its build_op..", "_NonAtrousConvoluton will (in the 1D case) use conv1d as its conv_op (to be precise it uses self._conv1d which calls conv1d).", "This is the tf.nn.conv1d which is of course very different from tf.layers.conv1d we called in the beginning..", "As noted, tf.nn.conv1d produces the deprecation warning..", "I strongly suspect that somewhere along this chain, the conversion from the \"user friendly\" channels_first/last (used by the tf.layers interface) to the more generic format such as NCHW/NHWC (used by the lower-level ops) wasn't updated properly to using NCW/NWC for 1D convolutions.", "The short and boring answer: See this github issue.", "Apparently this will not yet be fixed in TF 1.7."], "sent_idxs": [101, 1045, 2001, 6603, 1996, 2168, 2518, 1998, 2115, 3160, 12774, 2033, 2000, 2298, 2046, 2023, 1012, 102, 101, 4214, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 2941, 3463, 1999, 1037, 2146, 5460, 1997, 9530, 6767, 7630, 3508, 1011, 3141, 4280, 1013, 4725, 2108, 2170, 1006, 23092, 2478, 23092, 2478, 23092, 1012, 1012, 1012, 1007, 1012, 102, 101, 2000, 2022, 10480, 1006, 13558, 2023, 2065, 2025, 4699, 1007, 1024, 102, 101, 9014, 1012, 9530, 2615, 2487, 2094, 3594, 9014, 1012, 1035, 9530, 2615, 1006, 2918, 2465, 2005, 2035, 9530, 2615, 9014, 1007, 1012, 1012, 102, 101, 9014, 1012, 1035, 9530, 2615, 4455, 1050, 2078, 1035, 23092, 1012, 9530, 6767, 7630, 3508, 2004, 2049, 1035, 9530, 6767, 7630, 3508, 1035, 6728, 1012, 1012, 102, 101, 1050, 2078, 1035, 23092, 1012, 9530, 6767, 7630, 3508, 3594, 1035, 2007, 23058, 3406, 14479, 2818, 2004, 2049, 9530, 2615, 1035, 6728, 1012, 1012, 102, 101, 1035, 2007, 23058, 3406, 14479, 2818, 4152, 1035, 2512, 4017, 13288, 8663, 6767, 7630, 3508, 2004, 2049, 3857, 1035, 6728, 1012, 1012, 102, 101, 1035, 2512, 4017, 13288, 8663, 6767, 7630, 2669, 2097, 1006, 1999, 1996, 1015, 2094, 2553, 1007, 2224, 9530, 2615, 2487, 2094, 2004, 2049, 9530, 2615, 1035, 6728, 1006, 2000, 2022, 10480, 2009, 3594, 2969, 1012, 1035, 9530, 2615, 2487, 2094, 2029, 4455, 9530, 2615, 2487, 2094, 1007, 1012, 102, 101, 2023, 2003, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2487, 2094, 2029, 2003, 1997, 2607, 2200, 2367, 2013, 1056, 2546, 1012, 9014, 1012, 9530, 2615, 2487, 2094, 2057, 2170, 1999, 1996, 2927, 1012, 1012, 102, 101, 2004, 3264, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 9530, 2615, 2487, 2094, 7137, 1996, 2139, 28139, 10719, 5432, 1012, 1012, 102, 101, 1045, 6118, 8343, 2008, 4873, 2247, 2023, 4677, 1010, 1996, 7584, 2013, 1996, 1000, 5310, 5379, 1000, 6833, 1035, 2034, 1013, 2197, 1006, 2109, 2011, 1996, 1056, 2546, 1012, 9014, 8278, 1007, 2000, 1996, 2062, 12391, 4289, 2107, 2004, 13316, 2232, 2860, 1013, 18699, 16526, 1006, 2109, 2011, 1996, 2896, 1011, 2504, 23092, 1007, 2347, 1005, 1056, 7172, 7919, 2000, 2478, 13316, 2860, 1013, 22064, 2278, 2005, 1015, 2094, 9530, 6767, 7630, 9285, 1012, 102, 101, 1996, 2460, 1998, 11771, 3437, 1024, 2156, 2023, 21025, 2705, 12083, 3277, 1012, 102, 101, 4593, 2023, 2097, 2025, 2664, 2022, 4964, 1999, 1056, 2546, 1015, 1012, 1021, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 18, 59, 72, 97, 125, 151, 175, 225, 263, 286, 362, 377, 393], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44273882", "vertexSet": [[{"sent_id": 7, "name": "tf.train", "pos": [273, 277]}], [{"sent_id": 1, "name": "tf.group", "pos": [79, 83]}], [{"sent_id": 7, "name": "tf.estimator", "pos": [242, 248]}, {"sent_id": 7, "name": "tf.estimator", "pos": [287, 293]}], [{"sent_id": 0, "name": "tf.saved_model", "pos": [23, 29]}, {"sent_id": 4, "name": "tf.saved_model", "pos": [158, 164]}], [{"sent_id": 7, "name": "tf.train.scaffold", "pos": [273, 281]}], [{"sent_id": 0, "name": "tf.saved_model.builder", "pos": [23, 31]}], [{"sent_id": 4, "name": "tf.saved_model.main_op", "pos": [158, 168]}], [{"sent_id": 7, "name": "tf.estimator.estimatorspec", "pos": [287, 299]}], [{"sent_id": 4, "name": "tf.saved_model.main_op.main_op", "pos": [158, 172]}], [{"sent_id": 0, "name": "tf.saved_model.builder.savedmodelbuilder", "pos": [23, 38]}]], "sents": ["You can specify an \"initialization\" operation when you add a meta graph to your SavedModel bundle with tf.saved_model.builder.SavedModelBuilder.add_meta_graph, using the main_op or legacy_init_op kwarg.", "You can either use a single operation, or group together a number of operations with tf.group if you need more than one.", "Note that in Cloud ML Engine, You'll have to use the legacy_init_op.", "However in future runtime_versions you will be able to use main_op \n(IIRC, starting with runtime_version == 1.2)", "The saved_model module provides a built in tf.saved_model.main_op.main_op  to wrap up common initialization actions in a single op (local variable initialization, and table initialization).", "So in summary, code should look like this (adapted from this example):", "<code>Code Snippet</code>.", "NOTE: If you are using the high level libraries (such as tf.estimator) this should be the default, and if you need to specify additional initialization actions you can specify them as part of the tf.train.Scaffold object that you pass to your tf.estimator.EstimatorSpec in your model_fn."], "sent_idxs": [101, 2017, 2064, 20648, 2019, 1000, 3988, 3989, 1000, 3169, 2043, 2017, 5587, 1037, 18804, 10629, 2000, 2115, 5552, 5302, 9247, 14012, 2007, 1056, 2546, 1012, 5552, 1035, 2944, 1012, 12508, 1012, 5552, 5302, 9247, 8569, 23891, 2099, 1012, 5587, 1035, 18804, 1035, 10629, 1010, 2478, 1996, 2364, 1035, 6728, 2030, 8027, 1035, 1999, 4183, 1035, 6728, 6448, 2906, 2290, 1012, 102, 101, 2017, 2064, 2593, 2224, 1037, 2309, 3169, 1010, 2030, 2177, 2362, 1037, 2193, 1997, 3136, 2007, 1056, 2546, 1012, 2177, 2065, 2017, 2342, 2062, 2084, 2028, 1012, 102, 101, 3602, 2008, 1999, 6112, 19875, 3194, 1010, 2017, 1005, 2222, 2031, 2000, 2224, 1996, 8027, 1035, 1999, 4183, 1035, 6728, 1012, 102, 101, 2174, 1999, 2925, 2448, 7292, 1035, 4617, 2017, 2097, 2022, 2583, 2000, 2224, 2364, 1035, 6728, 1006, 2462, 11890, 1010, 3225, 2007, 2448, 7292, 1035, 2544, 1027, 1027, 1015, 1012, 1016, 1007, 102, 101, 1996, 5552, 1035, 2944, 11336, 3640, 1037, 2328, 1999, 1056, 2546, 1012, 5552, 1035, 2944, 1012, 2364, 1035, 6728, 1012, 2364, 1035, 6728, 2000, 10236, 2039, 2691, 3988, 3989, 4506, 1999, 1037, 2309, 6728, 1006, 2334, 8023, 3988, 3989, 1010, 1998, 2795, 3988, 3989, 1007, 1012, 102, 101, 2061, 1999, 12654, 1010, 3642, 2323, 2298, 2066, 2023, 1006, 5967, 2013, 2023, 2742, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 1024, 2065, 2017, 2024, 2478, 1996, 2152, 2504, 8860, 1006, 2107, 2004, 1056, 2546, 1012, 9765, 9581, 4263, 1007, 2023, 2323, 2022, 1996, 12398, 1010, 1998, 2065, 2017, 2342, 2000, 20648, 3176, 3988, 3989, 4506, 2017, 2064, 20648, 2068, 2004, 2112, 1997, 1996, 1056, 2546, 1012, 3345, 1012, 8040, 10354, 10371, 4874, 2008, 2017, 3413, 2000, 2115, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 9765, 9581, 6591, 5051, 2278, 1999, 2115, 2944, 1035, 1042, 2078, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [5, 8], [5, 9], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [6, 8], [6, 9], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 8], [7, 9], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 9], [9, 0], [9, 1], [9, 2], [9, 3], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8]], "sent_ends": [0, 62, 91, 114, 148, 196, 214, 228, 307], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51088806", "vertexSet": [[{"sent_id": 1, "name": "tf.add", "pos": [29, 33]}], [{"sent_id": 1, "name": "tf.matmul", "pos": [34, 40]}], [{"sent_id": 3, "name": "tf.session", "pos": [85, 89]}]], "sents": ["You should not create any operations while executing the graph!", "Each time when you call apply_feature_extraction you put a new operation tf.add(tf.matmul(...) to your graph.", "As a result your graph gets bloated.", "First, create a fully defined graph that contains all variables and operations you need and then just execute ops within a tf.Session that are defined in the graph.", "In your case that might look like this:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2323, 2025, 3443, 2151, 3136, 2096, 23448, 1996, 10629, 999, 102, 101, 2169, 2051, 2043, 2017, 2655, 6611, 1035, 3444, 1035, 14676, 2017, 2404, 1037, 2047, 3169, 1056, 2546, 1012, 5587, 1006, 1056, 2546, 1012, 13523, 12274, 2140, 1006, 1012, 1012, 1012, 1007, 2000, 2115, 10629, 1012, 102, 101, 2004, 1037, 2765, 2115, 10629, 4152, 1038, 4135, 4383, 1012, 102, 101, 2034, 1010, 3443, 1037, 3929, 4225, 10629, 2008, 3397, 2035, 10857, 1998, 3136, 2017, 2342, 1998, 2059, 2074, 15389, 23092, 2306, 1037, 1056, 2546, 1012, 5219, 2008, 2024, 4225, 1999, 1996, 10629, 1012, 102, 101, 1999, 2115, 2553, 2008, 2453, 2298, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 13, 50, 62, 97, 108, 122], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60732212", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [14, 19]}], [{"sent_id": 0, "name": "tf.keras.callbacks", "pos": [14, 22]}], [{"sent_id": 0, "name": "tf.keras.callbacks.callback", "pos": [14, 25]}]], "sents": ["You can achieve this functionality by creating a class which sub-classes tf.keras.callbacks.Callback and use the object of that class as callback to model.fit.", "<code>Code Snippet</code>.", "Then you can load the csv file and plot model's loss, learning rate, metrics, etc.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 6162, 2023, 15380, 2011, 4526, 1037, 2465, 2029, 4942, 1011, 4280, 1056, 2546, 1012, 17710, 8180, 1012, 2655, 12221, 1012, 2655, 5963, 1998, 2224, 1996, 4874, 1997, 2008, 2465, 2004, 2655, 5963, 2000, 2944, 1012, 4906, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 2017, 2064, 7170, 1996, 20116, 2615, 5371, 1998, 5436, 2944, 1005, 1055, 3279, 1010, 4083, 3446, 1010, 12046, 2015, 1010, 4385, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 41, 55, 80, 94], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43007712", "vertexSet": [[{"sent_id": 1, "name": "tf.fill", "pos": [48, 52]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [25, 29]}, {"sent_id": 1, "name": "tf.tensor", "pos": [68, 72]}], [{"sent_id": 0, "name": "tf.constant", "pos": [6, 10]}]], "sents": ["The shape argument of the tf.constant() op expects a static shape, so you can't use a tf.Tensor as part of the argument.", "Fortunately there is another op that will suffice: tf.fill(), which allows the shape (its dims argument) to be a tf.Tensor.", "This means you can define bias as:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 4338, 6685, 1997, 1996, 1056, 2546, 1012, 5377, 1006, 1007, 6728, 24273, 1037, 10763, 4338, 1010, 2061, 2017, 2064, 1005, 1056, 2224, 1037, 1056, 2546, 1012, 23435, 2004, 2112, 1997, 1996, 6685, 1012, 102, 101, 14599, 2045, 2003, 2178, 6728, 2008, 2097, 10514, 26989, 3401, 1024, 1056, 2546, 1012, 6039, 1006, 1007, 1010, 2029, 4473, 1996, 4338, 1006, 2049, 11737, 2015, 6685, 1007, 2000, 2022, 1037, 1056, 2546, 1012, 23435, 1012, 102, 101, 2023, 2965, 2017, 2064, 9375, 13827, 2004, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 36, 74, 84, 98], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55628261", "vertexSet": [[{"sent_id": 4, "name": "tf.train", "pos": [60, 64]}], [{"sent_id": 0, "name": "tf.variable", "pos": [16, 20]}], [{"sent_id": 4, "name": "tf.graphkeys", "pos": [85, 91]}], [{"sent_id": 4, "name": "tf.train.optimizer", "pos": [60, 68]}]], "sents": ["I don't think it is possible to modify trainable attribute of the tf.Variable.", "However, there are multiple workarounds.", "Suppose you have two variables:", "<code>Code Snippet</code>.", "When you are using tf.train.Optimizer class and its sub-classes to optimize, by default it takes variables from tf.GraphKeys.TRAINABLE_VARIABLES collection.", "Every variable that you define with trainable=True is added to this collection by default.", "What you can do is to clear this collection and append to it only those variables that you're willing to optimize.", "For example, if I want to optimize only v1 but not v2:", "<code>Code Snippet</code>.", "Another way is to use var_list keyword argument of the optimizer and pass those variables you want to be updated during training (during execution of the train_op):", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2123, 1005, 1056, 2228, 2009, 2003, 2825, 2000, 19933, 3345, 3085, 17961, 1997, 1996, 1056, 2546, 1012, 8023, 1012, 102, 101, 2174, 1010, 2045, 2024, 3674, 2147, 24490, 2015, 1012, 102, 101, 6814, 2017, 2031, 2048, 10857, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2043, 2017, 2024, 2478, 1056, 2546, 1012, 3345, 1012, 23569, 27605, 6290, 2465, 1998, 2049, 4942, 1011, 4280, 2000, 23569, 27605, 4371, 1010, 2011, 12398, 2009, 3138, 10857, 2013, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3345, 3085, 1035, 10857, 3074, 1012, 102, 101, 2296, 8023, 2008, 2017, 9375, 2007, 3345, 3085, 1027, 2995, 2003, 2794, 2000, 2023, 3074, 2011, 12398, 1012, 102, 101, 2054, 2017, 2064, 2079, 2003, 2000, 3154, 2023, 3074, 1998, 10439, 10497, 2000, 2009, 2069, 2216, 10857, 2008, 2017, 1005, 2128, 5627, 2000, 23569, 27605, 4371, 1012, 102, 101, 2005, 2742, 1010, 2065, 1045, 2215, 2000, 23569, 27605, 4371, 2069, 1058, 2487, 2021, 2025, 1058, 2475, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2178, 2126, 2003, 2000, 2224, 13075, 1035, 2862, 3145, 18351, 6685, 1997, 1996, 23569, 27605, 6290, 1998, 3413, 2216, 10857, 2017, 2215, 2000, 2022, 7172, 2076, 2731, 1006, 2076, 7781, 1997, 1996, 3345, 1035, 6728, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 22, 33, 41, 55, 99, 119, 148, 168, 182, 221, 235], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55566010", "vertexSet": [[{"sent_id": 0, "name": "tf.tile", "pos": [4, 8]}], [{"sent_id": 8, "name": "tf.map_fn", "pos": [187, 194]}], [{"sent_id": 0, "name": "tf.concat", "pos": [17, 22]}], [{"sent_id": 0, "name": "tf.expand_dims", "pos": [9, 16]}]], "sents": ["You can use tf.tile and tf.expand_dims with tf.concat.", "An example:", "<code>Code Snippet</code>.", "Performance", "You can use follow code to compare speed.", "<code>Code Snippet</code>.", "The vectorization method 10000 iterations takes 1.48s and the loop 10000 iterations takes 5.76s when a.shape=(2,4,5) and b.shape=(2,3,5) on my 8GB GPU memory.", "But the vectorization method takes 3.28s and the loop time is 317.23s when a.shape=(20,40,5) and b.shape=(20,40,5).", "The vectorization method will be significantly faster than the tf.map_fn() and python loop."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 14090, 1998, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 2007, 1056, 2546, 1012, 9530, 11266, 1012, 102, 101, 2019, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2836, 102, 101, 2017, 2064, 2224, 3582, 3642, 2000, 12826, 3177, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 9207, 3989, 4118, 6694, 2692, 27758, 2015, 3138, 1015, 1012, 4466, 2015, 1998, 1996, 7077, 6694, 2692, 27758, 2015, 3138, 1019, 1012, 6146, 2015, 2043, 1037, 1012, 4338, 1027, 1006, 1016, 1010, 1018, 1010, 1019, 1007, 1998, 1038, 1012, 4338, 1027, 1006, 1016, 1010, 1017, 1010, 1019, 1007, 2006, 2026, 1022, 18259, 14246, 2226, 3638, 1012, 102, 101, 2021, 1996, 9207, 3989, 4118, 3138, 1017, 1012, 2654, 2015, 1998, 1996, 7077, 2051, 2003, 26628, 1012, 2603, 2015, 2043, 1037, 1012, 4338, 1027, 1006, 2322, 1010, 2871, 1010, 1019, 1007, 1998, 1038, 1012, 4338, 1027, 1006, 2322, 1010, 2871, 1010, 1019, 1007, 1012, 102, 101, 1996, 9207, 3989, 4118, 2097, 2022, 6022, 5514, 2084, 1996, 1056, 2546, 1012, 4949, 1035, 1042, 2078, 1006, 1007, 1998, 18750, 7077, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 24, 29, 43, 46, 57, 71, 130, 176, 201], "sent_pos": [0, 0, 0, 0, 1, 1, 1, 1, 0, 4, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50337385", "vertexSet": [[{"sent_id": 1, "name": "tf.data.dataset.from_generator", "pos": [68, 79]}], [{"sent_id": 0, "name": "tf.data.dataset.from_tensor_slices", "pos": [6, 19]}], [{"sent_id": 0, "name": "tf.tensor", "pos": [34, 38]}], [{"sent_id": 0, "name": "tf.sparsetensor", "pos": [40, 46]}]], "sents": ["As you have noticed, tf.data.Dataset.from_tensor_slices() only works on objects that can be converted to a (dense) tf.Tensor or a tf.SparseTensor.", "The easiest way to get variable-length NumPy data into a Dataset is to use tf.data.Dataset.from_generator(), as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2004, 2017, 2031, 4384, 1010, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 23435, 1035, 25609, 1006, 1007, 2069, 2573, 2006, 5200, 2008, 2064, 2022, 4991, 2000, 1037, 1006, 9742, 1007, 1056, 2546, 1012, 23435, 2030, 1037, 1056, 2546, 1012, 20288, 25808, 2953, 1012, 102, 101, 1996, 25551, 2126, 2000, 2131, 8023, 1011, 3091, 16371, 8737, 2100, 2951, 2046, 1037, 2951, 13462, 2003, 2000, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 13103, 1006, 1007, 1010, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [0, 3], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 48, 86, 100], "sent_pos": [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47916567", "vertexSet": [[{"sent_id": 1, "name": "tf.nn.dropout", "pos": [22, 30]}], [{"sent_id": 1, "name": "tf.layers.dropout", "pos": [36, 43]}]], "sents": ["Well, I cannot make a comment...", "The only thing came to my mind is that tf.nn.dropout uses keep_prob while tf.layers.dropout uses dropout rate.", "Please check if this rate is too high."], "sent_idxs": [101, 2092, 1010, 1045, 3685, 2191, 1037, 7615, 1012, 1012, 1012, 102, 101, 1996, 2069, 2518, 2234, 2000, 2026, 2568, 2003, 2008, 1056, 2546, 1012, 1050, 2078, 1012, 4530, 5833, 3594, 2562, 1035, 4013, 2497, 2096, 1056, 2546, 1012, 9014, 1012, 4530, 5833, 3594, 4530, 5833, 3446, 1012, 102, 101, 3531, 4638, 2065, 2023, 3446, 2003, 2205, 2152, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [1]}], "na_triple": [], "sent_ends": [0, 12, 49, 60], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54082302", "vertexSet": [[{"sent_id": 2, "name": "tf.keras", "pos": [67, 72]}], [{"sent_id": 2, "name": "tf.keras.backend", "pos": [67, 75]}], [{"sent_id": 2, "name": "tf.keras.backend.function", "pos": [67, 77]}]], "sents": ["Using model.layer[1].output does not produce an output, it simply returns the tensor definition of the output.", "In order to actually produce an output, you need to run your data through the model and specify model.layer[1].output as the output.", "You can do this by using tf.keras.backend.function (documentation), which will return Numpy arrays.", "A similar question to yours can be found here.", "The following should work for your example if you only want the output from model.layers[1].output and if you convert your data to a Numpy array for input:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2478, 2944, 1012, 6741, 1031, 1015, 1033, 1012, 6434, 2515, 2025, 3965, 2019, 6434, 1010, 2009, 3432, 5651, 1996, 23435, 6210, 1997, 1996, 6434, 1012, 102, 101, 1999, 2344, 2000, 2941, 3965, 2019, 6434, 1010, 2017, 2342, 2000, 2448, 2115, 2951, 2083, 1996, 2944, 1998, 20648, 2944, 1012, 6741, 1031, 1015, 1033, 1012, 6434, 2004, 1996, 6434, 1012, 102, 101, 2017, 2064, 2079, 2023, 2011, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 3853, 1006, 12653, 1007, 1010, 2029, 2097, 2709, 16371, 8737, 2100, 27448, 1012, 102, 101, 1037, 2714, 3160, 2000, 6737, 2064, 2022, 2179, 2182, 1012, 102, 101, 1996, 2206, 2323, 2147, 2005, 2115, 2742, 2065, 2017, 2069, 2215, 1996, 6434, 2013, 2944, 1012, 9014, 1031, 1015, 1033, 1012, 6434, 1998, 2065, 2017, 10463, 2115, 2951, 2000, 1037, 16371, 8737, 2100, 9140, 2005, 7953, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 27, 60, 90, 102, 141, 155], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45154875", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [7, 12]}], [{"sent_id": 2, "name": "tf.py_func", "pos": [54, 62]}], [{"sent_id": 0, "name": "tf.nn.sigmoid", "pos": [7, 16]}]], "sents": ["Why cant you use the tf.nn.sigmoid() function?.", "<code>Code Snippet</code>.", "If you want to call a numpy function in the graph, you can use tf.py_func (The code will be executed in CPU only):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2339, 2064, 2102, 2017, 2224, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1006, 1007, 3853, 1029, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 2215, 2000, 2655, 1037, 16371, 8737, 2100, 3853, 1999, 1996, 10629, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 1052, 2100, 1035, 4569, 2278, 1006, 1996, 3642, 2097, 2022, 6472, 1999, 17368, 2069, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 36, 74, 88], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41900893", "vertexSet": [[{"sent_id": 5, "name": "tf.nn", "pos": [77, 82]}], [{"sent_id": 8, "name": "tf.exp", "pos": [193, 198]}], [{"sent_id": 7, "name": "tf.pow", "pos": [140, 144]}], [{"sent_id": 7, "name": "tf.reduce_sum", "pos": [133, 139]}], [{"sent_id": 8, "name": "tf.reciprocal", "pos": [186, 190]}], [{"sent_id": 5, "name": "tf.nn.sigmoid_cross_entropy_with_logits", "pos": [77, 95]}]], "sents": ["Problem: your gradients are zero, therefore your weights doesn't change.", "You provide single dimension (batch_size, 1) to softmax.", "This makes output of softmax a constant (1).", "This makes it's gradient be zero.", "Solutions:", "If you're doing a logistic regression, please use tf.nn.sigmoid_cross_entropy_with_logits(y_values, y_)", "If you're doing a linear regression, please use (i.e.", "don't use softmax):\ncost = tf.reduce_sum(tf.pow(y_ - y_values, 2))/(2*n_samples)", "If you insist on mixing softmax and MSE, please use following instead of softmax:\ny = tf.reciprocal(1 + tf.exp(-y_values))"], "sent_idxs": [101, 3291, 1024, 2115, 17978, 2015, 2024, 5717, 1010, 3568, 2115, 15871, 2987, 1005, 1056, 2689, 1012, 102, 101, 2017, 3073, 2309, 9812, 1006, 14108, 1035, 2946, 1010, 1015, 1007, 2000, 3730, 17848, 1012, 102, 101, 2023, 3084, 6434, 1997, 3730, 17848, 1037, 5377, 1006, 1015, 1007, 1012, 102, 101, 2023, 3084, 2009, 1005, 1055, 17978, 2022, 5717, 1012, 102, 101, 7300, 1024, 102, 101, 2065, 2017, 1005, 2128, 2725, 1037, 8833, 6553, 26237, 1010, 3531, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 9033, 21693, 9314, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1061, 1035, 5300, 1010, 1061, 1035, 1007, 102, 101, 2065, 2017, 1005, 2128, 2725, 1037, 7399, 26237, 1010, 3531, 2224, 1006, 1045, 1012, 1041, 1012, 102, 101, 2123, 1005, 1056, 2224, 3730, 17848, 1007, 1024, 3465, 1027, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1056, 2546, 1012, 23776, 1006, 1061, 1035, 1011, 1061, 1035, 5300, 1010, 1016, 1007, 1007, 1013, 1006, 1016, 1008, 1050, 1035, 8168, 1007, 102, 101, 2065, 2017, 18292, 2006, 6809, 3730, 17848, 1998, 5796, 2063, 1010, 3531, 2224, 2206, 2612, 1997, 3730, 17848, 1024, 1061, 1027, 1056, 2546, 1012, 28309, 1006, 1015, 1009, 1056, 2546, 1012, 4654, 2361, 1006, 1011, 1061, 1035, 5300, 1007, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 18, 35, 49, 60, 64, 104, 122, 164, 206], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62153886", "vertexSet": [[{"sent_id": 6, "name": "tf.keras", "pos": [143, 148]}], [{"sent_id": 1, "name": "tf.reduce_mean", "pos": [40, 46]}], [{"sent_id": 6, "name": "tf.keras.backend", "pos": [143, 151]}], [{"sent_id": 6, "name": "tf.keras.backend.function", "pos": [143, 153]}]], "sents": ["As far as i understand, temp_predict and temp_predict  are numpy arrays.", "So the only way you can end up with tensors is because you are using tf.reduce_mean.", "You can replace it with np.mean.", "This will only  work if dice_coef has no tensorflow ops.", "If it does then you will have to replace them with numpy functions.", "Once you do that, then you wouldn't have to open new sessions.", "And also instead of creating a new model at the end of every epoch (intermediate_layer_model), you can construct a keras function using tf.keras.backend.function more about it here."], "sent_idxs": [101, 2004, 2521, 2004, 1045, 3305, 1010, 8915, 8737, 1035, 16014, 1998, 8915, 8737, 1035, 16014, 2024, 16371, 8737, 2100, 27448, 1012, 102, 101, 2061, 1996, 2069, 2126, 2017, 2064, 2203, 2039, 2007, 23435, 2015, 2003, 2138, 2017, 2024, 2478, 1056, 2546, 1012, 5547, 1035, 2812, 1012, 102, 101, 2017, 2064, 5672, 2009, 2007, 27937, 1012, 2812, 1012, 102, 101, 2023, 2097, 2069, 2147, 2065, 18740, 1035, 24873, 2546, 2038, 2053, 23435, 12314, 23092, 1012, 102, 101, 2065, 2009, 2515, 2059, 2017, 2097, 2031, 2000, 5672, 2068, 2007, 16371, 8737, 2100, 4972, 1012, 102, 101, 2320, 2017, 2079, 2008, 1010, 2059, 2017, 2876, 1005, 1056, 2031, 2000, 2330, 2047, 6521, 1012, 102, 101, 1998, 2036, 2612, 1997, 4526, 1037, 2047, 2944, 2012, 1996, 2203, 1997, 2296, 25492, 1006, 7783, 1035, 6741, 1035, 2944, 1007, 1010, 2017, 2064, 9570, 1037, 17710, 8180, 3853, 2478, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 3853, 2062, 2055, 2009, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 23, 48, 59, 76, 94, 112, 159], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0]}, {"title": "57571405", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [12, 17]}, {"sent_id": 1, "name": "tf.keras", "pos": [32, 37]}, {"sent_id": 1, "name": "tf.keras", "pos": [40, 45]}, {"sent_id": 3, "name": "tf.keras", "pos": [93, 98]}, {"sent_id": 8, "name": "tf.keras", "pos": [229, 234]}, {"sent_id": 13, "name": "tf.keras", "pos": [346, 351]}, {"sent_id": 13, "name": "tf.keras", "pos": [379, 384]}], [{"sent_id": 12, "name": "tf.session", "pos": [321, 325]}], [{"sent_id": 0, "name": "tf.keras.input", "pos": [12, 19]}, {"sent_id": 1, "name": "tf.keras.input", "pos": [40, 47]}, {"sent_id": 8, "name": "tf.keras.input", "pos": [229, 236]}, {"sent_id": 13, "name": "tf.keras.input", "pos": [346, 353]}], [{"sent_id": 0, "name": "tf.placeholder", "pos": [6, 11]}, {"sent_id": 1, "name": "tf.placeholder", "pos": [62, 67]}, {"sent_id": 7, "name": "tf.placeholder", "pos": [192, 197]}, {"sent_id": 7, "name": "tf.placeholder", "pos": [205, 210]}, {"sent_id": 9, "name": "tf.placeholder", "pos": [266, 271]}, {"sent_id": 10, "name": "tf.placeholder", "pos": [277, 282]}, {"sent_id": 12, "name": "tf.placeholder", "pos": [333, 338]}]], "sents": ["You can't mix tf.placeholder and tf.keras.Input.", "In other words, if you want to use the tf.keras API then use tf.keras.Input, or if you want to use the tf native API the go with tf.placeholder.", "In addition, your choice will reflect other parts of your code.", "Assuming that you want to go with the tf.keras API, the approach you should partake is the following:", "<code>Code Snippet</code>.", "Please note that this approach is relevant if we consider that you use TF 1.14 as you mentioned in the comments.", "On TF 2.0, this is less complicated and more intuitive.", "On the other hand, if you want to stick to the TF 1.14 native API, and use tf.placeholder, then you should build the graph using tf.placeholder as nodes that you will use to feed the data.", "Moreover, regarding your question whether tf.keras.Input returns a placeholder - It does return a placeholder node, that you can use to feed your data.", "But it does not return a tf.placeholder.", "The usage of tf.placeholder is the following:", "<code>Code Snippet</code>.", "As you can see, a static graph is created, and then its nodes are executed with a tf.Session while feeding data in the graph using the tf.placeholder.", "On the other hand, tf.keras.Input serves the same purpose (That is why in the documentation is called a placeholder), but its use case is relevant with the tf.keras API and not with the tf native API."], "sent_idxs": [101, 2017, 2064, 1005, 1056, 4666, 1056, 2546, 1012, 2173, 14528, 1998, 1056, 2546, 1012, 17710, 8180, 1012, 7953, 1012, 102, 101, 1999, 2060, 2616, 1010, 2065, 2017, 2215, 2000, 2224, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 2059, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 7953, 1010, 2030, 2065, 2017, 2215, 2000, 2224, 1996, 1056, 2546, 3128, 17928, 1996, 2175, 2007, 1056, 2546, 1012, 2173, 14528, 1012, 102, 101, 1999, 2804, 1010, 2115, 3601, 2097, 8339, 2060, 3033, 1997, 2115, 3642, 1012, 102, 101, 10262, 2008, 2017, 2215, 2000, 2175, 2007, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 1010, 1996, 3921, 2017, 2323, 2112, 13808, 2003, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3531, 3602, 2008, 2023, 3921, 2003, 7882, 2065, 2057, 5136, 2008, 2017, 2224, 1056, 2546, 1015, 1012, 2403, 2004, 2017, 3855, 1999, 1996, 7928, 1012, 102, 101, 2006, 1056, 2546, 1016, 1012, 1014, 1010, 2023, 2003, 2625, 8552, 1998, 2062, 29202, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 2065, 2017, 2215, 2000, 6293, 2000, 1996, 1056, 2546, 1015, 1012, 2403, 3128, 17928, 1010, 1998, 2224, 1056, 2546, 1012, 2173, 14528, 1010, 2059, 2017, 2323, 3857, 1996, 10629, 2478, 1056, 2546, 1012, 2173, 14528, 2004, 14164, 2008, 2017, 2097, 2224, 2000, 5438, 1996, 2951, 1012, 102, 101, 9308, 1010, 4953, 2115, 3160, 3251, 1056, 2546, 1012, 17710, 8180, 1012, 7953, 5651, 1037, 2173, 14528, 1011, 2009, 2515, 2709, 1037, 2173, 14528, 13045, 1010, 2008, 2017, 2064, 2224, 2000, 5438, 2115, 2951, 1012, 102, 101, 2021, 2009, 2515, 2025, 2709, 1037, 1056, 2546, 1012, 2173, 14528, 1012, 102, 101, 1996, 8192, 1997, 1056, 2546, 1012, 2173, 14528, 2003, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2004, 2017, 2064, 2156, 1010, 1037, 10763, 10629, 2003, 2580, 1010, 1998, 2059, 2049, 14164, 2024, 6472, 2007, 1037, 1056, 2546, 1012, 5219, 2096, 8521, 2951, 1999, 1996, 10629, 2478, 1996, 1056, 2546, 1012, 2173, 14528, 1012, 102, 101, 2006, 1996, 2060, 2192, 1010, 1056, 2546, 1012, 17710, 8180, 1012, 7953, 4240, 1996, 2168, 3800, 1006, 2008, 2003, 2339, 1999, 1996, 12653, 2003, 2170, 1037, 2173, 14528, 1007, 1010, 2021, 2049, 2224, 2553, 2003, 7882, 2007, 1996, 1056, 2546, 1012, 17710, 8180, 17928, 1998, 2025, 2007, 1996, 1056, 2546, 3128, 17928, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 21, 69, 84, 111, 125, 152, 169, 222, 259, 273, 287, 301, 340, 395], "sent_pos": [0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45174251", "vertexSet": [[{"sent_id": 2, "name": "tf.summary", "pos": [62, 66]}], [{"sent_id": 0, "name": "tf.gather_nd", "pos": [4, 11]}], [{"sent_id": 2, "name": "tf.summary.image", "pos": [62, 68]}]], "sents": ["You can use tf.gather_nd(), but you will need to modify the shapes of the palette and logits to obtain the desired image, for example:", "<code>Code Snippet</code>.", "The resulting tensor can be directly fed to a tf.summary.image(), but depending on your implementation you should upsample it before the summary."], "sent_idxs": [101, 2017, 2064, 2224, 1056, 2546, 1012, 8587, 1035, 1050, 2094, 1006, 1007, 1010, 2021, 2017, 2097, 2342, 2000, 19933, 1996, 10466, 1997, 1996, 27396, 1998, 8833, 12762, 2000, 6855, 1996, 9059, 3746, 1010, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 4525, 23435, 2064, 2022, 3495, 7349, 2000, 1037, 1056, 2546, 1012, 12654, 1012, 3746, 1006, 1007, 1010, 2021, 5834, 2006, 2115, 7375, 2017, 2323, 11139, 16613, 2571, 2009, 2077, 1996, 12654, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 38, 52, 87], "sent_pos": [0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "46566904", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [1, 7]}, {"sent_id": 6, "name": "tf.contrib", "pos": [174, 180]}], [{"sent_id": 0, "name": "tf.contrib.learn", "pos": [1, 9]}], [{"sent_id": 6, "name": "tf.contrib.layers", "pos": [174, 182]}], [{"sent_id": 0, "name": "tf.contrib.learn.dnnregressor", "pos": [1, 16]}], [{"sent_id": 6, "name": "tf.contrib.layers.fully_connected", "pos": [174, 186]}]], "sents": ["tf.contrib.learn.DNNRegressor hides too many details, which is great if everything's working right away, but pretty frustrating when it needs some debugging.", "For example, it's fairly possible that the learning rate is too large.", "You won't see the learning rate in your code, because it's chosen by DNNRegressor.", "By default, it's 0.05, which is reasonable for many applications, but may be too large in your particular case.", "I suggest you instantiate the optimizer yourself AdagradOptimizer(learning_rate) and pass it to DNNRegressor.", "It may be also possible that initial weights are too big.", "DNNRegressor uses tf.contrib.layers.fully_connected layers without overriding the \nweights_initializer and biases_initializer.", "Just as before, the default values are pretty reasonable, but if you want it to be different, you simply have no control over it.", "What I usually do to check if the neural network is at least somehow working is reduce the training set to few examples and try to overfit the neural network.", "This experiment is very fast, so I can try various learning rates and other hyperparameters to find a sweet spot, and then move on to a bigger dataset.", "Further troubleshooting: visualize the distributions of activations in each layer, the distribution of gradients or weights in tensorboard to narrow down the problem."], "sent_idxs": [101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 4553, 1012, 1040, 10695, 2890, 17603, 24137, 2099, 17382, 2205, 2116, 4751, 1010, 2029, 2003, 2307, 2065, 2673, 1005, 1055, 2551, 2157, 2185, 1010, 2021, 3492, 25198, 2043, 2009, 3791, 2070, 2139, 8569, 12588, 1012, 102, 101, 2005, 2742, 1010, 2009, 1005, 1055, 7199, 2825, 2008, 1996, 4083, 3446, 2003, 2205, 2312, 1012, 102, 101, 2017, 2180, 1005, 1056, 2156, 1996, 4083, 3446, 1999, 2115, 3642, 1010, 2138, 2009, 1005, 1055, 4217, 2011, 1040, 10695, 2890, 17603, 24137, 2099, 1012, 102, 101, 2011, 12398, 1010, 2009, 1005, 1055, 1014, 1012, 5709, 1010, 2029, 2003, 9608, 2005, 2116, 5097, 1010, 2021, 2089, 2022, 2205, 2312, 1999, 2115, 3327, 2553, 1012, 102, 101, 1045, 6592, 2017, 7107, 13143, 1996, 23569, 27605, 6290, 4426, 15262, 16307, 7361, 3775, 4328, 6290, 1006, 4083, 1035, 3446, 1007, 1998, 3413, 2009, 2000, 1040, 10695, 2890, 17603, 24137, 2099, 1012, 102, 101, 2009, 2089, 2022, 2036, 2825, 2008, 3988, 15871, 2024, 2205, 2502, 1012, 102, 101, 1040, 10695, 2890, 17603, 24137, 2099, 3594, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 3929, 1035, 4198, 9014, 2302, 2058, 21930, 1996, 15871, 1035, 3988, 17629, 1998, 13827, 2229, 1035, 3988, 17629, 1012, 102, 101, 2074, 2004, 2077, 1010, 1996, 12398, 5300, 2024, 3492, 9608, 1010, 2021, 2065, 2017, 2215, 2009, 2000, 2022, 2367, 1010, 2017, 3432, 2031, 2053, 2491, 2058, 2009, 1012, 102, 101, 2054, 1045, 2788, 2079, 2000, 4638, 2065, 1996, 15756, 2897, 2003, 2012, 2560, 5064, 2551, 2003, 5547, 1996, 2731, 2275, 2000, 2261, 4973, 1998, 3046, 2000, 2058, 8873, 2102, 1996, 15756, 2897, 1012, 102, 101, 2023, 7551, 2003, 2200, 3435, 1010, 2061, 1045, 2064, 3046, 2536, 4083, 6165, 1998, 2060, 23760, 28689, 22828, 2015, 2000, 2424, 1037, 4086, 3962, 1010, 1998, 2059, 2693, 2006, 2000, 1037, 7046, 2951, 13462, 1012, 102, 101, 2582, 13460, 23416, 2075, 1024, 5107, 4697, 1996, 20611, 1997, 13791, 2015, 1999, 2169, 6741, 1010, 1996, 4353, 1997, 17978, 2015, 2030, 15871, 1999, 23435, 6277, 2000, 4867, 2091, 1996, 3291, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 44, 62, 89, 118, 152, 166, 203, 233, 268, 305, 339], "sent_pos": [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60283571", "vertexSet": [[{"sent_id": 14, "name": "tf.io", "pos": [394, 398]}], [{"sent_id": 12, "name": "tf.data", "pos": [348, 352]}], [{"sent_id": 2, "name": "tf.train", "pos": [71, 75]}, {"sent_id": 3, "name": "tf.train", "pos": [83, 87]}, {"sent_id": 4, "name": "tf.train", "pos": [141, 145]}], [{"sent_id": 14, "name": "tf.io.parse_tensor", "pos": [394, 403]}], [{"sent_id": 2, "name": "tf.train.floatlist", "pos": [71, 78]}, {"sent_id": 3, "name": "tf.train.floatlist", "pos": [83, 90]}, {"sent_id": 4, "name": "tf.train.floatlist", "pos": [141, 148]}], [{"sent_id": 12, "name": "tf.data.tfrecorddataset", "pos": [348, 360]}]], "sents": ["The function _floats_feature described in the Tensorflow-Guide expects a scalar (either float32 or float64) as input.", "<code>Code Snippet</code>.", "As you can see the inputted scalar is written into a list (value=[value]) which is subsequently given to tf.train.FloatList as input.", "tf.train.FloatList expects an iterator that outputs a float in each iteration (as the list does).", "If your feature is not a scalar but a vectur, _float_feature can be rewritten to pass the iterator directly to tf.train.FloatList (instead of putting it into a list first).", "<code>Code Snippet</code>.", "However if your feature has two or more dimensions this solution does not work anymore.", "Like @mmry described in his answer in this case flattening your feature or splitting it into several one-dimensional features would be a solution.", "The disadvantage of these two approaches is that the information about the actual shape of the feature is lost if no further effort is invested.", "Another possibility to write an example message for a higher dimensional array is to convert the array into a byte string and then use the _bytes_feature function described in the Tensorflow-Guide to write the example message for it.", "The example message is then serialized and written into a TFRecord file.", "<code>Code Snippet</code>.", "The serialized example messages stored in the TFRecord file can be accessed via tf.data.TFRecordDataset.", "After the example messages have been parsed, the original array needs to be restored from the byte string it was converted to.", "This is possible via tf.io.parse_tensor.", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 3853, 1035, 24885, 1035, 3444, 2649, 1999, 1996, 23435, 12314, 1011, 5009, 24273, 1037, 26743, 2099, 1006, 2593, 14257, 16703, 2030, 14257, 21084, 1007, 2004, 7953, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2004, 2017, 2064, 2156, 1996, 7953, 3064, 26743, 2099, 2003, 2517, 2046, 1037, 2862, 1006, 3643, 1027, 1031, 3643, 1033, 1007, 2029, 2003, 3525, 2445, 2000, 1056, 2546, 1012, 3345, 1012, 14257, 9863, 2004, 7953, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 14257, 9863, 24273, 2019, 2009, 6906, 4263, 2008, 27852, 1037, 14257, 1999, 2169, 27758, 1006, 2004, 1996, 2862, 2515, 1007, 1012, 102, 101, 2065, 2115, 3444, 2003, 2025, 1037, 26743, 2099, 2021, 1037, 2310, 6593, 3126, 1010, 1035, 14257, 1035, 3444, 2064, 2022, 2128, 15773, 2000, 3413, 1996, 2009, 6906, 4263, 3495, 2000, 1056, 2546, 1012, 3345, 1012, 14257, 9863, 1006, 2612, 1997, 5128, 2009, 2046, 1037, 2862, 2034, 1007, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 2065, 2115, 3444, 2038, 2048, 2030, 2062, 9646, 2023, 5576, 2515, 2025, 2147, 4902, 1012, 102, 101, 2066, 1030, 3461, 2854, 2649, 1999, 2010, 3437, 1999, 2023, 2553, 4257, 6528, 2075, 2115, 3444, 2030, 14541, 2009, 2046, 2195, 2028, 1011, 8789, 2838, 2052, 2022, 1037, 5576, 1012, 102, 101, 1996, 20502, 1997, 2122, 2048, 8107, 2003, 2008, 1996, 2592, 2055, 1996, 5025, 4338, 1997, 1996, 3444, 2003, 2439, 2065, 2053, 2582, 3947, 2003, 11241, 1012, 102, 101, 2178, 6061, 2000, 4339, 2019, 2742, 4471, 2005, 1037, 3020, 8789, 9140, 2003, 2000, 10463, 1996, 9140, 2046, 1037, 24880, 5164, 1998, 2059, 2224, 1996, 1035, 27507, 1035, 3444, 3853, 2649, 1999, 1996, 23435, 12314, 1011, 5009, 2000, 4339, 1996, 2742, 4471, 2005, 2009, 1012, 102, 101, 1996, 2742, 4471, 2003, 2059, 27289, 1998, 2517, 2046, 1037, 1056, 19699, 8586, 8551, 5371, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 27289, 2742, 7696, 8250, 1999, 1996, 1056, 19699, 8586, 8551, 5371, 2064, 2022, 11570, 3081, 1056, 2546, 1012, 2951, 1012, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1012, 102, 101, 2044, 1996, 2742, 7696, 2031, 2042, 11968, 6924, 1010, 1996, 2434, 9140, 3791, 2000, 2022, 5854, 2013, 1996, 24880, 5164, 2009, 2001, 4991, 2000, 1012, 102, 101, 2023, 2003, 2825, 3081, 1056, 2546, 1012, 22834, 1012, 11968, 3366, 1035, 23435, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 30, 44, 82, 110, 160, 174, 192, 224, 252, 299, 317, 331, 362, 389, 405, 419], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34258214", "vertexSet": [[{"sent_id": 1, "name": "tf.train", "pos": [38, 42]}, {"sent_id": 4, "name": "tf.train", "pos": [153, 157]}, {"sent_id": 8, "name": "tf.train", "pos": [265, 269]}], [{"sent_id": 4, "name": "tf.train.shuffle_batch", "pos": [153, 161]}], [{"sent_id": 8, "name": "tf.train.start_queue_runners", "pos": [265, 275]}], [{"sent_id": 1, "name": "tf.train.string_input_producer", "pos": [38, 48]}]], "sents": ["Yes - what you want is to use a combination of two things.", "First, randomly shuffle the order in which you input your datafiles, by reading from them using a tf.train.string_input_producer with shuffle=True that feeds into whatever input method you use (if you can put your examples into tf.Example proto format, that's easy to use with parse_example).", "To be very clear, you put the list of filenames in the string_input_producer and then read them with another method such as read_file, etc.", "Second, you need to mix at a finer granularity.", "You can accomplish this by feeding the input examples into a tf.train.shuffle_batch node with a large capacity and large value of min_after_dequeue.", "One particularly nice way is to use a shuffle_batch_join that receives input from multiple files, so that you get a lot of mixing.", "Set the capacity of the batch big enough to mix well without exhausting your RAM.", "Tens of thousands of examples usually works pretty well.", "Keep in mind that the batch functions add a QueueRunner to the QUEUE_RUNNERS collection, so you need to run tf.train.start_queue_runners()"], "sent_idxs": [101, 2748, 1011, 2054, 2017, 2215, 2003, 2000, 2224, 1037, 5257, 1997, 2048, 2477, 1012, 102, 101, 2034, 1010, 18154, 23046, 1996, 2344, 1999, 2029, 2017, 7953, 2115, 2951, 8873, 4244, 1010, 2011, 3752, 2013, 2068, 2478, 1037, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 2007, 23046, 1027, 2995, 2008, 14172, 2046, 3649, 7953, 4118, 2017, 2224, 1006, 2065, 2017, 2064, 2404, 2115, 4973, 2046, 1056, 2546, 1012, 2742, 15053, 4289, 1010, 2008, 1005, 1055, 3733, 2000, 2224, 2007, 11968, 3366, 1035, 2742, 1007, 1012, 102, 101, 2000, 2022, 2200, 3154, 1010, 2017, 2404, 1996, 2862, 1997, 5371, 18442, 2015, 1999, 1996, 5164, 1035, 7953, 1035, 3135, 1998, 2059, 3191, 2068, 2007, 2178, 4118, 2107, 2004, 3191, 1035, 5371, 1010, 4385, 1012, 102, 101, 2117, 1010, 2017, 2342, 2000, 4666, 2012, 1037, 26954, 12604, 7934, 3012, 1012, 102, 101, 2017, 2064, 14570, 2023, 2011, 8521, 1996, 7953, 4973, 2046, 1037, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 13045, 2007, 1037, 2312, 3977, 1998, 2312, 3643, 1997, 8117, 1035, 2044, 1035, 2139, 4226, 5657, 1012, 102, 101, 2028, 3391, 3835, 2126, 2003, 2000, 2224, 1037, 23046, 1035, 14108, 1035, 3693, 2008, 8267, 7953, 2013, 3674, 6764, 1010, 2061, 2008, 2017, 2131, 1037, 2843, 1997, 6809, 1012, 102, 101, 2275, 1996, 3977, 1997, 1996, 14108, 2502, 2438, 2000, 4666, 2092, 2302, 15095, 2075, 2115, 8223, 1012, 102, 101, 15295, 1997, 5190, 1997, 4973, 2788, 2573, 3492, 2092, 1012, 102, 101, 2562, 1999, 2568, 2008, 1996, 14108, 4972, 5587, 1037, 24240, 23195, 2000, 1996, 24240, 1035, 7190, 3074, 1010, 2061, 2017, 2342, 2000, 2448, 1056, 2546, 1012, 3345, 1012, 2707, 1035, 24240, 1035, 7190, 1006, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 16, 89, 126, 141, 179, 210, 229, 241, 278], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0]}, {"title": "56678835", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [6, 11]}, {"sent_id": 1, "name": "tf.keras", "pos": [27, 32]}, {"sent_id": 4, "name": "tf.keras", "pos": [63, 68]}, {"sent_id": 4, "name": "tf.keras", "pos": [72, 77]}], [{"sent_id": 4, "name": "tf.keras.layers", "pos": [72, 79]}], [{"sent_id": 4, "name": "tf.keras.sequential", "pos": [63, 70]}], [{"sent_id": 4, "name": "tf.keras.layers.dense", "pos": [72, 81]}]], "sents": ["The error is not with tf.keras.", "In your model definition, you use layers from keras and not tf.keras.", "That's why your'e getting the error when you remove the import.", "Replace your model definition with:", "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])", "Now, you do not need to import keras from tensorflow."], "sent_idxs": [101, 1996, 7561, 2003, 2025, 2007, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 1999, 2115, 2944, 6210, 1010, 2017, 2224, 9014, 2013, 17710, 8180, 1998, 2025, 1056, 2546, 1012, 17710, 8180, 1012, 102, 101, 2008, 1005, 1055, 2339, 2115, 1005, 1041, 2893, 1996, 7561, 2043, 2017, 6366, 1996, 12324, 1012, 102, 101, 5672, 2115, 2944, 6210, 2007, 1024, 102, 101, 2944, 1027, 1056, 2546, 1012, 17710, 8180, 1012, 25582, 1006, 1031, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 1006, 3197, 1027, 1015, 1010, 7953, 1035, 4338, 1027, 1031, 1015, 1033, 1007, 1033, 1007, 102, 101, 2085, 1010, 2017, 2079, 2025, 2342, 2000, 12324, 17710, 8180, 2013, 23435, 12314, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 13, 34, 52, 60, 97, 113], "sent_pos": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "60097784", "vertexSet": [[{"sent_id": 4, "name": "tf.tanh", "pos": [144, 149]}], [{"sent_id": 1, "name": "tf.concat", "pos": [35, 40]}], [{"sent_id": 0, "name": "tf.layers", "pos": [6, 10]}, {"sent_id": 6, "name": "tf.layers", "pos": [172, 176]}]], "sents": ["First thing, for the tf.layers.dot to work, both inputs should have the same shape.", "To perform a concatenation, you can use tf.concat([h_t, h_s]).", "The new shape will depend on the axis over which the concatenation is performed.", "Lets suppose that both h_t and h_s have the shape [a, b], if the concatenation is done over the axis 0, then the new shape would be [2a, b] and if it is done over the axis 1, the resulting shape would be [a, 2b]", "Then you can apply the tf.tanh to the input or create a customize layer that does it for you.", "Update:", "Since the tf.layers.dot is performed over 3d data who happen not to match on the second axis (axis = 1), the concatenation can be done only on that axis and the resulting shape would be [ 1, 10 + 12, 64 ]"], "sent_idxs": [101, 2034, 2518, 1010, 2005, 1996, 1056, 2546, 1012, 9014, 1012, 11089, 2000, 2147, 1010, 2119, 20407, 2323, 2031, 1996, 2168, 4338, 1012, 102, 101, 2000, 4685, 1037, 9530, 16280, 9323, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 9530, 11266, 1006, 1031, 1044, 1035, 1056, 1010, 1044, 1035, 1055, 1033, 1007, 1012, 102, 101, 1996, 2047, 4338, 2097, 12530, 2006, 1996, 8123, 2058, 2029, 1996, 9530, 16280, 9323, 2003, 2864, 1012, 102, 101, 11082, 6814, 2008, 2119, 1044, 1035, 1056, 1998, 1044, 1035, 1055, 2031, 1996, 4338, 1031, 1037, 1010, 1038, 1033, 1010, 2065, 1996, 9530, 16280, 9323, 2003, 2589, 2058, 1996, 8123, 1014, 1010, 2059, 1996, 2047, 4338, 2052, 2022, 1031, 23409, 1010, 1038, 1033, 1998, 2065, 2009, 2003, 2589, 2058, 1996, 8123, 1015, 1010, 1996, 4525, 4338, 2052, 2022, 1031, 1037, 1010, 1016, 2497, 1033, 102, 101, 2059, 2017, 2064, 6611, 1996, 1056, 2546, 1012, 9092, 2232, 2000, 1996, 7953, 2030, 3443, 1037, 7661, 4697, 6741, 2008, 2515, 2009, 2005, 2017, 1012, 102, 101, 10651, 1024, 102, 101, 2144, 1996, 1056, 2546, 1012, 9014, 1012, 11089, 2003, 2864, 2058, 7605, 2951, 2040, 4148, 2025, 2000, 2674, 2006, 1996, 2117, 8123, 1006, 8123, 1027, 1015, 1007, 1010, 1996, 9530, 16280, 9323, 2064, 2022, 2589, 2069, 2006, 2008, 8123, 1998, 1996, 4525, 4338, 2052, 2022, 1031, 1015, 1010, 2184, 1009, 2260, 1010, 4185, 1033, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 24, 53, 72, 138, 165, 169, 225], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "54673843", "vertexSet": [[{"sent_id": 3, "name": "tf.estimator", "pos": [57, 63]}], [{"sent_id": 3, "name": "tf.estimator.inputs", "pos": [57, 65]}], [{"sent_id": 3, "name": "tf.estimator.inputs.pandas_input_fn", "pos": [57, 73]}]], "sents": ["You need to convert your ydata Dataframe to pandas.Series", "<code>Code Snippet</code>.", "Checked with random data, it's working.", "I'm actually surprised, it look like in newer TF versions tf.estimator.inputs.pandas_input_fn does not accept dataframe as labels."], "sent_idxs": [101, 2017, 2342, 2000, 10463, 2115, 21076, 6790, 2951, 15643, 2000, 25462, 2015, 1012, 2186, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 7039, 2007, 6721, 2951, 1010, 2009, 1005, 1055, 2551, 1012, 102, 101, 1045, 1005, 1049, 2941, 4527, 1010, 2009, 2298, 2066, 1999, 10947, 1056, 2546, 4617, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 20407, 1012, 25462, 2015, 1035, 7953, 1035, 1042, 2078, 2515, 2025, 5138, 2951, 15643, 2004, 10873, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 16, 30, 42, 82], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43730448", "vertexSet": [[{"sent_id": 7, "name": "tf.train", "pos": [132, 136]}, {"sent_id": 7, "name": "tf.train", "pos": [144, 148]}], [{"sent_id": 7, "name": "tf.train.adamoptimizer", "pos": [132, 142]}], [{"sent_id": 7, "name": "tf.train.gradientdescentoptimizer", "pos": [144, 156]}]], "sents": ["Here are some suggestions that may improve your accuracy:", "First of all, your hidden layer, which is of size 1024, seems too large.", "This may cause overfitting.", "I would try to reduce it to around 50-100 or so, see whether it improves and continue from there.", "In addition, regarding this line:", "<code>Code Snippet</code>.", "0.5 learning rate might be too high, try to reduce it (to 0.01, 0.001 or so) and see what happens.", "Finally, you can also try to use tf.train.AdamOptimizer instead of tf.train.GradientDescentOptimizer, as in many cases it performs better."], "sent_idxs": [101, 2182, 2024, 2070, 15690, 2008, 2089, 5335, 2115, 10640, 1024, 102, 101, 2034, 1997, 2035, 1010, 2115, 5023, 6741, 1010, 2029, 2003, 1997, 2946, 9402, 2549, 1010, 3849, 2205, 2312, 1012, 102, 101, 2023, 2089, 3426, 2058, 8873, 13027, 1012, 102, 101, 1045, 2052, 3046, 2000, 5547, 2009, 2000, 2105, 2753, 1011, 2531, 2030, 2061, 1010, 2156, 3251, 2009, 24840, 1998, 3613, 2013, 2045, 1012, 102, 101, 1999, 2804, 1010, 4953, 2023, 2240, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1014, 1012, 1019, 4083, 3446, 2453, 2022, 2205, 2152, 1010, 3046, 2000, 5547, 2009, 1006, 2000, 1014, 1012, 5890, 1010, 1014, 1012, 25604, 2030, 2061, 1007, 1998, 2156, 2054, 6433, 1012, 102, 101, 2633, 1010, 2017, 2064, 2036, 3046, 2000, 2224, 1056, 2546, 1012, 3345, 1012, 4205, 7361, 3775, 4328, 6290, 2612, 1997, 1056, 2546, 1012, 3345, 1012, 17978, 6155, 13013, 7361, 3775, 4328, 6290, 1010, 2004, 1999, 2116, 3572, 2009, 10438, 2488, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 12, 33, 42, 67, 76, 90, 123, 166], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56544952", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [9, 14]}, {"sent_id": 3, "name": "tf.keras", "pos": [41, 46]}], [{"sent_id": 1, "name": "tf.keras.backend", "pos": [9, 17]}, {"sent_id": 3, "name": "tf.keras.backend", "pos": [41, 49]}], [{"sent_id": 1, "name": "tf.keras.backend.resize_images", "pos": [9, 22]}, {"sent_id": 3, "name": "tf.keras.backend.resize_images", "pos": [41, 54]}]], "sents": ["The only differences I found:", "tf.keras.backend.resize_images advantages:", "supports different tensor channel orders (see data_format argument).", "tf.keras.backend.resize_images disadvantages:", "can only upsize, but not downsize (as height_factor and width_factor must be a positive integer)."], "sent_idxs": [101, 1996, 2069, 5966, 1045, 2179, 1024, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 24501, 4697, 1035, 4871, 12637, 1024, 102, 101, 6753, 2367, 23435, 3149, 4449, 1006, 2156, 2951, 1035, 4289, 6685, 1007, 1012, 102, 101, 1056, 2546, 1012, 17710, 8180, 1012, 2067, 10497, 1012, 24501, 4697, 1035, 4871, 20502, 2015, 1024, 102, 101, 2064, 2069, 11139, 4697, 1010, 2021, 2025, 12482, 4697, 1006, 2004, 4578, 1035, 5387, 1998, 9381, 1035, 5387, 2442, 2022, 1037, 3893, 16109, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 8, 25, 40, 58, 85], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49495744", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [7, 11]}], [{"sent_id": 2, "name": "tf.cond", "pos": [55, 60]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [7, 14]}]], "sents": ["Absolutely, take note that your tf.data.Dataset pipeline contains both your data and labels.", "Create yourself another .map() function just as you've done.", "It should receive the data and labels, and in there you can use tf.cond to either apply the transformation or not apply it depending on the class of the label.", "Note that you can also transform your label if necessary (it's probably not necessary in the case of classification, but if you had bounding boxes they would need to flip too, for example)."], "sent_idxs": [101, 7078, 1010, 2202, 3602, 2008, 2115, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 13117, 3397, 2119, 2115, 2951, 1998, 10873, 1012, 102, 101, 3443, 4426, 2178, 1012, 4949, 1006, 1007, 3853, 2074, 2004, 2017, 1005, 2310, 2589, 1012, 102, 101, 2009, 2323, 4374, 1996, 2951, 1998, 10873, 1010, 1998, 1999, 2045, 2017, 2064, 2224, 1056, 2546, 1012, 9530, 2094, 2000, 2593, 6611, 1996, 8651, 2030, 2025, 6611, 2009, 5834, 2006, 1996, 2465, 1997, 1996, 3830, 1012, 102, 101, 3602, 2008, 2017, 2064, 2036, 10938, 2115, 3830, 2065, 4072, 1006, 2009, 1005, 1055, 2763, 2025, 4072, 1999, 1996, 2553, 1997, 5579, 1010, 2021, 2065, 2017, 2018, 5391, 2075, 8378, 2027, 2052, 2342, 2000, 11238, 2205, 1010, 2005, 2742, 1007, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 23, 40, 78, 121], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41525110", "vertexSet": [[{"sent_id": 2, "name": "tf.variable", "pos": [49, 53]}], [{"sent_id": 4, "name": "tf.graphkeys", "pos": [119, 125]}], [{"sent_id": 2, "name": "tf.get_variable", "pos": [42, 48]}], [{"sent_id": 4, "name": "tf.get_collection", "pos": [112, 118]}], [{"sent_id": 1, "name": "tf.initialize_all_variables", "pos": [18, 27]}, {"sent_id": 3, "name": "tf.initialize_all_variables", "pos": [73, 82]}, {"sent_id": 5, "name": "tf.initialize_all_variables", "pos": [144, 153]}]], "sents": ["You need to define your init_op (i.e.", "call tf.initialize_all_variables()) after you declared all variables.", "Creating a variable via tf.get_variable or tf.Variable places it in GLOBAL_VARIABLES collection (unless otherwise specified with collections kwarg).", "tf.initialize_all_variables() takes a look at this collection and creates an op that initializes variables listed.", "To see GLOBAL_VARIABLES collection, you can use tf.get_collection with tf.GraphKeys.GLOBAL_VARIABLES as argument.", "TL;DR Place init_op = tf.initialize_all_variables() after the graph was created."], "sent_idxs": [101, 2017, 2342, 2000, 9375, 2115, 1999, 4183, 1035, 6728, 1006, 1045, 1012, 1041, 1012, 102, 101, 2655, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 1007, 2044, 2017, 4161, 2035, 10857, 1012, 102, 101, 4526, 1037, 8023, 3081, 1056, 2546, 1012, 2131, 1035, 8023, 2030, 1056, 2546, 1012, 8023, 3182, 2009, 1999, 3795, 1035, 10857, 3074, 1006, 4983, 4728, 9675, 2007, 6407, 6448, 2906, 2290, 1007, 1012, 102, 101, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 3138, 1037, 2298, 2012, 2023, 3074, 1998, 9005, 2019, 6728, 2008, 3988, 10057, 10857, 3205, 1012, 102, 101, 2000, 2156, 3795, 1035, 10857, 3074, 1010, 2017, 2064, 2224, 1056, 2546, 1012, 2131, 1035, 3074, 2007, 1056, 2546, 1012, 10629, 14839, 2015, 1012, 3795, 1035, 10857, 2004, 6685, 1012, 102, 101, 1056, 2140, 1025, 2852, 2173, 1999, 4183, 1035, 6728, 1027, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 2044, 1996, 10629, 2001, 2580, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 16, 37, 72, 101, 133, 162], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "35110972", "vertexSet": [[{"sent_id": 0, "name": "tf.log", "pos": [8, 12]}], [{"sent_id": 1, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [38, 55]}]], "sents": ["One likely source of trouble is the tf.log(y_conv), which will emit NaN values for any zeroes in y_conv.", "The tf.nn.softmax_cross_entropy_with_logits() operator offers a numerically stable (and more efficient) version of your loss calculation.", "The following should work better:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2028, 3497, 3120, 1997, 4390, 2003, 1996, 1056, 2546, 1012, 8833, 1006, 1061, 1035, 9530, 2615, 1007, 1010, 2029, 2097, 12495, 2102, 16660, 5300, 2005, 2151, 5717, 2229, 1999, 1061, 1035, 9530, 2615, 1012, 102, 101, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 1007, 6872, 4107, 1037, 15973, 2135, 6540, 1006, 1998, 2062, 8114, 1007, 2544, 1997, 2115, 3279, 17208, 1012, 102, 101, 1996, 2206, 2323, 2147, 2488, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 36, 75, 83, 97], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48171297", "vertexSet": [[{"sent_id": 6, "name": "tf.graph", "pos": [175, 179]}], [{"sent_id": 0, "name": "tf.train", "pos": [9, 13]}, {"sent_id": 3, "name": "tf.train", "pos": [89, 93]}, {"sent_id": 6, "name": "tf.train", "pos": [152, 156]}], [{"sent_id": 0, "name": "tf.train.saver", "pos": [9, 16]}, {"sent_id": 3, "name": "tf.train.saver", "pos": [89, 96]}, {"sent_id": 6, "name": "tf.train.saver", "pos": [152, 159]}]], "sents": ["In order to save or restore anything, tf.train.Saver needs a session, and session is bound to a particular graph instance (like in your example).", "This means that the saver is practically meaningless without a session.", "I guess this was the main motivation not to have an explicit graph binding in a saver.", "I think what might be interested to you is defer_build attribute in tf.train.Saver:", "defer_build: If True, defer adding the save and restore ops to the build() call.", "In that case build() should be called before finalizing the graph or using the saver.", "This way you can create a tf.train.Saver that is not bound to any graph and call build() later for a particular tf.Graph instance."], "sent_idxs": [101, 1999, 2344, 2000, 3828, 2030, 9239, 2505, 1010, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 3791, 1037, 5219, 1010, 1998, 5219, 2003, 5391, 2000, 1037, 3327, 10629, 6013, 1006, 2066, 1999, 2115, 2742, 1007, 1012, 102, 101, 2023, 2965, 2008, 1996, 3828, 2099, 2003, 8134, 25120, 2302, 1037, 5219, 1012, 102, 101, 1045, 3984, 2023, 2001, 1996, 2364, 14354, 2025, 2000, 2031, 2019, 13216, 10629, 8031, 1999, 1037, 3828, 2099, 1012, 102, 101, 1045, 2228, 2054, 2453, 2022, 4699, 2000, 2017, 2003, 13366, 2121, 1035, 3857, 17961, 1999, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 1024, 102, 101, 13366, 2121, 1035, 3857, 1024, 2065, 2995, 1010, 13366, 2121, 5815, 1996, 3828, 1998, 9239, 23092, 2000, 1996, 3857, 1006, 1007, 2655, 1012, 102, 101, 1999, 2008, 2553, 3857, 1006, 1007, 2323, 2022, 2170, 2077, 2345, 6026, 1996, 10629, 2030, 2478, 1996, 3828, 2099, 1012, 102, 101, 2023, 2126, 2017, 2064, 3443, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2008, 2003, 2025, 5391, 2000, 2151, 10629, 1998, 2655, 3857, 1006, 1007, 2101, 2005, 1037, 3327, 1056, 2546, 1012, 10629, 6013, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 52, 73, 98, 123, 145, 182], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]}, {"title": "53577920", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [9, 13]}, {"sent_id": 1, "name": "tf.train", "pos": [48, 52]}, {"sent_id": 5, "name": "tf.train", "pos": [145, 149]}, {"sent_id": 5, "name": "tf.train", "pos": [152, 156]}], [{"sent_id": 1, "name": "tf.train.saver", "pos": [48, 55]}, {"sent_id": 5, "name": "tf.train.saver", "pos": [152, 159]}], [{"sent_id": 0, "name": "tf.train.checkpoint", "pos": [9, 15]}, {"sent_id": 5, "name": "tf.train.checkpoint", "pos": [145, 151]}]], "sents": ["According to the new Tensorflow version, tf.train.Checkpoint is the preferable way of saving and restoring a model:", "Checkpoint.save and Checkpoint.restore write and read object-based\n  checkpoints, in contrast to tf.train.Saver which writes and reads\n  variable.name based checkpoints.", "Object-based checkpointing saves a\n  graph of dependencies between Python objects (Layers, Optimizers,\n  Variables, etc.)", "with named edges, and this graph is used to match\n  variables when restoring a checkpoint.", "It can be more robust to\n  changes in the Python program, and helps to support restore-on-create\n  for variables when executing eagerly.", "Prefer tf.train.Checkpoint over\n  tf.train.Saver for new code.", "Here is an example:", "<code>Code Snippet</code>.", "More information and example here."], "sent_idxs": [101, 2429, 2000, 1996, 2047, 23435, 12314, 2544, 1010, 1056, 2546, 1012, 3345, 1012, 26520, 2003, 1996, 9544, 3085, 2126, 1997, 7494, 1998, 16487, 1037, 2944, 1024, 102, 101, 26520, 1012, 3828, 1998, 26520, 1012, 9239, 4339, 1998, 3191, 4874, 1011, 2241, 26520, 2015, 1010, 1999, 5688, 2000, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2029, 7009, 1998, 9631, 8023, 1012, 2171, 2241, 26520, 2015, 1012, 102, 101, 4874, 1011, 2241, 26520, 2075, 13169, 1037, 10629, 1997, 12530, 15266, 2090, 18750, 5200, 1006, 9014, 1010, 23569, 27605, 16750, 1010, 10857, 1010, 4385, 1012, 1007, 102, 101, 2007, 2315, 7926, 1010, 1998, 2023, 10629, 2003, 2109, 2000, 2674, 10857, 2043, 16487, 1037, 26520, 1012, 102, 101, 2009, 2064, 2022, 2062, 15873, 2000, 3431, 1999, 1996, 18750, 2565, 1010, 1998, 7126, 2000, 2490, 9239, 1011, 2006, 1011, 3443, 2005, 10857, 2043, 23448, 17858, 1012, 102, 101, 9544, 1056, 2546, 1012, 3345, 1012, 26520, 2058, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2005, 2047, 3642, 1012, 102, 101, 2182, 2003, 2019, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2062, 2592, 1998, 2742, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 28, 67, 95, 114, 143, 164, 171, 185, 193], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42600540", "vertexSet": [[{"sent_id": 0, "name": "tf.variable", "pos": [8, 12]}], [{"sent_id": 1, "name": "tf.constant", "pos": [38, 42]}, {"sent_id": 4, "name": "tf.constant", "pos": [122, 126]}, {"sent_id": 4, "name": "tf.constant", "pos": [134, 138]}], [{"sent_id": 0, "name": "tf.while_loop", "pos": [18, 24]}], [{"sent_id": 4, "name": "tf.constant_initializer", "pos": [122, 129]}]], "sents": ["This is actually a subtle issue with tf.Variable objects in TensorFlow's tf.while_loop().", "TensorFlow becomes confused because it appears that the tf.constant() with which you're initializing the variable is a value created inside the loop (even though it's clearly loop invariant), but all variables get hoisted outside the loop.", "The easiest resolution is to move the creation of the variable outside the loop:", "<code>Code Snippet</code>.", "(Another possible resolution would be to use a tf.constant_initializer() rather than a tf.constant() when creating the variable.)"], "sent_idxs": [101, 2023, 2003, 2941, 1037, 11259, 3277, 2007, 1056, 2546, 1012, 8023, 5200, 1999, 23435, 12314, 1005, 1055, 1056, 2546, 1012, 2096, 1035, 7077, 1006, 1007, 1012, 102, 101, 23435, 12314, 4150, 5457, 2138, 2009, 3544, 2008, 1996, 1056, 2546, 1012, 5377, 1006, 1007, 2007, 2029, 2017, 1005, 2128, 3988, 6026, 1996, 8023, 2003, 1037, 3643, 2580, 2503, 1996, 7077, 1006, 2130, 2295, 2009, 1005, 1055, 4415, 7077, 23915, 1007, 1010, 2021, 2035, 10857, 2131, 27269, 2648, 1996, 7077, 1012, 102, 101, 1996, 25551, 5813, 2003, 2000, 2693, 1996, 4325, 1997, 1996, 8023, 2648, 1996, 7077, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1006, 2178, 2825, 5813, 2052, 2022, 2000, 2224, 1037, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1006, 1007, 2738, 2084, 1037, 1056, 2546, 1012, 5377, 1006, 1007, 2043, 4526, 1996, 8023, 1012, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 28, 81, 98, 112, 147], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48889633", "vertexSet": [[{"sent_id": 1, "name": "tf.train", "pos": [31, 35]}], [{"sent_id": 1, "name": "tf.train.example", "pos": [31, 37]}], [{"sent_id": 8, "name": "tf.parse_example", "pos": [215, 222]}], [{"sent_id": 8, "name": "tf.decode_json_example", "pos": [192, 202]}]], "sents": ["More or less, but not directly.", "There is an API to feed JSON data, but this must be a JSON mapping of a tf.train.Example protocol buffers object.", "However these objects cannot hold hierarchical data like what you show, only collections of features corresponding to either integers, floats or bytes.", "In your case, you could be something like this:", "<code>Code Snippet</code>.", "Note that strings must be given as arrays of bytes, which need to be encoded in Base64 in the JSON format of protocol buffers.", "However, this does not allow for records with multiple diseases, since, as I said, it does not support hierarchical structure.", "You could have one record per patient and disease instead, for example.", "If you actually get to the point where you have that JSON document as input, you would use tf.decode_json_example to obtain the protocol buffer binary representations of the objects, and then tf.parse_example to actually obtain the tensors."], "sent_idxs": [101, 2062, 2030, 2625, 1010, 2021, 2025, 3495, 1012, 102, 101, 2045, 2003, 2019, 17928, 2000, 5438, 1046, 3385, 2951, 1010, 2021, 2023, 2442, 2022, 1037, 1046, 3385, 12375, 1997, 1037, 1056, 2546, 1012, 3345, 1012, 2742, 8778, 17698, 2015, 4874, 1012, 102, 101, 2174, 2122, 5200, 3685, 2907, 25835, 2951, 2066, 2054, 2017, 2265, 1010, 2069, 6407, 1997, 2838, 7978, 2000, 2593, 24028, 1010, 24885, 2030, 27507, 1012, 102, 101, 1999, 2115, 2553, 1010, 2017, 2071, 2022, 2242, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 7817, 2442, 2022, 2445, 2004, 27448, 1997, 27507, 1010, 2029, 2342, 2000, 2022, 12359, 1999, 2918, 21084, 1999, 1996, 1046, 3385, 4289, 1997, 8778, 17698, 2015, 1012, 102, 101, 2174, 1010, 2023, 2515, 2025, 3499, 2005, 2636, 2007, 3674, 7870, 1010, 2144, 1010, 2004, 1045, 2056, 1010, 2009, 2515, 2025, 2490, 25835, 3252, 1012, 102, 101, 2017, 2071, 2031, 2028, 2501, 2566, 5776, 1998, 4295, 2612, 1010, 2005, 2742, 1012, 102, 101, 2065, 2017, 2941, 2131, 2000, 1996, 2391, 2073, 2017, 2031, 2008, 1046, 3385, 6254, 2004, 7953, 1010, 2017, 2052, 2224, 1056, 2546, 1012, 21933, 3207, 1035, 1046, 3385, 1035, 2742, 2000, 6855, 1996, 8778, 17698, 12441, 15066, 1997, 1996, 5200, 1010, 1998, 2059, 1056, 2546, 1012, 11968, 3366, 1035, 2742, 2000, 2941, 6855, 1996, 23435, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 10, 43, 70, 83, 97, 128, 155, 171, 230], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34077445", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [36, 40]}], [{"sent_id": 1, "name": "tf.expand_dims", "pos": [68, 75]}], [{"sent_id": 0, "name": "tf.train.string_input_producer", "pos": [36, 46]}]], "sents": ["This is an error due to the incompatible shapes of img1_path (a 0-D, or scalar, Tensor) and the expected string_tensor argument to tf.train.string_input_producer (a 1-D, or vector, Tensor).", "Fortunately, the solution is simple, using tf.expand_dims to convert img1_path to a (one-element) vector:", "<code>Code Snippet</code>.", "Keeping track of the shapes of tensors can be tricky, so TensorFlow provides the Tensor.get_shape() method, which you can call to get information about the shape of any tensor object.", "For example:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2003, 2019, 7561, 2349, 2000, 1996, 25876, 10466, 1997, 10047, 2290, 2487, 1035, 4130, 1006, 1037, 1014, 1011, 1040, 1010, 2030, 26743, 2099, 1010, 23435, 1007, 1998, 1996, 3517, 5164, 1035, 23435, 6685, 2000, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 1006, 1037, 1015, 1011, 1040, 1010, 2030, 9207, 1010, 23435, 1007, 1012, 102, 101, 14599, 1010, 1996, 5576, 2003, 3722, 1010, 2478, 1056, 2546, 1012, 7818, 1035, 11737, 2015, 2000, 10463, 10047, 2290, 2487, 1035, 4130, 2000, 1037, 1006, 2028, 1011, 5783, 1007, 9207, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 4363, 2650, 1997, 1996, 10466, 1997, 23435, 2015, 2064, 2022, 24026, 1010, 2061, 23435, 12314, 3640, 1996, 23435, 1012, 2131, 1035, 4338, 1006, 1007, 4118, 1010, 2029, 2017, 2064, 2655, 2000, 2131, 2592, 2055, 1996, 4338, 1997, 2151, 23435, 4874, 1012, 102, 101, 2005, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 59, 92, 106, 149, 154, 168], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47810332", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [6, 10]}], [{"sent_id": 1, "name": "tf.decode_raw", "pos": [57, 64]}], [{"sent_id": 0, "name": "tf.train.shuffle_batch", "pos": [6, 14]}]], "sents": ["The error is raised because tf.train.shuffle_batch needs to know the shape of your tensors to be able to batch them (items in a batch must have all the same shape).", "In principle, however, raw data can have different sizes, so tf.decode_raw doesn't set any shape for your tensor.", "In the comments, you mention that all your images have shape (192,81,2), so you only need to set that shape in the image tensor before returning from read_and_decode:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 7561, 2003, 2992, 2138, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 3791, 2000, 2113, 1996, 4338, 1997, 2115, 23435, 2015, 2000, 2022, 2583, 2000, 14108, 2068, 1006, 5167, 1999, 1037, 14108, 2442, 2031, 2035, 1996, 2168, 4338, 1007, 1012, 102, 101, 1999, 6958, 1010, 2174, 1010, 6315, 2951, 2064, 2031, 2367, 10826, 1010, 2061, 1056, 2546, 1012, 21933, 3207, 1035, 6315, 2987, 1005, 1056, 2275, 2151, 4338, 2005, 2115, 23435, 1012, 102, 101, 1999, 1996, 7928, 1010, 2017, 5254, 2008, 2035, 2115, 4871, 2031, 4338, 1006, 17613, 1010, 6282, 1010, 1016, 1007, 1010, 2061, 2017, 2069, 2342, 2000, 2275, 2008, 4338, 1999, 1996, 3746, 23435, 2077, 4192, 2013, 3191, 1035, 1998, 1035, 21933, 3207, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 43, 75, 119, 133], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47942146", "vertexSet": [[{"sent_id": 0, "name": "tf.estimator", "pos": [9, 15]}], [{"sent_id": 0, "name": "tf.estimator.inputs", "pos": [9, 17]}], [{"sent_id": 0, "name": "tf.estimator.inputs.numpy_input_fn", "pos": [9, 26]}]], "sents": ["The 128 example evaluation metric comes from the tf.estimator.inputs.numpy_input_fn function that I was using to create the input function for my examples.", "The API for this function located here specifies 128 to be the default argument if another argument for batch_size is not provided.", "Providing the batch_size argument with the desired number of examples to be evaluated will rectify the problem above."], "sent_idxs": [101, 1996, 11899, 2742, 9312, 12046, 3310, 2013, 1996, 1056, 2546, 1012, 9765, 9581, 4263, 1012, 20407, 1012, 16371, 8737, 2100, 1035, 7953, 1035, 1042, 2078, 3853, 2008, 1045, 2001, 2478, 2000, 3443, 1996, 7953, 3853, 2005, 2026, 4973, 1012, 102, 101, 1996, 17928, 2005, 2023, 3853, 2284, 2182, 27171, 11899, 2000, 2022, 1996, 12398, 6685, 2065, 2178, 6685, 2005, 14108, 1035, 2946, 2003, 2025, 3024, 1012, 102, 101, 4346, 1996, 14108, 1035, 2946, 6685, 2007, 1996, 9059, 2193, 1997, 4973, 2000, 2022, 16330, 2097, 28667, 27351, 1996, 3291, 2682, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 41, 68, 92], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41864069", "vertexSet": [[{"sent_id": 7, "name": "tf.nn", "pos": [180, 185]}], [{"sent_id": 7, "name": "tf.reduce_mean", "pos": [172, 178]}], [{"sent_id": 7, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [180, 197]}]], "sents": ["I'm not sure why it's not implemented, but perhaps there is a workaround.", "The KL divergence is defined as:", "KL(prob_a, prob_b) = Sum(prob_a * log(prob_a/prob_b))", "The cross entropy H, on the other hand, is defined as:", "H(prob_a, prob_b) = -Sum(prob_a * log(prob_b))", "So, if you create a variable y = prob_a/prob_b, you could obtain the KL divergence by calling negative H(proba_a, y).", "In Tensorflow notation, something like:", "KL = tf.reduce_mean(-tf.nn.softmax_cross_entropy_with_logits(prob_a, y))"], "sent_idxs": [101, 1045, 1005, 1049, 2025, 2469, 2339, 2009, 1005, 1055, 2025, 7528, 1010, 2021, 3383, 2045, 2003, 1037, 2147, 24490, 1012, 102, 101, 1996, 1047, 2140, 17856, 17905, 2003, 4225, 2004, 1024, 102, 101, 1047, 2140, 1006, 4013, 2497, 1035, 1037, 1010, 4013, 2497, 1035, 1038, 1007, 1027, 7680, 1006, 4013, 2497, 1035, 1037, 1008, 8833, 1006, 4013, 2497, 1035, 1037, 1013, 4013, 2497, 1035, 1038, 1007, 1007, 102, 101, 1996, 2892, 23077, 1044, 1010, 2006, 1996, 2060, 2192, 1010, 2003, 4225, 2004, 1024, 102, 101, 1044, 1006, 4013, 2497, 1035, 1037, 1010, 4013, 2497, 1035, 1038, 1007, 1027, 1011, 7680, 1006, 4013, 2497, 1035, 1037, 1008, 8833, 1006, 4013, 2497, 1035, 1038, 1007, 1007, 102, 101, 2061, 1010, 2065, 2017, 3443, 1037, 8023, 1061, 1027, 4013, 2497, 1035, 1037, 1013, 4013, 2497, 1035, 1038, 1010, 2017, 2071, 6855, 1996, 1047, 2140, 17856, 17905, 2011, 4214, 4997, 1044, 1006, 4013, 3676, 1035, 1037, 1010, 1061, 1007, 1012, 102, 101, 1999, 23435, 12314, 14869, 1010, 2242, 2066, 1024, 102, 101, 1047, 2140, 1027, 1056, 2546, 1012, 5547, 1035, 2812, 1006, 1011, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 1006, 4013, 2497, 1035, 1037, 1010, 1061, 1007, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 22, 33, 69, 85, 116, 158, 168, 207], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41255301", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [19, 24]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [19, 27]}], [{"sent_id": 0, "name": "tf.reduce_max", "pos": [10, 16]}]], "sents": ["Your training loop creates new TensorFlow operations (tf.reduce_max(), tf.nn.softmax() and tf.histogram_summary()) in each iteration, which will lead to more memory being consumed over time.", "TensorFlow is most efficient when you run the same graph many times, because it can amortize the cost of optimizing the graph over multiple executions.", "Therefore,\nto get the best performance, you should revise your program so that you create each of these operations once, before the for x_test_batch in batches: loop, and then re-use the same operations in each iteration."], "sent_idxs": [101, 2115, 2731, 7077, 9005, 2047, 23435, 12314, 3136, 1006, 1056, 2546, 1012, 5547, 1035, 4098, 1006, 1007, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1007, 1998, 1056, 2546, 1012, 2010, 3406, 13113, 1035, 12654, 1006, 1007, 1007, 1999, 2169, 27758, 1010, 2029, 2097, 2599, 2000, 2062, 3638, 2108, 10202, 2058, 2051, 1012, 102, 101, 23435, 12314, 2003, 2087, 8114, 2043, 2017, 2448, 1996, 2168, 10629, 2116, 2335, 1010, 2138, 2009, 2064, 16095, 3775, 4371, 1996, 3465, 1997, 23569, 27605, 6774, 1996, 10629, 2058, 3674, 22679, 1012, 102, 101, 3568, 1010, 2000, 2131, 1996, 2190, 2836, 1010, 2017, 2323, 7065, 5562, 2115, 2565, 2061, 2008, 2017, 3443, 2169, 1997, 2122, 3136, 2320, 1010, 2077, 1996, 2005, 1060, 1035, 3231, 1035, 14108, 1999, 14108, 2229, 1024, 7077, 1010, 1998, 2059, 2128, 1011, 2224, 1996, 2168, 3136, 1999, 2169, 27758, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 57, 91, 143], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45267542", "vertexSet": [[{"sent_id": 1, "name": "tf.variable", "pos": [47, 51]}], [{"sent_id": 0, "name": "tf.constant", "pos": [17, 21]}, {"sent_id": 1, "name": "tf.constant", "pos": [42, 46]}], [{"sent_id": 0, "name": "tf.constant_initializer", "pos": [17, 24]}]], "sents": ["If you want initialize weight by some predefined value you can use tf.constant_initializer.", "If you don't want train this weights, you can define them as tf.constant not tf.Variable", "<code>Code Snippet</code>."], "sent_idxs": [101, 2065, 2017, 2215, 3988, 4697, 3635, 2011, 2070, 3653, 3207, 23460, 2094, 3643, 2017, 2064, 2224, 1056, 2546, 1012, 5377, 1035, 3988, 17629, 1012, 102, 101, 2065, 2017, 2123, 1005, 1056, 2215, 3345, 2023, 15871, 1010, 2017, 2064, 9375, 2068, 2004, 1056, 2546, 1012, 5377, 2025, 1056, 2546, 1012, 8023, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [1]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 52, 66], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41540092", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [93, 99]}], [{"sent_id": 4, "name": "tf.random_normal", "pos": [135, 141]}, {"sent_id": 6, "name": "tf.random_normal", "pos": [151, 157]}, {"sent_id": 7, "name": "tf.random_normal", "pos": [172, 178]}, {"sent_id": 8, "name": "tf.random_normal", "pos": [228, 234]}], [{"sent_id": 2, "name": "tf.contrib.layers", "pos": [93, 101]}], [{"sent_id": 6, "name": "tf.random_normal_initializer", "pos": [151, 160]}, {"sent_id": 8, "name": "tf.random_normal_initializer", "pos": [228, 237]}], [{"sent_id": 2, "name": "tf.contrib.layers.convolution2d", "pos": [93, 108]}]], "sents": ["The book that you are reading (You didn't mention which one) might be using an older version of TensorFlow when the initial values for the weight tensor was passed through the weight_init argument.", "In the TensorFlow library version you are using (You didn't mention your TF version), probably that argument is replaced with weight_initializer.", "The latest (TensorFlow v0.12.0) documentation for tf.contrib.layers.convolution2d is here.", "To fix your problem, you can change the following line in your code:", "weight_init=tf.random_normal", "to", "weight_initializer=tf.random_normal_initializer()", "According to the documentation, by default, tf.random_normal_initialier uses a 0.0 mean, a standard deviation of 1.0 and the datatype to be tf.float32.", "You may change the arguments as per your need using this line instead:\nweight_initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)"], "sent_idxs": [101, 1996, 2338, 2008, 2017, 2024, 3752, 1006, 2017, 2134, 1005, 1056, 5254, 2029, 2028, 1007, 2453, 2022, 2478, 2019, 3080, 2544, 1997, 23435, 12314, 2043, 1996, 3988, 5300, 2005, 1996, 3635, 23435, 2001, 2979, 2083, 1996, 3635, 1035, 1999, 4183, 6685, 1012, 102, 101, 1999, 1996, 23435, 12314, 3075, 2544, 2017, 2024, 2478, 1006, 2017, 2134, 1005, 1056, 5254, 2115, 1056, 2546, 2544, 1007, 1010, 2763, 2008, 6685, 2003, 2999, 2007, 3635, 1035, 3988, 17629, 1012, 102, 101, 1996, 6745, 1006, 23435, 12314, 1058, 2692, 1012, 2260, 1012, 1014, 1007, 12653, 2005, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 9014, 1012, 9530, 6767, 7630, 3508, 2475, 2094, 2003, 2182, 1012, 102, 101, 2000, 8081, 2115, 3291, 1010, 2017, 2064, 2689, 1996, 2206, 2240, 1999, 2115, 3642, 1024, 102, 101, 3635, 1035, 1999, 4183, 1027, 1056, 2546, 1012, 6721, 1035, 3671, 102, 101, 2000, 102, 101, 3635, 1035, 3988, 17629, 1027, 1056, 2546, 1012, 6721, 1035, 3671, 1035, 3988, 17629, 1006, 1007, 102, 101, 2429, 2000, 1996, 12653, 1010, 2011, 12398, 1010, 1056, 2546, 1012, 6721, 1035, 3671, 1035, 3988, 3771, 3594, 1037, 1014, 1012, 1014, 2812, 1010, 1037, 3115, 24353, 1997, 1015, 1012, 1014, 1998, 1996, 2951, 13874, 2000, 2022, 1056, 2546, 1012, 14257, 16703, 1012, 102, 101, 2017, 2089, 2689, 1996, 9918, 2004, 2566, 2115, 2342, 2478, 2023, 2240, 2612, 1024, 3635, 1035, 3988, 17629, 1027, 1056, 2546, 1012, 6721, 1035, 3671, 1035, 3988, 17629, 1006, 2812, 1027, 1014, 1012, 1014, 1010, 2358, 14141, 6777, 1027, 1015, 1012, 1014, 1010, 6534, 1027, 3904, 1010, 26718, 18863, 1027, 1056, 2546, 1012, 14257, 16703, 1007, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 44, 78, 112, 129, 142, 145, 163, 208, 266], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57301368", "vertexSet": [[{"sent_id": 14, "name": "tf.math", "pos": [331, 335]}, {"sent_id": 15, "name": "tf.math", "pos": [395, 399]}], [{"sent_id": 3, "name": "tf.cond", "pos": [79, 84]}, {"sent_id": 6, "name": "tf.cond", "pos": [130, 135]}, {"sent_id": 14, "name": "tf.cond", "pos": [325, 330]}], [{"sent_id": 6, "name": "tf.no_op", "pos": [139, 145]}], [{"sent_id": 7, "name": "tf.group", "pos": [159, 163]}, {"sent_id": 7, "name": "tf.group", "pos": [195, 199]}], [{"sent_id": 14, "name": "tf.math.equal", "pos": [331, 337]}, {"sent_id": 15, "name": "tf.math.equal", "pos": [395, 401]}]], "sents": ["I think you can achieve this via passing train_ops to the estimator.", "Calling tensorflow ops alone inside an estimator model_fn has absolutely NO effect.", "Because by design the model_fn is called only once per training session, hence every op you put in it will also be executed only once.", "In addition to that, all tf.cond branches will be evaluated and executed during model_fn call.", "(you can verify this behavior by a simple conditional logging op.)", "The key to achieve gradient accumulation is:", "Wrap all your ops with tf.cond, combined with a tf.no_op as false_fn..", "Let train_op = tf.group(*accum_ops, [conditional_minimize_op, reset_ops]), but control your execution order via control_dependencies, because tf.group does not care..", "Pass your fully-loaded train_op to EstimatorSpec.", "Those ops passed to the estimator_spec or training_hooks can be executed dynamically during the training process.", "Here's my code, finetuning BERT with limited GPU memory:", "<code>Code Snippet</code>.", "I applied clipping on batch gradients, and simply took the average of them.", "This approach worked for me, but I recommend you watch your loss behavior closely on your dataset.", "Also, about tf.cond(tf.math.equal(do_update, 1.),...,...", "), do_update is a variable managed by a Hook, it will take value 1 for every gradient_accmulation_multiplier steps, so this statement has the exact same effect as tf.math.equal(global_step % gradient_accmulation_multiplier, 0).", "It's just another way.", "The code for the Hook is as follows:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2228, 2017, 2064, 6162, 2023, 3081, 4458, 3345, 1035, 23092, 2000, 1996, 9765, 9581, 4263, 1012, 102, 101, 4214, 23435, 12314, 23092, 2894, 2503, 2019, 9765, 9581, 4263, 2944, 1035, 1042, 2078, 2038, 7078, 2053, 3466, 1012, 102, 101, 2138, 2011, 2640, 1996, 2944, 1035, 1042, 2078, 2003, 2170, 2069, 2320, 2566, 2731, 5219, 1010, 6516, 2296, 6728, 2017, 2404, 1999, 2009, 2097, 2036, 2022, 6472, 2069, 2320, 1012, 102, 101, 1999, 2804, 2000, 2008, 1010, 2035, 1056, 2546, 1012, 9530, 2094, 5628, 2097, 2022, 16330, 1998, 6472, 2076, 2944, 1035, 1042, 2078, 2655, 1012, 102, 101, 1006, 2017, 2064, 20410, 2023, 5248, 2011, 1037, 3722, 18462, 15899, 6728, 1012, 1007, 102, 101, 1996, 3145, 2000, 6162, 17978, 20299, 2003, 1024, 102, 101, 10236, 2035, 2115, 23092, 2007, 1056, 2546, 1012, 9530, 2094, 1010, 4117, 2007, 1037, 1056, 2546, 1012, 2053, 1035, 6728, 2004, 6270, 1035, 1042, 2078, 1012, 1012, 102, 101, 2292, 3345, 1035, 6728, 1027, 1056, 2546, 1012, 2177, 1006, 1008, 16222, 2819, 1035, 23092, 1010, 1031, 18462, 1035, 18478, 1035, 6728, 1010, 25141, 1035, 23092, 1033, 1007, 1010, 2021, 2491, 2115, 7781, 2344, 3081, 2491, 1035, 12530, 15266, 1010, 2138, 1056, 2546, 1012, 2177, 2515, 2025, 2729, 1012, 1012, 102, 101, 3413, 2115, 3929, 1011, 8209, 3345, 1035, 6728, 2000, 9765, 9581, 6591, 5051, 2278, 1012, 102, 101, 2216, 23092, 2979, 2000, 1996, 9765, 9581, 4263, 1035, 28699, 2030, 2731, 1035, 18008, 2064, 2022, 6472, 8790, 3973, 2076, 1996, 2731, 2832, 1012, 102, 101, 2182, 1005, 1055, 2026, 3642, 1010, 2986, 8525, 5582, 14324, 2007, 3132, 14246, 2226, 3638, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1045, 4162, 12528, 4691, 2006, 14108, 17978, 2015, 1010, 1998, 3432, 2165, 1996, 2779, 1997, 2068, 1012, 102, 101, 2023, 3921, 2499, 2005, 2033, 1010, 2021, 1045, 16755, 2017, 3422, 2115, 3279, 5248, 4876, 2006, 2115, 2951, 13462, 1012, 102, 101, 2036, 1010, 2055, 1056, 2546, 1012, 9530, 2094, 1006, 1056, 2546, 1012, 8785, 1012, 5020, 1006, 2079, 1035, 10651, 1010, 1015, 1012, 1007, 1010, 1012, 1012, 1012, 1010, 1012, 1012, 1012, 102, 101, 1007, 1010, 2079, 1035, 10651, 2003, 1037, 8023, 3266, 2011, 1037, 8103, 1010, 2009, 2097, 2202, 3643, 1015, 2005, 2296, 17978, 1035, 16222, 12274, 13490, 1035, 4800, 24759, 3771, 4084, 1010, 2061, 2023, 4861, 2038, 1996, 6635, 2168, 3466, 2004, 1056, 2546, 1012, 8785, 1012, 5020, 1006, 3795, 1035, 3357, 1003, 17978, 1035, 16222, 12274, 13490, 1035, 4800, 24759, 3771, 1010, 1014, 1007, 1012, 102, 101, 2009, 1005, 1055, 2074, 2178, 2126, 1012, 102, 101, 1996, 3642, 2005, 1996, 8103, 2003, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 19, 40, 72, 98, 114, 124, 153, 205, 222, 248, 266, 280, 299, 321, 354, 420, 429, 440, 454], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57463879", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [11, 17]}], [{"sent_id": 0, "name": "tf.contrib.tpu", "pos": [11, 20]}], [{"sent_id": 0, "name": "tf.contrib.tpu.keras_to_tpu_model", "pos": [11, 30]}]], "sents": ["As of 2019-02-20, the function tf.contrib.tpu.keras_to_tpu_model has been deprecated.", "You should therefore re-attempt converting your model using the new Distribution Strategy function.", "An in depth guide on distributed training can be found here.", "I also noticed that you are using data type float as your input values.", "In CPython, the default bit value is 64bit.", "Currently, TPU\u2019s function most optimally with 16-bit floats therefore you should reduce your inputs to either 8-bit or 16-bit.", "The lower the bit value, the faster the processing will be for your model.", "Therefore it is also recommended to take advantage of Quantization, converting float weights to 8-bit integers.", "There are two types of quantized training: post-training quantization and quantization-aware training.", "For more information concerning TPU\u2019s on Google Cloud Platform you may refer to the Cloud TPU documentation, and for more information on TPU system architecture you may refer to this documentation by Google as it properly explains how TPU\u2019s are designed."], "sent_idxs": [101, 2004, 1997, 10476, 1011, 6185, 1011, 2322, 1010, 1996, 3853, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 1056, 14289, 1012, 17710, 8180, 1035, 2000, 1035, 1056, 14289, 1035, 2944, 2038, 2042, 2139, 28139, 12921, 1012, 102, 101, 2017, 2323, 3568, 2128, 1011, 3535, 16401, 2115, 2944, 2478, 1996, 2047, 4353, 5656, 3853, 1012, 102, 101, 2019, 1999, 5995, 5009, 2006, 5500, 2731, 2064, 2022, 2179, 2182, 1012, 102, 101, 1045, 2036, 4384, 2008, 2017, 2024, 2478, 2951, 2828, 14257, 2004, 2115, 7953, 5300, 1012, 102, 101, 1999, 18133, 22123, 8747, 1010, 1996, 12398, 2978, 3643, 2003, 4185, 16313, 1012, 102, 101, 2747, 1010, 1056, 14289, 1521, 1055, 3853, 2087, 15502, 2135, 2007, 2385, 1011, 2978, 24885, 3568, 2017, 2323, 5547, 2115, 20407, 2000, 2593, 1022, 1011, 2978, 2030, 2385, 1011, 2978, 1012, 102, 101, 1996, 2896, 1996, 2978, 3643, 1010, 1996, 5514, 1996, 6364, 2097, 2022, 2005, 2115, 2944, 1012, 102, 101, 3568, 2009, 2003, 2036, 6749, 2000, 2202, 5056, 1997, 24110, 3775, 9276, 1010, 16401, 14257, 15871, 2000, 1022, 1011, 2978, 24028, 1012, 102, 101, 2045, 2024, 2048, 4127, 1997, 24110, 23355, 2731, 1024, 2695, 1011, 2731, 24110, 3775, 9276, 1998, 24110, 3775, 9276, 1011, 5204, 2731, 1012, 102, 101, 2005, 2062, 2592, 7175, 1056, 14289, 1521, 1055, 2006, 8224, 6112, 4132, 2017, 2089, 6523, 2000, 1996, 6112, 1056, 14289, 12653, 1010, 1998, 2005, 2062, 2592, 2006, 1056, 14289, 2291, 4294, 2017, 2089, 6523, 2000, 2023, 12653, 2011, 8224, 2004, 2009, 7919, 7607, 2129, 1056, 14289, 1521, 1055, 2024, 2881, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 55, 69, 86, 101, 134, 152, 176, 201, 254], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62179455", "vertexSet": [[{"sent_id": 4, "name": "tf.keras", "pos": [109, 114]}], [{"sent_id": 4, "name": "tf.keras.utils", "pos": [109, 117]}], [{"sent_id": 4, "name": "tf.keras.utils.get_file", "pos": [109, 121]}]], "sents": ["Some ideas:", "You should use a combination of 1,2 and 3.", "If you save your files as TFRecords, you can read them in parallel, that's what they are designed for.", "Then, you will be able to use num_parallel_calls and interleave, because that way you don't have to wrap a py_func.", ".map doesn't have to wrap a .py_func, you could for example use tf.keras.utils.get_file.", "That way you also avoid using py_func and use num_parallel_calls efficiently.", "I still recommend using TFRecords, they are designed for this use case.", "Another option is to use an SSD to store your data instead of a Hard Disk.", "You can also look into the .cache function of the tf.Dataset API.", "Maybe you can try loading a random subset of the data, training multiple eopchs on that, and then in the mean time fetch another subset of the data (using tf.prefetch), and then train multiple epochs on that, and so on.", "This idea is more of a long shot as it might affect performance, but it just might work in your case."], "sent_idxs": [101, 2070, 4784, 1024, 102, 101, 2017, 2323, 2224, 1037, 5257, 1997, 1015, 1010, 1016, 1998, 1017, 1012, 102, 101, 2065, 2017, 3828, 2115, 6764, 2004, 1056, 19699, 8586, 8551, 2015, 1010, 2017, 2064, 3191, 2068, 1999, 5903, 1010, 2008, 1005, 1055, 2054, 2027, 2024, 2881, 2005, 1012, 102, 101, 2059, 1010, 2017, 2097, 2022, 2583, 2000, 2224, 16371, 2213, 1035, 5903, 1035, 4455, 1998, 6970, 19738, 3726, 1010, 2138, 2008, 2126, 2017, 2123, 1005, 1056, 2031, 2000, 10236, 1037, 1052, 2100, 1035, 4569, 2278, 1012, 102, 101, 1012, 4949, 2987, 1005, 1056, 2031, 2000, 10236, 1037, 1012, 1052, 2100, 1035, 4569, 2278, 1010, 2017, 2071, 2005, 2742, 2224, 1056, 2546, 1012, 17710, 8180, 1012, 21183, 12146, 1012, 2131, 1035, 5371, 1012, 102, 101, 2008, 2126, 2017, 2036, 4468, 2478, 1052, 2100, 1035, 4569, 2278, 1998, 2224, 16371, 2213, 1035, 5903, 1035, 4455, 18228, 1012, 102, 101, 1045, 2145, 16755, 2478, 1056, 19699, 8586, 8551, 2015, 1010, 2027, 2024, 2881, 2005, 2023, 2224, 2553, 1012, 102, 101, 2178, 5724, 2003, 2000, 2224, 2019, 7020, 2094, 2000, 3573, 2115, 2951, 2612, 1997, 1037, 2524, 9785, 1012, 102, 101, 2017, 2064, 2036, 2298, 2046, 1996, 1012, 17053, 3853, 1997, 1996, 1056, 2546, 1012, 2951, 13462, 17928, 1012, 102, 101, 2672, 2017, 2064, 3046, 10578, 1037, 6721, 16745, 1997, 1996, 2951, 1010, 2731, 3674, 1041, 7361, 18069, 2006, 2008, 1010, 1998, 2059, 1999, 1996, 2812, 2051, 18584, 2178, 16745, 1997, 1996, 2951, 1006, 2478, 1056, 2546, 1012, 3653, 7959, 10649, 1007, 1010, 1998, 2059, 3345, 3674, 25492, 2015, 2006, 2008, 1010, 1998, 2061, 2006, 1012, 102, 101, 2023, 2801, 2003, 2062, 1997, 1037, 2146, 2915, 2004, 2009, 2453, 7461, 2836, 1010, 2021, 2009, 2074, 2453, 2147, 1999, 2115, 2553, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 5, 19, 49, 87, 123, 146, 166, 186, 206, 263, 288], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44339261", "vertexSet": [[{"sent_id": 3, "name": "tf.shape", "pos": [140, 144]}], [{"sent_id": 3, "name": "tf.assert", "pos": [135, 139]}], [{"sent_id": 1, "name": "tf.reshape", "pos": [58, 64]}]], "sents": ["tensor.set_shape will refine the static shape information and throw an error if it is incompatible with current static shape information (in the TensorArray.stack() case it will let you set any value for the zeroth dimension's static shape information).", "tf.reshape can also be useful for asserting/filling in shape information, although it's not perfect.", "It will only throw an error if the size of the Tensor is wrong when the graph is executed (and may otherwise hide a shape error downstream).", "More complicated, but you can also set_shape for the static shape information and then use tf.Assert with tf.shape to check the Tensor's shape when the graph is executed."], "sent_idxs": [101, 23435, 1012, 2275, 1035, 4338, 2097, 25416, 3170, 1996, 10763, 4338, 2592, 1998, 5466, 2019, 7561, 2065, 2009, 2003, 25876, 2007, 2783, 10763, 4338, 2592, 1006, 1999, 1996, 23435, 2906, 9447, 1012, 9991, 1006, 1007, 2553, 2009, 2097, 2292, 2017, 2275, 2151, 3643, 2005, 1996, 5717, 2705, 9812, 1005, 1055, 10763, 4338, 2592, 1007, 1012, 102, 101, 1056, 2546, 1012, 24501, 3270, 5051, 2064, 2036, 2022, 6179, 2005, 27644, 1013, 8110, 1999, 4338, 2592, 1010, 2348, 2009, 1005, 1055, 2025, 3819, 1012, 102, 101, 2009, 2097, 2069, 5466, 2019, 7561, 2065, 1996, 2946, 1997, 1996, 23435, 2003, 3308, 2043, 1996, 10629, 2003, 6472, 1006, 1998, 2089, 4728, 5342, 1037, 4338, 7561, 13248, 1007, 1012, 102, 101, 2062, 8552, 1010, 2021, 2017, 2064, 2036, 2275, 1035, 4338, 2005, 1996, 10763, 4338, 2592, 1998, 2059, 2224, 1056, 2546, 1012, 20865, 2007, 1056, 2546, 1012, 4338, 2000, 4638, 1996, 23435, 1005, 1055, 4338, 2043, 1996, 10629, 2003, 6472, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 57, 84, 116, 158], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41915871", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [60, 64]}, {"sent_id": 3, "name": "tf.train", "pos": [123, 127]}, {"sent_id": 4, "name": "tf.train", "pos": [151, 155]}], [{"sent_id": 9, "name": "tf.errors", "pos": [290, 294]}], [{"sent_id": 3, "name": "tf.train.shuffle_batch", "pos": [123, 131]}, {"sent_id": 4, "name": "tf.train.shuffle_batch", "pos": [151, 159]}], [{"sent_id": 9, "name": "tf.errors.outofrangeerror", "pos": [290, 300]}], [{"sent_id": 2, "name": "tf.train.string_input_producer", "pos": [60, 70]}]], "sents": ["The input_pipeline function only creates the part of a (usually larger) graph that is responsible for producing batches of data.", "If you were to call input_pipeline twice - for whatever reason - you would be creating two different queues indeed.", "In general, the function tf.train.string_input_producer actually creates a queue node (or operation) in the currently active graph (which is the default graph unless you specify something different).", "read_my_file_format then reads from that queue and sequentially produces single \"example\" tensors, while tf.train.shuffle_batch then batches these into bundles of length batch_size each.", "However, the output of tf.train.shuffle_batch, two Tensors here that are returned from the input_pipeline function, only really takes on a (new) value when it is evaluated under a session.", "If you evaluate these tensors multiple times, they will contain different values - taken, through read_my_file_format, from files listed in the input queue.", "Think of it like so:", "<code>Code Snippet</code>.", "The boilerplate code to get it running is a bit more complex, e.g.", "because queues need to actually be started and stopped in the graph, because they will throw a tf.errors.OutOfRangeError when they run dry, etc.", "A more complete example could look like this:", "<code>Code Snippet</code>.", "For a deeper understanding, you might want to look at the Reading data documentation."], "sent_idxs": [101, 1996, 7953, 1035, 13117, 3853, 2069, 9005, 1996, 2112, 1997, 1037, 1006, 2788, 3469, 1007, 10629, 2008, 2003, 3625, 2005, 5155, 14108, 2229, 1997, 2951, 1012, 102, 101, 2065, 2017, 2020, 2000, 2655, 7953, 1035, 13117, 3807, 1011, 2005, 3649, 3114, 1011, 2017, 2052, 2022, 4526, 2048, 2367, 24240, 2015, 5262, 1012, 102, 101, 1999, 2236, 1010, 1996, 3853, 1056, 2546, 1012, 3345, 1012, 5164, 1035, 7953, 1035, 3135, 2941, 9005, 1037, 24240, 13045, 1006, 2030, 3169, 1007, 1999, 1996, 2747, 3161, 10629, 1006, 2029, 2003, 1996, 12398, 10629, 4983, 2017, 20648, 2242, 2367, 1007, 1012, 102, 101, 3191, 1035, 2026, 1035, 5371, 1035, 4289, 2059, 9631, 2013, 2008, 24240, 1998, 25582, 2135, 7137, 2309, 1000, 2742, 1000, 23435, 2015, 1010, 2096, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 2059, 14108, 2229, 2122, 2046, 26825, 1997, 3091, 14108, 1035, 2946, 2169, 1012, 102, 101, 2174, 1010, 1996, 6434, 1997, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1010, 2048, 23435, 2015, 2182, 2008, 2024, 2513, 2013, 1996, 7953, 1035, 13117, 3853, 1010, 2069, 2428, 3138, 2006, 1037, 1006, 2047, 1007, 3643, 2043, 2009, 2003, 16330, 2104, 1037, 5219, 1012, 102, 101, 2065, 2017, 16157, 2122, 23435, 2015, 3674, 2335, 1010, 2027, 2097, 5383, 2367, 5300, 1011, 2579, 1010, 2083, 3191, 1035, 2026, 1035, 5371, 1035, 4289, 1010, 2013, 6764, 3205, 1999, 1996, 7953, 24240, 1012, 102, 101, 2228, 1997, 2009, 2066, 2061, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 15635, 15725, 3642, 2000, 2131, 2009, 2770, 2003, 1037, 2978, 2062, 3375, 1010, 1041, 1012, 1043, 1012, 102, 101, 2138, 24240, 2015, 2342, 2000, 2941, 2022, 2318, 1998, 3030, 1999, 1996, 10629, 1010, 2138, 2027, 2097, 5466, 1037, 1056, 2546, 1012, 10697, 1012, 2041, 11253, 24388, 11510, 29165, 2043, 2027, 2448, 4318, 1010, 4385, 1012, 102, 101, 1037, 2062, 3143, 2742, 2071, 2298, 2066, 2023, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2005, 1037, 6748, 4824, 1010, 2017, 2453, 2215, 2000, 2298, 2012, 1996, 3752, 2951, 12653, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 28, 54, 98, 145, 192, 228, 236, 250, 270, 308, 319, 333, 351], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "50199499", "vertexSet": [[{"sent_id": 2, "name": "tf.contrib", "pos": [39, 45]}], [{"sent_id": 2, "name": "tf.is_tensor", "pos": [31, 37]}], [{"sent_id": 3, "name": "tf.executing_eagerly", "pos": [64, 70]}], [{"sent_id": 2, "name": "tf.contrib.framework", "pos": [39, 47]}], [{"sent_id": 2, "name": "tf.contrib.framework.is_tensor", "pos": [39, 51]}]], "sents": ["I am not sure if you can, but I think you probably don't need to.", "You already have the following tools:", "tf.is_tensor (previously tf.contrib.framework.is_tensor) that will return True for an EagerTensor.", "tf.executing_eagerly that returns True if you are, well, executing eagerly..", "I believe they should cover 99% of your needs -- and I would be curious to hear about your problem if it falls in that percentage left out."], "sent_idxs": [101, 1045, 2572, 2025, 2469, 2065, 2017, 2064, 1010, 2021, 1045, 2228, 2017, 2763, 2123, 1005, 1056, 2342, 2000, 1012, 102, 101, 2017, 2525, 2031, 1996, 2206, 5906, 1024, 102, 101, 1056, 2546, 1012, 2003, 1035, 23435, 1006, 3130, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 7705, 1012, 2003, 1035, 23435, 1007, 2008, 2097, 2709, 2995, 2005, 2019, 9461, 25808, 2953, 1012, 102, 101, 1056, 2546, 1012, 23448, 1035, 17858, 2008, 5651, 2995, 2065, 2017, 2024, 1010, 2092, 1010, 23448, 17858, 1012, 1012, 102, 101, 1045, 2903, 2027, 2323, 3104, 5585, 1003, 1997, 2115, 3791, 1011, 1011, 1998, 1045, 2052, 2022, 8025, 2000, 2963, 2055, 2115, 3291, 2065, 2009, 4212, 1999, 2008, 7017, 2187, 2041, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 21, 30, 63, 84, 117], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56460379", "vertexSet": [[{"sent_id": 0, "name": "tf.math", "pos": [12, 16]}, {"sent_id": 3, "name": "tf.math", "pos": [90, 94]}], [{"sent_id": 3, "name": "tf.math.count_nonzero", "pos": [90, 100]}], [{"sent_id": 0, "name": "tf.math.unsorted_segment_mean", "pos": [12, 24]}]], "sents": ["One way of computing per-row masked means is using tf.math.unsorted_segment_mean.", "Essentially, you can have one segment id per row, then replace the segment id for masked elements with an extra one.", "<code>Code Snippet</code>.", "However, since in this case the mask is precisely for elements that are nonzero, you can also just use tf.math.count_nonzero:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2028, 2126, 1997, 9798, 2566, 1011, 5216, 16520, 2965, 2003, 2478, 1056, 2546, 1012, 8785, 1012, 4895, 21748, 3064, 1035, 6903, 1035, 2812, 1012, 102, 101, 7687, 1010, 2017, 2064, 2031, 2028, 6903, 8909, 2566, 5216, 1010, 2059, 5672, 1996, 6903, 8909, 2005, 16520, 3787, 2007, 2019, 4469, 2028, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2174, 1010, 2144, 1999, 2023, 2553, 1996, 7308, 2003, 10785, 2005, 3787, 2008, 2024, 2512, 6290, 2080, 1010, 2017, 2064, 2036, 2074, 2224, 1056, 2546, 1012, 8785, 1012, 4175, 1035, 2512, 6290, 2080, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 26, 52, 66, 102, 116], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "63725072", "vertexSet": [[{"sent_id": 1, "name": "tf.keras", "pos": [16, 21]}], [{"sent_id": 1, "name": "tf.keras.preprocessing", "pos": [16, 26]}], [{"sent_id": 1, "name": "tf.keras.preprocessing.image_dataset_from_directory", "pos": [16, 35]}]], "sents": ["I had a similar issue.", "The solution was to take the underlying tf.keras.preprocessing.image_dataset_from_directory function and add the 'image_paths' variable to the return statement.", "This incurs no computational overhead as the filenames have already been retrieved.", "The main function code is taken from the GitHub at: https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/preprocessing/image_dataset.py#L34-L206", "See below:", "<code>Code Snippet</code>.", "Which would then work as:", "<code>Code Snippet</code>.", "train_paths returns a list of file strings."], "sent_idxs": [101, 1045, 2018, 1037, 2714, 3277, 1012, 102, 101, 1996, 5576, 2001, 2000, 2202, 1996, 10318, 1056, 2546, 1012, 17710, 8180, 1012, 17463, 3217, 9623, 7741, 1012, 3746, 1035, 2951, 13462, 1035, 2013, 1035, 14176, 3853, 1998, 5587, 1996, 1005, 3746, 1035, 10425, 1005, 8023, 2000, 1996, 2709, 4861, 1012, 102, 101, 2023, 4297, 9236, 2053, 15078, 8964, 2004, 1996, 5371, 18442, 2015, 2031, 2525, 2042, 5140, 1012, 102, 101, 1996, 2364, 3853, 3642, 2003, 2579, 2013, 1996, 21025, 2705, 12083, 2012, 1024, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 12314, 1013, 1038, 4135, 2497, 1013, 1058, 2475, 1012, 1017, 1012, 1014, 1013, 23435, 12314, 1013, 18750, 1013, 17710, 8180, 1013, 17463, 3217, 9623, 7741, 1013, 3746, 1035, 2951, 13462, 1012, 1052, 2100, 1001, 1048, 22022, 1011, 1048, 11387, 2575, 102, 101, 2156, 2917, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2029, 2052, 2059, 2147, 2004, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3345, 1035, 10425, 5651, 1037, 2862, 1997, 5371, 7817, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 8, 51, 69, 138, 143, 157, 165, 179, 191], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52187144", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [61, 65]}, {"sent_id": 2, "name": "tf.data", "pos": [160, 164]}], [{"sent_id": 0, "name": "tf.contrib", "pos": [21, 27]}], [{"sent_id": 0, "name": "tf.read_file", "pos": [10, 16]}], [{"sent_id": 2, "name": "tf.data.dataset", "pos": [160, 167]}], [{"sent_id": 0, "name": "tf.contrib.ffmpeg", "pos": [21, 31]}], [{"sent_id": 0, "name": "tf.data.tfrecorddataset", "pos": [61, 73]}], [{"sent_id": 0, "name": "tf.contrib.ffmpeg.decode_audio", "pos": [21, 36]}]], "sents": ["Although you could in theory read the files with tf.read_file and decode them with tf.contrib.ffmpeg.decode_audio, the usual approach for this kind of cases is to convert the data to TFRecord format and read it with a tf.data.TFRecordDataset.", "This blog post shows an example of how to do that, in your case you would need a script that reads each WAV file, decodes it and writes the vector of samples (I suppose as a 32-bit value would be the simplest way) in the file.", "Note that if you want to batch multiple audio files into a tensor either they must have all the same size or you would have to use tf.data.Dataset.padded_batch to form proper tensors."], "sent_idxs": [101, 2348, 2017, 2071, 1999, 3399, 3191, 1996, 6764, 2007, 1056, 2546, 1012, 3191, 1035, 5371, 1998, 21933, 3207, 2068, 2007, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 21461, 8737, 13910, 1012, 21933, 3207, 1035, 5746, 1010, 1996, 5156, 3921, 2005, 2023, 2785, 1997, 3572, 2003, 2000, 10463, 1996, 2951, 2000, 1056, 19699, 8586, 8551, 4289, 1998, 3191, 2009, 2007, 1037, 1056, 2546, 1012, 2951, 1012, 1056, 19699, 8586, 8551, 2850, 18260, 2102, 1012, 102, 101, 2023, 9927, 2695, 3065, 2019, 2742, 1997, 2129, 2000, 2079, 2008, 1010, 1999, 2115, 2553, 2017, 2052, 2342, 1037, 5896, 2008, 9631, 2169, 11333, 2615, 5371, 1010, 21933, 6155, 2009, 1998, 7009, 1996, 9207, 1997, 8168, 1006, 1045, 6814, 2004, 1037, 3590, 1011, 2978, 3643, 2052, 2022, 1996, 21304, 2126, 1007, 1999, 1996, 5371, 1012, 102, 101, 3602, 2008, 2065, 2017, 2215, 2000, 14108, 3674, 5746, 6764, 2046, 1037, 23435, 2593, 2027, 2442, 2031, 2035, 1996, 2168, 2946, 2030, 2017, 2052, 2031, 2000, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 20633, 1035, 14108, 2000, 2433, 5372, 23435, 2015, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5]], "sent_ends": [0, 75, 132, 178], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59280734", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [8, 12]}], [{"sent_id": 0, "name": "tf.estimator", "pos": [1, 7]}], [{"sent_id": 0, "name": "tf.train.profilerhook", "pos": [8, 16]}]], "sents": ["tf.estimator use tf.train.ProfilerHook works!", "just add a ProfilerHook in TrainSpec hooks!", "<code>Code Snippet</code>.", "then, you could get the tracing file like timeline-{}.json in model_dir/tracing, and open chrome chrome://tracing to visual!", "refs:\nhttps://stackoverflow.com/a/48278275/6494418"], "sent_idxs": [101, 1056, 2546, 1012, 9765, 9581, 4263, 2224, 1056, 2546, 1012, 3345, 1012, 6337, 25032, 14659, 2573, 999, 102, 101, 2074, 5587, 1037, 6337, 25032, 14659, 1999, 4499, 5051, 2278, 18008, 999, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1010, 2017, 2071, 2131, 1996, 16907, 5371, 2066, 17060, 1011, 1063, 1065, 1012, 1046, 3385, 1999, 2944, 1035, 16101, 1013, 16907, 1010, 1998, 2330, 18546, 18546, 1024, 1013, 1013, 16907, 2000, 5107, 999, 102, 101, 25416, 2015, 1024, 16770, 1024, 1013, 1013, 9991, 7840, 12314, 1012, 4012, 1013, 1037, 1013, 4466, 22907, 2620, 22907, 2629, 1013, 4185, 2683, 22932, 15136, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 19, 33, 47, 83, 110], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "41943422", "vertexSet": [[{"sent_id": 0, "name": "tf.global_variables_initializer", "pos": [8, 17]}], [{"sent_id": 0, "name": "tf.initialize_all_variables", "pos": [20, 29]}, {"sent_id": 2, "name": "tf.initialize_all_variables", "pos": [102, 111]}], [{"sent_id": 1, "name": "tf.session", "pos": [90, 94]}], [{"sent_id": 0, "name": "tf.operation", "pos": [33, 37]}], [{"sent_id": 1, "name": "tf.interactivesession", "pos": [78, 84]}], [{"sent_id": 3, "name": "tf.local_variables_initializer", "pos": [160, 169]}]], "sents": ["The two statements are equivalent: both tf.global_variables_initializer() and tf.initialize_all_variables() return a tf.Operation that, when run, will initialize the global variables in a model.", "Passing an operation to sess.run() or calling operation.run() are equivalent when you have created a tf.InteractiveSession, or are in a with tf.Session(): block.", "The tf.initialize_all_variables() function has been deprecated (and will be removed from TensorFlow 1.0) because its name is confusing: it does not initialize all variables (i.e.", "local variables must be initialized separately, using tf.local_variables_initializer()), and it doesn't immediately initialize the variables (instead it returns an operation that you have to run yourself)."], "sent_idxs": [101, 1996, 2048, 8635, 2024, 5662, 1024, 2119, 1056, 2546, 1012, 3795, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1998, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 2709, 1037, 1056, 2546, 1012, 3169, 2008, 1010, 2043, 2448, 1010, 2097, 3988, 4697, 1996, 3795, 10857, 1999, 1037, 2944, 1012, 102, 101, 4458, 2019, 3169, 2000, 7367, 4757, 1012, 2448, 1006, 1007, 2030, 4214, 3169, 1012, 2448, 1006, 1007, 2024, 5662, 2043, 2017, 2031, 2580, 1037, 1056, 2546, 1012, 9123, 8583, 10992, 1010, 2030, 2024, 1999, 1037, 2007, 1056, 2546, 1012, 5219, 1006, 1007, 1024, 3796, 1012, 102, 101, 1996, 1056, 2546, 1012, 3988, 4697, 1035, 2035, 1035, 10857, 1006, 1007, 3853, 2038, 2042, 2139, 28139, 12921, 1006, 1998, 2097, 2022, 3718, 2013, 23435, 12314, 1015, 1012, 1014, 1007, 2138, 2049, 2171, 2003, 16801, 1024, 2009, 2515, 2025, 3988, 4697, 2035, 10857, 1006, 1045, 1012, 1041, 1012, 102, 101, 2334, 10857, 2442, 2022, 3988, 3550, 10329, 1010, 2478, 1056, 2546, 1012, 2334, 1035, 10857, 1035, 3988, 17629, 1006, 1007, 1007, 1010, 1998, 2009, 2987, 1005, 1056, 3202, 3988, 4697, 1996, 10857, 1006, 2612, 2009, 5651, 2019, 3169, 2008, 2017, 2031, 2000, 2448, 4426, 1007, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}], "na_triple": [[0, 2], [0, 3], [0, 4], [0, 5], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 53, 100, 150, 198], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58114679", "vertexSet": [[{"sent_id": 0, "name": "tf.keras", "pos": [7, 12]}], [{"sent_id": 0, "name": "tf.keras.optimizers", "pos": [7, 16]}], [{"sent_id": 0, "name": "tf.keras.optimizers.sgd", "pos": [7, 19]}]], "sents": ["You can replace it with, tf.keras.optimizers.SGD() defined here.", "Here (skip to third point) is the official message, where TF team mentioned to use this keras optimizer."], "sent_idxs": [101, 2017, 2064, 5672, 2009, 2007, 1010, 1056, 2546, 1012, 17710, 8180, 1012, 23569, 27605, 16750, 1012, 22214, 2094, 1006, 1007, 4225, 2182, 1012, 102, 101, 2182, 1006, 13558, 2000, 2353, 2391, 1007, 2003, 1996, 2880, 4471, 1010, 2073, 1056, 2546, 2136, 3855, 2000, 2224, 2023, 17710, 8180, 23569, 27605, 6290, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 25, 53], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37877651", "vertexSet": [[{"sent_id": 0, "name": "tf.train", "pos": [17, 21]}], [{"sent_id": 3, "name": "tf.gather", "pos": [87, 91]}], [{"sent_id": 0, "name": "tf.train.shuffle_batch", "pos": [17, 25]}]], "sents": ["There's a short reference to how to do it in the docs using tf.train.shuffle_batch() and enqueue_many=True.", "If you can determine if an example is mislabeled using graph operations, then you can filter the result like so (adapted from another SO answer):", "<code>Code Snippet</code>.", "The tf.gather is just a way to get a zero-sized slice.", "In numpy it would just be X[[], ...]."], "sent_idxs": [101, 2045, 1005, 1055, 1037, 2460, 4431, 2000, 2129, 2000, 2079, 2009, 1999, 1996, 9986, 2015, 2478, 1056, 2546, 1012, 3345, 1012, 23046, 1035, 14108, 1006, 1007, 1998, 4372, 4226, 5657, 1035, 2116, 1027, 2995, 1012, 102, 101, 2065, 2017, 2064, 5646, 2065, 2019, 2742, 2003, 28616, 20470, 12260, 2094, 2478, 10629, 3136, 1010, 2059, 2017, 2064, 11307, 1996, 2765, 2066, 2061, 1006, 5967, 2013, 2178, 2061, 3437, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1996, 1056, 2546, 1012, 8587, 2003, 2074, 1037, 2126, 2000, 2131, 1037, 5717, 1011, 7451, 14704, 1012, 102, 101, 1999, 16371, 8737, 2100, 2009, 2052, 2074, 2022, 1060, 1031, 1031, 1033, 1010, 1012, 1012, 1012, 1033, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 37, 71, 85, 104, 124], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52622823", "vertexSet": [[{"sent_id": 2, "name": "tf.abs", "pos": [41, 45]}, {"sent_id": 2, "name": "tf.abs", "pos": [60, 64]}], [{"sent_id": 6, "name": "tf.squeeze", "pos": [150, 154]}], [{"sent_id": 2, "name": "tf.reduce_sum", "pos": [53, 59]}]], "sents": ["I tried solution by Rudresh Panchal and it doesn't work for me.", "Maybe due versions change.", "I found tipo in the first row: reduce_sum(tf.abs(x), 1) -> tf.reduce_sum(tf.abs(x), 1).", "Also, bool_mask has rank 2 instead of rank 1, which is required:\ntensor: N-D tensor.", "mask: K-D boolean tensor, K <= N and K must be known statically.", "In other words, the shape of bool_mask must be for example [6] not [1,6].", "tf.squeeze works well to reduce dimension.", "Corrected code which works for me:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1045, 2699, 5576, 2011, 21766, 16200, 4095, 6090, 18598, 1998, 2009, 2987, 1005, 1056, 2147, 2005, 2033, 1012, 102, 101, 2672, 2349, 4617, 2689, 1012, 102, 101, 1045, 2179, 5955, 2080, 1999, 1996, 2034, 5216, 1024, 5547, 1035, 7680, 1006, 1056, 2546, 1012, 14689, 1006, 1060, 1007, 1010, 1015, 1007, 1011, 1028, 1056, 2546, 1012, 5547, 1035, 7680, 1006, 1056, 2546, 1012, 14689, 1006, 1060, 1007, 1010, 1015, 1007, 1012, 102, 101, 2036, 1010, 22017, 2140, 1035, 7308, 2038, 4635, 1016, 2612, 1997, 4635, 1015, 1010, 2029, 2003, 3223, 1024, 23435, 1024, 1050, 1011, 1040, 23435, 1012, 102, 101, 7308, 1024, 1047, 1011, 1040, 22017, 20898, 23435, 1010, 1047, 1026, 1027, 1050, 1998, 1047, 2442, 2022, 2124, 10763, 3973, 1012, 102, 101, 1999, 2060, 2616, 1010, 1996, 4338, 1997, 22017, 2140, 1035, 7308, 2442, 2022, 2005, 2742, 1031, 1020, 1033, 2025, 1031, 1015, 1010, 1020, 1033, 1012, 102, 101, 1056, 2546, 1012, 11025, 2573, 2092, 2000, 5547, 9812, 1012, 102, 101, 13371, 3642, 2029, 2573, 2005, 2033, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 27, 72, 99, 122, 149, 161, 170, 184], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "62246104", "vertexSet": [[{"sent_id": 15, "name": "tf.abs", "pos": [376, 380]}], [{"sent_id": 15, "name": "tf.math", "pos": [381, 385]}], [{"sent_id": 10, "name": "tf.assign", "pos": [273, 277]}], [{"sent_id": 15, "name": "tf.math.tan", "pos": [381, 387]}], [{"sent_id": 10, "name": "tf.clip_by_value", "pos": [281, 289]}]], "sents": ["It seems to me (I can be mistaken) that constrained optimization (you can google for it in tensorflow) is not exactly the case for which tensroflow was designed.", "You may want to take a look at this repo, it may satisfy your needs, but as far as I understand, it's still not solving arbitrary constrained optimization, just some classification problems with labels and features, compatible with precision/recall scores.", "If you want to use constraints on the tensorflow variable (i.e.", "some function applied after gradient step - which you can do manually also - by taking variable values, doing manipulations, and reassigning then), it means that you will be cutting variables after each step done using gradient in general space.", "It's a question whether you will successfully reach the right optimization goal this way, or your variables will stuck at boundaries, because general gradient will point somewhere outside.", "My approach 1", "If your problem is simple enough.", "you can try to parametrize your x2 and x3 as x2 = x3 + t, and then try to do cutting in the graph:", "<code>Code Snippet</code>.", "Then, on a separate call additionally do", "sess.run(tf.assign(x2, tf.clip_by_value(x2, 1.0, 10.0)))", "But my opinion is that it won't work well.", "My approach 2", "I would also try to invent some loss terms to keep variables within constraints, which is more likely to work.", "For example, constraint for x2 to be in the interval [1,10] will be:", "loss += alpha*tf.abs(tf.math.tan(((x-5.5)/4.5)*pi/2))", "Here the expression under tan is brought to -pi/2,pi/2 and then tan function is used to make it grow very rapidly when it reaches boundaries.", "In this case I think you're more likely to find your optimum, but again the loss weight alpha might be too big and training will stuck somewhere nearby, if required value of x2 lies near the boundary.", "In this case you can try to use smaller alpha."], "sent_idxs": [101, 2009, 3849, 2000, 2033, 1006, 1045, 2064, 2022, 13534, 1007, 2008, 27570, 20600, 1006, 2017, 2064, 8224, 2005, 2009, 1999, 23435, 12314, 1007, 2003, 2025, 3599, 1996, 2553, 2005, 2029, 15295, 3217, 12314, 2001, 2881, 1012, 102, 101, 2017, 2089, 2215, 2000, 2202, 1037, 2298, 2012, 2023, 16360, 2080, 1010, 2009, 2089, 13225, 2115, 3791, 1010, 2021, 2004, 2521, 2004, 1045, 3305, 1010, 2009, 1005, 1055, 2145, 2025, 13729, 15275, 27570, 20600, 1010, 2074, 2070, 5579, 3471, 2007, 10873, 1998, 2838, 1010, 11892, 2007, 11718, 1013, 9131, 7644, 1012, 102, 101, 2065, 2017, 2215, 2000, 2224, 14679, 2006, 1996, 23435, 12314, 8023, 1006, 1045, 1012, 1041, 1012, 102, 101, 2070, 3853, 4162, 2044, 17978, 3357, 1011, 2029, 2017, 2064, 2079, 21118, 2036, 1011, 2011, 2635, 8023, 5300, 1010, 2725, 16924, 2015, 1010, 1998, 2128, 12054, 23773, 2075, 2059, 1007, 1010, 2009, 2965, 2008, 2017, 2097, 2022, 6276, 10857, 2044, 2169, 3357, 2589, 2478, 17978, 1999, 2236, 2686, 1012, 102, 101, 2009, 1005, 1055, 1037, 3160, 3251, 2017, 2097, 5147, 3362, 1996, 2157, 20600, 3125, 2023, 2126, 1010, 2030, 2115, 10857, 2097, 5881, 2012, 7372, 1010, 2138, 2236, 17978, 2097, 2391, 4873, 2648, 1012, 102, 101, 2026, 3921, 1015, 102, 101, 2065, 2115, 3291, 2003, 3722, 2438, 1012, 102, 101, 2017, 2064, 3046, 2000, 11498, 11368, 25709, 2115, 1060, 2475, 1998, 1060, 2509, 2004, 1060, 2475, 1027, 1060, 2509, 1009, 1056, 1010, 1998, 2059, 3046, 2000, 2079, 6276, 1999, 1996, 10629, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 1010, 2006, 1037, 3584, 2655, 5678, 2079, 102, 101, 7367, 4757, 1012, 2448, 1006, 1056, 2546, 1012, 23911, 1006, 1060, 2475, 1010, 1056, 2546, 1012, 12528, 1035, 2011, 1035, 3643, 1006, 1060, 2475, 1010, 1015, 1012, 1014, 1010, 2184, 1012, 1014, 1007, 1007, 1007, 102, 101, 2021, 2026, 5448, 2003, 2008, 2009, 2180, 1005, 1056, 2147, 2092, 1012, 102, 101, 2026, 3921, 1016, 102, 101, 1045, 2052, 2036, 3046, 2000, 1999, 15338, 2070, 3279, 3408, 2000, 2562, 10857, 2306, 14679, 1010, 2029, 2003, 2062, 3497, 2000, 2147, 1012, 102, 101, 2005, 2742, 1010, 27142, 2005, 1060, 2475, 2000, 2022, 1999, 1996, 13483, 1031, 1015, 1010, 2184, 1033, 2097, 2022, 1024, 102, 101, 3279, 1009, 1027, 6541, 1008, 1056, 2546, 1012, 14689, 1006, 1056, 2546, 1012, 8785, 1012, 9092, 1006, 1006, 1006, 1060, 1011, 1019, 1012, 1019, 1007, 1013, 1018, 1012, 1019, 1007, 1008, 14255, 1013, 1016, 1007, 1007, 102, 101, 2182, 1996, 3670, 2104, 9092, 2003, 2716, 2000, 1011, 14255, 1013, 1016, 1010, 14255, 1013, 1016, 1998, 2059, 9092, 3853, 2003, 2109, 2000, 2191, 2009, 4982, 2200, 5901, 2043, 2009, 6561, 7372, 1012, 102, 101, 1999, 2023, 2553, 1045, 2228, 2017, 1005, 2128, 2062, 3497, 2000, 2424, 2115, 23569, 28591, 1010, 2021, 2153, 1996, 3279, 3635, 6541, 2453, 2022, 2205, 2502, 1998, 2731, 2097, 5881, 4873, 3518, 1010, 2065, 3223, 3643, 1997, 1060, 2475, 3658, 2379, 1996, 6192, 1012, 102, 101, 1999, 2023, 2553, 2017, 2064, 3046, 2000, 2224, 3760, 6541, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 38, 91, 109, 160, 195, 200, 209, 243, 257, 267, 304, 318, 323, 348, 370, 408, 443, 489, 502], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48667570", "vertexSet": [[{"sent_id": 1, "name": "tf.get_variable", "pos": [36, 42]}], [{"sent_id": 0, "name": "tf.variable", "pos": [7, 11]}, {"sent_id": 1, "name": "tf.variable", "pos": [31, 35]}], [{"sent_id": 7, "name": "tf.nn", "pos": [114, 119]}], [{"sent_id": 7, "name": "tf.nn.rnn_cell", "pos": [114, 124]}], [{"sent_id": 7, "name": "tf.nn.rnn_cell.lstmcell", "pos": [114, 129]}]], "sents": ["First off, creating variables with tf.Variable won't make it reusable.", "That's one of the key differences between tf.Variable and tf.get_variable.", "See this example:", "<code>Code Snippet</code>.", "If you inspect the created variables, you'll see:", "<code>Code Snippet</code>.", "Next, RNN cells provide own mechanism for the reuse.", "E.g., for tf.nn.rnn_cell.LSTMCell it's reuse constructor argument:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2034, 2125, 1010, 4526, 10857, 2007, 1056, 2546, 1012, 8023, 2180, 1005, 1056, 2191, 2009, 2128, 10383, 3468, 1012, 102, 101, 2008, 1005, 1055, 2028, 1997, 1996, 3145, 5966, 2090, 1056, 2546, 1012, 8023, 1998, 1056, 2546, 1012, 2131, 1035, 8023, 1012, 102, 101, 2156, 2023, 2742, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2065, 2017, 22459, 1996, 2580, 10857, 1010, 2017, 1005, 2222, 2156, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2279, 1010, 29300, 2078, 4442, 3073, 2219, 7337, 2005, 1996, 2128, 8557, 1012, 102, 101, 1041, 1012, 1043, 1012, 1010, 2005, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 1048, 3367, 12458, 5349, 2009, 1005, 1055, 2128, 8557, 9570, 2953, 6685, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 21, 44, 50, 64, 78, 92, 107, 139, 153], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "57503202", "vertexSet": [[{"sent_id": 3, "name": "tf.keras", "pos": [50, 55]}], [{"sent_id": 3, "name": "tf.keras.model", "pos": [50, 57]}], [{"sent_id": 9, "name": "tf.set_random_seed", "pos": [169, 177]}]], "sents": ["If you set the random seed before building the graph, you will get reproducible results.", "For example, given a random input:", "<code>Code Snippet</code>.", "We can define one tf.keras.model:", "<code>Code Snippet</code>.", "Then another:", "<code>Code Snippet</code>.", "Now, if we perform inference on the random_input tensor, we will get the same results with both models because they had the same random seed to start.", "<code>Code Snippet</code>.", "To sum it up, no matter which random seed you use, if you set the random seed to the same number before building the graph with tf.set_random_seed(seed), you will get the same results."], "sent_idxs": [101, 2065, 2017, 2275, 1996, 6721, 6534, 2077, 2311, 1996, 10629, 1010, 2017, 2097, 2131, 16360, 14127, 21104, 3463, 1012, 102, 101, 2005, 2742, 1010, 2445, 1037, 6721, 7953, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2057, 2064, 9375, 2028, 1056, 2546, 1012, 17710, 8180, 1012, 2944, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2059, 2178, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2085, 1010, 2065, 2057, 4685, 28937, 2006, 1996, 6721, 1035, 7953, 23435, 1010, 2057, 2097, 2131, 1996, 2168, 3463, 2007, 2119, 4275, 2138, 2027, 2018, 1996, 2168, 6721, 6534, 2000, 2707, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2000, 7680, 2009, 2039, 1010, 2053, 3043, 2029, 6721, 6534, 2017, 2224, 1010, 2065, 2017, 2275, 1996, 6721, 6534, 2000, 1996, 2168, 2193, 2077, 2311, 1996, 10629, 2007, 1056, 2546, 1012, 2275, 1035, 6721, 1035, 6534, 1006, 6534, 1007, 1010, 2017, 2097, 2131, 1996, 2168, 3463, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 21, 31, 45, 59, 73, 78, 92, 126, 140, 189], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "58130094", "vertexSet": [[{"sent_id": 8, "name": "tf.data", "pos": [297, 301]}], [{"sent_id": 5, "name": "tf.train", "pos": [170, 174]}], [{"sent_id": 5, "name": "tf.train.byteslist", "pos": [170, 177]}], [{"sent_id": 8, "name": "tf.data.experimental", "pos": [297, 303]}], [{"sent_id": 8, "name": "tf.data.experimental.make_csv_dataset", "pos": [297, 311]}]], "sents": ["For starters, I need to know what type a TFRecord file can take, when using CSV types are removed.", "TFRecord accepts following datatypes-\nstring, byte, float32, float 64, bool, enum, int32, int64, uint32, uint64\nTalked here.", "Secondly, How can I convert data type:object into a type that a TFRecord can take?", "Here is an example from TF, it is a bit complicated to digest it at once but if you read it carefully it is easy.", "have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords?", "For string type data, you require tf.train.BytesList which returns a bytes_list from a string.", "When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers?", "Quick Note, I am using PANDAS to create a dataframe of the CSV file", "Instead of reading csv file using Pandas, I would recommend you to use tf.data.experimental.make_csv_dataset defined here.", "This will make this conversion process very faster than Pandas and will give you less compatibility issues to work with TF classes.", "If you use this function, then you will not need to read the csv file row by row but all at once using map() which uses eager execution.", "This is a good tutorial to get started.", "Accidentally edited wrong section of the post"], "sent_idxs": [101, 2005, 29400, 1010, 1045, 2342, 2000, 2113, 2054, 2828, 1037, 1056, 19699, 8586, 8551, 5371, 2064, 2202, 1010, 2043, 2478, 20116, 2615, 4127, 2024, 3718, 1012, 102, 101, 1056, 19699, 8586, 8551, 13385, 2206, 2951, 13874, 2015, 1011, 5164, 1010, 24880, 1010, 14257, 16703, 1010, 14257, 4185, 1010, 22017, 2140, 1010, 4372, 2819, 1010, 20014, 16703, 1010, 20014, 21084, 1010, 21318, 3372, 16703, 1010, 21318, 3372, 21084, 5720, 2182, 1012, 102, 101, 16378, 1010, 2129, 2064, 1045, 10463, 2951, 2828, 1024, 4874, 2046, 1037, 2828, 2008, 1037, 1056, 19699, 8586, 8551, 2064, 2202, 1029, 102, 101, 2182, 2003, 2019, 2742, 2013, 1056, 2546, 1010, 2009, 2003, 1037, 2978, 8552, 2000, 17886, 2009, 2012, 2320, 2021, 2065, 2017, 3191, 2009, 5362, 2009, 2003, 3733, 1012, 102, 101, 2031, 2048, 7753, 1006, 2097, 2695, 2742, 2917, 1007, 1997, 2048, 5200, 4127, 2008, 2024, 7817, 1010, 2129, 2064, 1045, 10463, 2008, 2951, 2000, 1996, 6149, 2828, 2005, 1056, 19699, 8586, 8551, 2015, 1029, 102, 101, 2005, 5164, 2828, 2951, 1010, 2017, 5478, 1056, 2546, 1012, 3345, 1012, 27507, 9863, 2029, 5651, 1037, 27507, 1035, 2862, 2013, 1037, 5164, 1012, 102, 101, 2043, 12324, 2075, 10047, 5327, 2000, 10439, 10497, 2951, 2013, 2169, 5216, 2012, 1037, 2051, 2046, 1996, 1056, 19699, 8586, 8551, 5371, 1010, 2151, 6040, 2030, 12653, 2052, 2022, 2307, 1010, 1045, 2031, 2042, 2559, 2005, 2070, 2051, 2012, 2023, 3291, 1998, 2009, 3849, 2045, 2064, 2069, 2022, 20014, 2015, 1010, 24885, 7953, 3064, 2046, 1037, 1056, 19699, 8586, 8551, 2021, 2054, 2055, 1037, 2862, 1013, 9140, 1997, 24028, 1029, 102, 101, 4248, 3602, 1010, 1045, 2572, 2478, 25462, 2015, 2000, 3443, 1037, 2951, 15643, 1997, 1996, 20116, 2615, 5371, 102, 101, 2612, 1997, 3752, 20116, 2615, 5371, 2478, 25462, 2015, 1010, 1045, 2052, 16755, 2017, 2000, 2224, 1056, 2546, 1012, 2951, 1012, 6388, 1012, 2191, 1035, 20116, 2615, 1035, 2951, 13462, 4225, 2182, 1012, 102, 101, 2023, 2097, 2191, 2023, 7584, 2832, 2200, 5514, 2084, 25462, 2015, 1998, 2097, 2507, 2017, 2625, 21778, 3314, 2000, 2147, 2007, 1056, 2546, 4280, 1012, 102, 101, 2065, 2017, 2224, 2023, 3853, 1010, 2059, 2017, 2097, 2025, 2342, 2000, 3191, 1996, 20116, 2615, 5371, 5216, 2011, 5216, 2021, 2035, 2012, 2320, 2478, 4949, 1006, 1007, 2029, 3594, 9461, 7781, 1012, 102, 101, 2023, 2003, 1037, 2204, 14924, 4818, 2000, 2131, 2318, 1012, 102, 101, 9554, 5493, 3308, 2930, 1997, 1996, 2695, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 0], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 28, 72, 96, 126, 162, 188, 260, 280, 315, 342, 377, 389, 398], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "55752196", "vertexSet": [[{"sent_id": 0, "name": "tf.contrib", "pos": [5, 11]}, {"sent_id": 2, "name": "tf.contrib", "pos": [88, 94]}], [{"sent_id": 0, "name": "tf.contrib.image", "pos": [5, 13]}], [{"sent_id": 0, "name": "tf.contrib.image.transform", "pos": [5, 15]}]], "sents": ["Functionality previous included in tf.contrib.image.transform is in the process of being upgraded to TF 2.0 and migrated to TensorFlow Addons (tfa.image).", "If you have an immediate need for a specific endpoint that has not yet been added, please file it as an issue in the Addons repo.", "Migration, deprecation, and rename fates for tf.contrib endpoints are detailed in the TensorFlow organization RFC here."], "sent_idxs": [101, 15380, 3025, 2443, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 3746, 1012, 10938, 2003, 1999, 1996, 2832, 1997, 2108, 9725, 2000, 1056, 2546, 1016, 1012, 1014, 1998, 13447, 2000, 23435, 12314, 5587, 5644, 1006, 1056, 7011, 1012, 3746, 1007, 1012, 102, 101, 2065, 2017, 2031, 2019, 6234, 2342, 2005, 1037, 3563, 2203, 8400, 2008, 2038, 2025, 2664, 2042, 2794, 1010, 3531, 5371, 2009, 2004, 2019, 3277, 1999, 1996, 5587, 5644, 16360, 2080, 1012, 102, 101, 9230, 1010, 2139, 28139, 10719, 1010, 1998, 14916, 14074, 26417, 2005, 1056, 2546, 1012, 9530, 18886, 2497, 2203, 26521, 2024, 6851, 1999, 1996, 23435, 12314, 3029, 14645, 2182, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 43, 76, 107], "sent_pos": [0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "42397517", "vertexSet": [[{"sent_id": 2, "name": "tf.train", "pos": [39, 43]}], [{"sent_id": 2, "name": "tf.session", "pos": [30, 34]}], [{"sent_id": 2, "name": "tf.train.server", "pos": [39, 45]}]], "sents": ["This line in the error message:", "<code>Code Snippet</code>.", "...suggests that your tf.Session is not connected to the tf.train.Server you created.", "In particular, it seems to be a local (or \"direct\") session that can only access devices in the local process.", "To fix this problem, when you create your session, pass server.target to the initializer.", "For example, depending on which API you are using to create the session, you might want to use one of the following:", "<code>Code Snippet</code>."], "sent_idxs": [101, 2023, 2240, 1999, 1996, 7561, 4471, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 6083, 2008, 2115, 1056, 2546, 1012, 5219, 2003, 2025, 4198, 2000, 1996, 1056, 2546, 1012, 3345, 1012, 8241, 2017, 2580, 1012, 102, 101, 1999, 3327, 1010, 2009, 3849, 2000, 2022, 1037, 2334, 1006, 2030, 1000, 3622, 1000, 1007, 5219, 2008, 2064, 2069, 3229, 5733, 1999, 1996, 2334, 2832, 1012, 102, 101, 2000, 8081, 2023, 3291, 1010, 2043, 2017, 3443, 2115, 5219, 1010, 3413, 8241, 1012, 4539, 2000, 1996, 3988, 17629, 1012, 102, 101, 2005, 2742, 1010, 5834, 2006, 2029, 17928, 2017, 2024, 2478, 2000, 3443, 1996, 5219, 1010, 2017, 2453, 2215, 2000, 2224, 2028, 1997, 1996, 2206, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 9, 23, 49, 77, 99, 126, 140], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "52702471", "vertexSet": [[{"sent_id": 0, "name": "tf.data", "pos": [1, 5]}], [{"sent_id": 0, "name": "tf.constant", "pos": [34, 38]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [1, 8]}]], "sents": ["tf.data.Dataset.from_tensor_slices((features, labels)) embed the features and labels arrays in your TensorFlow graph as tf.constant() operations.", "see more https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays"], "sent_idxs": [101, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 2013, 1035, 23435, 1035, 25609, 1006, 1006, 2838, 1010, 10873, 1007, 1007, 7861, 8270, 1996, 2838, 1998, 10873, 27448, 1999, 2115, 23435, 12314, 10629, 2004, 1056, 2546, 1012, 5377, 1006, 1007, 3136, 1012, 102, 101, 2156, 2062, 16770, 1024, 1013, 1013, 7479, 1012, 23435, 12314, 1012, 8917, 1013, 5009, 1013, 2951, 13462, 2015, 1001, 15077, 1035, 16371, 8737, 2100, 1035, 27448, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 43, 71], "sent_pos": [0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44479970", "vertexSet": [[{"sent_id": 0, "name": " tf.nn.static_rnn", "pos": [1, 11]}, {"sent_id": 1, "name": " tf.nn.static_rnn", "pos": [28, 38]}, {"sent_id": 2, "name": " tf.nn.static_rnn", "pos": [60, 70]}, {"sent_id": 9, "name": " tf.nn.static_rnn", "pos": [191, 201]}], [{"sent_id": 0, "name": "tf.nn.dynamic_rnn", "pos": [13, 23]}, {"sent_id": 5, "name": "tf.nn.dynamic_rnn", "pos": [124, 134]}, {"sent_id": 10, "name": "tf.nn.dynamic_rnn", "pos": [226, 236]}], [{"sent_id": 6, "name": "tf.while_loop", "pos": [143, 149]}]], "sents": ["tf.nn.static_rnn vs. tf.nn.dynamic_rnn.", "Internally, tf.nn.static_rnn creates an unrolled graph for a fixed RNN length.", "That means that, if you call tf.nn.static_rnn with inputs having 200 time-steps you are creating a static graph with 200 RNN steps.", "First, graph creation is slow.", "Second, you\u2019re unable to pass in longer sequences (> 200) than you've originally specified.", "tf.nn.dynamic_rnn solves this.", "It uses a tf.while_loop to dynamically construct the graph when it is executed.", "That means graph creation is faster and you can feed batches of variable size.", "What about performance?.", "You may think the tf.nn.static_rnn is faster than its dynamic counterpart because it pre-builds the graph.", "Please note, it is strongly encouraged to use tf.nn.dynamic_rnn.", "Reference: http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 5443, 1012, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1012, 102, 101, 16058, 1010, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 9005, 2019, 4895, 28402, 2098, 10629, 2005, 1037, 4964, 29300, 2078, 3091, 1012, 102, 101, 2008, 2965, 2008, 1010, 2065, 2017, 2655, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 2007, 20407, 2383, 3263, 2051, 1011, 4084, 2017, 2024, 4526, 1037, 10763, 10629, 2007, 3263, 29300, 2078, 4084, 1012, 102, 101, 2034, 1010, 10629, 4325, 2003, 4030, 1012, 102, 101, 2117, 1010, 2017, 1521, 2128, 4039, 2000, 3413, 1999, 2936, 10071, 1006, 1028, 3263, 1007, 2084, 2017, 1005, 2310, 2761, 9675, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 9611, 2015, 2023, 1012, 102, 101, 2009, 3594, 1037, 1056, 2546, 1012, 2096, 1035, 7077, 2000, 8790, 3973, 9570, 1996, 10629, 2043, 2009, 2003, 6472, 1012, 102, 101, 2008, 2965, 10629, 4325, 2003, 5514, 1998, 2017, 2064, 5438, 14108, 2229, 1997, 8023, 2946, 1012, 102, 101, 2054, 2055, 2836, 1029, 1012, 102, 101, 2017, 2089, 2228, 1996, 1056, 2546, 1012, 1050, 2078, 1012, 10763, 1035, 29300, 2078, 2003, 5514, 2084, 2049, 8790, 13637, 2138, 2009, 3653, 1011, 16473, 1996, 10629, 1012, 102, 101, 3531, 3602, 1010, 2009, 2003, 6118, 6628, 2000, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 8790, 1035, 29300, 2078, 1012, 102, 101, 4431, 1024, 8299, 1024, 1013, 1013, 7479, 1012, 3748, 19968, 1012, 4012, 1013, 2355, 1013, 5511, 1013, 29300, 3619, 1011, 1999, 1011, 23435, 12314, 1011, 1037, 1011, 6742, 1011, 5009, 1011, 1998, 1011, 25672, 24894, 14088, 1011, 2838, 1013, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 25, 52, 90, 99, 123, 139, 161, 179, 186, 216, 238, 279], "sent_pos": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "33729233", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [7, 12]}], [{"sent_id": 0, "name": "tf.reshape", "pos": [35, 41]}, {"sent_id": 1, "name": "tf.reshape", "pos": [52, 58]}], [{"sent_id": 0, "name": "tf.nn.bias_add", "pos": [7, 16]}]], "sents": ["The shape of the result of tf.nn.bias_add(value, bias) is always the same as the shape of value, so these calls to tf.reshape() are unnecessary.", "Occasionally, calls to tf.reshape() are used to add explicit information about the shape, but the recommended way to do this, per the FAQ, is to use the Tensor.set_shape() method to add shape information without adding a redundant operation to the graph."], "sent_idxs": [101, 1996, 4338, 1997, 1996, 2765, 1997, 1056, 2546, 1012, 1050, 2078, 1012, 13827, 1035, 5587, 1006, 3643, 1010, 13827, 1007, 2003, 2467, 1996, 2168, 2004, 1996, 4338, 1997, 3643, 1010, 2061, 2122, 4455, 2000, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 2024, 14203, 1012, 102, 101, 5681, 1010, 4455, 2000, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 2024, 2109, 2000, 5587, 13216, 2592, 2055, 1996, 4338, 1010, 2021, 1996, 6749, 2126, 2000, 2079, 2023, 1010, 2566, 1996, 6904, 4160, 1010, 2003, 2000, 2224, 1996, 23435, 1012, 2275, 1035, 4338, 1006, 1007, 4118, 2000, 5587, 4338, 2592, 2302, 5815, 1037, 21707, 3169, 2000, 1996, 10629, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 47, 109], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "43603466", "vertexSet": [[{"sent_id": 3, "name": "tf.summary.histogram", "pos": [72, 80]}, {"sent_id": 5, "name": "tf.summary.histogram", "pos": [124, 132]}], [{"sent_id": 10, "name": "tf.train.summarywriter", "pos": [219, 226]}], [{"sent_id": 2, "name": "tf.audio_summary", "pos": [33, 39]}], [{"sent_id": 4, "name": "tf.summary.scalar", "pos": [102, 109]}, {"sent_id": 9, "name": "tf.summary.scalar", "pos": [209, 216]}], [{"sent_id": 6, "name": "tf.image_summary", "pos": [135, 141]}], [{"sent_id": 7, "name": "tf.merge_all_summaries", "pos": [154, 164]}], [{"sent_id": 7, "name": "tf.summary.merge_all", "pos": [168, 176]}], [{"sent_id": 10, "name": "tf.summary.filewriter", "pos": [230, 237]}], [{"sent_id": 3, "name": "tf.contrib.deprecated.histogram_summary", "pos": [52, 68]}], [{"sent_id": 8, "name": "tf.merge_summary", "pos": [179, 185]}], [{"sent_id": 9, "name": "tf.scalar_summary", "pos": [198, 205]}], [{"sent_id": 4, "name": "tf.contrib.deprecated.scalar_summary", "pos": [83, 98]}], [{"sent_id": 6, "name": "tf.summary.image", "pos": [145, 151]}], [{"sent_id": 2, "name": "tf.summary.audio", "pos": [43, 49]}], [{"sent_id": 5, "name": "tf.histogram_summary", "pos": [112, 120]}]], "sents": ["In a new version of TF, all summary functions were renamed.", "Summary functions have been consolidated under the tf.summary\n  namespace.", "tf.audio_summary should be renamed to tf.summary.audio .", "tf.contrib.deprecated.histogram_summary should be renamed to tf.summary.histogram .", "tf.contrib.deprecated.scalar_summary should be renamed to tf.summary.scalar.", "tf.histogram_summary should be renamed to tf.summary.histogram.", "tf.image_summary should be renamed to tf.summary.image .", "tf.merge_all_summaries should be renamed to tf.summary.merge_all .", "tf.merge_summary should be renamed to tf.summary.merge .", "tf.scalar_summary should be renamed to tf.summary.scalar .", "tf.train.SummaryWriter should be renamed to tf.summary.FileWriter."], "sent_idxs": [101, 1999, 1037, 2047, 2544, 1997, 1056, 2546, 1010, 2035, 12654, 4972, 2020, 4096, 1012, 102, 101, 12654, 4972, 2031, 2042, 10495, 2104, 1996, 1056, 2546, 1012, 12654, 3415, 15327, 1012, 102, 101, 1056, 2546, 1012, 5746, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 5746, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2139, 28139, 12921, 1012, 2010, 3406, 13113, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 2010, 3406, 13113, 1012, 102, 101, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 2139, 28139, 12921, 1012, 26743, 2099, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1012, 102, 101, 1056, 2546, 1012, 2010, 3406, 13113, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 2010, 3406, 13113, 1012, 102, 101, 1056, 2546, 1012, 3746, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 3746, 1012, 102, 101, 1056, 2546, 1012, 13590, 1035, 2035, 1035, 7680, 7849, 3111, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 1012, 102, 101, 1056, 2546, 1012, 13590, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 13590, 1012, 102, 101, 1056, 2546, 1012, 26743, 2099, 1035, 12654, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 26743, 2099, 1012, 102, 101, 1056, 2546, 1012, 3345, 1012, 12654, 15994, 2323, 2022, 4096, 2000, 1056, 2546, 1012, 12654, 1012, 5371, 15994, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 7, "evidence": [0, 10]}, {"r": "S1", "h": 7, "t": 1, "evidence": [0, 10]}, {"r": "S1", "h": 2, "t": 13, "evidence": [0, 2]}, {"r": "S1", "h": 13, "t": 2, "evidence": [0, 2]}, {"r": "S1", "h": 4, "t": 12, "evidence": [0, 6]}, {"r": "S1", "h": 12, "t": 4, "evidence": [0, 6]}, {"r": "S1", "h": 5, "t": 6, "evidence": [0, 7]}, {"r": "S1", "h": 6, "t": 5, "evidence": [0, 7]}, {"r": "S1", "h": 8, "t": 0, "evidence": [0, 3]}, {"r": "S1", "h": 0, "t": 8, "evidence": [0, 3]}, {"r": "S1", "h": 10, "t": 3, "evidence": [0, 9]}, {"r": "S1", "h": 3, "t": 10, "evidence": [0, 9]}, {"r": "S1", "h": 11, "t": 3, "evidence": [0, 4]}, {"r": "S1", "h": 3, "t": 11, "evidence": [0, 4]}, {"r": "S1", "h": 14, "t": 0, "evidence": [0, 5]}, {"r": "S1", "h": 0, "t": 14, "evidence": [0, 5]}], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [1, 14], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [2, 14], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [3, 12], [3, 13], [3, 14], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [4, 10], [4, 11], [4, 13], [4, 14], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [5, 13], [5, 14], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 7], [6, 8], [6, 9], [6, 10], [6, 11], [6, 12], [6, 13], [6, 14], [7, 0], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 8], [7, 9], [7, 10], [7, 11], [7, 12], [7, 13], [7, 14], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [8, 14], [9, 0], [9, 1], [9, 2], [9, 3], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8], [9, 10], [9, 11], [9, 12], [9, 13], [9, 14], [10, 0], [10, 1], [10, 2], [10, 4], [10, 5], [10, 6], [10, 7], [10, 8], [10, 9], [10, 11], [10, 12], [10, 13], [10, 14], [11, 0], [11, 1], [11, 2], [11, 4], [11, 5], [11, 6], [11, 7], [11, 8], [11, 9], [11, 10], [11, 12], [11, 13], [11, 14], [12, 0], [12, 1], [12, 2], [12, 3], [12, 5], [12, 6], [12, 7], [12, 8], [12, 9], [12, 10], [12, 11], [12, 13], [12, 14], [13, 0], [13, 1], [13, 3], [13, 4], [13, 5], [13, 6], [13, 7], [13, 8], [13, 9], [13, 10], [13, 11], [13, 12], [13, 14], [14, 1], [14, 2], [14, 3], [14, 4], [14, 5], [14, 6], [14, 7], [14, 8], [14, 9], [14, 10], [14, 11], [14, 12], [14, 13]], "sent_ends": [0, 16, 32, 51, 82, 111, 134, 153, 178, 197, 218, 239], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 14, 14, 14, 14, 14, 14, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 15, 15, 15, 15, 15, 15, 15, 15, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 13, 13, 13, 13, 13, 13, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 11, 11, 11, 11, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 0, 0]}, {"title": "41470165", "vertexSet": [[{"sent_id": 0, "name": "tf.image", "pos": [43, 47]}, {"sent_id": 4, "name": "tf.image", "pos": [169, 173]}], [{"sent_id": 5, "name": "tf.import_graph_def", "pos": [214, 222]}, {"sent_id": 6, "name": "tf.import_graph_def", "pos": [228, 236]}], [{"sent_id": 0, "name": "tf.image.decode_jpeg", "pos": [43, 53]}, {"sent_id": 4, "name": "tf.image.decode_jpeg", "pos": [169, 179]}]], "sents": ["The model can only train on (and evaluate) JPEG images, because the GraphDef that you've saved in /root/tf_files/output_graph.pb only contains a tf.image.decode_jpeg() op, and uses the output of that op for making predictions.", "There are at least a couple of options for using other image formats:", "Feed in parsed images rather than JPEG data.", "In the current program, you feed in a JPEG-encoded image as a string value for the tensor \"DecodeJpeg/contents:0\".", "Instead, you can feed in a 3-D array of decoded image data for the tensor \"DecodeJpeg:0\" (which represents the output of the tf.image.decode_jpeg() op), and you can use NumPy, PIL, or some other Python library to create this array.", "Remap the image input in tf.import_graph_def().", "The tf.import_graph_def() function enables you to connect two different graphs together by remapping individual tensor values.", "For example, you could do something like the following to add a new image-processing op to the existing graph:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1996, 2944, 2064, 2069, 3345, 2006, 1006, 1998, 16157, 1007, 16545, 13910, 4871, 1010, 2138, 1996, 10629, 3207, 2546, 2008, 2017, 1005, 2310, 5552, 1999, 1013, 7117, 1013, 1056, 2546, 1035, 6764, 1013, 6434, 1035, 10629, 1012, 1052, 2497, 2069, 3397, 1037, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 1006, 1007, 6728, 1010, 1998, 3594, 1996, 6434, 1997, 2008, 6728, 2005, 2437, 20932, 1012, 102, 101, 2045, 2024, 2012, 2560, 1037, 3232, 1997, 7047, 2005, 2478, 2060, 3746, 11630, 1024, 102, 101, 5438, 1999, 11968, 6924, 4871, 2738, 2084, 16545, 13910, 2951, 1012, 102, 101, 1999, 1996, 2783, 2565, 1010, 2017, 5438, 1999, 1037, 16545, 13910, 1011, 12359, 3746, 2004, 1037, 5164, 3643, 2005, 1996, 23435, 1000, 21933, 3207, 3501, 5051, 2290, 1013, 8417, 1024, 1014, 1000, 1012, 102, 101, 2612, 1010, 2017, 2064, 5438, 1999, 1037, 1017, 1011, 1040, 9140, 1997, 21933, 5732, 3746, 2951, 2005, 1996, 23435, 1000, 21933, 3207, 3501, 5051, 2290, 1024, 1014, 1000, 1006, 2029, 5836, 1996, 6434, 1997, 1996, 1056, 2546, 1012, 3746, 1012, 21933, 3207, 1035, 16545, 13910, 1006, 1007, 6728, 1007, 1010, 1998, 2017, 2064, 2224, 16371, 8737, 2100, 1010, 14255, 2140, 1010, 2030, 2070, 2060, 18750, 3075, 2000, 3443, 2023, 9140, 1012, 102, 101, 2128, 2863, 2361, 1996, 3746, 7953, 1999, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 1012, 102, 101, 1996, 1056, 2546, 1012, 12324, 1035, 10629, 1035, 13366, 1006, 1007, 3853, 12939, 2017, 2000, 7532, 2048, 2367, 19287, 2362, 2011, 2128, 2863, 14853, 3265, 23435, 5300, 1012, 102, 101, 2005, 2742, 1010, 2017, 2071, 2079, 2242, 2066, 1996, 2206, 2000, 5587, 1037, 2047, 3746, 1011, 6364, 6728, 2000, 1996, 4493, 10629, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 69, 85, 98, 133, 206, 226, 256, 281, 295], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "37678281", "vertexSet": [[{"sent_id": 0, "name": "tf.nn", "pos": [6, 11]}], [{"sent_id": 2, "name": "tf.transpose", "pos": [69, 74]}], [{"sent_id": 0, "name": "tf.nn.embedding_lookup", "pos": [6, 18]}]], "sents": ["There is a function named tf.nn.embedding_lookup(params, ind)  which retrieves the rows of the params tensor.", "To achieve what you want, we can first transpose the tensor t from which you want to select certain columns from.", "Then look up the rows of tf.transpose(t) (columns of t).", "After the selection, we transpose the result back.", "<code>Code Snippet</code>."], "sent_idxs": [101, 2045, 2003, 1037, 3853, 2315, 1056, 2546, 1012, 1050, 2078, 1012, 7861, 8270, 4667, 1035, 2298, 6279, 1006, 11498, 5244, 1010, 27427, 1007, 2029, 12850, 2015, 1996, 10281, 1997, 1996, 11498, 5244, 23435, 1012, 102, 101, 2000, 6162, 2054, 2017, 2215, 1010, 2057, 2064, 2034, 9099, 20688, 1996, 23435, 1056, 2013, 2029, 2017, 2215, 2000, 7276, 3056, 7753, 2013, 1012, 102, 101, 2059, 2298, 2039, 1996, 10281, 1997, 1056, 2546, 1012, 9099, 20688, 1006, 1056, 1007, 1006, 7753, 1997, 1056, 1007, 1012, 102, 101, 2044, 1996, 4989, 1010, 2057, 9099, 20688, 1996, 2765, 2067, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 36, 62, 84, 97, 111], "sent_pos": [0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "56597079", "vertexSet": [[{"sent_id": 6, "name": "tf.compat", "pos": [188, 194]}], [{"sent_id": 6, "name": "tf.compat.v1", "pos": [188, 197]}], [{"sent_id": 6, "name": "tf.graph_util", "pos": [169, 176]}], [{"sent_id": 6, "name": "tf.compat.v1.graph_util", "pos": [188, 202]}], [{"sent_id": 6, "name": "tf.graph_util.convert_variables_to_constants", "pos": [169, 185]}], [{"sent_id": 6, "name": "tf.compat.v1.graph_util.convert_variables_to_constants", "pos": [188, 211]}]], "sents": ["I have also faced this same problem while migrating from tensorflow1.x to tensoflow2.0 beta.", "This problem can be solved by 2 methods:", "1st is to go to the tensflow2.0 docs search for the methods you have used and change the syntax for each line &.", "To use google's tf_ugrade_v2 script.", "tf_upgrade_v2 --infile your_tf1_script_file --outfile converted_tf2_file", "You try above command to change your tensorflow1.x script to tensorflow2.0, it will solve all your problem.", "Also, you can rename the method (Manual step by refering documentation)\nRename 'tf.graph_util.convert_variables_to_constants' to 'tf.compat.v1.graph_util.convert_variables_to_constants'", "The measure problem is that in tensorflow2.0 is that many syntax and function has changed try referring the tensoflow2.0 docs or use the google's tf_upgrade_v2 script"], "sent_idxs": [101, 1045, 2031, 2036, 4320, 2023, 2168, 3291, 2096, 28636, 2013, 23435, 12314, 2487, 1012, 1060, 2000, 15295, 11253, 8261, 2475, 1012, 1014, 8247, 1012, 102, 101, 2023, 3291, 2064, 2022, 13332, 2011, 1016, 4725, 1024, 102, 101, 3083, 2003, 2000, 2175, 2000, 1996, 15295, 12314, 2475, 1012, 1014, 9986, 2015, 3945, 2005, 1996, 4725, 2017, 2031, 2109, 1998, 2689, 1996, 20231, 2005, 2169, 2240, 1004, 1012, 102, 101, 2000, 2224, 8224, 1005, 1055, 1056, 2546, 1035, 1057, 24170, 1035, 1058, 2475, 5896, 1012, 102, 101, 1056, 2546, 1035, 12200, 1035, 1058, 2475, 1011, 1011, 1999, 8873, 2571, 2115, 1035, 1056, 2546, 2487, 1035, 5896, 1035, 5371, 1011, 1011, 2041, 8873, 2571, 4991, 1035, 1056, 2546, 2475, 1035, 5371, 102, 101, 2017, 3046, 2682, 3094, 2000, 2689, 2115, 23435, 12314, 2487, 1012, 1060, 5896, 2000, 23435, 12314, 2475, 1012, 1014, 1010, 2009, 2097, 9611, 2035, 2115, 3291, 1012, 102, 101, 2036, 1010, 2017, 2064, 14916, 14074, 1996, 4118, 1006, 6410, 3357, 2011, 6523, 2075, 12653, 1007, 14916, 14074, 1005, 1056, 2546, 1012, 10629, 1035, 21183, 4014, 1012, 10463, 1035, 10857, 1035, 2000, 1035, 5377, 2015, 1005, 2000, 1005, 1056, 2546, 1012, 4012, 4502, 2102, 1012, 1058, 2487, 1012, 10629, 1035, 21183, 4014, 1012, 10463, 1035, 10857, 1035, 2000, 1035, 5377, 2015, 1005, 102, 101, 1996, 5468, 3291, 2003, 2008, 1999, 23435, 12314, 2475, 1012, 1014, 2003, 2008, 2116, 20231, 1998, 3853, 2038, 2904, 3046, 7727, 1996, 15295, 11253, 8261, 2475, 1012, 1014, 9986, 2015, 2030, 2224, 1996, 8224, 1005, 1055, 1056, 2546, 1035, 12200, 1035, 1058, 2475, 5896, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [1, 0], [1, 2], [1, 3], [1, 4], [1, 5], [2, 0], [2, 1], [2, 3], [2, 4], [2, 5], [3, 0], [3, 1], [3, 2], [3, 4], [3, 5], [4, 0], [4, 1], [4, 2], [4, 3], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4]], "sent_ends": [0, 26, 37, 68, 85, 120, 149, 213, 259], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34272341", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy_with_logits", "pos": [44, 61]}], [{"sent_id": 0, "name": "tf.nn.softmax", "pos": [1, 9]}, {"sent_id": 2, "name": "tf.nn.softmax", "pos": [44, 52]}]], "sents": ["tf.nn.softmax computes the forward propagation through a softmax layer.", "You use it during evaluation of the model when you compute the probabilities that the model outputs.", "tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer.", "It is only used during training.", "The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them)."], "sent_idxs": [101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 24134, 2015, 1996, 2830, 20594, 2083, 1037, 3730, 17848, 6741, 1012, 102, 101, 2017, 2224, 2009, 2076, 9312, 1997, 1996, 2944, 2043, 2017, 24134, 1996, 4013, 3676, 14680, 2008, 1996, 2944, 27852, 1012, 102, 101, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1035, 2007, 1035, 8833, 12762, 24134, 2015, 1996, 3465, 2005, 1037, 3730, 17848, 6741, 1012, 102, 101, 2009, 2003, 2069, 2109, 2076, 2731, 1012, 102, 101, 1996, 8833, 12762, 2024, 1996, 4895, 12131, 9067, 3550, 8833, 4013, 3676, 14680, 6434, 1996, 2944, 1006, 1996, 5300, 6434, 2077, 1996, 3730, 17848, 3671, 3989, 2003, 4162, 2000, 2068, 1007, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 21, 43, 72, 81, 115], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49468139", "vertexSet": [[{"sent_id": 5, "name": "tf.session", "pos": [147, 151]}, {"sent_id": 8, "name": "tf.session", "pos": [236, 240]}], [{"sent_id": 0, "name": "tf.summary", "pos": [7, 11]}], [{"sent_id": 0, "name": "tf.summary.merge_all", "pos": [7, 15]}]], "sents": ["You can find the implementation of tf.summary.merge_all() here.", "It works by calling this function, which gets the collection from the graph returned by get_default_graph().", "The documentation for that function is as follows:", "<code>Code Snippet</code>.", "So, in your code without the session context manager, the problem is not necessarily the fact that you're not in a session; the problem is that the graph with the summary is not the default graph, and you have not entered a context (like the session) with that graph.", "There are a few different ways to \"solve\" this without using the with tf.Session(graph=graph) as sess: context manager:", "One option is to merge the summaries together while you still have graph as the default graph:", "<code>Code Snippet</code>.", "Another option is to explicitly __enter__() the session before merging the summaries (this is pretty much identical to what happens internally in python in the with tf.Session(graph=graph) as sess: statement):", "<code>Code Snippet</code>."], "sent_idxs": [101, 2017, 2064, 2424, 1996, 7375, 1997, 1056, 2546, 1012, 12654, 1012, 13590, 1035, 2035, 1006, 1007, 2182, 1012, 102, 101, 2009, 2573, 2011, 4214, 2023, 3853, 1010, 2029, 4152, 1996, 3074, 2013, 1996, 10629, 2513, 2011, 2131, 1035, 12398, 1035, 10629, 1006, 1007, 1012, 102, 101, 1996, 12653, 2005, 2008, 3853, 2003, 2004, 4076, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2061, 1010, 1999, 2115, 3642, 2302, 1996, 5219, 6123, 3208, 1010, 1996, 3291, 2003, 2025, 9352, 1996, 2755, 2008, 2017, 1005, 2128, 2025, 1999, 1037, 5219, 1025, 1996, 3291, 2003, 2008, 1996, 10629, 2007, 1996, 12654, 2003, 2025, 1996, 12398, 10629, 1010, 1998, 2017, 2031, 2025, 3133, 1037, 6123, 1006, 2066, 1996, 5219, 1007, 2007, 2008, 10629, 1012, 102, 101, 2045, 2024, 1037, 2261, 2367, 3971, 2000, 1000, 9611, 1000, 2023, 2302, 2478, 1996, 2007, 1056, 2546, 1012, 5219, 1006, 10629, 1027, 10629, 1007, 2004, 7367, 4757, 1024, 6123, 3208, 1024, 102, 101, 2028, 5724, 2003, 2000, 13590, 1996, 7680, 7849, 3111, 2362, 2096, 2017, 2145, 2031, 10629, 2004, 1996, 12398, 10629, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2178, 5724, 2003, 2000, 12045, 1035, 1035, 4607, 1035, 1035, 1006, 1007, 1996, 5219, 2077, 16468, 1996, 7680, 7849, 3111, 1006, 2023, 2003, 3492, 2172, 7235, 2000, 2054, 6433, 16058, 1999, 18750, 1999, 1996, 2007, 1056, 2546, 1012, 5219, 1006, 10629, 1027, 10629, 1007, 2004, 7367, 4757, 1024, 4861, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [1, 0], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 20, 46, 57, 71, 131, 164, 186, 200, 253, 267], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "39453210", "vertexSet": [[{"sent_id": 5, "name": "tf.variable.name", "pos": [199, 205]}], [{"sent_id": 0, "name": "tf.train", "pos": [14, 18]}, {"sent_id": 1, "name": "tf.train", "pos": [72, 76]}, {"sent_id": 3, "name": "tf.train", "pos": [107, 111]}], [{"sent_id": 0, "name": "tf.train.saver", "pos": [14, 21]}, {"sent_id": 1, "name": "tf.train.saver", "pos": [72, 79]}, {"sent_id": 3, "name": "tf.train.saver", "pos": [107, 114]}], [{"sent_id": 1, "name": "tf.all_variables", "pos": [45, 51]}]], "sents": ["The standard way to save variables in TensorFlow is to use a tf.train.Saver object.", "By default it saves all of the variables in your problem (i.e., the results of tf.all_variables()), but you can save variables selectively by passing the var_list optional argument to the tf.train.Saver constructor:", "<code>Code Snippet</code>.", "Note that if you pass a dictionary to the tf.train.Saver constructor (such as the weights and/or biases dictionaries from your question), TensorFlow will use the dictionary key (e.g.", "'wc1_0') as the name for the corresponding variable in any checkpoint files it creates or consumes.", "By default, or if you pass a list of tf.Variable objects to the constructor, TensorFlow will use the tf.Variable.name property instead.", "Passing a dictionary gives you the ability to share checkpoints between models that give different Variable.name properties to each variable.", "This detail is only important if you want to use the created checkpoints with another model."], "sent_idxs": [101, 1996, 3115, 2126, 2000, 3828, 10857, 1999, 23435, 12314, 2003, 2000, 2224, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 4874, 1012, 102, 101, 2011, 12398, 2009, 13169, 2035, 1997, 1996, 10857, 1999, 2115, 3291, 1006, 1045, 1012, 1041, 1012, 1010, 1996, 3463, 1997, 1056, 2546, 1012, 2035, 1035, 10857, 1006, 1007, 1007, 1010, 2021, 2017, 2064, 3828, 10857, 13228, 2135, 2011, 4458, 1996, 13075, 1035, 2862, 11887, 6685, 2000, 1996, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 9570, 2953, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 2065, 2017, 3413, 1037, 9206, 2000, 1996, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 9570, 2953, 1006, 2107, 2004, 1996, 15871, 1998, 1013, 2030, 13827, 2229, 4487, 7542, 12086, 2013, 2115, 3160, 1007, 1010, 23435, 12314, 2097, 2224, 1996, 9206, 3145, 1006, 1041, 1012, 1043, 1012, 102, 101, 1005, 15868, 2487, 1035, 1014, 1005, 1007, 2004, 1996, 2171, 2005, 1996, 7978, 8023, 1999, 2151, 26520, 6764, 2009, 9005, 2030, 16678, 2015, 1012, 102, 101, 2011, 12398, 1010, 2030, 2065, 2017, 3413, 1037, 2862, 1997, 1056, 2546, 1012, 8023, 5200, 2000, 1996, 9570, 2953, 1010, 23435, 12314, 2097, 2224, 1996, 1056, 2546, 1012, 8023, 1012, 2171, 3200, 2612, 1012, 102, 101, 4458, 1037, 9206, 3957, 2017, 1996, 3754, 2000, 3745, 26520, 2015, 2090, 4275, 2008, 2507, 2367, 8023, 1012, 2171, 5144, 2000, 2169, 8023, 1012, 102, 101, 2023, 6987, 2003, 2069, 2590, 2065, 2017, 2215, 2000, 2224, 1996, 2580, 26520, 2015, 2007, 2178, 2944, 1012, 102], "labels": [], "na_triple": [[0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2]], "sent_ends": [0, 24, 83, 97, 147, 173, 209, 235, 255], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45580471", "vertexSet": [[{"sent_id": 5, "name": "tf.tanh", "pos": [159, 164]}], [{"sent_id": 5, "name": "tf.atan", "pos": [148, 153]}]], "sents": ["Check out the Udacity self-driving-car models which take an input image from a dash cam and predict a steering angle (i.e.", "continuous scalar) to stay on the road...usually using a regression output after one or more fully connected layers on top of the CNN layers.", "https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models", "Here is a typical model:", "https://github.com/udacity/self-driving-car/tree/master/steering-models/community-models/autumn", "...it uses tf.atan() or you can use tf.tanh() or just linear to get your final output y.", "Use MSE for your loss function.", "Here is another example in keras...", "<code>Code Snippet</code>.", "They key difference from the MNIST examples is that instead of funneling down to a N-dim vector of logits into softmax w/ cross entropy loss take it down to a 1-dim vector w/ MSE loss."], "sent_idxs": [101, 4638, 2041, 1996, 20904, 6305, 3012, 2969, 1011, 4439, 1011, 2482, 4275, 2029, 2202, 2019, 7953, 3746, 2013, 1037, 11454, 11503, 1998, 16014, 1037, 9602, 6466, 1006, 1045, 1012, 1041, 1012, 102, 101, 7142, 26743, 2099, 1007, 2000, 2994, 2006, 1996, 2346, 1012, 1012, 1012, 2788, 2478, 1037, 26237, 6434, 2044, 2028, 2030, 2062, 3929, 4198, 9014, 2006, 2327, 1997, 1996, 13229, 9014, 1012, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 20904, 6305, 3012, 1013, 2969, 1011, 4439, 1011, 2482, 1013, 3392, 1013, 3040, 1013, 9602, 1011, 4275, 1013, 2451, 1011, 4275, 102, 101, 2182, 2003, 1037, 5171, 2944, 1024, 102, 101, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 20904, 6305, 3012, 1013, 2969, 1011, 4439, 1011, 2482, 1013, 3392, 1013, 3040, 1013, 9602, 1011, 4275, 1013, 2451, 1011, 4275, 1013, 7114, 102, 101, 1012, 1012, 1012, 2009, 3594, 1056, 2546, 1012, 29533, 2078, 1006, 1007, 2030, 2017, 2064, 2224, 1056, 2546, 1012, 9092, 2232, 1006, 1007, 2030, 2074, 7399, 2000, 2131, 2115, 2345, 6434, 1061, 1012, 102, 101, 2224, 5796, 2063, 2005, 2115, 3279, 3853, 1012, 102, 101, 2182, 2003, 2178, 2742, 1999, 17710, 8180, 1012, 1012, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2027, 3145, 4489, 2013, 1996, 24098, 2923, 4973, 2003, 2008, 2612, 1997, 25102, 2075, 2091, 2000, 1037, 1050, 1011, 11737, 9207, 1997, 8833, 12762, 2046, 3730, 17848, 1059, 1013, 2892, 23077, 3279, 2202, 2009, 2091, 2000, 1037, 1015, 1011, 11737, 9207, 1059, 1013, 5796, 2063, 3279, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 33, 66, 99, 107, 142, 177, 187, 199, 213, 262], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "36088778", "vertexSet": [[{"sent_id": 3, "name": "tf.transpose", "pos": [108, 113]}], [{"sent_id": 3, "name": "tf.reshape", "pos": [98, 104]}], [{"sent_id": 0, "name": "tf.gather", "pos": [16, 20]}, {"sent_id": 1, "name": "tf.gather", "pos": [69, 73]}]], "sents": ["This is possible in TensorFlow, but slightly inconvenient, because tf.gather() currently only works with one-dimensional indices, and only selects slices from the 0th dimension of a tensor.", "However, it is still possible to solve your problem efficiently, by transforming the arguments so that they can be passed to tf.gather():", "<code>Code Snippet</code>.", "Note that, since this uses tf.reshape() and not tf.transpose(), it doesn't need to modify the (potentially large) data in the logits tensor, so it should be fairly efficient."], "sent_idxs": [101, 2023, 2003, 2825, 1999, 23435, 12314, 1010, 2021, 3621, 4297, 2239, 8159, 11638, 1010, 2138, 1056, 2546, 1012, 8587, 1006, 1007, 2747, 2069, 2573, 2007, 2028, 1011, 8789, 29299, 1010, 1998, 2069, 27034, 25609, 2013, 1996, 1014, 2705, 9812, 1997, 1037, 23435, 1012, 102, 101, 2174, 1010, 2009, 2003, 2145, 2825, 2000, 9611, 2115, 3291, 18228, 1010, 2011, 17903, 1996, 9918, 2061, 2008, 2027, 2064, 2022, 2979, 2000, 1056, 2546, 1012, 8587, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 3602, 2008, 1010, 2144, 2023, 3594, 1056, 2546, 1012, 24501, 3270, 5051, 1006, 1007, 1998, 2025, 1056, 2546, 1012, 9099, 20688, 1006, 1007, 1010, 2009, 2987, 1005, 1056, 2342, 2000, 19933, 1996, 1006, 9280, 2312, 1007, 2951, 1999, 1996, 8833, 12762, 23435, 1010, 2061, 2009, 2323, 2022, 7199, 8114, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [3]}, {"r": "S1", "h": 0, "t": 1, "evidence": [3]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 45, 77, 91, 143], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "48932957", "vertexSet": [[{"sent_id": 5, "name": "tf.nn.rnn_cell.basicrnncell", "pos": [101, 116]}], [{"sent_id": 5, "name": "tf.contrib.rnn.basicrnncell", "pos": [117, 131]}], [{"sent_id": 3, "name": "tf.summary", "pos": [45, 49]}], [{"sent_id": 2, "name": "tf.get_variable", "pos": [33, 39]}], [{"sent_id": 3, "name": "tf.summary.histogram", "pos": [45, 53]}]], "sents": ["But I'd like to log the weight variables to summary writer.", "Is there any way to do this?", "You can get a variable via tf.get_variable() function.", "tf.summary.histogram accepts the tensor instance, so it'd be easier to use Graph.get_tensor_by_name():", "<code>Code Snippet</code>.", "By the way, do we use tf.nn.rnn_cell.BasicRNNCell or tf.contrib.rnn.BasicRNNCell?", "Or are they identical?", "Yes, they are synonyms, but I prefer to use tf.nn.rnn_cell package, because everything in tf.contrib is sort of experimental and can be changed in 1.x versions."], "sent_idxs": [101, 2021, 1045, 1005, 1040, 2066, 2000, 8833, 1996, 3635, 10857, 2000, 12654, 3213, 1012, 102, 101, 2003, 2045, 2151, 2126, 2000, 2079, 2023, 1029, 102, 101, 2017, 2064, 2131, 1037, 8023, 3081, 1056, 2546, 1012, 2131, 1035, 8023, 1006, 1007, 3853, 1012, 102, 101, 1056, 2546, 1012, 12654, 1012, 2010, 3406, 13113, 13385, 1996, 23435, 6013, 1010, 2061, 2009, 1005, 1040, 2022, 6082, 2000, 2224, 10629, 1012, 2131, 1035, 23435, 1035, 2011, 1035, 2171, 1006, 1007, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2011, 1996, 2126, 1010, 2079, 2057, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 1012, 3937, 6826, 5897, 3363, 2030, 1056, 2546, 1012, 9530, 18886, 2497, 1012, 29300, 2078, 1012, 3937, 6826, 5897, 3363, 1029, 102, 101, 2030, 2024, 2027, 7235, 1029, 102, 101, 2748, 1010, 2027, 2024, 10675, 2015, 1010, 2021, 1045, 9544, 2000, 2224, 1056, 2546, 1012, 1050, 2078, 1012, 29300, 2078, 1035, 3526, 7427, 1010, 2138, 2673, 1999, 1056, 2546, 1012, 9530, 18886, 2497, 2003, 4066, 1997, 6388, 1998, 2064, 2022, 2904, 1999, 1015, 1012, 1060, 4617, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [5, 7]}, {"r": "S1", "h": 1, "t": 0, "evidence": [5, 7]}], "na_triple": [[0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 0], [2, 1], [2, 3], [2, 4], [3, 0], [3, 1], [3, 2], [3, 4], [4, 0], [4, 1], [4, 2], [4, 3]], "sent_ends": [0, 16, 26, 44, 79, 93, 133, 140, 189], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "51089993", "vertexSet": [[{"sent_id": 0, "name": "tf.keras.layers", "pos": [18, 25]}, {"sent_id": 1, "name": "tf.keras.layers", "pos": [51, 58]}], [{"sent_id": 0, "name": "tf.layers", "pos": [1, 5]}, {"sent_id": 1, "name": "tf.layers", "pos": [43, 47]}, {"sent_id": 4, "name": "tf.layers", "pos": [120, 124]}], [{"sent_id": 1, "name": "tf.keras.layers.dense", "pos": [51, 60]}]], "sents": ["tf.layers module is Tensorflow attempt at creating a Keras like API whereas tf.keras.layers is a compatibility wrapper.", "In fact, most of the implementation refers back to tf.layers, for example the tf.keras.layers.Dense inherits the core implementation:", "<code>Code Snippet</code>.", "Because the tf.keras compatibility module is checked into the Tensorflow repo separately, it might lack behind what Keras actually offers.", "I would use Keras directly or tf.layers but not necessarily mix them."], "sent_idxs": [101, 1056, 2546, 1012, 9014, 11336, 2003, 23435, 12314, 3535, 2012, 4526, 1037, 17710, 8180, 2066, 17928, 6168, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 2003, 1037, 21778, 10236, 4842, 1012, 102, 101, 1999, 2755, 1010, 2087, 1997, 1996, 7375, 5218, 2067, 2000, 1056, 2546, 1012, 9014, 1010, 2005, 2742, 1996, 1056, 2546, 1012, 17710, 8180, 1012, 9014, 1012, 9742, 22490, 2015, 1996, 4563, 7375, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2138, 1996, 1056, 2546, 1012, 17710, 8180, 21778, 11336, 2003, 7039, 2046, 1996, 23435, 12314, 16360, 2080, 10329, 1010, 2009, 2453, 3768, 2369, 2054, 17710, 8180, 2941, 4107, 1012, 102, 101, 1045, 2052, 2224, 17710, 8180, 3495, 2030, 1056, 2546, 1012, 9014, 2021, 2025, 9352, 4666, 2068, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 4]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 4]}], "na_triple": [[0, 2], [1, 2], [2, 0], [2, 1]], "sent_ends": [0, 32, 67, 81, 112, 131], "sent_pos": [0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0]}, {"title": "49896013", "vertexSet": [[{"sent_id": 1, "name": "tf.concat", "pos": [32, 37]}], [{"sent_id": 0, "name": "tf.stack", "pos": [1, 5]}]], "sents": ["tf.stack concatenate the given tensors along a new dimension.", "If you want to concatenate across an existing dimension, use tf.concat:", "<code>Code Snippet</code>."], "sent_idxs": [101, 1056, 2546, 1012, 9991, 9530, 16280, 12556, 1996, 2445, 23435, 2015, 2247, 1037, 2047, 9812, 1012, 102, 101, 2065, 2017, 2215, 2000, 9530, 16280, 12556, 2408, 2019, 4493, 9812, 1010, 2224, 1056, 2546, 1012, 9530, 11266, 1024, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 18, 39, 53], "sent_pos": [0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45183635", "vertexSet": [[{"sent_id": 2, "name": "tf.nn.cross_entropy", "pos": [79, 88]}], [{"sent_id": 2, "name": "tf.nn.softmax", "pos": [53, 61]}, {"sent_id": 2, "name": "tf.nn.softmax", "pos": [68, 76]}], [{"sent_id": 2, "name": "tf.nn.softmax_cross_entropy", "pos": [53, 65]}]], "sents": ["Above answers have enough description for the asked question.", "Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions.", "Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()", "You can find prominent difference between them in a resource intensive model."], "sent_idxs": [101, 2682, 6998, 2031, 2438, 6412, 2005, 1996, 2356, 3160, 1012, 102, 101, 5815, 2000, 2008, 1010, 23435, 12314, 2038, 23569, 27605, 6924, 1996, 3169, 1997, 11243, 1996, 13791, 3853, 2059, 20177, 3465, 2478, 2049, 2219, 13791, 2628, 2011, 3465, 4972, 1012, 102, 101, 6516, 2009, 2003, 1037, 2204, 3218, 2000, 2224, 1024, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1035, 2892, 1035, 23077, 1006, 1007, 2058, 1056, 2546, 1012, 1050, 2078, 1012, 3730, 17848, 1006, 1007, 1025, 1056, 2546, 1012, 1050, 2078, 1012, 2892, 1035, 23077, 1006, 1007, 102, 101, 2017, 2064, 2424, 4069, 4489, 2090, 2068, 1999, 1037, 7692, 11806, 2944, 1012, 102], "labels": [{"r": "S1", "h": 2, "t": 0, "evidence": [1, 2]}, {"r": "S1", "h": 0, "t": 2, "evidence": [1, 2]}, {"r": "S1", "h": 2, "t": 1, "evidence": [1, 2]}, {"r": "S1", "h": 1, "t": 2, "evidence": [1, 2]}], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 12, 43, 91, 106], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "34402643", "vertexSet": [[{"sent_id": 8, "name": "tf.train", "pos": [234, 238]}], [{"sent_id": 8, "name": "tf.train.saver", "pos": [234, 241]}]], "sents": ["The issue here is that you are running the \"train_step\" target, which performs much more work than just inference.", "In particular, it attempts to update the variables W and b with the result of the gradient descent step.", "The error message", "<code>Code Snippet</code>.", "...means that one of the nodes you attempted to run (\"train_step/update_W/ApplyGradientDescent\") expected a mutable input (with type float_ref) but it got an immutable input (with type float) because the value was fed in.", "There are (at least) two possible solutions:", "If you only want to see predictions for a given input and given weights, fetch \"softmax:0\" instead of \"train_step\" in the call to Session::Run().", "If you want to perform training in C++, do not feed W and b, but instead assign values to those variables, then continue to execute \"train_step\".", "You may find it easier to create a tf.train.Saver when you build the graph in Python, and then invoke the operations that it produces to save and restore values from a checkpoint."], "sent_idxs": [101, 1996, 3277, 2182, 2003, 2008, 2017, 2024, 2770, 1996, 1000, 3345, 1035, 3357, 1000, 4539, 1010, 2029, 10438, 2172, 2062, 2147, 2084, 2074, 28937, 1012, 102, 101, 1999, 3327, 1010, 2009, 4740, 2000, 10651, 1996, 10857, 1059, 1998, 1038, 2007, 1996, 2765, 1997, 1996, 17978, 6934, 3357, 1012, 102, 101, 1996, 7561, 4471, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 1012, 1012, 1012, 2965, 2008, 2028, 1997, 1996, 14164, 2017, 4692, 2000, 2448, 1006, 1000, 3345, 1035, 3357, 1013, 10651, 1035, 1059, 1013, 6611, 16307, 11638, 6155, 13013, 1000, 1007, 3517, 1037, 14163, 10880, 7953, 1006, 2007, 2828, 14257, 1035, 25416, 1007, 2021, 2009, 2288, 2019, 10047, 28120, 3085, 7953, 1006, 2007, 2828, 14257, 1007, 2138, 1996, 3643, 2001, 7349, 1999, 1012, 102, 101, 2045, 2024, 1006, 2012, 2560, 1007, 2048, 2825, 7300, 1024, 102, 101, 2065, 2017, 2069, 2215, 2000, 2156, 20932, 2005, 1037, 2445, 7953, 1998, 2445, 15871, 1010, 18584, 1000, 3730, 17848, 1024, 1014, 1000, 2612, 1997, 1000, 3345, 1035, 3357, 1000, 1999, 1996, 2655, 2000, 5219, 1024, 1024, 2448, 1006, 1007, 1012, 102, 101, 2065, 2017, 2215, 2000, 4685, 2731, 1999, 1039, 1009, 1009, 1010, 2079, 2025, 5438, 1059, 1998, 1038, 1010, 2021, 2612, 23911, 5300, 2000, 2216, 10857, 1010, 2059, 3613, 2000, 15389, 1000, 3345, 1035, 3357, 1000, 1012, 102, 101, 2017, 2089, 2424, 2009, 6082, 2000, 3443, 1037, 1056, 2546, 1012, 3345, 1012, 3828, 2099, 2043, 2017, 3857, 1996, 10629, 1999, 18750, 1010, 1998, 2059, 1999, 6767, 3489, 1996, 3136, 2008, 2009, 7137, 2000, 3828, 1998, 9239, 5300, 2013, 1037, 26520, 1012, 102], "labels": [], "na_triple": [[0, 1], [1, 0]], "sent_ends": [0, 27, 50, 55, 69, 133, 145, 187, 225, 269], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "45347628", "vertexSet": [[{"sent_id": 0, "name": "tf.gradients", "pos": [1, 6]}, {"sent_id": 1, "name": "tf.gradients", "pos": [69, 74]}, {"sent_id": 3, "name": "tf.gradients", "pos": [148, 153]}], [{"sent_id": 2, "name": "tf.train.optimizer.compute_gradients", "pos": [88, 101]}]], "sents": ["tf.gradients does not allow you to compute Jacobians, it aggregates the gradients of each input for every output (something like the summation of each column of the actual Jacobian matrix).", "In fact, there is no \"good\" way of computing Jacobians in TensorFlow (basically you have to call tf.gradients once per output, see this issue).", "With respect to tf.train.Optimizer.compute_gradients, yes, its result is basically the same, but taking care of some details automatically and with slightly more convenient output format.", "If you look at the implementation, you will see that, at its core, is a call to tf.gradients (in this case aliased to gradients.gradients), but it is useful for optimizer implementations to have the surrounding logic already implemented.", "Also, having it as a method allows for extensible behaviour in subclasses, either to implement some kind of optimization strategy (not very likely at the compute_gradients step, really) or for auxiliary purposes, like tracing or debugging."], "sent_idxs": [101, 1056, 2546, 1012, 17978, 2015, 2515, 2025, 3499, 2017, 2000, 24134, 6213, 7066, 1010, 2009, 9572, 2015, 1996, 17978, 2015, 1997, 2169, 7953, 2005, 2296, 6434, 1006, 2242, 2066, 1996, 7680, 28649, 1997, 2169, 5930, 1997, 1996, 5025, 6213, 2937, 8185, 1007, 1012, 102, 101, 1999, 2755, 1010, 2045, 2003, 2053, 1000, 2204, 1000, 2126, 1997, 9798, 6213, 7066, 1999, 23435, 12314, 1006, 10468, 2017, 2031, 2000, 2655, 1056, 2546, 1012, 17978, 2015, 2320, 2566, 6434, 1010, 2156, 2023, 3277, 1007, 1012, 102, 101, 2007, 4847, 2000, 1056, 2546, 1012, 3345, 1012, 23569, 27605, 6290, 1012, 24134, 1035, 17978, 2015, 1010, 2748, 1010, 2049, 2765, 2003, 10468, 1996, 2168, 1010, 2021, 2635, 2729, 1997, 2070, 4751, 8073, 1998, 2007, 3621, 2062, 14057, 6434, 4289, 1012, 102, 101, 2065, 2017, 2298, 2012, 1996, 7375, 1010, 2017, 2097, 2156, 2008, 1010, 2012, 2049, 4563, 1010, 2003, 1037, 2655, 2000, 1056, 2546, 1012, 17978, 2015, 1006, 1999, 2023, 2553, 14593, 2098, 2000, 17978, 2015, 1012, 17978, 2015, 1007, 1010, 2021, 2009, 2003, 6179, 2005, 23569, 27605, 6290, 24977, 2000, 2031, 1996, 4193, 7961, 2525, 7528, 1012, 102, 101, 2036, 1010, 2383, 2009, 2004, 1037, 4118, 4473, 2005, 4654, 25808, 7028, 9164, 1999, 4942, 26266, 2229, 1010, 2593, 2000, 10408, 2070, 2785, 1997, 20600, 5656, 1006, 2025, 2200, 3497, 2012, 1996, 24134, 1035, 17978, 2015, 3357, 1010, 2428, 1007, 2030, 2005, 9830, 5682, 1010, 2066, 16907, 2030, 2139, 8569, 12588, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 2, 3, 4]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 2, 3, 4]}], "na_triple": [], "sent_ends": [0, 45, 84, 127, 185, 239], "sent_pos": [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "44983945", "vertexSet": [[{"sent_id": 0, "name": "tf.gather_nd", "pos": [17, 24]}, {"sent_id": 1, "name": "tf.gather_nd", "pos": [27, 34]}], [{"sent_id": 0, "name": "tf.gather", "pos": [12, 16]}, {"sent_id": 0, "name": "tf.gather", "pos": [17, 21]}, {"sent_id": 1, "name": "tf.gather", "pos": [27, 31]}, {"sent_id": 1, "name": "tf.gather", "pos": [38, 42]}, {"sent_id": 2, "name": "tf.gather", "pos": [64, 68]}]], "sents": ["The closest function in tensorflow to np.take are tf.gather and tf.gather_nd.", "tf.gather_nd is more general than tf.gather (and np.take) as it can slices through several dimensions at once.", "A noticeable restriction of tf.gather[_nd] compared to np.take is that they slice through the first dimensions of the tensor only -- you can't slice through inner dimensions.", "When you want to slice through an arbitrary dimension (as in your case), you need to transpose the array to put the slice dimensions first, gather, then transpose back."], "sent_idxs": [101, 1996, 7541, 3853, 1999, 23435, 12314, 2000, 27937, 1012, 2202, 2024, 1056, 2546, 1012, 8587, 1998, 1056, 2546, 1012, 8587, 1035, 1050, 2094, 1012, 102, 101, 1056, 2546, 1012, 8587, 1035, 1050, 2094, 2003, 2062, 2236, 2084, 1056, 2546, 1012, 8587, 1006, 1998, 27937, 1012, 2202, 1007, 2004, 2009, 2064, 25609, 2083, 2195, 9646, 2012, 2320, 1012, 102, 101, 1037, 17725, 16840, 1997, 1056, 2546, 1012, 8587, 1031, 1035, 1050, 2094, 1033, 4102, 2000, 27937, 1012, 2202, 2003, 2008, 2027, 14704, 2083, 1996, 2034, 9646, 1997, 1996, 23435, 2069, 1011, 1011, 2017, 2064, 1005, 1056, 14704, 2083, 5110, 9646, 1012, 102, 101, 2043, 2017, 2215, 2000, 14704, 2083, 2019, 15275, 9812, 1006, 2004, 1999, 2115, 2553, 1007, 1010, 2017, 2342, 2000, 9099, 20688, 1996, 9140, 2000, 2404, 1996, 14704, 9646, 2034, 1010, 8587, 1010, 2059, 9099, 20688, 2067, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}], "na_triple": [], "sent_ends": [0, 26, 59, 102, 141], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "47526013", "vertexSet": [[{"sent_id": 0, "name": "tf.metrics.accuracy", "pos": [22, 29]}, {"sent_id": 2, "name": "tf.metrics.accuracy", "pos": [73, 80]}], [{"sent_id": 3, "name": "tf.metric.mean_squared_error", "pos": [94, 104]}]], "sents": ["Turns out, since this is a multi-class Linear Regression problem, and not a classification problem, that tf.metrics.accuracy is not the right approach.", "Instead of displaying the accuracy of my model in terms of percentage, I instead focused on reducing the Mean Square Error (MSE) instead.", "From looking at other examples, tf.metrics.accuracy is never used for Linear Regression, and only classification.", "Normally tf.metric.mean_squared_error is the right approach.", "I implemented two ways of calculating the total MSE of my predictions to my testing data...", "<code>Code Snippet</code>.", "OR", "<code>Code Snippet</code>.", "They both do the same but obviously the second approach is more concise.", "There's a good explanation of how to measure the accuracy of a Linear Regression model here."], "sent_idxs": [101, 4332, 2041, 1010, 2144, 2023, 2003, 1037, 4800, 1011, 2465, 7399, 26237, 3291, 1010, 1998, 2025, 1037, 5579, 3291, 1010, 2008, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 2003, 2025, 1996, 2157, 3921, 1012, 102, 101, 2612, 1997, 14962, 1996, 10640, 1997, 2026, 2944, 1999, 3408, 1997, 7017, 1010, 1045, 2612, 4208, 2006, 8161, 1996, 2812, 2675, 7561, 1006, 5796, 2063, 1007, 2612, 1012, 102, 101, 2013, 2559, 2012, 2060, 4973, 1010, 1056, 2546, 1012, 12046, 2015, 1012, 10640, 2003, 2196, 2109, 2005, 7399, 26237, 1010, 1998, 2069, 5579, 1012, 102, 101, 5373, 1056, 2546, 1012, 12046, 1012, 2812, 1035, 19942, 1035, 7561, 2003, 1996, 2157, 3921, 1012, 102, 101, 1045, 7528, 2048, 3971, 1997, 20177, 1996, 2561, 5796, 2063, 1997, 2026, 20932, 2000, 2026, 5604, 2951, 1012, 1012, 1012, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2030, 102, 101, 1026, 3642, 1028, 3642, 1055, 3490, 29519, 1026, 1013, 3642, 1028, 1012, 102, 101, 2027, 2119, 2079, 1996, 2168, 2021, 5525, 1996, 2117, 3921, 2003, 2062, 9530, 18380, 1012, 102, 101, 2045, 1005, 1055, 1037, 2204, 7526, 1997, 2129, 2000, 5468, 1996, 10640, 1997, 1037, 7399, 26237, 2944, 2182, 1012, 102], "labels": [{"r": "S1", "h": 0, "t": 1, "evidence": [0, 1, 2, 3]}, {"r": "S1", "h": 1, "t": 0, "evidence": [0, 1, 2, 3]}], "na_triple": [], "sent_ends": [0, 36, 66, 92, 110, 132, 146, 149, 163, 180, 201], "sent_pos": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {"title": "59531820", "vertexSet": [[{"sent_id": 1, "name": "tf.data.dataset.prefetch", "pos": [52, 63]}], [{"sent_id": 0, "name": "tf.data.dataset", "pos": [1, 8]}, {"sent_id": 1, "name": "tf.data.dataset", "pos": [40, 47]}, {"sent_id": 1, "name": "tf.data.dataset", "pos": [52, 59]}], [{"sent_id": 1, "name": "tf.data.dataset.cache", "pos": [40, 49]}]], "sents": ["tf.data.Dataset is a more abstract object whose job is to define the data pipeline.", "If you want to save intermediate results to speed up your data pipeline you can use tf.data.Dataset.cache() or tf.data.Dataset.prefetch() (more on it here)", "If you are interested in saving the sequence of operations in your data pipeline, I assume there is no such thing and you need to keep the code for data pipeline.", "I am not aware of any method that can extract the graph of data pipeline of Dataset API.", "If anyone is aware of that please add to the answer."], "sent_idxs": [101, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 2003, 1037, 2062, 10061, 4874, 3005, 3105, 2003, 2000, 9375, 1996, 2951, 13117, 1012, 102, 101, 2065, 2017, 2215, 2000, 3828, 7783, 3463, 2000, 3177, 2039, 2115, 2951, 13117, 2017, 2064, 2224, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 17053, 1006, 1007, 2030, 1056, 2546, 1012, 2951, 1012, 2951, 13462, 1012, 3653, 7959, 10649, 1006, 1007, 1006, 2062, 2006, 2009, 2182, 1007, 102, 101, 2065, 2017, 2024, 4699, 1999, 7494, 1996, 5537, 1997, 3136, 1999, 2115, 2951, 13117, 1010, 1045, 7868, 2045, 2003, 2053, 2107, 2518, 1998, 2017, 2342, 2000, 2562, 1996, 3642, 2005, 2951, 13117, 1012, 102, 101, 1045, 2572, 2025, 5204, 1997, 2151, 4118, 2008, 2064, 14817, 1996, 10629, 1997, 2951, 13117, 1997, 2951, 13462, 17928, 1012, 102, 101, 2065, 3087, 2003, 5204, 1997, 2008, 3531, 5587, 2000, 1996, 3437, 1012, 102], "labels": [{"r": "S1", "h": 1, "t": 0, "evidence": [0, 1]}, {"r": "S1", "h": 0, "t": 1, "evidence": [0, 1]}, {"r": "S1", "h": 1, "t": 2, "evidence": [0, 1]}, {"r": "S1", "h": 2, "t": 1, "evidence": [0, 1]}], "na_triple": [[0, 2], [2, 0]], "sent_ends": [0, 23, 72, 107, 129, 143], "sent_pos": [0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]