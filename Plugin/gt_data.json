{
    "tf.nn.rnn_cell.multirnncell":
    {
        "tf.keras.layers.stackedrnncells":
        [
            {
                "title": "55221698",
                "h": "tf.nn.rnn_cell.multirnncell",
                "t": "tf.keras.layers.stackedrnncells",
                "sentences":
                [
                    "If you need some \"old-style\" tensorflow stacked LSTM, you can use tf.nn.rnn_cell.MultiRNNCell (now it's deprecated and replaced with tf.keras.layers.StackedRNNCells):"
                ]
            }
        ]
    },
    "tf.keras.layers.stackedrnncells":
    {
        "tf.nn.rnn_cell.multirnncell":
        [
            {
                "title": "55221698",
                "h": "tf.nn.rnn_cell.multirnncell",
                "t": "tf.keras.layers.stackedrnncells",
                "sentences":
                [
                    "If you need some \"old-style\" tensorflow stacked LSTM, you can use tf.nn.rnn_cell.MultiRNNCell (now it's deprecated and replaced with tf.keras.layers.StackedRNNCells):"
                ]
            }
        ]
    },
    "tf.nn.static_rnn":
    {
        "tf.contrib.rnn.static_rnn":
        [
            {
                "title": "42539696",
                "h": "tf.nn.static_rnn",
                "t": "tf.contrib.rnn.static_rnn",
                "sentences":
                [
                    "Note: before version 1.2 of TensorFlow, the namespace tf.nn.static_rnn did not exist, but only tf.contrib.rnn.static_rnn (which is now an alias for tf.nn.static_rnn)."
                ]
            }
        ],
        "tf.nn.rnn":
        [
            {
                "title": "42539696",
                "h": "tf.nn.rnn",
                "t": "tf.nn.static_rnn",
                "sentences":
                [
                    "tf.nn.rnn is equivalent to tf.nn.static_rnn."
                ]
            }
        ]
    },
    "tf.contrib.rnn.static_rnn":
    {
        "tf.nn.static_rnn":
        [
            {
                "title": "42539696",
                "h": "tf.nn.static_rnn",
                "t": "tf.contrib.rnn.static_rnn",
                "sentences":
                [
                    "Note: before version 1.2 of TensorFlow, the namespace tf.nn.static_rnn did not exist, but only tf.contrib.rnn.static_rnn (which is now an alias for tf.nn.static_rnn)."
                ]
            }
        ]
    },
    "tf.nn.rnn":
    {
        "tf.nn.static_rnn":
        [
            {
                "title": "42539696",
                "h": "tf.nn.rnn",
                "t": "tf.nn.static_rnn",
                "sentences":
                [
                    "tf.nn.rnn is equivalent to tf.nn.static_rnn."
                ]
            }
        ],
        "tf.nn.dynamic_rnn":
        [
            {
                "title": "36267491",
                "h": "tf.nn.rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "For RNNs specifically, there are two options: tf.nn.rnn, and tf.nn.dynamic_rnn.",
                    "Neither one creates or destroys graphs temporarily.",
                    "The first function creates T subgraphs, where T is the length of the python list of inputs you provide (that is, inputs is a len T python list of shape [batch, depth] tensors).",
                    "tf.nn.rnn always expects a fixed number of time steps.",
                    "Note, you can control which subgraphs are run for a given step by passing the sequence_length parameter; the function then uses conditional evaluation (tf.cond) to determine what operations get run.",
                    "In contrast, dynamic_rnn uses a special TensorFlow while loop and other trickery that introduces a limited type of loop in the graph structure.",
                    "In this case, there is exactly one subgraph for the \"time step\", and it gets run over and over until your input has been processed.",
                    "In this case your input is a 3D tensor with dimensions [batch, time, depth] (it can be [time, batch, depth] if you set time_major=True); and the first two dimensions can vary from step to step."
                ]
            },
            {
                "title": "38753139",
                "h": "tf.nn.rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "The tf.nn.rnn() and tf.nn.dynamic_rnn() functions accept an argument cell of type tf.nn.rnn_cell.RNNCell."
                ]
            }
        ]
    },
    "tf.group":
    {
        "tf.add_to_collection":
        [
            {
                "title": "53902375",
                "h": "tf.group",
                "t": "tf.add_to_collection",
                "sentences":
                [
                    "tf.group creates an operation inside the computational graph that once evaluated executes all the tensors in the group:",
                    "tf.add_to_collection instead, creates a group of operations not inside the computational graph, but only in the python script.",
                    "In your example, using tf.group or tf.add_to_collection + tf.get_collection is the same: you just need all the operations executed in parallel, hence sess.run(op) and sess.run(tf.get_collection('coll')) have the same behaviour.",
                    "But in the case of the export of a computational graph (that's just an example to make you understand a possible scenario), you can't rely upon a python list, hence you have to use tf.group"
                ]
            }
        ]
    },
    "tf.add_to_collection":
    {
        "tf.group":
        [
            {
                "title": "53902375",
                "h": "tf.group",
                "t": "tf.add_to_collection",
                "sentences":
                [
                    "tf.group creates an operation inside the computational graph that once evaluated executes all the tensors in the group:",
                    "tf.add_to_collection instead, creates a group of operations not inside the computational graph, but only in the python script.",
                    "In your example, using tf.group or tf.add_to_collection + tf.get_collection is the same: you just need all the operations executed in parallel, hence sess.run(op) and sess.run(tf.get_collection('coll')) have the same behaviour.",
                    "But in the case of the export of a computational graph (that's just an example to make you understand a possible scenario), you can't rely upon a python list, hence you have to use tf.group"
                ]
            }
        ]
    },
    "tf.nn.moments":
    {
        "tf.nn.fused_batch_norm":
        [
            {
                "title": "44809583",
                "h": "tf.nn.moments",
                "t": "tf.nn.fused_batch_norm",
                "sentences":
                [
                    "tf.nn.moments is computing the sample variance, whereas tf.nn.fused_batch_norm is computing the unbiased variance estimator.",
                    "The difference between the two is a factor n/n-1, where n is your sample size."
                ]
            }
        ]
    },
    "tf.nn.fused_batch_norm":
    {
        "tf.nn.moments":
        [
            {
                "title": "44809583",
                "h": "tf.nn.moments",
                "t": "tf.nn.fused_batch_norm",
                "sentences":
                [
                    "tf.nn.moments is computing the sample variance, whereas tf.nn.fused_batch_norm is computing the unbiased variance estimator.",
                    "The difference between the two is a factor n/n-1, where n is your sample size."
                ]
            }
        ],
        "tf.nn.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.nn.fused_batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.nn.batch_normalization accepts tensors of any rank greater than 1.."
                ]
            }
        ],
        "tf.contrib.layers.batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.batch_norm_with_global_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.layers.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.layers.batch_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you.."
                ]
            }
        ]
    },
    "tf.contrib.learn.estimator":
    {
        "tf.estimator.estimator":
        [
            {
                "title": "45186929",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.estimator.estimator",
                "sentences":
                [
                    "My guess now is that the tf.contrib.learn.Estimator is an early prototype that got replaced by the tf.estimator.Estimator.",
                    "According to the docs everything in tf.contrib is unstable API that may change at any time and it looks like the tf.estimator module is the stable API that “evolved” from the tf.contrib.learn module."
                ]
            },
            {
                "title": "47249767",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.estimator.estimator",
                "sentences":
                [
                    "First up, you should stop using tf.contrib.learn.Estimator in favor of tf.estimator.Estimator, because contrib is an experimental module, and classes that have graduated to the core API (such es Estimator) automatically get deprecated."
                ]
            }
        ],
        "tf.contrib.learn.linearestimator":
        [
            {
                "title": "44654244",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.contrib.learn.linearestimator",
                "sentences":
                [
                    "You can also use a tf.contrib.learn.Estimator to do this for you.",
                    "You just give it a model_fn that constructs your model, and a model_dir to save the model in, and it will take care of the rest.",
                    "Of course, there's already a linear model in tf.contrib.learn.LinearEstimator.",
                    "With estimators, you'd just call fit(...) whenever you have new data and it will load your variables and continue running the training steps you've defined."
                ]
            }
        ]
    },
    "tf.estimator.estimator":
    {
        "tf.contrib.learn.estimator":
        [
            {
                "title": "45186929",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.estimator.estimator",
                "sentences":
                [
                    "My guess now is that the tf.contrib.learn.Estimator is an early prototype that got replaced by the tf.estimator.Estimator.",
                    "According to the docs everything in tf.contrib is unstable API that may change at any time and it looks like the tf.estimator module is the stable API that “evolved” from the tf.contrib.learn module."
                ]
            },
            {
                "title": "47249767",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.estimator.estimator",
                "sentences":
                [
                    "First up, you should stop using tf.contrib.learn.Estimator in favor of tf.estimator.Estimator, because contrib is an experimental module, and classes that have graduated to the core API (such es Estimator) automatically get deprecated."
                ]
            }
        ],
        "tf.keras.models":
        [
            {
                "title": "49334135",
                "h": "tf.estimator.estimator",
                "t": "tf.keras.models",
                "sentences":
                [
                    "but I think you're better off using a tf.estimator.Estimator, though some people prefer tf.keras.Models.",
                    "Note if you use estimators the variables will be saved after training, so you won't need to re-train each time."
                ]
            }
        ],
        "tf.contrib.learn.experiment":
        [
            {
                "title": "47288830",
                "h": "tf.estimator.estimator",
                "t": "tf.contrib.learn.experiment",
                "sentences":
                [
                    "Just like tf.estimator.Estimator (and the derived classes) is a high-level API that hides matrix multiplications, saving checkpoints and so on, tf.contrib.learn.Experiment tries to hide the boilerplate you'd need to do for distributed computation, namely tf.train.ClusterSpec, tf.train.Server, jobs, tasks, etc."
                ]
            }
        ],
        "tf.contrib.tpu.tpuestimator":
        [
            {
                "title": "54742449",
                "h": "tf.estimator.estimator",
                "t": "tf.contrib.tpu.tpuestimator",
                "sentences":
                [
                    "If you actually don't need TPU inference support you can create a tf.estimator.Estimator instead of a tf.contrib.tpu.TPUEstimator one, using the same model_fn and trained model."
                ]
            }
        ]
    },
    "tf.metrics":
    {
        "tf.keras.metrics":
        [
            {
                "title": "53197786",
                "h": "tf.metrics",
                "t": "tf.keras.metrics",
                "sentences":
                [
                    "The metrics in tf.metrics are stateful; they create variables to accumulate partial results in, so you shouldn't expect them to auto-reset.",
                    "Instead use the metrics in tf.contrib.metrics or tf.keras.metrics and session.run the ops to reset them accordingly."
                ]
            }
        ],
        "tf.contrib.metrics":
        [
            {
                "title": "53197786",
                "h": "tf.metrics",
                "t": "tf.contrib.metrics",
                "sentences":
                [
                    "The metrics in tf.metrics are stateful; they create variables to accumulate partial results in, so you shouldn't expect them to auto-reset.",
                    "Instead use the metrics in tf.contrib.metrics or tf.keras.metrics and session.run the ops to reset them accordingly."
                ]
            }
        ]
    },
    "tf.keras.metrics":
    {
        "tf.metrics":
        [
            {
                "title": "53197786",
                "h": "tf.metrics",
                "t": "tf.keras.metrics",
                "sentences":
                [
                    "The metrics in tf.metrics are stateful; they create variables to accumulate partial results in, so you shouldn't expect them to auto-reset.",
                    "Instead use the metrics in tf.contrib.metrics or tf.keras.metrics and session.run the ops to reset them accordingly."
                ]
            }
        ]
    },
    "tf.contrib.metrics":
    {
        "tf.metrics":
        [
            {
                "title": "53197786",
                "h": "tf.metrics",
                "t": "tf.contrib.metrics",
                "sentences":
                [
                    "The metrics in tf.metrics are stateful; they create variables to accumulate partial results in, so you shouldn't expect them to auto-reset.",
                    "Instead use the metrics in tf.contrib.metrics or tf.keras.metrics and session.run the ops to reset them accordingly."
                ]
            }
        ]
    },
    "tf.nn.softmax_cross_entropy_with_logits":
    {
        "tf.nn.sparse_softmax_cross_entropy_with_logits":
        [
            {
                "title": "34243720",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).",
                    "The cross entropy is a summary metric: it sums across the elements.",
                    "If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.",
                    "Edited 2016-02-07: \nIf you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.",
                    "This function was added after release 0.6.0."
                ]
            },
            {
                "title": "64213624",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences": [
                  "actually tf.nn.softmax_cross_entropy_with_logits does not impose the restriction that labels must be one-hot encoded, so you can go ahead and use non-one-hot label vectors.",
                  "you might be confusing this with tf.nn.sparse_softmax_cross_entropy_with_logits which does impose this restriction."
                ]
            },
            {
                "title": "41639889",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "To fix this problem, you should replace tf.nn.softmax_cross_entropy_with_logits() with its sparse counterpart, tf.nn.sparse_softmax_cross_entropy_with_logits(), which can handle input data in the format that you are using:"
                ]
            },
            {
                "title": "38101834",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax_cross_entropy_with_logits",
                    "NOTE: While the classes are mutually exclusive, their probabilities\nneed not be.",
                    "All that is required is that each row of labels is a\nvalid probability distribution.",
                    "If they are not, the computation of\nthe gradient will be incorrect.",
                    "tf.nn.sparse_softmax_cross_entropy_with_logits",
                    "NOTE: For this operation, the probability of a given label is\nconsidered exclusive.",
                    "That is, soft classes are not allowed, and the\nlabels vector must provide a single specific index for the true class\nfor each row of logits (each minibatch entry)."
                ]
            },
            {
                "title": "36088396",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "If your problem is a single-class problem, then I assume that your y_ tensor is a one-hot encoding of the true labels for your examples (for example because you also pass them to an op like tf.nn.softmax_cross_entropy_with_logits().",
                    "(Also, consider using tf.nn.sparse_softmax_cross_entropy_with_logits() as your loss function, because it may be more efficient."
                ]
            }
        ],
        "tf.nn.softmax":
        [
            {
                "title": "34243720",
                "h": "tf.nn.softmax",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax produces just the result of applying the softmax function to an input tensor.",
                    "The softmax \"squishes\" the inputs so that sum(input) = 1:  it's a way of normalizing.",
                    "The shape of output of a softmax is the same as the input: it just normalizes the values.",
                    "The outputs of softmax can be interpreted as probabilities.",
                    "In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).",
                    "The cross entropy is a summary metric: it sums across the elements.",
                    "If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way."
                ]
            },
            {
                "title": "34272341",
                "h": "tf.nn.softmax",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax computes the forward propagation through a softmax layer.",
                    "You use it during evaluation of the model when you compute the probabilities that the model outputs.",
                    "tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer.",
                    "It is only used during training.",
                    "The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them)."
                ]
            }
        ],
        "tf.nn.sigmoid_cross_entropy_with_logits":
        [
            {
                "title": "47889528",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "binary_crossentropy (and tf.nn.sigmoid_cross_entropy_with_logits under the hood) is for binary multi-label classification (labels are independent)..",
                    "categorical_crossentropy (and tf.nn.softmax_cross_entropy_with_logits under the hood) is for multi-class classification (classes are exclusive).."
                ]
            },
            {
                "title": "45459557",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "I think that you should use tf.nn.sigmoid_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits because you use sigmoid and 1 neuron in output layer."
                ]
            },
            {
                "title": "39783314",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sigmoid_cross_entropy_with_logits",
                "sentences":
                [
                    "The warning just informs you that tf.nn.softmax_cross_entropy_with_logits will apply a softmax on the input logits, before computing cross-entropy.",
                    "This warning seems really to avoid applying softmax twice, as the cross-entropy results would be very different.",
                    "As the warning states, this implementation is for improving performance, with the caveat that you should not put your own softmax layer as input (which is somewhat convenient, in practice).",
                    "If the forced softmax hinders your computation, perhaps another API could help: tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits.",
                    "But whatever is applied on the input logits, tf.nn.softmax_cross_entropy_with_logits will apply a softmax before computing the cross-entropy."
                ]
            }
        ],
        "tf.nn.weighted_cross_entropy_with_logits":
        [
            {
                "title": "39783314",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.weighted_cross_entropy_with_logits",
                "sentences":
                [
                    "The warning just informs you that tf.nn.softmax_cross_entropy_with_logits will apply a softmax on the input logits, before computing cross-entropy.",
                    "This warning seems really to avoid applying softmax twice, as the cross-entropy results would be very different.",
                    "As the warning states, this implementation is for improving performance, with the caveat that you should not put your own softmax layer as input (which is somewhat convenient, in practice).",
                    "If the forced softmax hinders your computation, perhaps another API could help: tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits.",
                    "The implementation does not seem to indicate, though, that any scaling will impact the result.",
                    "I guess a linear scaling function should be fine, as long as it preserves the original logits repartition.",
                    "But whatever is applied on the input logits, tf.nn.softmax_cross_entropy_with_logits will apply a softmax before computing the cross-entropy."
                ]
            }
        ]
    },
    "tf.nn.sparse_softmax_cross_entropy_with_logits":
    {
        "tf.nn.softmax_cross_entropy_with_logits":
        [
            {
                "title": "34243720",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).",
                    "The cross entropy is a summary metric: it sums across the elements.",
                    "If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.",
                    "Edited 2016-02-07: \nIf you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.",
                    "This function was added after release 0.6.0."
                ]
            },
            {
                "title": "64213624",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences": [
                  "actually tf.nn.softmax_cross_entropy_with_logits does not impose the restriction that labels must be one-hot encoded, so you can go ahead and use non-one-hot label vectors.",
                  "you might be confusing this with tf.nn.sparse_softmax_cross_entropy_with_logits which does impose this restriction."
                ]
            },
            {
                "title": "41639889",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "To fix this problem, you should replace tf.nn.softmax_cross_entropy_with_logits() with its sparse counterpart, tf.nn.sparse_softmax_cross_entropy_with_logits(), which can handle input data in the format that you are using:"
                ]
            },
            {
                "title": "38101834",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax_cross_entropy_with_logits",
                    "NOTE: While the classes are mutually exclusive, their probabilities\nneed not be.",
                    "All that is required is that each row of labels is a\nvalid probability distribution.",
                    "If they are not, the computation of\nthe gradient will be incorrect.",
                    "tf.nn.sparse_softmax_cross_entropy_with_logits",
                    "NOTE: For this operation, the probability of a given label is\nconsidered exclusive.",
                    "That is, soft classes are not allowed, and the\nlabels vector must provide a single specific index for the true class\nfor each row of logits (each minibatch entry)."
                ]
            },
            {
                "title": "36088396",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sparse_softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "If your problem is a single-class problem, then I assume that your y_ tensor is a one-hot encoding of the true labels for your examples (for example because you also pass them to an op like tf.nn.softmax_cross_entropy_with_logits().",
                    "(Also, consider using tf.nn.sparse_softmax_cross_entropy_with_logits() as your loss function, because it may be more efficient."
                ]
            }
        ]
    },
    "tf.fifoqueue":
    {
        "tf.randomshufflequeue":
        [
            {
                "title": "34596212",
                "h": "tf.fifoqueue",
                "t": "tf.randomshufflequeue",
                "sentences":
                [
                    "TensorFlow supports the simple tf.FIFOQueue that produces elements in the order they were enqueued, and the more advanced tf.RandomShuffleQueue that produces elements in a random order."
                ]
            }
        ]
    },
    "tf.randomshufflequeue":
    {
        "tf.fifoqueue":
        [
            {
                "title": "34596212",
                "h": "tf.fifoqueue",
                "t": "tf.randomshufflequeue",
                "sentences":
                [
                    "TensorFlow supports the simple tf.FIFOQueue that produces elements in the order they were enqueued, and the more advanced tf.RandomShuffleQueue that produces elements in a random order."
                ]
            }
        ]
    },
    "tf.metrics.auc":
    {
        "tf.keras.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.metrics.auc",
                "t": "tf.keras.metrics.auc",
                "sentences":
                [
                    "Please note the tf.metrics.auc() mentioned below is obsolete.",
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ],
        "tf.compat.v1.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.metrics.auc",
                "t": "tf.compat.v1.metrics.auc",
                "sentences":
                [
                    "Please note the tf.metrics.auc() mentioned below is obsolete.",
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ]
    },
    "tf.keras.metrics.auc":
    {
        "tf.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.metrics.auc",
                "t": "tf.keras.metrics.auc",
                "sentences":
                [
                    "Please note the tf.metrics.auc() mentioned below is obsolete.",
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ],
        "tf.compat.v1.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.compat.v1.metrics.auc",
                "t": "tf.keras.metrics.auc",
                "sentences":
                [
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ]
    },
    "tf.compat.v1.metrics.auc":
    {
        "tf.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.metrics.auc",
                "t": "tf.compat.v1.metrics.auc",
                "sentences":
                [
                    "Please note the tf.metrics.auc() mentioned below is obsolete.",
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ],
        "tf.keras.metrics.auc":
        [
            {
                "title": "50068955",
                "h": "tf.compat.v1.metrics.auc",
                "t": "tf.keras.metrics.auc",
                "sentences":
                [
                    "There is tf.compat.v1.metrics.auc as a drop-in substitute, but even that is deprecated now, and the recommendation is to use tf.keras.metrics.AUC."
                ]
            }
        ]
    },
    "tf.image.decode_image":
    {
        "tf.image.decode_jpeg":
        [
            {
                "title": "48347687",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_jpeg",
                "sentences":
                [
                    "The issue comes from the fact that you use tf.image.decode_image instead of tf.image.decode_jpeg.",
                    "The first one doesn't return any shape because of some issues described here and here."
                ]
            },
            {
                "title": "48347663",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_jpeg",
                "sentences":
                [
                    "The issue here actually comes from the fact that tf.image.decode_image doesn't return the shape of the image.",
                    "The problem comes from the fact that tf.image.decode_image also handles .gif, which returns a 4D tensor, whereas .jpg and .png return 3D images.",
                    "Therefore, the correct shape cannot be returned.",
                    "The solution is to simply use tf.image.decode_jpeg or tf.image.decode_png (both work the same and can be used on .png and .jpg images)."
                ]
            }
        ],
        "tf.image.decode_png":
        [
            {
                "title": "48347663",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_png",
                "sentences":
                [
                    "The issue here actually comes from the fact that tf.image.decode_image doesn't return the shape of the image.",
                    "The problem comes from the fact that tf.image.decode_image also handles .gif, which returns a 4D tensor, whereas .jpg and .png return 3D images.",
                    "Therefore, the correct shape cannot be returned.",
                    "The solution is to simply use tf.image.decode_jpeg or tf.image.decode_png (both work the same and can be used on .png and .jpg images)."
                ]
            }
        ]
    },
    "tf.image.decode_jpeg":
    {
        "tf.image.decode_image":
        [
            {
                "title": "48347687",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_jpeg",
                "sentences":
                [
                    "The issue comes from the fact that you use tf.image.decode_image instead of tf.image.decode_jpeg.",
                    "The first one doesn't return any shape because of some issues described here and here."
                ]
            },
            {
                "title": "48347663",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_jpeg",
                "sentences":
                [
                    "The issue here actually comes from the fact that tf.image.decode_image doesn't return the shape of the image.",
                    "The problem comes from the fact that tf.image.decode_image also handles .gif, which returns a 4D tensor, whereas .jpg and .png return 3D images.",
                    "Therefore, the correct shape cannot be returned.",
                    "The solution is to simply use tf.image.decode_jpeg or tf.image.decode_png (both work the same and can be used on .png and .jpg images)."
                ]
            }
        ],
        "tf.image.resize":
        [
            {
                "title": "44289344",
                "h": "tf.image.decode_jpeg",
                "t": "tf.image.resize",
                "sentences":
                [
                    "tf.image.decode_jpeg returns a tf.uint8 tensor.",
                    "tf.image.resize returns a tf.float32 tensor."
                ]
            }
        ]
    },
    "tf.train.summarywriter":
    {
        "tf.summary.filewriter":
        [
            {
                "title": "41483033",
                "h": "tf.train.summarywriter",
                "t": "tf.summary.filewriter",
                "sentences":
                [
                    "tf.train.SummaryWriter is deprecated, instead use tf.summary.FileWriter.",
                    "The interface and behavior is the same; this is just a rename."
                ]
            },
            {
                "title": "43304425",
                "h": "tf.summary.filewriter",
                "t": "tf.train.summarywriter",
                "sentences":
                [
                    "For future reference of anyone in the same situation, changing tf.summary.FileWriter() to tf.train.SummaryWriter() solved this issue and allowed for graph visualisation in Tensorboard.",
                    "As I thought, it seems like FileWriter may be deprecated (although it does oddly still appear when searching through tf methods in the IDE)"
                ]
            },
            {
                "title": "43603466",
                "h": "tf.train.summarywriter",
                "t": "tf.summary.filewriter",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.train.SummaryWriter should be renamed to tf.summary.FileWriter."
                ]
            }
        ]
    },
    "tf.summary.filewriter":
    {
        "tf.train.summarywriter":
        [
            {
                "title": "41483033",
                "h": "tf.train.summarywriter",
                "t": "tf.summary.filewriter",
                "sentences":
                [
                    "tf.train.SummaryWriter is deprecated, instead use tf.summary.FileWriter.",
                    "The interface and behavior is the same; this is just a rename."
                ]
            },
            {
                "title": "43304425",
                "h": "tf.summary.filewriter",
                "t": "tf.train.summarywriter",
                "sentences":
                [
                    "For future reference of anyone in the same situation, changing tf.summary.FileWriter() to tf.train.SummaryWriter() solved this issue and allowed for graph visualisation in Tensorboard.",
                    "As I thought, it seems like FileWriter may be deprecated (although it does oddly still appear when searching through tf methods in the IDE)"
                ]
            },
            {
                "title": "43603466",
                "h": "tf.train.summarywriter",
                "t": "tf.summary.filewriter",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.train.SummaryWriter should be renamed to tf.summary.FileWriter."
                ]
            }
        ]
    },
    "tf.contrib.layers.batch_norm":
    {
        "tf.nn.batch_norm_with_global_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.contrib.layers.batch_norm",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.layers.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.layers.batch_normalization",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you..",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.fused_batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.batch_norm":
        [
            {
                "title": "42144422",
                "h": "tf.contrib.layers.batch_norm",
                "t": "tf.nn.batch_norm",
                "sentences":
                [
                    "tf.contrib refers to a high level machine learning API over Tensorflow.",
                    "So tf.contrib.layers.batch_norm basically acts as a wrapper over tf.nn.batch_normalization making it easier to use but less flexible than the latter.",
                    "So, depending upon the application you can use whatever fits your needs.",
                    "For example in case of tf.nn.batch_norm you will have to give a symbolic link to mean and variance tensors by yourself, but tf.contrib.layers.batch_norm will take care of that for you."
                ]
            }
        ]
    },
    "tf.nn.batch_norm_with_global_normalization":
    {
        "tf.contrib.layers.batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.contrib.layers.batch_norm",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.layers.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.nn.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.nn.fused_batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ]
    },
    "tf.layers.batch_normalization":
    {
        "tf.contrib.layers.batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.layers.batch_normalization",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you..",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.batch_norm_with_global_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.nn.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.layers.batch_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you.."
                ]
            },
            {
                "title": "47143624",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_normalization",
                "sentences":
                [
                    "To your second question: in tensorflow, you can use a high-level tf.layers.batch_normalization function, or a low-level tf.nn.batch_normalization."
                ]
            },
            {
                "title": "47984274",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_normalization",
                "sentences":
                [
                    "Simply use tf.layers.batch_normalization.",
                    "Update: tf.nn.batch_normalization is fine too.",
                    "It's a more low-level function that requires you manage mean and variance tensors yourself.",
                    "In fact, tf.layers.batch_normalization is a wrapper over tf.nn.",
                    "* functions, which also includes tf.nn.fused_batch_norm (a faster fused version)."
                ]
            }
        ],
        "tf.nn.fused_batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.fused_batch_norm",
                "t": "tf.layers.batch_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you.."
                ]
            }
        ]
    },
    "tf.nn.batch_normalization":
    {
        "tf.contrib.layers.batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.contrib.layers.batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.contrib.layers.batch_norm is the early implementation of batch norm, before it's graduated to the core API (i.e., tf.layers).",
                    "The use of it is not recommended because it may be dropped in the future releases.."
                ]
            }
        ],
        "tf.nn.batch_norm_with_global_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.nn.batch_norm_with_global_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.nn.batch_norm_with_global_normalization is another deprecated op.",
                    "Currently, delegates the call to tf.nn.batch_normalization, but likely to be dropped in the future.."
                ]
            }
        ],
        "tf.layers.batch_normalization":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.layers.batch_normalization",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.layers.batch_normalization is a high-level wrapper over the previous ops.",
                    "The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible.",
                    "Usually, this should be the default choice for you.."
                ]
            },
            {
                "title": "47143624",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_normalization",
                "sentences":
                [
                    "To your second question: in tensorflow, you can use a high-level tf.layers.batch_normalization function, or a low-level tf.nn.batch_normalization."
                ]
            },
            {
                "title": "47984274",
                "h": "tf.layers.batch_normalization",
                "t": "tf.nn.batch_normalization",
                "sentences":
                [
                    "Simply use tf.layers.batch_normalization.",
                    "Update: tf.nn.batch_normalization is fine too.",
                    "It's a more low-level function that requires you manage mean and variance tensors yourself.",
                    "In fact, tf.layers.batch_normalization is a wrapper over tf.nn.",
                    "* functions, which also includes tf.nn.fused_batch_norm (a faster fused version)."
                ]
            }
        ],
        "tf.nn.fused_batch_norm":
        [
            {
                "title": "48006315",
                "h": "tf.nn.batch_normalization",
                "t": "tf.nn.fused_batch_norm",
                "sentences":
                [
                    "Just to add to the list, there're several more ways to do batch-norm in tensorflow:",
                    "tf.nn.batch_normalization is a low-level op.",
                    "The caller is responsible to handle mean and variance tensors themselves..",
                    "tf.nn.fused_batch_norm is another low-level op, similar to the previous one.",
                    "The difference is that it's optimized for 4D input tensors, which is the usual case in convolutional neural networks.",
                    "tf.nn.batch_normalization accepts tensors of any rank greater than 1.."
                ]
            }
        ]
    },
    "tf.contrib.learn.linearestimator":
    {
        "tf.contrib.learn.estimator":
        [
            {
                "title": "44654244",
                "h": "tf.contrib.learn.estimator",
                "t": "tf.contrib.learn.linearestimator",
                "sentences":
                [
                    "You can also use a tf.contrib.learn.Estimator to do this for you.",
                    "You just give it a model_fn that constructs your model, and a model_dir to save the model in, and it will take care of the rest.",
                    "Of course, there's already a linear model in tf.contrib.learn.LinearEstimator.",
                    "With estimators, you'd just call fit(...) whenever you have new data and it will load your variables and continue running the training steps you've defined."
                ]
            }
        ]
    },
    "tf.metrics.sparse_average_precision_at_k":
    {
        "tf.metrics.average_precision_at_k":
        [
            {
                "title": "52055189",
                "h": "tf.metrics.sparse_average_precision_at_k",
                "t": "tf.metrics.average_precision_at_k",
                "sentences":
                [
                    "The tf.metrics.sparse_average_precision_at_k will be replaced by tf.metrics.average_precision_at_k.",
                    "And by browsing the code in tensorflow, you will find that when your inputs are y_true and y_pred, this function will actually transform the y_pred to y_pred_idx, by using top_k function.",
                    "y_true is a tensor of shape (batch_size, num_labels), and y_pred is of shape (batch_size, num_classes)"
                ]
            }
        ]
    },
    "tf.metrics.average_precision_at_k":
    {
        "tf.metrics.sparse_average_precision_at_k":
        [
            {
                "title": "52055189",
                "h": "tf.metrics.sparse_average_precision_at_k",
                "t": "tf.metrics.average_precision_at_k",
                "sentences":
                [
                    "The tf.metrics.sparse_average_precision_at_k will be replaced by tf.metrics.average_precision_at_k.",
                    "And by browsing the code in tensorflow, you will find that when your inputs are y_true and y_pred, this function will actually transform the y_pred to y_pred_idx, by using top_k function.",
                    "y_true is a tensor of shape (batch_size, num_labels), and y_pred is of shape (batch_size, num_classes)"
                ]
            }
        ]
    },
    "tf.get_variable":
    {
        "tf.variable":
        [
            {
                "title": "37603765",
                "h": "tf.get_variable",
                "t": "tf.variable",
                "sentences":
                [
                    "Use tf.get_variable() instead of tf.Variable() to ensure that the variables are shared where possible."
                ]
            },
            {
                "title": "51732374",
                "h": "tf.get_variable",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.name_scope is ignored by variables created via tf.get_variable() which is usually used by tf.layers functions.",
                    "This is in contrast to variables created via tf.Variable."
                ]
            },
            {
                "title": "48667570",
                "h": "tf.variable",
                "t": "tf.get_variable",
                "sentences":
                [
                    "First off, creating variables with tf.Variable won't make it reusable.",
                    "That's one of the key differences between tf.Variable and tf.get_variable."
                ]
            }
        ],
        "tf.constant_initializer":
        [
            {
                "title": "38820162",
                "h": "tf.constant_initializer",
                "t": "tf.get_variable",
                "sentences":
                [
                    "The tf.constant_initializer() function might not accept a tf.Tensor as an argument, but tf.get_variable() does accept a tf.Tensor as its initializer argument.",
                    "The reason tf.constant_initializer() doesn't take an arbitrary tensor is that it is designed to initialize variables of many different shapes with the same constant value for each element."
                ]
            }
        ]
    },
    "tf.variable":
    {
        "tf.get_variable":
        [
            {
                "title": "37603765",
                "h": "tf.get_variable",
                "t": "tf.variable",
                "sentences":
                [
                    "Use tf.get_variable() instead of tf.Variable() to ensure that the variables are shared where possible."
                ]
            },
            {
                "title": "51732374",
                "h": "tf.get_variable",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.name_scope is ignored by variables created via tf.get_variable() which is usually used by tf.layers functions.",
                    "This is in contrast to variables created via tf.Variable."
                ]
            },
            {
                "title": "48667570",
                "h": "tf.variable",
                "t": "tf.get_variable",
                "sentences":
                [
                    "First off, creating variables with tf.Variable won't make it reusable.",
                    "That's one of the key differences between tf.Variable and tf.get_variable."
                ]
            }
        ],
        "tf.tensor":
        [
            {
                "title": "37624060",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "Most TensorFlow tensors (tf.Tensor objects) are immutable, so you cannot simply assign a value to them.",
                    "However, if you created the tensor as a tf.Variable, you can assign a value to it by calling Variable.assign()."
                ]
            },
            {
                "title": "42965020",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "For the sake of concreteness, I'm assuming that func1 and func2 in your example are tf.Tensor objects.",
                    "TensorFlow does not cache intermediate tensor values between calls to tf.Session.run().",
                    "The reason for this is simple: in a typical neural network workload (training or inference), most intermediate values become invalid between runs of the graph, because they are a function of the input (which changes from step to step) and the current state (which changes during training).",
                    "If you want to save a value for later use, you must explicitly assign it to a tf.Variable, or store it in some other stateful object, such as a tf.FIFOQueue."
                ]
            },
            {
                "title": "44486260",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.Tensor cannot be changed, but tf.Variable can."
                ]
            },
            {
                "title": "59571286",
                "h": "tf.variable",
                "t": "tf.tensor",
                "sentences":
                [
                    "The cause of this is that tf.add has trouble working with tf.Variable types but works fine for tf.Tensor types."
                ]
            }
        ],
        "tf.constant":
        [
            {
                "title": "63435824",
                "h": "tf.variable",
                "t": "tf.constant",
                "sentences":
                [
                    "What was wrong: You cannot convert a tf.variable to a numpy array using graph execution and need to use a tf.constant instead."
                ]
            },
            {
                "title": "43536220",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.constant() and tf.placeholder() are nodes in the graph (ops or operations).",
                    "On the other hand tf.Variable() is a class."
                ]
            },
            {
                "title": "57959532",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "you will not be able to do this for a 'tf.constant()', as it is a constant variable and does not support having its values changed.",
                    "If you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.Variable.",
                    "However these require predefined dimensions, and cannot have their sizes changed as desired in your question."
                ]
            },
            {
                "title": "45267542",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "If you don't want train this weights, you can define them as tf.constant not tf.Variable"
                ]
            }
        ],
        "tf.placeholder":
        [
            {
                "title": "41352964",
                "h": "tf.placeholder",
                "t": "tf.variable",
                "sentences":
                [
                    "TensorFlow's tf.placeholder() tensors do not require you to specify a shape, in order to allow you to feed tensors of different shapes in a later tf.Session.run() call.",
                    "By default, a placeholder has a completely unconstrained shape, but you can constrain it by passing the optional shape argument.",
                    "Note that tf.Variable objects typically do require a shape when you create them, and this shape is inferred from the first argument to the initializer."
                ]
            },
            {
                "title": "43536220",
                "h": "tf.placeholder",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.constant() and tf.placeholder() are nodes in the graph (ops or operations).",
                    "On the other hand tf.Variable() is a class."
                ]
            },
            {
                "title": "40649185",
                "h": "tf.variable",
                "t": "tf.placeholder",
                "sentences":
                [
                    "As you've noticed, TensorFlow optimizers (i.e.",
                    "subclasses of tf.train.Optimizer) operate on tf.Variable objects because they need to be able to assign new values to those objects, and in TensorFlow only variables support an assign operation.",
                    "If you use a tf.placeholder(), there's nothing to update, because the value of a placeholder is immutable within each step."
                ]
            }
        ]
    },
    "tf.layers":
    {
        "tf.keras.layer":
        [
            {
                "title": "50049491",
                "h": "tf.layers",
                "t": "tf.keras.layer",
                "sentences":
                [
                    "tf.layers and tf.keras.layer classes are generally interchangeable and in fact at head (and thus by the next release - 1.9), the former actually inherits from the latter.",
                    "TensorFlow is moving towards consolidating on tf.keras APIs for constructing models as that makes state ownership more explicit (e.g., parameters are \"owned\" by the Layer object, as opposed to the functional style where all model parameters are put in a \"collection\" associated with the complete graph).",
                    "This style works well for both eager execution and graph construction (support for eager execution is improving with every release).",
                    "I'd recommend using tf.keras.layers and tf.keras.Model."
                ]
            }
        ],
        "tf.compat.v1.layers":
        [
            {
                "title": "60409372",
                "h": "tf.layers",
                "t": "tf.compat.v1.layers",
                "sentences":
                [
                    "For example, tf.layers does not exist anymore in Tensorflow v2.",
                    "You can use tf.compat.v1.layers (see for example the Conv2D function) instead, but this is a temporary fix, as these functions will be removed in a future version."
                ]
            }
        ],
        "tf.keras.layers":
        [
            {
                "title": "53534547",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras.",
                    "So to best prepare for TF 2.0 you should start using tf.keras.layers instead."
                ]
            },
            {
                "title": "54718798",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "Since TensorFlow 1.12, tf.layers are merely wrappers around tf.keras.layers.",
                    "Convolutional tf.layers just inherit from the convolutional tf.keras.layers, see source code here:",
                    "With the integration of Keras into TensorFlow, it would make little sense to maintain several different layer implementations.",
                    "tf.keras is becoming the de-facto high-level API for TensorFlow, therefore tf.layers are now just wrappers around tf.keras.layers."
                ]
            },
            {
                "title": "58538279",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "So I faced the same error but discovered that my version of tensorflow (which \n is 2.0) moved layers from the tf package (tf.layers) to tf.keras.",
                    "An easy fix would be to replace tf.layers with tf.keras.layers"
                ]
            },
            {
                "title": "51089993",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "tf.layers module is Tensorflow attempt at creating a Keras like API whereas tf.keras.layers is a compatibility wrapper.",
                    "In fact, most of the implementation refers back to tf.layers, for example the tf.keras.layers.Dense inherits the core implementation:",
                    "I would use Keras directly or tf.layers but not necessarily mix them."
                ]
            }
        ],
        "tf.contrib.layers":
        [
            {
                "title": "53534547",
                "h": "tf.contrib.layers",
                "t": "tf.layers",
                "sentences":
                [
                    "The core functionality corresponding to tf.contrib.layers is in tf.layers.",
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras."
                ]
            }
        ],
        " tf.keras.layers":
        [
            {
                "title": "55413120",
                "h": "tf.layers",
                "t": " tf.keras.layers",
                "sentences":
                [
                    "As per official docs, tf.layers are wrappers around tf.keras.layers."
                ]
            }
        ]
    },
    "tf.keras.layer":
    {
        "tf.layers":
        [
            {
                "title": "50049491",
                "h": "tf.layers",
                "t": "tf.keras.layer",
                "sentences":
                [
                    "tf.layers and tf.keras.layer classes are generally interchangeable and in fact at head (and thus by the next release - 1.9), the former actually inherits from the latter.",
                    "TensorFlow is moving towards consolidating on tf.keras APIs for constructing models as that makes state ownership more explicit (e.g., parameters are \"owned\" by the Layer object, as opposed to the functional style where all model parameters are put in a \"collection\" associated with the complete graph).",
                    "This style works well for both eager execution and graph construction (support for eager execution is improving with every release).",
                    "I'd recommend using tf.keras.layers and tf.keras.Model."
                ]
            }
        ]
    },
    "tf.variable_scope":
    {
        "tf.compat.v1.variable_scope":
        [
            {
                "title": "58574655",
                "h": "tf.variable_scope",
                "t": "tf.compat.v1.variable_scope",
                "sentences":
                [
                    "There is no tf.variable_scope() in TensorFlow 2.0.",
                    "If you want your code to be compatible with older versions of TensorFlow, you can use tf.compat.v1.variable_scope()"
                ]
            }
        ],
        "tf.name_scope":
        [
            {
                "title": "39981684",
                "h": "tf.variable_scope",
                "t": "tf.name_scope",
                "sentences":
                [
                    "Just use tf.variable_scope instead of tf.name_scope.",
                    "tf.name_scope doesn't add prefixes to the variables created with tf.get_variable()."
                ]
            },
            {
                "title": "34232533",
                "h": "tf.name_scope",
                "t": "tf.variable_scope",
                "sentences":
                [
                    "If you created a var with tf.get_variable and you try to change the prefix of your variable names by using the tf.name_scope context manager, this won't prevent the Tensorflow of raising an exception.",
                    "Only tf.variable_scope context manager will effectively change the name of your var in this case.",
                    "In summary, tf.name_scope just add a prefix to all tensor created in that scope (except the vars created with tf.get_variable), and tf.variable_scope add a prefix to the variables created with tf.get_variable."
                ]
            },
            {
                "title": "51732374",
                "h": "tf.variable_scope",
                "t": "tf.name_scope",
                "sentences":
                [
                    "You can try using tf.variable_scope instead.",
                    "tf.name_scope is ignored by variables created via tf.get_variable() which is usually used by tf.layers functions."
                ]
            }
        ]
    },
    "tf.compat.v1.variable_scope":
    {
        "tf.variable_scope":
        [
            {
                "title": "58574655",
                "h": "tf.variable_scope",
                "t": "tf.compat.v1.variable_scope",
                "sentences":
                [
                    "There is no tf.variable_scope() in TensorFlow 2.0.",
                    "If you want your code to be compatible with older versions of TensorFlow, you can use tf.compat.v1.variable_scope()"
                ]
            }
        ]
    },
    "tf.sparse_to_dense":
    {
        "tf.sparsetensor":
        [
            {
                "title": "55796211",
                "h": "tf.sparse_to_dense",
                "t": "tf.sparsetensor",
                "sentences":
                [
                    "However, tf.sparse_to_dense is deprecated recently.",
                    "Thus, use tf.SparseTensor and then use tf.sparse.to_dense to get the same result as above"
                ]
            }
        ]
    },
    "tf.sparsetensor":
    {
        "tf.sparse_to_dense":
        [
            {
                "title": "55796211",
                "h": "tf.sparse_to_dense",
                "t": "tf.sparsetensor",
                "sentences":
                [
                    "However, tf.sparse_to_dense is deprecated recently.",
                    "Thus, use tf.SparseTensor and then use tf.sparse.to_dense to get the same result as above"
                ]
            }
        ],
        "tf.tensor":
        [
            {
                "title": "42562650",
                "h": "tf.sparsetensor",
                "t": "tf.tensor",
                "sentences":
                [
                    "The closest thing TensorFlow has to scipy.sparse.coo_matrix is tf.SparseTensor, which is the sparse equivalent of tf.Tensor.",
                    "A tf.SparseTensor is a slight generalization of COO matrices, where the tensor is represented as three dense tf.Tensor objects:",
                    "indices: An N x D matrix of tf.int64 values in which each row represents the coordinates of a non-zero value.",
                    "N is the number of non-zeroes, and D is the rank of the equivalent dense tensor (2 in the case of a matrix)..",
                    "values: A length-N vector of values, where element i is the value of the element whose coordinates are given on row i of indices..",
                    "dense_shape: A length-D vector of tf.int64, representing the shape of the equivalent dense tensor.."
                ]
            }
        ]
    },
    "tf.tensor":
    {
        "tf.variable":
        [
            {
                "title": "37624060",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "Most TensorFlow tensors (tf.Tensor objects) are immutable, so you cannot simply assign a value to them.",
                    "However, if you created the tensor as a tf.Variable, you can assign a value to it by calling Variable.assign()."
                ]
            },
            {
                "title": "42965020",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "For the sake of concreteness, I'm assuming that func1 and func2 in your example are tf.Tensor objects.",
                    "TensorFlow does not cache intermediate tensor values between calls to tf.Session.run().",
                    "The reason for this is simple: in a typical neural network workload (training or inference), most intermediate values become invalid between runs of the graph, because they are a function of the input (which changes from step to step) and the current state (which changes during training).",
                    "If you want to save a value for later use, you must explicitly assign it to a tf.Variable, or store it in some other stateful object, such as a tf.FIFOQueue."
                ]
            },
            {
                "title": "44486260",
                "h": "tf.tensor",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.Tensor cannot be changed, but tf.Variable can."
                ]
            },
            {
                "title": "59571286",
                "h": "tf.variable",
                "t": "tf.tensor",
                "sentences":
                [
                    "The cause of this is that tf.add has trouble working with tf.Variable types but works fine for tf.Tensor types."
                ]
            }
        ],
        "tf.placeholder":
        [
            {
                "title": "46525453",
                "h": "tf.tensor",
                "t": "tf.placeholder",
                "sentences":
                [
                    "If the key is a tf.Tensor, the value may be a Python scalar, string,\n  list, or numpy ndarray that can be converted to the same dtype as that\n  tensor.",
                    "Additionally, if the key is a tf.placeholder, the shape of the\n  value will be checked for compatibility with the placeholder."
                ]
            }
        ],
        "tf.sparsetensor":
        [
            {
                "title": "42562650",
                "h": "tf.sparsetensor",
                "t": "tf.tensor",
                "sentences":
                [
                    "The closest thing TensorFlow has to scipy.sparse.coo_matrix is tf.SparseTensor, which is the sparse equivalent of tf.Tensor.",
                    "A tf.SparseTensor is a slight generalization of COO matrices, where the tensor is represented as three dense tf.Tensor objects:",
                    "indices: An N x D matrix of tf.int64 values in which each row represents the coordinates of a non-zero value.",
                    "N is the number of non-zeroes, and D is the rank of the equivalent dense tensor (2 in the case of a matrix)..",
                    "values: A length-N vector of values, where element i is the value of the element whose coordinates are given on row i of indices..",
                    "dense_shape: A length-D vector of tf.int64, representing the shape of the equivalent dense tensor.."
                ]
            }
        ]
    },
    " tf.nn.dymanic_rnn":
    {
        "tf.keras.layers.lstm":
        [
            {
                "title": "62771771",
                "h": " tf.nn.dymanic_rnn",
                "t": "tf.keras.layers.lstm",
                "sentences":
                [
                    "Most importantly, those \"pure Tensorflow\" RNN cells are not made to be used with the keras RNN anyway -- they were used with tf.nn.dymanic_rnn for example, which is now deprecated and also found only in the compat.v1 module.",
                    "I would recommend that you simply use tf.keras.layers.LSTM directly as it's much faster anyway -- it allows for the use of highly optimized GPU kernels."
                ]
            }
        ]
    },
    "tf.keras.layers.lstm":
    {
        " tf.nn.dymanic_rnn":
        [
            {
                "title": "62771771",
                "h": " tf.nn.dymanic_rnn",
                "t": "tf.keras.layers.lstm",
                "sentences":
                [
                    "Most importantly, those \"pure Tensorflow\" RNN cells are not made to be used with the keras RNN anyway -- they were used with tf.nn.dymanic_rnn for example, which is now deprecated and also found only in the compat.v1 module.",
                    "I would recommend that you simply use tf.keras.layers.LSTM directly as it's much faster anyway -- it allows for the use of highly optimized GPU kernels."
                ]
            }
        ]
    },
    "tf.logging":
    {
        "tf.compat.v1.logging":
        [
            {
                "title": "55318851",
                "h": "tf.logging",
                "t": "tf.compat.v1.logging",
                "sentences":
                [
                    "tf.logging was for Logging and Summary Operations and in TF 2.0 it has been removed in favor of the open-source absl-py, and to make the main tf.",
                    "* namespace has functions that will be used more often.",
                    "In TF.2 lesser used functions are gone or moved into sub-packages like tf.math",
                    "So instead of tf.logging you could:",
                    "tf_upgrade_v2 will upgrade script and changes tf.logging to tf.compat.v1.logging."
                ]
            }
        ]
    },
    "tf.compat.v1.logging":
    {
        "tf.logging":
        [
            {
                "title": "55318851",
                "h": "tf.logging",
                "t": "tf.compat.v1.logging",
                "sentences":
                [
                    "tf.logging was for Logging and Summary Operations and in TF 2.0 it has been removed in favor of the open-source absl-py, and to make the main tf.",
                    "* namespace has functions that will be used more often.",
                    "In TF.2 lesser used functions are gone or moved into sub-packages like tf.math",
                    "So instead of tf.logging you could:",
                    "tf_upgrade_v2 will upgrade script and changes tf.logging to tf.compat.v1.logging."
                ]
            }
        ]
    },
    "tf.keras.models":
    {
        "tf.estimator.estimator":
        [
            {
                "title": "49334135",
                "h": "tf.estimator.estimator",
                "t": "tf.keras.models",
                "sentences":
                [
                    "but I think you're better off using a tf.estimator.Estimator, though some people prefer tf.keras.Models.",
                    "Note if you use estimators the variables will be saved after training, so you won't need to re-train each time."
                ]
            }
        ]
    },
    "tf.read_file":
    {
        "tf.textlinereader":
        [
            {
                "title": "35530400",
                "h": "tf.read_file",
                "t": "tf.textlinereader",
                "sentences":
                [
                    "The tf.read_file() op reads the entire contents of the given file into a single string, whereas tf.decode_csv() op expects each element of its input to be a single record (i.e.",
                    "Therefore you need something that reads one line at a time, which the tf.TextLineReader supports."
                ]
            }
        ],
        "tf.decode_csv":
        [
            {
                "title": "35530400",
                "h": "tf.read_file",
                "t": "tf.decode_csv",
                "sentences":
                [
                    "The tf.read_file() op reads the entire contents of the given file into a single string, whereas tf.decode_csv() op expects each element of its input to be a single record (i.e."
                ]
            }
        ],
        "tf.wholefilereader":
        [
            {
                "title": "34345827",
                "h": "tf.read_file",
                "t": "tf.wholefilereader",
                "sentences":
                [
                    "Use tf.read_file(filename) rather than tf.WholeFileReader() to read your image files.",
                    "tf.read_file() is a stateless op that consumes a single filename and produces a single string containing the contents of the file.",
                    "It has the advantage that it's a pure function, so it's easy to associate data with the input and the output."
                ]
            }
        ]
    },
    "tf.textlinereader":
    {
        "tf.read_file":
        [
            {
                "title": "35530400",
                "h": "tf.read_file",
                "t": "tf.textlinereader",
                "sentences":
                [
                    "The tf.read_file() op reads the entire contents of the given file into a single string, whereas tf.decode_csv() op expects each element of its input to be a single record (i.e.",
                    "Therefore you need something that reads one line at a time, which the tf.TextLineReader supports."
                ]
            }
        ]
    },
    "tf.decode_csv":
    {
        "tf.read_file":
        [
            {
                "title": "35530400",
                "h": "tf.read_file",
                "t": "tf.decode_csv",
                "sentences":
                [
                    "The tf.read_file() op reads the entire contents of the given file into a single string, whereas tf.decode_csv() op expects each element of its input to be a single record (i.e."
                ]
            }
        ]
    },
    "tf.autograph.to_graph":
    {
        "tf.function":
        [
            {
                "title": "63669319",
                "h": "tf.autograph.to_graph",
                "t": "tf.function",
                "sentences":
                [
                    "While, actually the doc tf.autograph.to_graph explains the relation ship between autograpsh and tf.funciton directly:",
                    "Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code.",
                    "It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired.",
                    "Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable.",
                    "Internally, tf.function uses to_graph."
                ]
            }
        ]
    },
    "tf.function":
    {
        "tf.autograph.to_graph":
        [
            {
                "title": "63669319",
                "h": "tf.autograph.to_graph",
                "t": "tf.function",
                "sentences":
                [
                    "While, actually the doc tf.autograph.to_graph explains the relation ship between autograpsh and tf.funciton directly:",
                    "Unlike tf.function, to_graph is a low-level transpiler that converts Python code to TensorFlow graph code.",
                    "It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired.",
                    "Another difference from tf.function is that to_graph will not wrap the graph into a TensorFlow function or a Python callable.",
                    "Internally, tf.function uses to_graph."
                ]
            }
        ]
    },
    "tf.random.normal":
    {
        "tf.random_normal":
        [
            {
                "title": "59953445",
                "h": "tf.random.normal",
                "t": "tf.random_normal",
                "sentences":
                [
                    "Tensorflow 2.0 comes with new aliases for random_normal.",
                    "Using tf.random.normal instead of tf.random_normal should execute successfully."
                ]
            }
        ]
    },
    "tf.random_normal":
    {
        "tf.random.normal":
        [
            {
                "title": "59953445",
                "h": "tf.random.normal",
                "t": "tf.random_normal",
                "sentences":
                [
                    "Tensorflow 2.0 comes with new aliases for random_normal.",
                    "Using tf.random.normal instead of tf.random_normal should execute successfully."
                ]
            }
        ]
    },
    "tf.contrib.layers.relu":
    {
        "tf.nn.relu":
        [
            {
                "title": "42774074",
                "h": "tf.contrib.layers.relu",
                "t": "tf.nn.relu",
                "sentences":
                [
                    "The latter is not an activation function but a fully_connected layer that has its activation function preset as nn.relu:",
                    "Aliases for fully_connected which set a default activation function\n  are available: relu, relu6 and linear.",
                    "Summarily, tf.contrib.layers.relu is an alias for a fully_connected layer with relu activation while tf.nn.relu is the REctified Linear Unit activation function itself."
                ]
            }
        ]
    },
    "tf.nn.relu":
    {
        "tf.contrib.layers.relu":
        [
            {
                "title": "42774074",
                "h": "tf.contrib.layers.relu",
                "t": "tf.nn.relu",
                "sentences":
                [
                    "The latter is not an activation function but a fully_connected layer that has its activation function preset as nn.relu:",
                    "Aliases for fully_connected which set a default activation function\n  are available: relu, relu6 and linear.",
                    "Summarily, tf.contrib.layers.relu is an alias for a fully_connected layer with relu activation while tf.nn.relu is the REctified Linear Unit activation function itself."
                ]
            }
        ],
        "tf.maximum":
        [
            {
                "title": "54223597",
                "h": "tf.nn.relu",
                "t": "tf.maximum",
                "sentences":
                [
                    "they are basically the same.",
                    "Both take a tensor as input and return a tensor.",
                    "The only difference is the supported types.",
                    "The tf.nn.relu supports the following types:",
                    "float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8.",
                    "while  tf.maximum supports a subset of the above types:",
                    "half, float32, float64, int32, int64."
                ]
            }
        ]
    },
    "tf.nn.softmax":
    {
        "tf.nn.sigmoid":
        [
            {
                "title": "50080463",
                "h": "tf.nn.softmax",
                "t": "tf.nn.sigmoid",
                "sentences":
                [
                    "You need to use - y_ = tf.nn.softmax(tf.matmul(X,W)+b)",
                    "instead of :",
                    "y_ = tf.nn.sigmoid(tf.matmul(X,W)+b)",
                    "as the MNIST data set has multi-class labels (sigmoid is used in case of 2 classes)."
                ]
            }
        ],
        "tf.nn.softmax_cross_entropy_with_logits":
        [
            {
                "title": "34243720",
                "h": "tf.nn.softmax",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax produces just the result of applying the softmax function to an input tensor.",
                    "The softmax \"squishes\" the inputs so that sum(input) = 1:  it's a way of normalizing.",
                    "The shape of output of a softmax is the same as the input: it just normalizes the values.",
                    "The outputs of softmax can be interpreted as probabilities.",
                    "In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).",
                    "The cross entropy is a summary metric: it sums across the elements.",
                    "If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way."
                ]
            },
            {
                "title": "34272341",
                "h": "tf.nn.softmax",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "tf.nn.softmax computes the forward propagation through a softmax layer.",
                    "You use it during evaluation of the model when you compute the probabilities that the model outputs.",
                    "tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer.",
                    "It is only used during training.",
                    "The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them)."
                ]
            }
        ],
        "tf.sigmoid":
        [
            {
                "title": "50090357",
                "h": "tf.nn.softmax",
                "t": "tf.sigmoid",
                "sentences":
                [
                    "tf.nn.softmax() computes probability distribution over classes (output neurons), if you have just 1 output neuron then probability distribution over 1 neuron will always be 1.0.",
                    "I would suggest to use tf.sigmoid() in combination with tf.greater(), e.g:",
                    "<code>Code Snippet</code>."
                ]
            }
        ],
        "tf.contrib.layers.softmax":
        [
            {
                "title": "46686881",
                "h": "tf.nn.softmax",
                "t": "tf.contrib.layers.softmax",
                "sentences":
                [
                    "Once that was done, you could use tf.nn.softmax, applying it separately to both of your logit tensors.",
                    "There is also a tf.contrib.layers.softmax which allows you to apply the softmax on the final axis of a tensor with greater than 2 dimensions, but it doesn't look like you need anything like that.",
                    "tf.nn.softmax should work here."
                ]
            }
        ],
        "tf.sigmoid ":
        [
            {
                "title": "47761718",
                "h": "tf.nn.softmax",
                "t": "tf.sigmoid ",
                "sentences":
                [
                    "Instead of tf.nn.softmax, you could also use tf.sigmoid on a single logit, then set the other output to one minus that."
                ]
            }
        ],
        "tf.nn.softmax_cross_entropy":
        [
            {
                "title": "45183635",
                "h": "tf.nn.softmax_cross_entropy",
                "t": "tf.nn.softmax",
                "sentences":
                [
                    "Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions.",
                    "Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()"
                ]
            }
        ]
    },
    "tf.nn.sigmoid":
    {
        "tf.nn.softmax":
        [
            {
                "title": "50080463",
                "h": "tf.nn.softmax",
                "t": "tf.nn.sigmoid",
                "sentences":
                [
                    "You need to use - y_ = tf.nn.softmax(tf.matmul(X,W)+b)",
                    "instead of :",
                    "y_ = tf.nn.sigmoid(tf.matmul(X,W)+b)",
                    "as the MNIST data set has multi-class labels (sigmoid is used in case of 2 classes)."
                ]
            }
        ],
        "tf.nn.sigmoid_cross_entropy_with_logits":
        [
            {
                "title": "55601831",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.sigmoid",
                "sentences":
                [
                    "The problem was that the tf.nn.sigmoid_cross_entropy_with_logits runs the logits through a sigmoid which is of course not used at validation time since the loss operation is only called during train time.",
                    "make sure to run the network outputs through a tf.nn.sigmoid at validation/test time like this:"
                ]
            }
        ]
    },
    "tf.placeholder":
    {
        "tf.tensor":
        [
            {
                "title": "46525453",
                "h": "tf.tensor",
                "t": "tf.placeholder",
                "sentences":
                [
                    "If the key is a tf.Tensor, the value may be a Python scalar, string,\n  list, or numpy ndarray that can be converted to the same dtype as that\n  tensor.",
                    "Additionally, if the key is a tf.placeholder, the shape of the\n  value will be checked for compatibility with the placeholder."
                ]
            }
        ],
        "tf.compat.v1.placeholder":
        [
            {
                "title": "56562119",
                "h": "tf.compat.v1.placeholder",
                "t": "tf.placeholder",
                "sentences":
                [
                    "If you are converting the code from tensorflow v1 to tensorflow v2, You must implement tf.compat.v1 and  Placeholder is present at tf.compat.v1.placeholder but this can only be executed in eager mode off.",
                    "TensorFlow released the eager execution mode, for which each node is immediately executed after definition.",
                    "Statements using tf.placeholder are thus no longer valid."
                ]
            },
            {
                "title": "60316058",
                "h": "tf.placeholder",
                "t": "tf.compat.v1.placeholder",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: The concept of Placeholders, tf.placeholder will not be available in Tensorflow 2.x (>= 2.0) by default, as the Default Execution Mode is Eager Execution.",
                    "Equivalent command for TF Placeholder in version 2.x is tf.compat.v1.placeholder."
                ]
            }
        ],
        "tf.variable":
        [
            {
                "title": "41352964",
                "h": "tf.placeholder",
                "t": "tf.variable",
                "sentences":
                [
                    "TensorFlow's tf.placeholder() tensors do not require you to specify a shape, in order to allow you to feed tensors of different shapes in a later tf.Session.run() call.",
                    "By default, a placeholder has a completely unconstrained shape, but you can constrain it by passing the optional shape argument.",
                    "Note that tf.Variable objects typically do require a shape when you create them, and this shape is inferred from the first argument to the initializer."
                ]
            },
            {
                "title": "43536220",
                "h": "tf.placeholder",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.constant() and tf.placeholder() are nodes in the graph (ops or operations).",
                    "On the other hand tf.Variable() is a class."
                ]
            },
            {
                "title": "40649185",
                "h": "tf.variable",
                "t": "tf.placeholder",
                "sentences":
                [
                    "As you've noticed, TensorFlow optimizers (i.e.",
                    "subclasses of tf.train.Optimizer) operate on tf.Variable objects because they need to be able to assign new values to those objects, and in TensorFlow only variables support an assign operation.",
                    "If you use a tf.placeholder(), there's nothing to update, because the value of a placeholder is immutable within each step."
                ]
            }
        ],
        "tf.constant":
        [
            {
                "title": "54372304",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "In this code you construct a dictionary where the values are tensors.",
                    "Like you said, this won't work for a VarLenFeature.",
                    "Instead of using tf.constant try using tf.placeholder for a a FixedLenFeature and tf.sparse_placeholder for a VarLenFeature."
                ]
            },
            {
                "title": "51356461",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "Note that the above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations.",
                    "This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.",
                    "As an alternative, you can define the Dataset in terms of tf.placeholder() tensors, and feed the NumPy arrays when you initialize an Iterator over the dataset."
                ]
            },
            {
                "title": "57959532",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "you will not be able to do this for a 'tf.constant()', as it is a constant variable and does not support having its values changed.",
                    "If you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.Variable.",
                    "However these require predefined dimensions, and cannot have their sizes changed as desired in your question."
                ]
            }
        ]
    },
    "tf.compat.v1.layers":
    {
        "tf.layers":
        [
            {
                "title": "60409372",
                "h": "tf.layers",
                "t": "tf.compat.v1.layers",
                "sentences":
                [
                    "For example, tf.layers does not exist anymore in Tensorflow v2.",
                    "You can use tf.compat.v1.layers (see for example the Conv2D function) instead, but this is a temporary fix, as these functions will be removed in a future version."
                ]
            }
        ]
    },
    "tf.layers.flatten":
    {
        "tf.contrib.layers.flatten":
        [
            {
                "title": "50259637",
                "h": "tf.layers.flatten",
                "t": "tf.contrib.layers.flatten",
                "sentences":
                [
                    "Moreover, avoid using contrib package layers, when they already exist in the main package (use tf.layers.flatten instead of tf.contrib.layers.flatten)."
                ]
            },
            {
                "title": "50259737",
                "h": "tf.layers.flatten",
                "t": "tf.contrib.layers.flatten",
                "sentences":
                [
                    "The biggest difference between np.flatten and tf.layers.flatten (or tf.contrib.layers.flatten) is that numpy operations are applicable only to static nd arrays, while tensorflow operations can work with dynamic tensors.",
                    "If the data is already a tensor, use any of the flatten ops provided by tensorflow.",
                    "Between those, tf.layers.flatten is better choice since tf.layers API is more stable than tf.contrib."
                ]
            }
        ]
    },
    "tf.contrib.layers.flatten":
    {
        "tf.layers.flatten":
        [
            {
                "title": "50259637",
                "h": "tf.layers.flatten",
                "t": "tf.contrib.layers.flatten",
                "sentences":
                [
                    "Moreover, avoid using contrib package layers, when they already exist in the main package (use tf.layers.flatten instead of tf.contrib.layers.flatten)."
                ]
            },
            {
                "title": "50259737",
                "h": "tf.layers.flatten",
                "t": "tf.contrib.layers.flatten",
                "sentences":
                [
                    "The biggest difference between np.flatten and tf.layers.flatten (or tf.contrib.layers.flatten) is that numpy operations are applicable only to static nd arrays, while tensorflow operations can work with dynamic tensors.",
                    "If the data is already a tensor, use any of the flatten ops provided by tensorflow.",
                    "Between those, tf.layers.flatten is better choice since tf.layers API is more stable than tf.contrib."
                ]
            }
        ]
    },
    "tf.train.get_global_step":
    {
        "tf.train.get_or_create_global_step":
        [
            {
                "title": "48340802",
                "h": "tf.train.get_global_step",
                "t": "tf.train.get_or_create_global_step",
                "sentences":
                [
                    "You can get the current global step via tf.train.get_global_step() or via tf.train.get_or_create_global_step() function.",
                    "The latter should be called before training starts."
                ]
            },
            {
                "title": "51898852",
                "h": "tf.train.get_global_step",
                "t": "tf.train.get_or_create_global_step",
                "sentences":
                [
                    "tf.train.get_global_step really just creates a variable, but making sure it is not trainable and adding it to the right collections.",
                    "Moreover, you can use tf.train.get_or_create_global_step if you are not sure which part of the graph definition will go first."
                ]
            }
        ],
        "tf.train.global_step":
        [
            {
                "title": "51898852",
                "h": "tf.train.get_global_step",
                "t": "tf.train.global_step",
                "sentences":
                [
                    "tf.train.get_global_step really just creates a variable, but making sure it is not trainable and adding it to the right collections.",
                    "The is one more function, tf.train.global_step, but I don't think it serves any purpose currently, since it just runs the given tensor in the given session and returns its value as an integer."
                ]
            }
        ]
    },
    "tf.train.get_or_create_global_step":
    {
        "tf.train.get_global_step":
        [
            {
                "title": "48340802",
                "h": "tf.train.get_global_step",
                "t": "tf.train.get_or_create_global_step",
                "sentences":
                [
                    "You can get the current global step via tf.train.get_global_step() or via tf.train.get_or_create_global_step() function.",
                    "The latter should be called before training starts."
                ]
            },
            {
                "title": "51898852",
                "h": "tf.train.get_global_step",
                "t": "tf.train.get_or_create_global_step",
                "sentences":
                [
                    "tf.train.get_global_step really just creates a variable, but making sure it is not trainable and adding it to the right collections.",
                    "Moreover, you can use tf.train.get_or_create_global_step if you are not sure which part of the graph definition will go first."
                ]
            }
        ],
        "tf.train.global_step":
        [
            {
                "title": "51898852",
                "h": "tf.train.get_or_create_global_step",
                "t": "tf.train.global_step",
                "sentences":
                [
                    "Moreover, you can use tf.train.get_or_create_global_step if you are not sure which part of the graph definition will go first.",
                    "The is one more function, tf.train.global_step, but I don't think it serves any purpose currently, since it just runs the given tensor in the given session and returns its value as an integer."
                ]
            }
        ]
    },
    "tf.keras.layers.conv2d":
    {
        "tf.layers.max_pooling2d":
        [
            {
                "title": "51089448",
                "h": "tf.keras.layers.conv2d",
                "t": "tf.layers.max_pooling2d",
                "sentences":
                [
                    "tf.keras.layers.Conv2d is a tensorflow-keras layer while tf.layers.max_pooling2d is a tensorflow 'native layer'",
                    "You cannot use a native layer directly within a Keras model, as it will be missing certain attributes required by the Keras API.",
                    "However, it is possible to use native layer if wrapped within a tensorflow-keras Lambda layer."
                ]
            }
        ],
        "tf.nn.conv2d":
        [
            {
                "title": "58533519",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.conv2d",
                "sentences":
                [
                    "tf.nn.conv2d is functional api and tf.keras.layers.Conv2D is layer-class api.",
                    "It's quite as similar as the relationship between torch.nn.functional.conv2d and torch.nn.Conv2D."
                ]
            }
        ]
    },
    "tf.layers.max_pooling2d":
    {
        "tf.keras.layers.conv2d":
        [
            {
                "title": "51089448",
                "h": "tf.keras.layers.conv2d",
                "t": "tf.layers.max_pooling2d",
                "sentences":
                [
                    "tf.keras.layers.Conv2d is a tensorflow-keras layer while tf.layers.max_pooling2d is a tensorflow 'native layer'",
                    "You cannot use a native layer directly within a Keras model, as it will be missing certain attributes required by the Keras API.",
                    "However, it is possible to use native layer if wrapped within a tensorflow-keras Lambda layer."
                ]
            }
        ]
    },
    "tf.keras.layers.dense":
    {
        "tf.layers.dense":
        [
            {
                "title": "50048405",
                "h": "tf.keras.layers.dense",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "Looks like you're using the Dense layer from the wrong package: it should be tf.keras.layers.Dense rather than tf.layers.Dense.",
                    "Note that though they have the same class name and lots of similar parameters, in fact they have nothing in common: tf.layers.Dense is a high-level tensorflow API, not related to keras."
                ]
            }
        ]
    },
    "tf.layers.dense":
    {
        "tf.keras.layers.dense":
        [
            {
                "title": "50048405",
                "h": "tf.keras.layers.dense",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "Looks like you're using the Dense layer from the wrong package: it should be tf.keras.layers.Dense rather than tf.layers.Dense.",
                    "Note that though they have the same class name and lots of similar parameters, in fact they have nothing in common: tf.layers.Dense is a high-level tensorflow API, not related to keras."
                ]
            }
        ],
        " tf.keras.layers.dense":
        [
            {
                "title": "55431562",
                "h": " tf.keras.layers.dense",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "Using tf.keras.layers.Dense() is actually a preferred way in newest tensorflow version 1.13 (tf.layers.dense() is deprectated)."
                ]
            }
        ],
        "tf.nn.xw_plus_b":
        [
            {
                "title": "53761947",
                "h": "tf.nn.xw_plus_b",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "tf.nn.xw_plus_b is a low-level operation that only computes x*W+b and requires existing variables.",
                    "tf.layers.dense is a high-level \"layer\" that creates variables, apply activation can set constrains and apply regularization."
                ]
            }
        ]
    },
    "tf.graphkeys.model_variables":
    {
        "tf.graphkeys.global_variables":
        [
            {
                "title": "45960834",
                "h": "tf.graphkeys.model_variables",
                "t": "tf.graphkeys.global_variables",
                "sentences":
                [
                    "It is preferable that you use tf.GraphKeys.MODEL_VARIABLES in this case instead of tf.GraphKeys.GLOBAL_VARIABLES, which has a more general purpose and is likely to contain other unrelated variables as well."
                ]
            }
        ]
    },
    "tf.graphkeys.global_variables":
    {
        "tf.graphkeys.model_variables":
        [
            {
                "title": "45960834",
                "h": "tf.graphkeys.model_variables",
                "t": "tf.graphkeys.global_variables",
                "sentences":
                [
                    "It is preferable that you use tf.GraphKeys.MODEL_VARIABLES in this case instead of tf.GraphKeys.GLOBAL_VARIABLES, which has a more general purpose and is likely to contain other unrelated variables as well."
                ]
            }
        ]
    },
    "tf.contrib.layers.linear":
    {
        "tf.contrib.layers.fully_connected":
        [
            {
                "title": "41628828",
                "h": "tf.contrib.layers.linear",
                "t": "tf.contrib.layers.fully_connected",
                "sentences":
                [
                    "The tf.contrib.layers.linear() function appears to have been removed, but you can use tf.contrib.layers.fully_connected(…, activation_fn=None) to achieve the same effect."
                ]
            }
        ]
    },
    "tf.contrib.layers.fully_connected":
    {
        "tf.contrib.layers.linear":
        [
            {
                "title": "41628828",
                "h": "tf.contrib.layers.linear",
                "t": "tf.contrib.layers.fully_connected",
                "sentences":
                [
                    "The tf.contrib.layers.linear() function appears to have been removed, but you can use tf.contrib.layers.fully_connected(…, activation_fn=None) to achieve the same effect."
                ]
            }
        ]
    },
    "tf.train.saver":
    {
        "tf.saved_model.builder.savedmodelbuilder":
        [
            {
                "title": "45925593",
                "h": "tf.train.saver",
                "t": "tf.saved_model.builder.savedmodelbuilder",
                "sentences":
                [
                    "tf.train.Saver is primarily used for saving and restoring checkpoints.",
                    "However, since you want prediction, I would suggest using a tf.saved_model.builder.SavedModelBuilder to build a SavedModel binary."
                ]
            }
        ],
        "tf.saved_model":
        [
            {
                "title": "47235448",
                "h": "tf.train.saver",
                "t": "tf.saved_model",
                "sentences":
                [
                    "Though there have been many solutions, most of them is based on tf.train.Saver.",
                    "When we load a .ckpt saved by Saver, we have to either redefine the tensorflow network or use some weird and hard-remembered name, e.g.",
                    "'placehold_0:0','dense/Adam/Weight:0'.",
                    "Here I recommend to use tf.saved_model, one simplest example given below, your can learn more from Serving a TensorFlow Model:"
                ]
            }
        ]
    },
    "tf.saved_model.builder.savedmodelbuilder":
    {
        "tf.train.saver":
        [
            {
                "title": "45925593",
                "h": "tf.train.saver",
                "t": "tf.saved_model.builder.savedmodelbuilder",
                "sentences":
                [
                    "tf.train.Saver is primarily used for saving and restoring checkpoints.",
                    "However, since you want prediction, I would suggest using a tf.saved_model.builder.SavedModelBuilder to build a SavedModel binary."
                ]
            }
        ]
    },
    "tf.metrics.recall_at_k":
    {
        "tf.metrics.recall_at_top_k":
        [
            {
                "title": "53791390",
                "h": "tf.metrics.recall_at_k",
                "t": "tf.metrics.recall_at_top_k",
                "sentences":
                [
                    "First, let's talk about the difference between tf.metrics.recall_at_k and tf.metrics.recall_at_top_k.",
                    "If you look at open source code, you will find precision_at_k is a simple wrapper around precision_at_top_k.",
                    "precision_at_k applies tf.nn.top_k first, and then calls precision_at_top_k.",
                    "Documentation shows that precision_at_k expects a float tensor of logits values, but precision_at_top_k expects integer tensor the predictions to be the indices of the top k classes.",
                    "So if your value is a logit score values, you should use precision_at_k."
                ]
            }
        ]
    },
    "tf.metrics.recall_at_top_k":
    {
        "tf.metrics.recall_at_k":
        [
            {
                "title": "53791390",
                "h": "tf.metrics.recall_at_k",
                "t": "tf.metrics.recall_at_top_k",
                "sentences":
                [
                    "First, let's talk about the difference between tf.metrics.recall_at_k and tf.metrics.recall_at_top_k.",
                    "If you look at open source code, you will find precision_at_k is a simple wrapper around precision_at_top_k.",
                    "precision_at_k applies tf.nn.top_k first, and then calls precision_at_top_k.",
                    "Documentation shows that precision_at_k expects a float tensor of logits values, but precision_at_top_k expects integer tensor the predictions to be the indices of the top k classes.",
                    "So if your value is a logit score values, you should use precision_at_k."
                ]
            }
        ]
    },
    "tf.logging.info":
    {
        "tf.get_logger":
        [
            {
                "title": "62979584",
                "h": "tf.logging.info",
                "t": "tf.get_logger",
                "sentences":
                [
                    "tf.logging.info('embedding_name: %s', FLAGS.embedding_dimension) is indeed an out-date way of doing this.",
                    "It is no longer supported.",
                    "You can use tf.get_logger as an alternative."
                ]
            }
        ]
    },
    "tf.get_logger":
    {
        "tf.logging.info":
        [
            {
                "title": "62979584",
                "h": "tf.logging.info",
                "t": "tf.get_logger",
                "sentences":
                [
                    "tf.logging.info('embedding_name: %s', FLAGS.embedding_dimension) is indeed an out-date way of doing this.",
                    "It is no longer supported.",
                    "You can use tf.get_logger as an alternative."
                ]
            }
        ]
    },
    "tf.contrib.data.unbatch":
    {
        "tf.data.dataset.unbatch":
        [
            {
                "title": "48472420",
                "h": "tf.contrib.data.unbatch",
                "t": "tf.data.dataset.unbatch",
                "sentences":
                [
                    "I think you're just looking for the transformation tf.contrib.data.unbatch.",
                    "If elements of the dataset are shaped [B, a0, a1, ...], where B may vary from element to element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].",
                    "From TF 2.0, you can use directly tf.data.Dataset.unbatch:"
                ]
            }
        ]
    },
    "tf.data.dataset.unbatch":
    {
        "tf.contrib.data.unbatch":
        [
            {
                "title": "48472420",
                "h": "tf.contrib.data.unbatch",
                "t": "tf.data.dataset.unbatch",
                "sentences":
                [
                    "I think you're just looking for the transformation tf.contrib.data.unbatch.",
                    "If elements of the dataset are shaped [B, a0, a1, ...], where B may vary from element to element, then for each element in the dataset, the unbatched dataset will contain B consecutive elements of shape [a0, a1, ...].",
                    "From TF 2.0, you can use directly tf.data.Dataset.unbatch:"
                ]
            }
        ]
    },
    "tf.stack":
    {
        "tf.tile":
        [
            {
                "title": "53633664",
                "h": "tf.stack",
                "t": "tf.tile",
                "sentences":
                [
                    "You can use tf.stack like you suggest, although tf.tile would probably be more obvious (and may save memory, although I'm not sure how it is implemented really):"
                ]
            }
        ],
        "tf.concat":
        [
            {
                "title": "49896013",
                "h": "tf.stack",
                "t": "tf.concat",
                "sentences":
                [
                    "tf.stack concatenate the given tensors along a new dimension.",
                    "If you want to concatenate across an existing dimension, use tf.concat:"
                ]
            }
        ]
    },
    "tf.tile":
    {
        "tf.stack":
        [
            {
                "title": "53633664",
                "h": "tf.stack",
                "t": "tf.tile",
                "sentences":
                [
                    "You can use tf.stack like you suggest, although tf.tile would probably be more obvious (and may save memory, although I'm not sure how it is implemented really):"
                ]
            }
        ]
    },
    "tf.io.parse_single_example":
    {
        "tf.io.parse_example":
        [
            {
                "title": "64080452",
                "h": "tf.io.parse_single_example",
                "t": "tf.io.parse_example",
                "sentences":
                [
                    "Where in tf.io.parse_single_example   Parses a serialized Example protos given in serialized which is similar to tf.io.parse_example, except tf.io.parse_example is serialized in batches.",
                    "You can use tf.io.parse_example for performance advantage over batching samples."
                ]
            }
        ]
    },
    "tf.io.parse_example":
    {
        "tf.io.parse_single_example":
        [
            {
                "title": "64080452",
                "h": "tf.io.parse_single_example",
                "t": "tf.io.parse_example",
                "sentences":
                [
                    "Where in tf.io.parse_single_example   Parses a serialized Example protos given in serialized which is similar to tf.io.parse_example, except tf.io.parse_example is serialized in batches.",
                    "You can use tf.io.parse_example for performance advantage over batching samples."
                ]
            }
        ],
        "tf.train.example":
        [
            {
                "title": "57776691",
                "h": "tf.train.example",
                "t": "tf.io.parse_example",
                "sentences":
                [
                    "It looks like your serialized_tf_example placeholder has shape=[] which is a single example.",
                    "You should pass a single tf.train.Example, serialized as a string:",
                    "If you want to feed a batch of examples instead of a single serialized example, you need to use shape=[None] and tf.io.parse_example instead."
                ]
            }
        ]
    },
    "tf.nn.sigmoid_cross_entropy_with_logits":
    {
        "tf.nn.softmax_cross_entropy_with_logits":
        [
            {
                "title": "47889528",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "binary_crossentropy (and tf.nn.sigmoid_cross_entropy_with_logits under the hood) is for binary multi-label classification (labels are independent)..",
                    "categorical_crossentropy (and tf.nn.softmax_cross_entropy_with_logits under the hood) is for multi-class classification (classes are exclusive).."
                ]
            },
            {
                "title": "45459557",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.softmax_cross_entropy_with_logits",
                "sentences":
                [
                    "I think that you should use tf.nn.sigmoid_cross_entropy_with_logits instead of tf.nn.softmax_cross_entropy_with_logits because you use sigmoid and 1 neuron in output layer."
                ]
            },
            {
                "title": "39783314",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.sigmoid_cross_entropy_with_logits",
                "sentences":
                [
                    "The warning just informs you that tf.nn.softmax_cross_entropy_with_logits will apply a softmax on the input logits, before computing cross-entropy.",
                    "This warning seems really to avoid applying softmax twice, as the cross-entropy results would be very different.",
                    "As the warning states, this implementation is for improving performance, with the caveat that you should not put your own softmax layer as input (which is somewhat convenient, in practice).",
                    "If the forced softmax hinders your computation, perhaps another API could help: tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits.",
                    "But whatever is applied on the input logits, tf.nn.softmax_cross_entropy_with_logits will apply a softmax before computing the cross-entropy."
                ]
            }
        ],
        "tf.nn.sigmoid":
        [
            {
                "title": "55601831",
                "h": "tf.nn.sigmoid_cross_entropy_with_logits",
                "t": "tf.nn.sigmoid",
                "sentences":
                [
                    "The problem was that the tf.nn.sigmoid_cross_entropy_with_logits runs the logits through a sigmoid which is of course not used at validation time since the loss operation is only called during train time.",
                    "make sure to run the network outputs through a tf.nn.sigmoid at validation/test time like this:"
                ]
            }
        ]
    },
    "tf.keras.layers.Conv2DTranspose":
    {
        "tf.nn.conv2d_transpose":
        [
            {
                "title": "46040638",
                "h": "tf.keras.layers.Conv2DTranspose",
                "t": "tf.nn.conv2d_transpose",
                "sentences":
                [
                    "The problem is that a convolution with 'SAME' padding and a stride of 2 will have an output shape of 24*14 for all the following input shapes:",
                    "48*27, 48*28, 47*27 and 47*28",
                    "Therefore tf.keras.layers.Conv2DTranspose can not know the correct output shape by itself and it goes with the symmetric case of doubling each of the dimensions: 24*14 -> 48*28",
                    "If you want to use a different output shape you can use the lower level deconvolution: tf.nn.conv2d_transpose",
                    "Using it allows you to specify the output shape."
                ]
            },
            {
                "title": "50307810",
                "h": "tf.nn.conv2d_transpose",
                "t": "tf.keras.layers.Conv2DTranspose",
                "sentences":
                [
                    "The input args filter to tf.nn.conv2d_transpose is the weights matrix itself and not just the size of the filter.",
                    "Note: Its better to use tf.keras.layers.Conv2DTranspose API, where the filters arg is the filter size and the weights initialization happens within."
                ]
            },
            {
                "title": "57560621",
                "h": "tf.nn.conv2d_transpose",
                "t": "tf.keras.layers.Conv2DTranspose",
                "sentences":
                [
                    "While tf.nn.conv2d_transpose represents an operation in the computational graph, tf.keras.layers.Conv2DTranspose defines the entire layer.",
                    "Being more precise tf.nn.conv2d_transpose applies a convolutional filter to the inputs.",
                    "tf.keras.layers.Conv2DTranspose, on the other hand, first creates trainable variables that will serve as filter according to the arguments given, and then it internally calls some conv2d_transpose operation.",
                    "Based on the arguments, it also applies some other operations as adding bias, applying non-linearity, or normalizing the weights or inputs.",
                    "With tf.keras.layers.Conv2DTranspose you do not specify output shape as it is computed from filter size, input size, and stride."
                ]
            }
        ]
    },
    "tf.nn.conv2d_transpose":
    {
        "tf.keras.layers.Conv2DTranspose":
        [
            {
                "title": "46040638",
                "h": "tf.keras.layers.Conv2DTranspose",
                "t": "tf.nn.conv2d_transpose",
                "sentences":
                [
                    "The problem is that a convolution with 'SAME' padding and a stride of 2 will have an output shape of 24*14 for all the following input shapes:",
                    "48*27, 48*28, 47*27 and 47*28",
                    "Therefore tf.keras.layers.Conv2DTranspose can not know the correct output shape by itself and it goes with the symmetric case of doubling each of the dimensions: 24*14 -> 48*28",
                    "If you want to use a different output shape you can use the lower level deconvolution: tf.nn.conv2d_transpose",
                    "Using it allows you to specify the output shape."
                ]
            },
            {
                "title": "50307810",
                "h": "tf.nn.conv2d_transpose",
                "t": "tf.keras.layers.Conv2DTranspose",
                "sentences":
                [
                    "The input args filter to tf.nn.conv2d_transpose is the weights matrix itself and not just the size of the filter.",
                    "Note: Its better to use tf.keras.layers.Conv2DTranspose API, where the filters arg is the filter size and the weights initialization happens within."
                ]
            },
            {
                "title": "57560621",
                "h": "tf.nn.conv2d_transpose",
                "t": "tf.keras.layers.Conv2DTranspose",
                "sentences":
                [
                    "While tf.nn.conv2d_transpose represents an operation in the computational graph, tf.keras.layers.Conv2DTranspose defines the entire layer.",
                    "Being more precise tf.nn.conv2d_transpose applies a convolutional filter to the inputs.",
                    "tf.keras.layers.Conv2DTranspose, on the other hand, first creates trainable variables that will serve as filter according to the arguments given, and then it internally calls some conv2d_transpose operation.",
                    "Based on the arguments, it also applies some other operations as adding bias, applying non-linearity, or normalizing the weights or inputs.",
                    "With tf.keras.layers.Conv2DTranspose you do not specify output shape as it is computed from filter size, input size, and stride."
                ]
            }
        ],
        "tf.keras.layers.conv2dtranspose":
        [
            {
                "title": "55199043",
                "h": "tf.keras.layers.conv2dtranspose",
                "t": "tf.nn.conv2d_transpose",
                "sentences":
                [
                    "TensoFlow 2.0 has tf.keras.layers.Conv2DTranspose.",
                    "Keras is the default high level API for TF 2.0, but it still have tf.nn.conv2d_transpose for low level applications"
                ]
            }
        ]
    },
    "tf.map_fn":
    {
        "tf.mul":
        [
            {
                "title": "40981831",
                "h": "tf.map_fn",
                "t": "tf.mul",
                "sentences":
                [
                    "The TensorFlow function tf.map_fn(fn, elems) allows you to apply a function (fn) to each slice of a tensor (elems).",
                    "It may also be possible to implement your operation more concisely using broadcasting on the tf.mul() operator, which uses NumPy broadcasting semantics, and the axis argument to tf.reduce_sum()."
                ]
            }
        ],
        "tf.reduce_sum":
        [
            {
                "title": "41471616",
                "h": "tf.map_fn",
                "t": "tf.reduce_sum",
                "sentences":
                [
                    "The TensorFlow Python API includes the tf.map_fn(fn, elems) higher-order operator, which allows you to specify a (Python) function fn that will be applied to each slice of elems in the 0th dimension (i.e.",
                    "to each row if elems is a matrix).",
                    "Note that, while tf.map_fn() is very general, it may be more efficient to use specialized ops that either broadcast their arguments on one or more dimensions (e.g.",
                    "tf.multiply()), or reduce in parallel across one or more dimensions (e.g.",
                    "However, tf.map_fn() is useful when there is no built-in operator to do what you want."
                ]
            }
        ],
        "tf.py_func":
        [
            {
                "title": "50359422",
                "h": "tf.py_func",
                "t": "tf.map_fn",
                "sentences":
                [
                    "So the main issue is that you have to use tf.py_func() to map a Python function (scipy.integrate.quad() in this case) on a tensor.",
                    "tf.map_fn() will map other TensorFlow operations and passes and expects tensors as operands."
                ]
            }
        ]
    },
    "tf.mul":
    {
        "tf.map_fn":
        [
            {
                "title": "40981831",
                "h": "tf.map_fn",
                "t": "tf.mul",
                "sentences":
                [
                    "The TensorFlow function tf.map_fn(fn, elems) allows you to apply a function (fn) to each slice of a tensor (elems).",
                    "It may also be possible to implement your operation more concisely using broadcasting on the tf.mul() operator, which uses NumPy broadcasting semantics, and the axis argument to tf.reduce_sum()."
                ]
            }
        ]
    },
    "tf.nn.lrn":
    {
        "tf.nn.local_response_normalization":
        [
            {
                "title": "37377303",
                "h": "tf.nn.lrn",
                "t": "tf.nn.local_response_normalization",
                "sentences":
                [
                    "tf.nn.lrn is a short for tf.nn.local_response_normalization."
                ]
            }
        ]
    },
    "tf.nn.local_response_normalization":
    {
        "tf.nn.lrn":
        [
            {
                "title": "37377303",
                "h": "tf.nn.lrn",
                "t": "tf.nn.local_response_normalization",
                "sentences":
                [
                    "tf.nn.lrn is a short for tf.nn.local_response_normalization."
                ]
            }
        ]
    },
    "tf.keras.layers.Conv2D":
    {
        "tf.nn.convolution":
        [
            {
                "title": "42932979",
                "h": "tf.keras.layers.Conv2D",
                "t": "tf.nn.convolution",
                "sentences":
                [
                    "For convolution, they are the same.",
                    "More precisely, tf.keras.layers.Conv2D (actually _Conv) uses tf.nn.convolution as the backend."
                ]
            }
        ],
        "tf.nn.conv2d":
        [
            {
                "title": "49225225",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "It's confusing, but TensorFlow has two conv2d methods: tf.nn.conv2d and tf.keras.layers.Conv2D.",
                    "If you want to filter an image with a known kernel, call tf.nn.conv2d.",
                    "If you want to create a layer in a convolutional neural network (CNN) that will determine its filters programmatically, call tf.keras.layers.Conv2D."
                ]
            },
            {
                "title": "43587561",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.nn.conv2d(...) is the core, low-level convolution functionality provided by TensorFlow.",
                    "Note, that in current TensorFlow versions, parts of layers are now in core, too, e.g.",
                    "tf.keras.layers.Conv2D.",
                    "The difference is simply, that tf.nn.conv2d is an op, that does convolution, nothing else.",
                    "tf.keras.layers.Conv2D does more, e.g.",
                    "it also creates variables for the kernel and the biases amongst other things."
                ]
            },
            {
                "title": "48537309",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.nn.conv2d is a low-level implementation in Tensorflow, which exposes the GPU API as it is.",
                    "There is another high-level implementation as well, tf.keras.layers.Conv2D which only allows you to pass a two element tuple, with height stride and width stride.",
                    "But, if you want to use the low-level API (maybe due to more control over the parameters), you should always keep batch and column stride to 1."
                ]
            },
            {
                "title": "47321605",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "As others mentioned the parameters are different especially the \"filter(s)\".",
                    "tf.nn.conv2d takes a tensor as a filter, which means you can specify the weight decay (or maybe other properties) like the following in cifar10 code.",
                    "(Whether you want/need to have weight decay in conv layer is another question.)",
                    "I'm not quite sure how to set weight decay in tf.keras.layers.Conv2D since it only take an integer as filters.",
                    "On the other hand, tf.keras.layers.Conv2D handles activation and bias automatically while you have to write additional codes for these if you use tf.nn.conv2d."
                ]
            },
            {
                "title": "45308609",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "I would use tf.nn.conv2d when loading a pretrained model (example code: https://github.com/ry/tensorflow-vgg16), and tf.keras.layers.Conv2D for a model trained from scratch."
                ]
            }
        ],
        "tf.keras.layers.Conv1D":
        [
            {
                "title": "48223162",
                "h": "tf.keras.layers.Conv1D",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.keras.layers.Conv1D is used when you slide your convolution kernels along 1 dimensions (i.e.",
                    "you reuse the same weights, sliding them along 1 dimensions), whereas tf.keras.layers.Conv2D is used when you slide your convolution kernels along 2 dimensions (i.e.",
                    "you reuse the same weights, sliding them along 2 dimensions).",
                    "So the typical use case for tf.keras.layers.Conv2D is if you have a 2D image.",
                    "And possible use-cases for tf.keras.layers.Conv1D are, for example:",
                    "Convolutions in Time.",
                    "Convolutions on Piano notes."
                ]
            }
        ],
        "tf.contrib.layers.conv2d":
        [
            {
                "title": "43587561",
                "h": "tf.contrib.layers.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.contrib.layers.conv2d(...) is part of a higher-level API build around core-TensorFlow.",
                    "Note, that in current TensorFlow versions, parts of layers are now in core, too, e.g.",
                    "tf.keras.layers.Conv2D."
                ]
            }
        ]
    },
    "tf.nn.convolution":
    {
        "tf.keras.layers.Conv2D":
        [
            {
                "title": "42932979",
                "h": "tf.keras.layers.Conv2D",
                "t": "tf.nn.convolution",
                "sentences":
                [
                    "For convolution, they are the same.",
                    "More precisely, tf.keras.layers.Conv2D (actually _Conv) uses tf.nn.convolution as the backend."
                ]
            }
        ],
        "tf.nn.conv2d":
        [
            {
                "title": "47777046",
                "h": "tf.nn.conv2d",
                "t": "tf.nn.convolution",
                "sentences":
                [
                    "tf.nn.conv2d computes a 2-D convolution given 4-D input and filter tensors, while tf.nn.convolution computes sums of N-D convolutions.",
                    "Both return a Tensor with same type of input."
                ]
            }
        ]
    },
    "tf.train.global_step":
    {
        "tf.train.get_global_step":
        [
            {
                "title": "51898852",
                "h": "tf.train.get_global_step",
                "t": "tf.train.global_step",
                "sentences":
                [
                    "tf.train.get_global_step really just creates a variable, but making sure it is not trainable and adding it to the right collections.",
                    "The is one more function, tf.train.global_step, but I don't think it serves any purpose currently, since it just runs the given tensor in the given session and returns its value as an integer."
                ]
            }
        ],
        "tf.train.get_or_create_global_step":
        [
            {
                "title": "51898852",
                "h": "tf.train.get_or_create_global_step",
                "t": "tf.train.global_step",
                "sentences":
                [
                    "Moreover, you can use tf.train.get_or_create_global_step if you are not sure which part of the graph definition will go first.",
                    "The is one more function, tf.train.global_step, but I don't think it serves any purpose currently, since it just runs the given tensor in the given session and returns its value as an integer."
                ]
            }
        ]
    },
    "tf.keras.losses.SparseCategoricalCrossentropy":
    {
        "tf.keras.losses.CategoricalCrossentropy":
        [
            {
                "title": "59454480",
                "h": "tf.keras.losses.SparseCategoricalCrossentropy",
                "t": "tf.keras.losses.CategoricalCrossentropy",
                "sentences":
                [
                    "Try different possibilities like \ntf.keras.losses.SparseCategoricalCrossentropy if your labels are one-hot vectors."
                ]
            }
        ]
    },
    "tf.keras.losses.CategoricalCrossentropy":
    {
        "tf.keras.losses.SparseCategoricalCrossentropy":
        [
            {
                "title": "59454480",
                "h": "tf.keras.losses.SparseCategoricalCrossentropy",
                "t": "tf.keras.losses.CategoricalCrossentropy",
                "sentences":
                [
                    "Try different possibilities like \ntf.keras.losses.SparseCategoricalCrossentropy if your labels are one-hot vectors."
                ]
            }
        ]
    },
    "tf.compat.v2":
    {
        "tf.compat.v1":
        [
            {
                "title": "59018552",
                "h": "tf.compat.v2",
                "t": "tf.compat.v1",
                "sentences":
                [
                    "tf.compat is the compatibility module.",
                    "v1 is the version 1.x of TensorFlow, while v2 is the version 2.x of TensorFlow (so although strange, you have tf2 inside tf2, through tf.compat.v2).",
                    "However, using the compatibility module is often not needed and I recommend to do not reply on it too much (usually it is better to find the tf2 equivalent way of doing something, instead of writing code in the tf1 style, through tf.compat.v1)"
                ]
            }
        ]
    },
    "tf.compat.v1":
    {
        "tf.compat.v2":
        [
            {
                "title": "59018552",
                "h": "tf.compat.v2",
                "t": "tf.compat.v1",
                "sentences":
                [
                    "tf.compat is the compatibility module.",
                    "v1 is the version 1.x of TensorFlow, while v2 is the version 2.x of TensorFlow (so although strange, you have tf2 inside tf2, through tf.compat.v2).",
                    "However, using the compatibility module is often not needed and I recommend to do not reply on it too much (usually it is better to find the tf2 equivalent way of doing something, instead of writing code in the tf1 style, through tf.compat.v1)"
                ]
            }
        ]
    },
    "tf.nn.conv1d":
    {
        "tf.nn.conv2d": [
            {
                "title": "55217354",
                "h": "tf.nn.conv1d",
                "t": "tf.nn.conv2d",
                "sentences":
                [
                    "tf.nn.conv1d just call the tf.nn.conv2d",
                    "This is the description of tf.nn.conv1d:",
                    "Internally, this op reshapes the input tensors and invokes tf.nn.conv2d."
                ]
            }
        ]
    },
    "tf.nn.conv2d":
    {
        "tf.nn.conv1d": [
            {
                "title": "55217354",
                "h": "tf.nn.conv1d",
                "t": "tf.nn.conv2d",
                "sentences":
                [
                    "tf.nn.conv1d just call the tf.nn.conv2d",
                    "This is the description of tf.nn.conv1d:",
                    "Internally, this op reshapes the input tensors and invokes tf.nn.conv2d."
                ]
            }
        ],
        "tf.nn.convolution":
        [
            {
                "title": "47777046",
                "h": "tf.nn.conv2d",
                "t": "tf.nn.convolution",
                "sentences":
                [
                    "tf.nn.conv2d computes a 2-D convolution given 4-D input and filter tensors, while tf.nn.convolution computes sums of N-D convolutions.",
                    "Both return a Tensor with same type of input."
                ]
            }
        ],
        "tf.keras.layers.Conv2D":
        [
            {
                "title": "49225225",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "It's confusing, but TensorFlow has two conv2d methods: tf.nn.conv2d and tf.keras.layers.Conv2D.",
                    "If you want to filter an image with a known kernel, call tf.nn.conv2d.",
                    "If you want to create a layer in a convolutional neural network (CNN) that will determine its filters programmatically, call tf.keras.layers.Conv2D."
                ]
            },
            {
                "title": "43587561",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.nn.conv2d(...) is the core, low-level convolution functionality provided by TensorFlow.",
                    "Note, that in current TensorFlow versions, parts of layers are now in core, too, e.g.",
                    "tf.keras.layers.Conv2D.",
                    "The difference is simply, that tf.nn.conv2d is an op, that does convolution, nothing else.",
                    "tf.keras.layers.Conv2D does more, e.g.",
                    "it also creates variables for the kernel and the biases amongst other things."
                ]
            },
            {
                "title": "48537309",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.nn.conv2d is a low-level implementation in Tensorflow, which exposes the GPU API as it is.",
                    "There is another high-level implementation as well, tf.keras.layers.Conv2D which only allows you to pass a two element tuple, with height stride and width stride.",
                    "But, if you want to use the low-level API (maybe due to more control over the parameters), you should always keep batch and column stride to 1."
                ]
            },
            {
                "title": "47321605",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "As others mentioned the parameters are different especially the \"filter(s)\".",
                    "tf.nn.conv2d takes a tensor as a filter, which means you can specify the weight decay (or maybe other properties) like the following in cifar10 code.",
                    "(Whether you want/need to have weight decay in conv layer is another question.)",
                    "I'm not quite sure how to set weight decay in tf.keras.layers.Conv2D since it only take an integer as filters.",
                    "On the other hand, tf.keras.layers.Conv2D handles activation and bias automatically while you have to write additional codes for these if you use tf.nn.conv2d."
                ]
            },
            {
                "title": "45308609",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "I would use tf.nn.conv2d when loading a pretrained model (example code: https://github.com/ry/tensorflow-vgg16), and tf.keras.layers.Conv2D for a model trained from scratch."
                ]
            }
        ],
        "tf.contrib.layers.conv2d":
        [
            {
                "title": "43587561",
                "h": "tf.nn.conv2d",
                "t": "tf.contrib.layers.conv2d",
                "sentences":
                [
                    "tf.nn.conv2d(...) is the core, low-level convolution functionality provided by TensorFlow.",
                    "tf.contrib.layers.conv2d(...) is part of a higher-level API build around core-TensorFlow."
                ]
            }
        ],
        "tf.keras.layers.conv2d":
        [
            {
                "title": "58533519",
                "h": "tf.nn.conv2d",
                "t": "tf.keras.layers.conv2d",
                "sentences":
                [
                    "tf.nn.conv2d is functional api and tf.keras.layers.Conv2D is layer-class api.",
                    "It's quite as similar as the relationship between torch.nn.functional.conv2d and torch.nn.Conv2D."
                ]
            }
        ]
    },
    "tf.compat.v1.placeholder":
    {
        "tf.placeholder":
        [
            {
                "title": "56562119",
                "h": "tf.compat.v1.placeholder",
                "t": "tf.placeholder",
                "sentences":
                [
                    "If you are converting the code from tensorflow v1 to tensorflow v2, You must implement tf.compat.v1 and  Placeholder is present at tf.compat.v1.placeholder but this can only be executed in eager mode off.",
                    "TensorFlow released the eager execution mode, for which each node is immediately executed after definition.",
                    "Statements using tf.placeholder are thus no longer valid."
                ]
            },
            {
                "title": "60316058",
                "h": "tf.placeholder",
                "t": "tf.compat.v1.placeholder",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: The concept of Placeholders, tf.placeholder will not be available in Tensorflow 2.x (>= 2.0) by default, as the Default Execution Mode is Eager Execution.",
                    "Equivalent command for TF Placeholder in version 2.x is tf.compat.v1.placeholder."
                ]
            }
        ]
    },
    "tf.metrics.mean_squared_error":
    {
        "tf.losses.mean_squared_error":
        [
            {
                "title": "47021208",
                "h": "tf.metrics.mean_squared_error",
                "t": "tf.losses.mean_squared_error",
                "sentences":
                [
                    "tf.metrics.mean_squared_error() is meant to compute the MSE on a whole dataset for instance, so you should not be using it if you want the result for batches independantly.",
                    "For that, use tf.losses.mean_squared_error(a, b, loss_collection=None) for instance."
                ]
            }
        ]
    },
    "tf.losses.mean_squared_error":
    {
        "tf.metrics.mean_squared_error":
        [
            {
                "title": "47021208",
                "h": "tf.metrics.mean_squared_error",
                "t": "tf.losses.mean_squared_error",
                "sentences":
                [
                    "tf.metrics.mean_squared_error() is meant to compute the MSE on a whole dataset for instance, so you should not be using it if you want the result for batches independantly.",
                    "For that, use tf.losses.mean_squared_error(a, b, loss_collection=None) for instance."
                ]
            }
        ],
        "tf.nn.l2_loss":
        [
            {
                "title": "44522505",
                "h": "tf.losses.mean_squared_error",
                "t": "tf.nn.l2_loss",
                "sentences":
                [
                    "And notice that your error argument to tf.losses.mean_squared_error is a scope name, not the name of the returned operation.",
                    "This can be confusing, as other operations, such as tf.nn.l2_loss, accept a name argument."
                ]
            }
        ]
    },
    "tf.nn.dropout":
    {
        "tf.layers.dropout":
        [
            {
                "title": "48085077",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "On the training phase they are identical (as long as \"drop rate\" and \"keep rate\" are consistent).",
                    "However, for evaluation (test) phase they are completely different.",
                    "tf.nn.dropout will still do random dropping while tf.layers.dropout won't drop anything (transparent layer).",
                    "In most cases it make sense to use tf.layers.dropout."
                ]
            },
            {
                "title": "51306350",
                "h": "tf.layers.dropout",
                "t": "tf.nn.dropout",
                "sentences":
                [
                    "The latter function also does not have an argument for a training switch.",
                    "It is not to be confused with tf.layers.dropout, which wraps tf.nn.dropout and has a training argument.",
                    "As we can see in the implementation, the layers version returns either the result of nn.dropout or the identity depending on the training switch.",
                    "To avoid any confusion: layers.dropout eventually calls the \"keras layers\" version of dropout which is the implementation linked above."
                ]
            },
            {
                "title": "48549654",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "The tf.nn.dropout scaled the weights by 1./keep prob during training phase, while tf.layers.dropout scaled the weights by 1./(1-rate)."
                ]
            },
            {
                "title": "47809165",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "Instead of using tf.nn.dropout, consider using tf.layers.dropout.",
                    "It's a wrapper over tf.nn.dropout, but it comes with a boolean to activate or deactivate the dropout.",
                    "(You can set the boolean the same way that the prob in my example)."
                ]
            },
            {
                "title": "47916567",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "The only thing came to my mind is that tf.nn.dropout uses keep_prob while tf.layers.dropout uses dropout rate."
                ]
            }
        ],
        "tf.contrib.nn.alpha_dropout":
        [
            {
                "title": "51306350",
                "h": "tf.contrib.nn.alpha_dropout",
                "t": "tf.nn.dropout",
                "sentences":
                [
                    "tf.contrib.nn.alpha_dropout should be seen as an analogue to tf.nn.dropout.",
                    "The latter function also does not have an argument for a training switch.",
                    "It is not to be confused with tf.layers.dropout, which wraps tf.nn.dropout and has a training argument.",
                    "As we can see in the implementation, the layers version returns either the result of nn.dropout or the identity depending on the training switch.",
                    "It should be relatively easy to define your own wrapper around alpha_dropout in a similar manner."
                ]
            }
        ],
        "tf.clip_by_norm":
        [
            {
                "title": "39646025",
                "h": "tf.nn.dropout",
                "t": "tf.clip_by_norm",
                "sentences":
                [
                    "tf.nn.dropout does not impose any norm constraint.",
                    "I believe what you're looking for is to \"process the gradients before applying them\" using tf.clip_by_norm."
                ]
            }
        ]
    },
    "tf.layers.dropout":
    {
        "tf.nn.dropout":
        [
            {
                "title": "48085077",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "On the training phase they are identical (as long as \"drop rate\" and \"keep rate\" are consistent).",
                    "However, for evaluation (test) phase they are completely different.",
                    "tf.nn.dropout will still do random dropping while tf.layers.dropout won't drop anything (transparent layer).",
                    "In most cases it make sense to use tf.layers.dropout."
                ]
            },
            {
                "title": "51306350",
                "h": "tf.layers.dropout",
                "t": "tf.nn.dropout",
                "sentences":
                [
                    "The latter function also does not have an argument for a training switch.",
                    "It is not to be confused with tf.layers.dropout, which wraps tf.nn.dropout and has a training argument.",
                    "As we can see in the implementation, the layers version returns either the result of nn.dropout or the identity depending on the training switch.",
                    "To avoid any confusion: layers.dropout eventually calls the \"keras layers\" version of dropout which is the implementation linked above."
                ]
            },
            {
                "title": "48549654",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "The tf.nn.dropout scaled the weights by 1./keep prob during training phase, while tf.layers.dropout scaled the weights by 1./(1-rate)."
                ]
            },
            {
                "title": "47809165",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "Instead of using tf.nn.dropout, consider using tf.layers.dropout.",
                    "It's a wrapper over tf.nn.dropout, but it comes with a boolean to activate or deactivate the dropout.",
                    "(You can set the boolean the same way that the prob in my example)."
                ]
            },
            {
                "title": "47916567",
                "h": "tf.nn.dropout",
                "t": "tf.layers.dropout",
                "sentences":
                [
                    "The only thing came to my mind is that tf.nn.dropout uses keep_prob while tf.layers.dropout uses dropout rate."
                ]
            }
        ]
    },
    "tf.data.experimental":
    {
        "tf.contrib.data":
        [
            {
                "title": "54384163",
                "h": "tf.data.experimental",
                "t": "tf.contrib.data",
                "sentences":
                [
                    "Note that I am looking at tf.data.experimental, as the tf.contrib.data one is deprecated."
                ]
            }
        ]
    },
    "tf.contrib.data":
    {
        "tf.data.experimental":
        [
            {
                "title": "54384163",
                "h": "tf.data.experimental",
                "t": "tf.contrib.data",
                "sentences":
                [
                    "Note that I am looking at tf.data.experimental, as the tf.contrib.data one is deprecated."
                ]
            }
        ],
        "tf.train.string_input_producer":
        [
            {
                "title": "44551409",
                "h": "tf.train.string_input_producer",
                "t": "tf.contrib.data",
                "sentences":
                [
                    "As Nicolas observes, the tf.train.string_input_producer() API does not give you the ability to detect when the end of an epoch is reached; instead it concatenates together all epochs into one long batch.",
                    "For this reason, we recently added (in TensorFlow 1.2) the tf.contrib.data API, which makes it possible to express more sophisticated pipelines, including your use case."
                ]
            }
        ]
    },
    "tf.gather":
    {
        "tf.gather_nd":
        [
            {
                "title": "48791133",
                "h": "tf.gather",
                "t": "tf.gather_nd",
                "sentences":
                [
                    "Whereas in tf.gather indices defines slices into the first dimension of Tensor, in tf.gather_nd, indices defines slices into the first N dimensions of the Tensor, where N = indices.shape[-1]"
                ]
            },
            {
                "title": "44983945",
                "h": "tf.gather",
                "t": "tf.gather_nd",
                "sentences":
                [
                    "The closest function in tensorflow to np.take are tf.gather and tf.gather_nd.",
                    "tf.gather_nd is more general than tf.gather (and np.take) as it can slices through several dimensions at once."
                ]
            }
        ],
        "tf.boolean_mask":
        [
            {
                "title": "51586823",
                "h": "tf.boolean_mask",
                "t": "tf.gather",
                "sentences":
                [
                    "You can also shorten this a bit and use tf.boolean_mask instead of tf.where and tf.gather:"
                ]
            }
        ]
    },
    "tf.gather_nd":
    {
        "tf.gather":
        [
            {
                "title": "48791133",
                "h": "tf.gather",
                "t": "tf.gather_nd",
                "sentences":
                [
                    "Whereas in tf.gather indices defines slices into the first dimension of Tensor, in tf.gather_nd, indices defines slices into the first N dimensions of the Tensor, where N = indices.shape[-1]"
                ]
            },
            {
                "title": "44983945",
                "h": "tf.gather",
                "t": "tf.gather_nd",
                "sentences":
                [
                    "The closest function in tensorflow to np.take are tf.gather and tf.gather_nd.",
                    "tf.gather_nd is more general than tf.gather (and np.take) as it can slices through several dimensions at once."
                ]
            }
        ]
    },
    "tf.keras.layers":
    {
        "tf.layers":
        [
            {
                "title": "53534547",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras.",
                    "So to best prepare for TF 2.0 you should start using tf.keras.layers instead."
                ]
            },
            {
                "title": "54718798",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "Since TensorFlow 1.12, tf.layers are merely wrappers around tf.keras.layers.",
                    "Convolutional tf.layers just inherit from the convolutional tf.keras.layers, see source code here:",
                    "With the integration of Keras into TensorFlow, it would make little sense to maintain several different layer implementations.",
                    "tf.keras is becoming the de-facto high-level API for TensorFlow, therefore tf.layers are now just wrappers around tf.keras.layers."
                ]
            },
            {
                "title": "58538279",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "So I faced the same error but discovered that my version of tensorflow (which \n is 2.0) moved layers from the tf package (tf.layers) to tf.keras.",
                    "An easy fix would be to replace tf.layers with tf.keras.layers"
                ]
            },
            {
                "title": "51089993",
                "h": "tf.layers",
                "t": "tf.keras.layers",
                "sentences":
                [
                    "tf.layers module is Tensorflow attempt at creating a Keras like API whereas tf.keras.layers is a compatibility wrapper.",
                    "In fact, most of the implementation refers back to tf.layers, for example the tf.keras.layers.Dense inherits the core implementation:",
                    "I would use Keras directly or tf.layers but not necessarily mix them."
                ]
            }
        ]
    },
    "tf.contrib.layers":
    {
        "tf.layers":
        [
            {
                "title": "53534547",
                "h": "tf.contrib.layers",
                "t": "tf.layers",
                "sentences":
                [
                    "The core functionality corresponding to tf.contrib.layers is in tf.layers.",
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras."
                ]
            }
        ],
        "tf.contrib":
        [
            {
                "title": "55518654",
                "h": "tf.contrib",
                "t": "tf.contrib.layers",
                "sentences":
                [
                    "According to an RFC document from August 2018, tf.contrib will be deleted with some of its parts becoming standalone projects (such as tensorflow/probability).",
                    "This not the case of tf.conrib.layers.",
                    "Even tf.layers (which was distilled from tf.contrib.layers) will be no longer supported."
                ]
            }
        ]
    },
    "tf.contrib":
    {
        "tf.keras":
        [
            {
                "title": "53534547",
                "h": "tf.contrib",
                "t": "tf.keras",
                "sentences":
                [
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras."
                ]
            }
        ],
        "tf.contrib.layers":
        [
            {
                "title": "55518654",
                "h": "tf.contrib",
                "t": "tf.contrib.layers",
                "sentences":
                [
                    "According to an RFC document from August 2018, tf.contrib will be deleted with some of its parts becoming standalone projects (such as tensorflow/probability).",
                    "This not the case of tf.conrib.layers.",
                    "Even tf.layers (which was distilled from tf.contrib.layers) will be no longer supported."
                ]
            }
        ]
    },
    "tf.keras":
    {
        "tf.contrib":
        [
            {
                "title": "53534547",
                "h": "tf.contrib",
                "t": "tf.keras",
                "sentences":
                [
                    "If your goal is to prepare your code for TF 2.0, consider that tf.contrib will be removed entirely (either split from TF or integrated into it) and that tf.layers too will be removed and the high-level API will reside under tf.keras."
                ]
            }
        ]
    },
    "tf.image.resize":
    {
        "tf.image.decode_jpeg":
        [
            {
                "title": "44289344",
                "h": "tf.image.decode_jpeg",
                "t": "tf.image.resize",
                "sentences":
                [
                    "tf.image.decode_jpeg returns a tf.uint8 tensor.",
                    "tf.image.resize returns a tf.float32 tensor."
                ]
            }
        ],
        "tf.image.pad_to_bounding_box":
        [
            {
                "title": "57427659",
                "h": "tf.image.resize",
                "t": "tf.image.pad_to_bounding_box",
                "sentences":
                [
                    "If you want to resize, use tf.image.resize:",
                    "If, instead, you want to pad, use tf.image.pad_to_bounding_box (just replace it in the above preprocess function and adapt the parameters as needed)."
                ]
            }
        ],
        "tf.image.resize_with_pad":
        [
            {
                "title": "53894805",
                "h": "tf.image.resize",
                "t": "tf.image.resize_with_pad",
                "sentences":
                [
                    "According to docs tf.image.resize:",
                    "Resize images to size using the specified method.",
                    "Resized images will be distorted if their original aspect ratio is not\n  the same as size.",
                    "To avoid distortions see\n  tf.image.resize_with_pad."
                ]
            }
        ]
    },
    "tf.compute_gradients":
    {
        "tf.gradients":
        [
            {
                "title": "40115530",
                "h": "tf.compute_gradients",
                "t": "tf.gradients",
                "sentences":
                [
                    "tf.compute_gradients computes derivative of a scalar quantity.",
                    "If the quantity provided isn't scalar, it turns it into scalar by summing up the components which is what's happening in your example",
                    "To compute full Hessian you need n calls to tf.gradients, The example is here."
                ]
            }
        ]
    },
    "tf.gradients":
    {
        "tf.compute_gradients":
        [
            {
                "title": "40115530",
                "h": "tf.compute_gradients",
                "t": "tf.gradients",
                "sentences":
                [
                    "tf.compute_gradients computes derivative of a scalar quantity.",
                    "If the quantity provided isn't scalar, it turns it into scalar by summing up the components which is what's happening in your example",
                    "To compute full Hessian you need n calls to tf.gradients, The example is here."
                ]
            }
        ],
        "tf.train.optimizer.compute_gradients":
        [
            {
                "title": "45347628",
                "h": "tf.gradients",
                "t": "tf.train.optimizer.compute_gradients",
                "sentences":
                [
                    "tf.gradients does not allow you to compute Jacobians, it aggregates the gradients of each input for every output (something like the summation of each column of the actual Jacobian matrix).",
                    "With respect to tf.train.Optimizer.compute_gradients, yes, its result is basically the same, but taking care of some details automatically and with slightly more convenient output format.",
                    "If you look at the implementation, you will see that, at its core, is a call to tf.gradients (in this case aliased to gradients.gradients), but it is useful for optimizer implementations to have the surrounding logic already implemented.",
                    "Also, having it as a method allows for extensible behaviour in subclasses, either to implement some kind of optimization strategy (not very likely at the compute_gradients step, really) or for auxiliary purposes, like tracing or debugging."
                ]
            }
        ]
    },
    "tf.reduce_sum":
    {
        "tf.map_fn":
        [
            {
                "title": "41471616",
                "h": "tf.map_fn",
                "t": "tf.reduce_sum",
                "sentences":
                [
                    "The TensorFlow Python API includes the tf.map_fn(fn, elems) higher-order operator, which allows you to specify a (Python) function fn that will be applied to each slice of elems in the 0th dimension (i.e.",
                    "to each row if elems is a matrix).",
                    "Note that, while tf.map_fn() is very general, it may be more efficient to use specialized ops that either broadcast their arguments on one or more dimensions (e.g.",
                    "tf.multiply()), or reduce in parallel across one or more dimensions (e.g.",
                    "However, tf.map_fn() is useful when there is no built-in operator to do what you want."
                ]
            }
        ],
        "tf.multiply":
        [
            {
                "title": "47583685",
                "h": "tf.multiply",
                "t": "tf.reduce_sum",
                "sentences":
                [
                    "What tf.multiply(X, X) does is essentially multiplying each element of the matrix with itself, like",
                    "whereas tf.reduce_sum(_, axis=1) takes a sum of each row, so the result for the previous example will be"
                ]
            }
        ]
    },
    "tf.train.exponentialmovingaverage":
    {
        "tf.train.momentumoptimizer":
        [
            {
                "title": "49716693",
                "h": "tf.train.exponentialmovingaverage",
                "t": "tf.train.momentumoptimizer",
                "sentences":
                [
                    "tf.train.ExponentialMovingAverage implements just the exponential moving average: the shadow_variable is the moving average at the current time step and it's updated using the formula you posted.",
                    "Every time you execute the node that holds the moving average, what happens is just the execution of that formula.",
                    "The tf.train.MomentumOptimizer, instead, is a way more complex object.",
                    "In short, it implements a parameter update algorithm called Gradient Descent with Momentum that computes the gradient of the model parameters and executes the update step of every single network parameter using the computed gradient + the momentum term that's accumulated over the training steps.",
                    "The momentum term is, of course, the moving average of the gradients.",
                    "But the two functions execute different operations and have different aims."
                ]
            }
        ]
    },
    "tf.train.momentumoptimizer":
    {
        "tf.train.exponentialmovingaverage":
        [
            {
                "title": "49716693",
                "h": "tf.train.exponentialmovingaverage",
                "t": "tf.train.momentumoptimizer",
                "sentences":
                [
                    "tf.train.ExponentialMovingAverage implements just the exponential moving average: the shadow_variable is the moving average at the current time step and it's updated using the formula you posted.",
                    "Every time you execute the node that holds the moving average, what happens is just the execution of that formula.",
                    "The tf.train.MomentumOptimizer, instead, is a way more complex object.",
                    "In short, it implements a parameter update algorithm called Gradient Descent with Momentum that computes the gradient of the model parameters and executes the update step of every single network parameter using the computed gradient + the momentum term that's accumulated over the training steps.",
                    "The momentum term is, of course, the moving average of the gradients.",
                    "But the two functions execute different operations and have different aims."
                ]
            }
        ]
    },
    "tf.nn.dynamic_rnn":
    {
        "tf.nn.rnn":
        [
            {
                "title": "36267491",
                "h": "tf.nn.rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "For RNNs specifically, there are two options: tf.nn.rnn, and tf.nn.dynamic_rnn.",
                    "Neither one creates or destroys graphs temporarily.",
                    "The first function creates T subgraphs, where T is the length of the python list of inputs you provide (that is, inputs is a len T python list of shape [batch, depth] tensors).",
                    "tf.nn.rnn always expects a fixed number of time steps.",
                    "Note, you can control which subgraphs are run for a given step by passing the sequence_length parameter; the function then uses conditional evaluation (tf.cond) to determine what operations get run.",
                    "In contrast, dynamic_rnn uses a special TensorFlow while loop and other trickery that introduces a limited type of loop in the graph structure.",
                    "In this case, there is exactly one subgraph for the \"time step\", and it gets run over and over until your input has been processed.",
                    "In this case your input is a 3D tensor with dimensions [batch, time, depth] (it can be [time, batch, depth] if you set time_major=True); and the first two dimensions can vary from step to step."
                ]
            },
            {
                "title": "38753139",
                "h": "tf.nn.rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "The tf.nn.rnn() and tf.nn.dynamic_rnn() functions accept an argument cell of type tf.nn.rnn_cell.RNNCell."
                ]
            }
        ],
        " tf.nn.static_rnn":
        [
            {
                "title": "44479970",
                "h": " tf.nn.static_rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "tf.nn.static_rnn vs. tf.nn.dynamic_rnn.",
                    "Internally, tf.nn.static_rnn creates an unrolled graph for a fixed RNN length.",
                    "That means that, if you call tf.nn.static_rnn with inputs having 200 time-steps you are creating a static graph with 200 RNN steps.",
                    "First, graph creation is slow.",
                    "Second, you’re unable to pass in longer sequences (> 200) than you've originally specified.",
                    "tf.nn.dynamic_rnn solves this.",
                    "It uses a tf.while_loop to dynamically construct the graph when it is executed.",
                    "That means graph creation is faster and you can feed batches of variable size.",
                    "What about performance?.",
                    "You may think the tf.nn.static_rnn is faster than its dynamic counterpart because it pre-builds the graph.",
                    "Please note, it is strongly encouraged to use tf.nn.dynamic_rnn."
                ]
            }
        ]
    },
    "tf.keras.layers.Conv1D":
    {
        "tf.keras.layers.Conv2D":
        [
            {
                "title": "48223162",
                "h": "tf.keras.layers.Conv1D",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.keras.layers.Conv1D is used when you slide your convolution kernels along 1 dimensions (i.e.",
                    "you reuse the same weights, sliding them along 1 dimensions), whereas tf.keras.layers.Conv2D is used when you slide your convolution kernels along 2 dimensions (i.e.",
                    "you reuse the same weights, sliding them along 2 dimensions).",
                    "So the typical use case for tf.keras.layers.Conv2D is if you have a 2D image.",
                    "And possible use-cases for tf.keras.layers.Conv1D are, for example:",
                    "Convolutions in Time.",
                    "Convolutions on Piano notes."
                ]
            }
        ]
    },
    "tf.contrib.metrics.accuracy":
    {
        "tf.metrics.accuracy":
        [
            {
                "title": "48910328",
                "h": "tf.contrib.metrics.accuracy",
                "t": "tf.metrics.accuracy",
                "sentences":
                [
                    "tf.contrib.metrics.accuracy instead of tf.metrics.accuracy.",
                    "But this gives some residual value at the end like 0.800000011920929 instead of 0.8.",
                    "It's also deprecated as pointed out by OP in comments."
                ]
            }
        ]
    },
    "tf.metrics.accuracy":
    {
        "tf.contrib.metrics.accuracy":
        [
            {
                "title": "48910328",
                "h": "tf.contrib.metrics.accuracy",
                "t": "tf.metrics.accuracy",
                "sentences":
                [
                    "tf.contrib.metrics.accuracy instead of tf.metrics.accuracy.",
                    "But this gives some residual value at the end like 0.800000011920929 instead of 0.8.",
                    "It's also deprecated as pointed out by OP in comments."
                ]
            }
        ],
        "tf.metrics.precision":
        [
            {
                "title": "48063821",
                "h": "tf.metrics.precision",
                "t": "tf.metrics.accuracy",
                "sentences":
                [
                    "tf.metrics.precision is meant to be used in binary classification problems only, and its arguments must be all 0 or 1 since, as the docs say, they will be converted to bool.",
                    "However, since your manual calculation uses tf.argmax, it looks more like a multiclass classification problem, in which case you don't usually talk about precision/recall, but just accuracy, so you can look at tf.metrics.accuracy, and pass tf.argmax(self.input_y, 1) and self.predictions as parameters."
                ]
            }
        ],
        "tf.metric.mean_squared_error":
        [
            {
                "title": "47526013",
                "h": "tf.metrics.accuracy",
                "t": "tf.metric.mean_squared_error",
                "sentences":
                [
                    "Turns out, since this is a multi-class Linear Regression problem, and not a classification problem, that tf.metrics.accuracy is not the right approach.",
                    "Instead of displaying the accuracy of my model in terms of percentage, I instead focused on reducing the Mean Square Error (MSE) instead.",
                    "From looking at other examples, tf.metrics.accuracy is never used for Linear Regression, and only classification.",
                    "Normally tf.metric.mean_squared_error is the right approach."
                ]
            }
        ]
    },
    "tf.diag_part":
    {
        "tf.linalg.tensor_diag_part":
        [
            {
                "title": "36744555",
                "h": "tf.diag_part",
                "t": "tf.linalg.tensor_diag_part",
                "sentences":
                [
                    "with tensorflow 0.8 its possible to extract the diagonal elements with tf.diag_part() (see documentation)",
                    "for tensorflow >= r1.12 its tf.linalg.tensor_diag_part (see documentation)"
                ]
            }
        ]
    },
    "tf.linalg.tensor_diag_part":
    {
        "tf.diag_part":
        [
            {
                "title": "36744555",
                "h": "tf.diag_part",
                "t": "tf.linalg.tensor_diag_part",
                "sentences":
                [
                    "with tensorflow 0.8 its possible to extract the diagonal elements with tf.diag_part() (see documentation)",
                    "for tensorflow >= r1.12 its tf.linalg.tensor_diag_part (see documentation)"
                ]
            }
        ]
    },
    "tf.name_scope":
    {
        "tf.variable_scope":
        [
            {
                "title": "39981684",
                "h": "tf.variable_scope",
                "t": "tf.name_scope",
                "sentences":
                [
                    "Just use tf.variable_scope instead of tf.name_scope.",
                    "tf.name_scope doesn't add prefixes to the variables created with tf.get_variable()."
                ]
            },
            {
                "title": "34232533",
                "h": "tf.name_scope",
                "t": "tf.variable_scope",
                "sentences":
                [
                    "If you created a var with tf.get_variable and you try to change the prefix of your variable names by using the tf.name_scope context manager, this won't prevent the Tensorflow of raising an exception.",
                    "Only tf.variable_scope context manager will effectively change the name of your var in this case.",
                    "In summary, tf.name_scope just add a prefix to all tensor created in that scope (except the vars created with tf.get_variable), and tf.variable_scope add a prefix to the variables created with tf.get_variable."
                ]
            },
            {
                "title": "51732374",
                "h": "tf.variable_scope",
                "t": "tf.name_scope",
                "sentences":
                [
                    "You can try using tf.variable_scope instead.",
                    "tf.name_scope is ignored by variables created via tf.get_variable() which is usually used by tf.layers functions."
                ]
            }
        ]
    },
    "tf.contrib.layers.xavier_initializer":
    {
        "tf.glorot_uniform_initializer":
        [
            {
                "title": "47987212",
                "h": "tf.contrib.layers.xavier_initializer",
                "t": "tf.glorot_uniform_initializer",
                "sentences":
                [
                    "Yes, tf.contrib.layers.xavier_initializer and tf.glorot_uniform_initializer both implement the same concept described in this JMLR paper: Understanding the difficulty of training deep feedforward neural networks, which can be seen in the code:",
                    "With typical values for fan_in, fan_out, mode = FAN_AVG , and uniform = True, both implementations sample values from the standard uniform distribution over the limit [-sqrt(3), sqrt(3))",
                    "Because tf.initializer has support for a wide variety of initialization strategies, it's highly likely that it will stay whereas the initialization from contrib which just has xavier_initialization will most probably be deprecated in future versions.",
                    "So, yes it's highly likely that in future versions the tf.contrib.layers.xavier_initialier way of initialization might go away."
                ]
            }
        ]
    },
    "tf.glorot_uniform_initializer":
    {
        "tf.contrib.layers.xavier_initializer":
        [
            {
                "title": "47987212",
                "h": "tf.contrib.layers.xavier_initializer",
                "t": "tf.glorot_uniform_initializer",
                "sentences":
                [
                    "Yes, tf.contrib.layers.xavier_initializer and tf.glorot_uniform_initializer both implement the same concept described in this JMLR paper: Understanding the difficulty of training deep feedforward neural networks, which can be seen in the code:",
                    "With typical values for fan_in, fan_out, mode = FAN_AVG , and uniform = True, both implementations sample values from the standard uniform distribution over the limit [-sqrt(3), sqrt(3))",
                    "Because tf.initializer has support for a wide variety of initialization strategies, it's highly likely that it will stay whereas the initialization from contrib which just has xavier_initialization will most probably be deprecated in future versions.",
                    "So, yes it's highly likely that in future versions the tf.contrib.layers.xavier_initialier way of initialization might go away."
                ]
            }
        ]
    },
    "tf.nn.batch_norm":
    {
        "tf.contrib.layers.batch_norm":
        [
            {
                "title": "42144422",
                "h": "tf.contrib.layers.batch_norm",
                "t": "tf.nn.batch_norm",
                "sentences":
                [
                    "tf.contrib refers to a high level machine learning API over Tensorflow.",
                    "So tf.contrib.layers.batch_norm basically acts as a wrapper over tf.nn.batch_normalization making it easier to use but less flexible than the latter.",
                    "So, depending upon the application you can use whatever fits your needs.",
                    "For example in case of tf.nn.batch_norm you will have to give a symbolic link to mean and variance tensors by yourself, but tf.contrib.layers.batch_norm will take care of that for you."
                ]
            }
        ]
    },
    "tf.sigmoid":
    {
        "tf.nn.softmax":
        [
            {
                "title": "50090357",
                "h": "tf.nn.softmax",
                "t": "tf.sigmoid",
                "sentences":
                [
                    "tf.nn.softmax() computes probability distribution over classes (output neurons), if you have just 1 output neuron then probability distribution over 1 neuron will always be 1.0.",
                    "I would suggest to use tf.sigmoid() in combination with tf.greater(), e.g:",
                    "<code>Code Snippet</code>."
                ]
            }
        ]
    },
    "tf.metrics.precision":
    {
        "tf.metrics.precision_at_thresholds":
        [
            {
                "title": "48063821",
                "h": "tf.metrics.precision",
                "t": "tf.metrics.precision_at_thresholds",
                "sentences":
                [
                    "tf.metrics.precision is meant to be used in binary classification problems only, and its arguments must be all 0 or 1 since, as the docs say, they will be converted to bool.",
                    "If you are indeed working in a binary classification problem but want to use the logits as parameter, you can look at tf.metrics.precision_at_thresholds, which allows you to specify the threshold at which a prediction will be considered true."
                ]
            }
        ],
        "tf.metrics.accuracy":
        [
            {
                "title": "48063821",
                "h": "tf.metrics.precision",
                "t": "tf.metrics.accuracy",
                "sentences":
                [
                    "tf.metrics.precision is meant to be used in binary classification problems only, and its arguments must be all 0 or 1 since, as the docs say, they will be converted to bool.",
                    "However, since your manual calculation uses tf.argmax, it looks more like a multiclass classification problem, in which case you don't usually talk about precision/recall, but just accuracy, so you can look at tf.metrics.accuracy, and pass tf.argmax(self.input_y, 1) and self.predictions as parameters."
                ]
            }
        ]
    },
    "tf.metrics.precision_at_thresholds":
    {
        "tf.metrics.precision":
        [
            {
                "title": "48063821",
                "h": "tf.metrics.precision",
                "t": "tf.metrics.precision_at_thresholds",
                "sentences":
                [
                    "tf.metrics.precision is meant to be used in binary classification problems only, and its arguments must be all 0 or 1 since, as the docs say, they will be converted to bool.",
                    "If you are indeed working in a binary classification problem but want to use the logits as parameter, you can look at tf.metrics.precision_at_thresholds, which allows you to specify the threshold at which a prediction will be considered true."
                ]
            }
        ]
    },
    "tf.image.pad_to_bounding_box":
    {
        "tf.image.resize":
        [
            {
                "title": "57427659",
                "h": "tf.image.resize",
                "t": "tf.image.pad_to_bounding_box",
                "sentences":
                [
                    "If you want to resize, use tf.image.resize:",
                    "If, instead, you want to pad, use tf.image.pad_to_bounding_box (just replace it in the above preprocess function and adapt the parameters as needed)."
                ]
            }
        ]
    },
    "tf.add":
    {
        "tf.add_n":
        [
            {
                "title": "34696282",
                "h": "tf.add",
                "t": "tf.add_n",
                "sentences":
                [
                    "String concatenation is implemented using the existing tf.add() operator, to match the behavior of NumPy's add operator (including broadcasting).",
                    "We have not yet added support for strings in tf.add_n() (or related ops like tf.reduce_sum()) but will consider this if there are use cases for it."
                ]
            }
        ],
        "tf.nn.bias_add":
        [
            {
                "title": "43135048",
                "h": "tf.add",
                "t": "tf.nn.bias_add",
                "sentences":
                [
                    "Unlike tf.add, the type of bias is allowed to differ from value in the case where both types are quantized.",
                    "tf.add is a general addition operation, while tf.nn.bias_add is to be used specifically for adding bias to the weights, which raises an exception if the dtypes aren't same."
                ]
            }
        ]
    },
    "tf.add_n":
    {
        "tf.add":
        [
            {
                "title": "34696282",
                "h": "tf.add",
                "t": "tf.add_n",
                "sentences":
                [
                    "String concatenation is implemented using the existing tf.add() operator, to match the behavior of NumPy's add operator (including broadcasting).",
                    "We have not yet added support for strings in tf.add_n() (or related ops like tf.reduce_sum()) but will consider this if there are use cases for it."
                ]
            }
        ]
    },
    "tf.shape":
    {
        "tf.constant":
        [
            {
                "title": "35855219",
                "h": "tf.constant",
                "t": "tf.shape",
                "sentences":
                [
                    "A tf.constant() has fixed size and value at graph construction time, so it probably isn't the right op for your application.",
                    "If you are trying to create a tensor with a dynamic size and the same (constant) value for every element, you can use tf.fill() and tf.shape() to create an appropriately-shaped tensor."
                ]
            }
        ]
    },
    "tf.constant_initializer":
    {
        "tf.get_variable":
        [
            {
                "title": "38820162",
                "h": "tf.constant_initializer",
                "t": "tf.get_variable",
                "sentences":
                [
                    "The tf.constant_initializer() function might not accept a tf.Tensor as an argument, but tf.get_variable() does accept a tf.Tensor as its initializer argument.",
                    "The reason tf.constant_initializer() doesn't take an arbitrary tensor is that it is designed to initialize variables of many different shapes with the same constant value for each element."
                ]
            }
        ],
        "tf.fill":
        [
            {
                "title": "38820162",
                "h": "tf.constant_initializer",
                "t": "tf.fill",
                "sentences":
                [
                    "Arguably we could make tf.constant_initializer() accept a scalar tf.Tensor, and then it would have semantics similar to tf.fill(), but we haven't had any demand for that yet."
                ]
            }
        ],
        "tf.global_variables_initializer":
        [
            {
                "title": "57441807",
                "h": "tf.global_variables_initializer",
                "t": "tf.constant_initializer",
                "sentences":
                [
                    "As for the distinction between tf.global_variables_initializer and tf.constant_initializer, they are something completely different:",
                    "tf.global_variables_initializer is an operation that you execute to initialize all variables in your graph.",
                    "It doesn't matter whether the variables will get initialized with tf.constant_initializer, tf.random_normal_initializer or tf.glorot_uniform_initializer.",
                    "You just pass that operation to a tf.Session so that the graph variables will get initialized.",
                    "tf.constant_initializer on the other hand, is just an initializer that you pass to the tf.Variable of your graph.",
                    "Then, when a tf.Session runs the operation tf.global_variables_initializer, the TF Graph will use the tf.constant_initializer to initialize the corresponding tf.Variable with the constant values provided."
                ]
            }
        ]
    },
    "tf.fill":
    {
        "tf.constant_initializer":
        [
            {
                "title": "38820162",
                "h": "tf.constant_initializer",
                "t": "tf.fill",
                "sentences":
                [
                    "Arguably we could make tf.constant_initializer() accept a scalar tf.Tensor, and then it would have semantics similar to tf.fill(), but we haven't had any demand for that yet."
                ]
            }
        ],
        "tf.constant":
        [
            {
                "title": "37380546",
                "h": "tf.constant",
                "t": "tf.fill",
                "sentences":
                [
                    "Note that tf.constant() does not accept a dynamic shape as an argument—for then it would not be constant!—but the similar tf.fill() op does."
                ]
            },
            {
                "title": "35855219",
                "h": "tf.constant",
                "t": "tf.fill",
                "sentences":
                [
                    "A tf.constant() has fixed size and value at graph construction time, so it probably isn't the right op for your application.",
                    "If you are trying to create a tensor with a dynamic size and the same (constant) value for every element, you can use tf.fill() and tf.shape() to create an appropriately-shaped tensor."
                ]
            }
        ]
    },
    "tf.nn.l2_loss":
    {
        "tf.losses.mean_squared_error":
        [
            {
                "title": "44522505",
                "h": "tf.losses.mean_squared_error",
                "t": "tf.nn.l2_loss",
                "sentences":
                [
                    "And notice that your error argument to tf.losses.mean_squared_error is a scope name, not the name of the returned operation.",
                    "This can be confusing, as other operations, such as tf.nn.l2_loss, accept a name argument."
                ]
            }
        ]
    },
    " tf.keras.layers.dense":
    {
        "tf.layers.dense":
        [
            {
                "title": "55431562",
                "h": " tf.keras.layers.dense",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "Using tf.keras.layers.Dense() is actually a preferred way in newest tensorflow version 1.13 (tf.layers.dense() is deprectated)."
                ]
            }
        ]
    },
    "tf.estimator.inputs.pandas_input_fn":
    {
        "tf.compat.v1.estimator.inputs.pandas_input_fn":
        [
            {
                "title": "62889525",
                "h": "tf.estimator.inputs.pandas_input_fn",
                "t": "tf.compat.v1.estimator.inputs.pandas_input_fn",
                "sentences":
                [
                    "In Tensorflow V1 While reading input from Pandas DataFrames for tf.estimator we used this command: tf.estimator.inputs.pandas_input_fn.",
                    "But now due to API changes, we will have to replace that command with this one: tf.compat.v1.estimator.inputs.pandas_input_fn."
                ]
            }
        ]
    },
    "tf.compat.v1.estimator.inputs.pandas_input_fn":
    {
        "tf.estimator.inputs.pandas_input_fn":
        [
            {
                "title": "62889525",
                "h": "tf.estimator.inputs.pandas_input_fn",
                "t": "tf.compat.v1.estimator.inputs.pandas_input_fn",
                "sentences":
                [
                    "In Tensorflow V1 While reading input from Pandas DataFrames for tf.estimator we used this command: tf.estimator.inputs.pandas_input_fn.",
                    "But now due to API changes, we will have to replace that command with this one: tf.compat.v1.estimator.inputs.pandas_input_fn."
                ]
            }
        ]
    },
    "tf.maximum":
    {
        "tf.nn.relu":
        [
            {
                "title": "54223597",
                "h": "tf.nn.relu",
                "t": "tf.maximum",
                "sentences":
                [
                    "they are basically the same.",
                    "Both take a tensor as input and return a tensor.",
                    "The only difference is the supported types.",
                    "The tf.nn.relu supports the following types:",
                    "float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8.",
                    "while  tf.maximum supports a subset of the above types:",
                    "half, float32, float64, int32, int64."
                ]
            }
        ]
    },
    "tf.image.resize_with_pad":
    {
        "tf.image.resize":
        [
            {
                "title": "53894805",
                "h": "tf.image.resize",
                "t": "tf.image.resize_with_pad",
                "sentences":
                [
                    "According to docs tf.image.resize:",
                    "Resize images to size using the specified method.",
                    "Resized images will be distorted if their original aspect ratio is not\n  the same as size.",
                    "To avoid distortions see\n  tf.image.resize_with_pad."
                ]
            }
        ]
    },
    "tf.test.is_gpu_available":
    {
        "tf.config.list_physical_devices":
        [
            {
                "title": "61724959",
                "h": "tf.test.is_gpu_available",
                "t": "tf.config.list_physical_devices",
                "sentences":
                [
                    "Note: tf.test.is_gpu_available is deprecated.",
                    "It will be removed in a future version.",
                    "Instructions for updating: Use tf.config.list_physical_devices('GPU') instead."
                ]
            }
        ]
    },
    "tf.config.list_physical_devices":
    {
        "tf.test.is_gpu_available":
        [
            {
                "title": "61724959",
                "h": "tf.test.is_gpu_available",
                "t": "tf.config.list_physical_devices",
                "sentences":
                [
                    "Note: tf.test.is_gpu_available is deprecated.",
                    "It will be removed in a future version.",
                    "Instructions for updating: Use tf.config.list_physical_devices('GPU') instead."
                ]
            }
        ]
    },
    "tf.tensordot":
    {
        "tf.einsum":
        [
            {
                "title": "43104527",
                "h": "tf.tensordot",
                "t": "tf.einsum",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation."
                ]
            }
        ],
        "tf.linalg.matmul":
        [
            {
                "title": "43104527",
                "h": "tf.tensordot",
                "t": "tf.linalg.matmul",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation.",
                    "However, for smaller matrices it may be more efficient to use tf.linalg.matmul() directly, because it would yield a simpler TensorFlow graph with fewer operations, and hence the per-operation invocation costs will be lower."
                ]
            }
        ]
    },
    "tf.einsum":
    {
        "tf.tensordot":
        [
            {
                "title": "43104527",
                "h": "tf.tensordot",
                "t": "tf.einsum",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation."
                ]
            }
        ],
        "tf.linalg.matmul":
        [
            {
                "title": "43104527",
                "h": "tf.einsum",
                "t": "tf.linalg.matmul",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation.",
                    "However, for smaller matrices it may be more efficient to use tf.linalg.matmul() directly, because it would yield a simpler TensorFlow graph with fewer operations, and hence the per-operation invocation costs will be lower."
                ]
            }
        ]
    },
    "tf.linalg.matmul":
    {
        "tf.tensordot":
        [
            {
                "title": "43104527",
                "h": "tf.tensordot",
                "t": "tf.linalg.matmul",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation.",
                    "However, for smaller matrices it may be more efficient to use tf.linalg.matmul() directly, because it would yield a simpler TensorFlow graph with fewer operations, and hence the per-operation invocation costs will be lower."
                ]
            }
        ],
        "tf.einsum":
        [
            {
                "title": "43104527",
                "h": "tf.einsum",
                "t": "tf.linalg.matmul",
                "sentences":
                [
                    "Both tf.tensordot() and tf.einsum() are syntactic sugar that wrap one or more invocations of tf.linalg.matmul() (although in some special cases tf.einsum() can reduce to the simpler elementwise tf.multiply()).",
                    "In the limit, I'd expect all three functions to have equivalent performance for the same computation.",
                    "However, for smaller matrices it may be more efficient to use tf.linalg.matmul() directly, because it would yield a simpler TensorFlow graph with fewer operations, and hence the per-operation invocation costs will be lower."
                ]
            }
        ]
    },
    "tf.fixedlenfeature":
    {
        "tf.io.fixedlenfeature":
        [
            {
                "title": "61071698",
                "h": "tf.fixedlenfeature",
                "t": "tf.io.fixedlenfeature",
                "sentences":
                [
                    "If you are using TensorFlow 2.x, FixedLenFeature has moved from tf.FixedLenFeature into tf.io.FixedLenFeature.",
                    "The use of tf.FixedLenFeature was marked as deprecated in previous versions."
                ]
            }
        ]
    },
    "tf.io.fixedlenfeature":
    {
        "tf.fixedlenfeature":
        [
            {
                "title": "61071698",
                "h": "tf.fixedlenfeature",
                "t": "tf.io.fixedlenfeature",
                "sentences":
                [
                    "If you are using TensorFlow 2.x, FixedLenFeature has moved from tf.FixedLenFeature into tf.io.FixedLenFeature.",
                    "The use of tf.FixedLenFeature was marked as deprecated in previous versions."
                ]
            }
        ]
    },
    "tf.realdiv":
    {
        "tf.truncatediv":
        [
            {
                "title": "50592353",
                "h": "tf.realdiv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments.",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ],
        "tf.math.truediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.realdiv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments."
                ]
            }
        ],
        "tf.math.floordiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.floordiv",
                "t": "tf.realdiv",
                "sentences":
                [
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:",
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments."
                ]
            }
        ]
    },
    "tf.truncatediv":
    {
        "tf.realdiv":
        [
            {
                "title": "50592353",
                "h": "tf.realdiv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments.",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ],
        "tf.math.truediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ],
        "tf.math.floordiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.floordiv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ]
    },
    "tf.math.divide":
    {
        "tf.math.floordiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.divide",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "tf.math.divide(x,y) essentially calls x/y, therefore the result depends on the behavior of the / operator in the environment in which the division is performed.",
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:"
                ]
            }
        ],
        "tf.math.truediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.math.divide",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.math.divide(x,y) essentially calls x/y, therefore the result depends on the behavior of the / operator in the environment in which the division is performed."
                ]
            }
        ]
    },
    "tf.math.truediv":
    {
        "tf.truncatediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ],
        "tf.realdiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.realdiv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments."
                ]
            }
        ],
        "tf.math.divide":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.math.divide",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.math.divide(x,y) essentially calls x/y, therefore the result depends on the behavior of the / operator in the environment in which the division is performed."
                ]
            }
        ],
        "tf.math.floordiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:"
                ]
            }
        ]
    },
    "tf.math.floordiv":
    {
        "tf.math.divide":
        [
            {
                "title": "50592353",
                "h": "tf.math.divide",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "tf.math.divide(x,y) essentially calls x/y, therefore the result depends on the behavior of the / operator in the environment in which the division is performed.",
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:"
                ]
            }
        ],
        "tf.math.truediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.truediv",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "tf.math.truediv - enforces python v3 division semantics, e.g.",
                    "if both arguments are integers they are first cast into float type (the documentation web page specifies which type of integer is converted to which type of float) and then the normal floating point division is applied:",
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:"
                ]
            }
        ],
        "tf.truncatediv":
        [
            {
                "title": "50592353",
                "h": "tf.math.floordiv",
                "t": "tf.truncatediv",
                "sentences":
                [
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:",
                    "tf.truncatediv - rounds the result of division towards zero:"
                ]
            }
        ],
        "tf.realdiv":
        [
            {
                "title": "50592353",
                "h": "tf.math.floordiv",
                "t": "tf.realdiv",
                "sentences":
                [
                    "tf.math.floordiv and tf.floor_div return the same result as tf.div if both arguments are integers and tf.floor(tf.div(x,y)) if both arguments are floating point numbers:",
                    "tf.realdiv - normal floating point division, this is the operation that tf.math.truediv calls after it casts its arguments."
                ]
            }
        ]
    },
    "tf.image.random_flip_left_right":
    {
        "tf.image.flip_left_right":
        [
            {
                "title": "49837908",
                "h": "tf.image.random_flip_left_right",
                "t": "tf.image.flip_left_right",
                "sentences":
                [
                    "If you want to augment you data randomly, use tf.image.random_flip_left_right rather than tf.image.flip_left_right.."
                ]
            }
        ]
    },
    "tf.image.flip_left_right":
    {
        "tf.image.random_flip_left_right":
        [
            {
                "title": "49837908",
                "h": "tf.image.random_flip_left_right",
                "t": "tf.image.flip_left_right",
                "sentences":
                [
                    "If you want to augment you data randomly, use tf.image.random_flip_left_right rather than tf.image.flip_left_right.."
                ]
            }
        ]
    },
    "tf.train.supervisor":
    {
        "tf.session":
        [
            {
                "title": "43722074",
                "h": "tf.train.supervisor",
                "t": "tf.session",
                "sentences":
                [
                    "When you use tf.train.Supervisor, the framework code automatically calls tf.train.start_queue_runners(sess) (along with initializing variables) at the beginning of the session.",
                    "If you switch back to using a raw tf.Session, you must call this manually to start the input pipeline."
                ]
            }
        ]
    },
    "tf.session":
    {
        "tf.train.supervisor":
        [
            {
                "title": "43722074",
                "h": "tf.train.supervisor",
                "t": "tf.session",
                "sentences":
                [
                    "When you use tf.train.Supervisor, the framework code automatically calls tf.train.start_queue_runners(sess) (along with initializing variables) at the beginning of the session.",
                    "If you switch back to using a raw tf.Session, you must call this manually to start the input pipeline."
                ]
            }
        ],
        "tf.compat.v1.session":
        [
            {
                "title": "59390988",
                "h": "tf.compat.v1.session",
                "t": "tf.session",
                "sentences":
                [
                    "you need to use tf.compat.v1.Session() as tf.session is deprecated."
                ]
            },
            {
                "title": "61896836",
                "h": "tf.compat.v1.session",
                "t": "tf.session",
                "sentences":
                [
                    "Eager Execution is enabled by default in TF 2.0, so you need to call .numpy() on the Tensor object.",
                    "If Eager Execution is disabled, you can build a graph and then run it through tf.compat.v1.Session in TF 2.x and tf.Session in TF 1.x"
                ]
            }
        ],
        "tf.interactivesession":
        [
            {
                "title": "33633839",
                "h": "tf.session",
                "t": "tf.interactivesession",
                "sentences":
                [
                    "The easiest[A] way to evaluate the actual value of a Tensor object is to pass it to the Session.run() method, or call Tensor.eval() when you have a default session (i.e.",
                    "in a with tf.Session(): block, or see below).",
                    "In general[B], you cannot print the value of a tensor without running some code in a session.",
                    "If you are experimenting with the programming model, and want an easy way to evaluate tensors, the tf.InteractiveSession lets you open a session at the start of your program, and then use that session for all Tensor.eval() (and Operation.run()) calls.",
                    "This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a Session object everywhere."
                ]
            }
        ]
    },
    "tf.import_graph_def":
    {
        "tf.train.import_meta_graph":
        [
            {
                "title": "50229276",
                "h": "tf.import_graph_def",
                "t": "tf.train.import_meta_graph",
                "sentences":
                [
                    "The tool import_pb_to_tensorboard.py uses tf.import_graph_def to import the graph and uses default name argument, which is \"import\" as documented.",
                    "Your code imports the graph through tf.train.import_meta_graph and uses default import_scope argument, which will not prefix imported tensor or operation name."
                ]
            },
            {
                "title": "36412802",
                "h": "tf.train.import_meta_graph",
                "t": "tf.import_graph_def",
                "sentences":
                [
                    "In general, when importing a graph for retraining, the tf.train.import_meta_graph() function should be used, as this function loads additional metadata (including the collections of variables).",
                    "The tf.import_graph_def() function is lower level, and does not populate these collections."
                ]
            }
        ]
    },
    "tf.train.import_meta_graph":
    {
        "tf.import_graph_def":
        [
            {
                "title": "50229276",
                "h": "tf.import_graph_def",
                "t": "tf.train.import_meta_graph",
                "sentences":
                [
                    "The tool import_pb_to_tensorboard.py uses tf.import_graph_def to import the graph and uses default name argument, which is \"import\" as documented.",
                    "Your code imports the graph through tf.train.import_meta_graph and uses default import_scope argument, which will not prefix imported tensor or operation name."
                ]
            },
            {
                "title": "36412802",
                "h": "tf.train.import_meta_graph",
                "t": "tf.import_graph_def",
                "sentences":
                [
                    "In general, when importing a graph for retraining, the tf.train.import_meta_graph() function should be used, as this function loads additional metadata (including the collections of variables).",
                    "The tf.import_graph_def() function is lower level, and does not populate these collections."
                ]
            }
        ]
    },
    "tf.where":
    {
        "tf.case":
        [
            {
                "title": "43095070",
                "h": "tf.where",
                "t": "tf.case",
                "sentences":
                [
                    "The only operation I'm aware of that evaluates a condition separately on each element of a vector is tf.where.",
                    "However, this only evaluates the truth of a single condition.",
                    "If you want to evaluate the truth of multiple conditions, over each element of a vector, I think you'll have to use tf.map_fn combined with tf.case.",
                    "AFAIK, tf.case is the only operation that evaluates the truth of many conditions on a given value:"
                ]
            }
        ],
        "tf.boolean_mask":
        [
            {
                "title": "51586823",
                "h": "tf.boolean_mask",
                "t": "tf.where",
                "sentences":
                [
                    "You can also shorten this a bit and use tf.boolean_mask instead of tf.where and tf.gather:"
                ]
            }
        ]
    },
    "tf.case":
    {
        "tf.where":
        [
            {
                "title": "43095070",
                "h": "tf.where",
                "t": "tf.case",
                "sentences":
                [
                    "The only operation I'm aware of that evaluates a condition separately on each element of a vector is tf.where.",
                    "However, this only evaluates the truth of a single condition.",
                    "If you want to evaluate the truth of multiple conditions, over each element of a vector, I think you'll have to use tf.map_fn combined with tf.case.",
                    "AFAIK, tf.case is the only operation that evaluates the truth of many conditions on a given value:"
                ]
            }
        ]
    },
    "tf.keras.backend.clear_session":
    {
        "tf.reset_default_graph":
        [
            {
                "title": "57732427",
                "h": "tf.keras.backend.clear_session",
                "t": "tf.reset_default_graph",
                "sentences":
                [
                    "If you use the tf.keras.backend.clear_session it will discard the values resides in the variable defined in the graph, leaving an empty vessel.",
                    "(It will free up your RAM space.",
                    "), now you can load weights from some other files.",
                    "If you use the tf.reset_default_graph() it will reset the graph and it will remove all the defined operations and their inter-connection with corresponding weights.",
                    "Now you have to load both models architecture and weights for the execution.",
                    "practically it seems it is doing same stuff cause it is tf.reset_default_graph() will be called internally while calling k.clear_session() but clear_session will also intiate the fresh graph for the new operation you can check the source code here"
                ]
            }
        ]
    },
    "tf.reset_default_graph":
    {
        "tf.keras.backend.clear_session":
        [
            {
                "title": "57732427",
                "h": "tf.keras.backend.clear_session",
                "t": "tf.reset_default_graph",
                "sentences":
                [
                    "If you use the tf.keras.backend.clear_session it will discard the values resides in the variable defined in the graph, leaving an empty vessel.",
                    "(It will free up your RAM space.",
                    "), now you can load weights from some other files.",
                    "If you use the tf.reset_default_graph() it will reset the graph and it will remove all the defined operations and their inter-connection with corresponding weights.",
                    "Now you have to load both models architecture and weights for the execution.",
                    "practically it seems it is doing same stuff cause it is tf.reset_default_graph() will be called internally while calling k.clear_session() but clear_session will also intiate the fresh graph for the new operation you can check the source code here"
                ]
            }
        ],
        "tf.compat.v1.reset_default_graph":
        [
            {
                "title": "58428014",
                "h": "tf.reset_default_graph",
                "t": "tf.compat.v1.reset_default_graph",
                "sentences":
                [
                    "Defining a new model is not enough to reset the incrementing index, because all models end up on the same underlying graph.",
                    "To reset the index, you must reset the underlying graph.",
                    "In TF 1.X, this is done via tf.reset_default_graph().",
                    "In TF 2.0, you can do this via the v1 compatibility API: tf.compat.v1.reset_default_graph() (the latter will also solve some deprecation warnings you might get with the latest versions of TF 1.X)"
                ]
            }
        ]
    },
    "tf.train.string_input_producer":
    {
        "tf.contrib.data":
        [
            {
                "title": "44551409",
                "h": "tf.train.string_input_producer",
                "t": "tf.contrib.data",
                "sentences":
                [
                    "As Nicolas observes, the tf.train.string_input_producer() API does not give you the ability to detect when the end of an epoch is reached; instead it concatenates together all epochs into one long batch.",
                    "For this reason, we recently added (in TensorFlow 1.2) the tf.contrib.data API, which makes it possible to express more sophisticated pipelines, including your use case."
                ]
            }
        ],
        "tf.train.batch":
        [
            {
                "title": "38726143",
                "h": "tf.train.string_input_producer",
                "t": "tf.train.batch",
                "sentences":
                [
                    "The tf.train.string_input_producer creates a shuffled queue with the given keys.",
                    "The tf.train.batch op create another queue that contains batches of data."
                ]
            }
        ],
        "tf.py_func":
        [
            {
                "title": "38726143",
                "h": "tf.py_func",
                "t": "tf.train.string_input_producer",
                "sentences":
                [
                    "The tf.py_func op inserts a call to regular python code inside of the TensorFlow graph, we need to specify the inputs and the number and types of the outputs.",
                    "The tf.train.string_input_producer creates a shuffled queue with the given keys."
                ]
            }
        ]
    },
    "tf.random.set_seed":
    {
        "tf.compat.v2.random.set_seed":
        [
            {
                "title": "60088810",
                "h": "tf.random.set_seed",
                "t": "tf.compat.v2.random.set_seed",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: For Tensorflow version greater than 2.0, if we want to set the Global Random Seed, the Command used is tf.random.set_seed.",
                    "If we are migrating from Tensorflow Version 1.x to 2.x, we can use the command, \ntf.compat.v2.random.set_seed."
                ]
            }
        ],
        "tf.random.uniform":
        [
            {
                "title": "60088810",
                "h": "tf.random.set_seed",
                "t": "tf.random.uniform",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: For Tensorflow version greater than 2.0, if we want to set the Global Random Seed, the Command used is tf.random.set_seed.",
                    "To set the Operation Level Seed (as answered above), we can use the command, tf.random.uniform([1], seed=1)."
                ]
            }
        ],
        "tf.set_random_seed":
        [
            {
                "title": "57043917",
                "h": "tf.set_random_seed",
                "t": "tf.random.set_seed",
                "sentences":
                [
                    "In TensorFlow 2.0 tf.set_random_seed(42) has changed to tf.random.set_seed(42)."
                ]
            }
        ]
    },
    "tf.compat.v2.random.set_seed":
    {
        "tf.random.set_seed":
        [
            {
                "title": "60088810",
                "h": "tf.random.set_seed",
                "t": "tf.compat.v2.random.set_seed",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: For Tensorflow version greater than 2.0, if we want to set the Global Random Seed, the Command used is tf.random.set_seed.",
                    "If we are migrating from Tensorflow Version 1.x to 2.x, we can use the command, \ntf.compat.v2.random.set_seed."
                ]
            }
        ]
    },
    "tf.random.uniform":
    {
        "tf.random.set_seed":
        [
            {
                "title": "60088810",
                "h": "tf.random.set_seed",
                "t": "tf.random.uniform",
                "sentences":
                [
                    "Tensorflow 2.0 Compatible Answer: For Tensorflow version greater than 2.0, if we want to set the Global Random Seed, the Command used is tf.random.set_seed.",
                    "To set the Operation Level Seed (as answered above), we can use the command, tf.random.uniform([1], seed=1)."
                ]
            }
        ]
    },
    "tf.contrib.seq2seq.greedyembeddinghelper":
    {
        "tf.contrib.seq2seq.sampleembeddinghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.greedyembeddinghelper",
                "t": "tf.contrib.seq2seq.sampleembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.GreedyEmbeddingHelper discards the inputs, and returns the argmax sampled token from the previous output.",
                    "NMT uses this helper in inference when sampling_temperature hyper-parameter is 0..",
                    "tf.contrib.seq2seq.SampleEmbeddingHelper does the same, but samples the token according to categorical (a.k.a.",
                    "generalized Bernoulli) distribution.",
                    "NMT uses this helper in inference when sampling_temperature > 0..",
                    "Note that both GreedyEmbeddingHelper and SampleEmbeddingHelper don't care what the decoder input is.",
                    "So in fact you can feed anything, but the zero tensor is the standard choice."
                ]
            }
        ],
        "tf.contrib.seq2seq.traininghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.traininghelper",
                "t": "tf.contrib.seq2seq.greedyembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.TrainingHelper is returning the next decoder input (which is usually ground truth).",
                    "This helper is used in training as indicated in the tutorial..",
                    "tf.contrib.seq2seq.GreedyEmbeddingHelper discards the inputs, and returns the argmax sampled token from the previous output.",
                    "NMT uses this helper in inference when sampling_temperature hyper-parameter is 0.."
                ]
            }
        ]
    },
    "tf.contrib.seq2seq.sampleembeddinghelper":
    {
        "tf.contrib.seq2seq.greedyembeddinghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.greedyembeddinghelper",
                "t": "tf.contrib.seq2seq.sampleembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.GreedyEmbeddingHelper discards the inputs, and returns the argmax sampled token from the previous output.",
                    "NMT uses this helper in inference when sampling_temperature hyper-parameter is 0..",
                    "tf.contrib.seq2seq.SampleEmbeddingHelper does the same, but samples the token according to categorical (a.k.a.",
                    "generalized Bernoulli) distribution.",
                    "NMT uses this helper in inference when sampling_temperature > 0..",
                    "Note that both GreedyEmbeddingHelper and SampleEmbeddingHelper don't care what the decoder input is.",
                    "So in fact you can feed anything, but the zero tensor is the standard choice."
                ]
            }
        ],
        "tf.contrib.seq2seq.traininghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.traininghelper",
                "t": "tf.contrib.seq2seq.sampleembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.TrainingHelper is returning the next decoder input (which is usually ground truth).",
                    "This helper is used in training as indicated in the tutorial..",
                    "tf.contrib.seq2seq.SampleEmbeddingHelper does the same, but samples the token according to categorical (a.k.a.",
                    "generalized Bernoulli) distribution.",
                    "NMT uses this helper in inference when sampling_temperature > 0.."
                ]
            }
        ]
    },
    "tf.contrib.seq2seq.traininghelper":
    {
        "tf.contrib.seq2seq.greedyembeddinghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.traininghelper",
                "t": "tf.contrib.seq2seq.greedyembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.TrainingHelper is returning the next decoder input (which is usually ground truth).",
                    "This helper is used in training as indicated in the tutorial..",
                    "tf.contrib.seq2seq.GreedyEmbeddingHelper discards the inputs, and returns the argmax sampled token from the previous output.",
                    "NMT uses this helper in inference when sampling_temperature hyper-parameter is 0.."
                ]
            }
        ],
        "tf.contrib.seq2seq.sampleembeddinghelper":
        [
            {
                "title": "48589833",
                "h": "tf.contrib.seq2seq.traininghelper",
                "t": "tf.contrib.seq2seq.sampleembeddinghelper",
                "sentences":
                [
                    "Internal details: NMT uses the entity called Helper to determine the next input in the decoder (see tf.contrib.seq2seq.Helper documentation).",
                    "In particular, tf.contrib.seq2seq.BasicDecoder relies solely on helper when it performs a step: the next_inputs that the are fed in to the subsequent cell is exactly the return value of Helper.next_inputs().",
                    "There are different implementations of Helper interface, e.g.,",
                    "tf.contrib.seq2seq.TrainingHelper is returning the next decoder input (which is usually ground truth).",
                    "This helper is used in training as indicated in the tutorial..",
                    "tf.contrib.seq2seq.SampleEmbeddingHelper does the same, but samples the token according to categorical (a.k.a.",
                    "generalized Bernoulli) distribution.",
                    "NMT uses this helper in inference when sampling_temperature > 0.."
                ]
            }
        ]
    },
    "tf.nn.xw_plus_b":
    {
        "tf.layers.dense":
        [
            {
                "title": "53761947",
                "h": "tf.nn.xw_plus_b",
                "t": "tf.layers.dense",
                "sentences":
                [
                    "tf.nn.xw_plus_b is a low-level operation that only computes x*W+b and requires existing variables.",
                    "tf.layers.dense is a high-level \"layer\" that creates variables, apply activation can set constrains and apply regularization."
                ]
            }
        ]
    },
    "tf.app.flags":
    {
        "tf.compat.v1.flags":
        [
            {
                "title": "58264744",
                "h": "tf.app.flags",
                "t": "tf.compat.v1.flags",
                "sentences":
                [
                    "If it is TF2.0 then you need to replace tf.app.flags with tf.compat.v1.flags defined here since it is no longer supported."
                ]
            }
        ]
    },
    "tf.compat.v1.flags":
    {
        "tf.app.flags":
        [
            {
                "title": "58264744",
                "h": "tf.app.flags",
                "t": "tf.compat.v1.flags",
                "sentences":
                [
                    "If it is TF2.0 then you need to replace tf.app.flags with tf.compat.v1.flags defined here since it is no longer supported."
                ]
            }
        ]
    },
    "tf.contrib.layers.conv2d":
    {
        "tf.nn.conv2d":
        [
            {
                "title": "43587561",
                "h": "tf.nn.conv2d",
                "t": "tf.contrib.layers.conv2d",
                "sentences":
                [
                    "tf.nn.conv2d(...) is the core, low-level convolution functionality provided by TensorFlow.",
                    "tf.contrib.layers.conv2d(...) is part of a higher-level API build around core-TensorFlow."
                ]
            }
        ],
        "tf.keras.layers.Conv2D":
        [
            {
                "title": "43587561",
                "h": "tf.contrib.layers.conv2d",
                "t": "tf.keras.layers.Conv2D",
                "sentences":
                [
                    "tf.contrib.layers.conv2d(...) is part of a higher-level API build around core-TensorFlow.",
                    "Note, that in current TensorFlow versions, parts of layers are now in core, too, e.g.",
                    "tf.keras.layers.Conv2D."
                ]
            }
        ]
    },
    "tf.wholefilereader":
    {
        "tf.read_file":
        [
            {
                "title": "34345827",
                "h": "tf.read_file",
                "t": "tf.wholefilereader",
                "sentences":
                [
                    "Use tf.read_file(filename) rather than tf.WholeFileReader() to read your image files.",
                    "tf.read_file() is a stateless op that consumes a single filename and produces a single string containing the contents of the file.",
                    "It has the advantage that it's a pure function, so it's easy to associate data with the input and the output."
                ]
            }
        ]
    },
    "tf.constant":
    {
        "tf.variable":
        [
            {
                "title": "63435824",
                "h": "tf.variable",
                "t": "tf.constant",
                "sentences":
                [
                    "What was wrong: You cannot convert a tf.variable to a numpy array using graph execution and need to use a tf.constant instead."
                ]
            },
            {
                "title": "43536220",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "tf.constant() and tf.placeholder() are nodes in the graph (ops or operations).",
                    "On the other hand tf.Variable() is a class."
                ]
            },
            {
                "title": "57959532",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "you will not be able to do this for a 'tf.constant()', as it is a constant variable and does not support having its values changed.",
                    "If you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.Variable.",
                    "However these require predefined dimensions, and cannot have their sizes changed as desired in your question."
                ]
            },
            {
                "title": "45267542",
                "h": "tf.constant",
                "t": "tf.variable",
                "sentences":
                [
                    "If you don't want train this weights, you can define them as tf.constant not tf.Variable"
                ]
            }
        ],
        "tf.fill":
        [
            {
                "title": "37380546",
                "h": "tf.constant",
                "t": "tf.fill",
                "sentences":
                [
                    "Note that tf.constant() does not accept a dynamic shape as an argument—for then it would not be constant!—but the similar tf.fill() op does."
                ]
            },
            {
                "title": "35855219",
                "h": "tf.constant",
                "t": "tf.fill",
                "sentences":
                [
                    "A tf.constant() has fixed size and value at graph construction time, so it probably isn't the right op for your application.",
                    "If you are trying to create a tensor with a dynamic size and the same (constant) value for every element, you can use tf.fill() and tf.shape() to create an appropriately-shaped tensor."
                ]
            }
        ],
        "tf.sparse_placeholder":
        [
            {
                "title": "54372304",
                "h": "tf.constant",
                "t": "tf.sparse_placeholder",
                "sentences":
                [
                    "In this code you construct a dictionary where the values are tensors.",
                    "Like you said, this won't work for a VarLenFeature.",
                    "Instead of using tf.constant try using tf.placeholder for a a FixedLenFeature and tf.sparse_placeholder for a VarLenFeature."
                ]
            }
        ],
        "tf.placeholder":
        [
            {
                "title": "54372304",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "In this code you construct a dictionary where the values are tensors.",
                    "Like you said, this won't work for a VarLenFeature.",
                    "Instead of using tf.constant try using tf.placeholder for a a FixedLenFeature and tf.sparse_placeholder for a VarLenFeature."
                ]
            },
            {
                "title": "51356461",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "Note that the above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations.",
                    "This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.",
                    "As an alternative, you can define the Dataset in terms of tf.placeholder() tensors, and feed the NumPy arrays when you initialize an Iterator over the dataset."
                ]
            },
            {
                "title": "57959532",
                "h": "tf.constant",
                "t": "tf.placeholder",
                "sentences":
                [
                    "you will not be able to do this for a 'tf.constant()', as it is a constant variable and does not support having its values changed.",
                    "If you want to change values within tensorflow data structures it is best to either pass values to a tf.placeholder or use a tf.Variable.",
                    "However these require predefined dimensions, and cannot have their sizes changed as desired in your question."
                ]
            }
        ],
        "tf.shape":
        [
            {
                "title": "35855219",
                "h": "tf.constant",
                "t": "tf.shape",
                "sentences":
                [
                    "A tf.constant() has fixed size and value at graph construction time, so it probably isn't the right op for your application.",
                    "If you are trying to create a tensor with a dynamic size and the same (constant) value for every element, you can use tf.fill() and tf.shape() to create an appropriately-shaped tensor."
                ]
            }
        ]
    },
    "tf.contrib.learn.experiment":
    {
        "tf.estimator.estimator":
        [
            {
                "title": "47288830",
                "h": "tf.estimator.estimator",
                "t": "tf.contrib.learn.experiment",
                "sentences":
                [
                    "Just like tf.estimator.Estimator (and the derived classes) is a high-level API that hides matrix multiplications, saving checkpoints and so on, tf.contrib.learn.Experiment tries to hide the boilerplate you'd need to do for distributed computation, namely tf.train.ClusterSpec, tf.train.Server, jobs, tasks, etc."
                ]
            }
        ]
    },
    "tf.math.truediv":
    {
        "tf.math.floordiv":
        [
            {
                "title": "59579870",
                "h": "tf.math.truediv",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "Assuming you're using Python3, the / operator is overloaded to the tf.math.truediv (i.e., floating-point division, which corresponds to the RealDiv op of TensorFlow).",
                    "In Python2, the / operator may be doing integer division, in which case it's overloaded in a dtype-dependent way.",
                    "For floating dtypes, it's tf.math.truediv, for integer dtypes, it's tf.math.floordiv (integer floor division)."
                ]
            }
        ]
    },
    "tf.math.floordiv":
    {
        "tf.math.truediv":
        [
            {
                "title": "59579870",
                "h": "tf.math.truediv",
                "t": "tf.math.floordiv",
                "sentences":
                [
                    "Assuming you're using Python3, the / operator is overloaded to the tf.math.truediv (i.e., floating-point division, which corresponds to the RealDiv op of TensorFlow).",
                    "In Python2, the / operator may be doing integer division, in which case it's overloaded in a dtype-dependent way.",
                    "For floating dtypes, it's tf.math.truediv, for integer dtypes, it's tf.math.floordiv (integer floor division)."
                ]
            }
        ]
    },
    "tf.contrib.image.transform":
    {
        "tf.contrib.image.angles_to_projective_transforms":
        [
            {
                "title": "53855704",
                "h": "tf.contrib.image.transform",
                "t": "tf.contrib.image.angles_to_projective_transforms",
                "sentences":
                [
                    "tf.contrib.image.transform() receives a projective transform matrix.",
                    "tf.contrib.image.angles_to_projective_transforms() generates projective transforms from the rotation angles.",
                    "Both accept tensors as arguments, so you can just call the underlying functions."
                ]
            }
        ]
    },
    "tf.contrib.image.angles_to_projective_transforms":
    {
        "tf.contrib.image.transform":
        [
            {
                "title": "53855704",
                "h": "tf.contrib.image.transform",
                "t": "tf.contrib.image.angles_to_projective_transforms",
                "sentences":
                [
                    "tf.contrib.image.transform() receives a projective transform matrix.",
                    "tf.contrib.image.angles_to_projective_transforms() generates projective transforms from the rotation angles.",
                    "Both accept tensors as arguments, so you can just call the underlying functions."
                ]
            }
        ]
    },
    "tf.contrib.nn.alpha_dropout":
    {
        "tf.nn.dropout":
        [
            {
                "title": "51306350",
                "h": "tf.contrib.nn.alpha_dropout",
                "t": "tf.nn.dropout",
                "sentences":
                [
                    "tf.contrib.nn.alpha_dropout should be seen as an analogue to tf.nn.dropout.",
                    "The latter function also does not have an argument for a training switch.",
                    "It is not to be confused with tf.layers.dropout, which wraps tf.nn.dropout and has a training argument.",
                    "As we can see in the implementation, the layers version returns either the result of nn.dropout or the identity depending on the training switch.",
                    "It should be relatively easy to define your own wrapper around alpha_dropout in a similar manner."
                ]
            }
        ]
    },
    "tf.nn.max_pool":
    {
        "tf.reduce_max":
        [
            {
                "title": "36853403",
                "h": "tf.nn.max_pool",
                "t": "tf.reduce_max",
                "sentences":
                [
                    "tf.nn.max_pool does not support pooling over the depth dimension which is why you get an error.",
                    "You can use a max reduction instead to achieve what you're looking for:",
                    "tf.reduce_max(input_tensor, reduction_indices=[3], keep_dims=True)"
                ]
            },
            {
                "title": "43576298",
                "h": "tf.reduce_max",
                "t": "tf.nn.max_pool",
                "sentences":
                [
                    "It seems like you simply want to find the largest value over one of the dimensions which may be of dynamic size.",
                    "If that is the case, you are probably better off using the tf.reduce_max() function instead of tf.nn.max_pool()."
                ]
            }
        ]
    },
    "tf.reduce_max":
    {
        "tf.nn.max_pool":
        [
            {
                "title": "36853403",
                "h": "tf.nn.max_pool",
                "t": "tf.reduce_max",
                "sentences":
                [
                    "tf.nn.max_pool does not support pooling over the depth dimension which is why you get an error.",
                    "You can use a max reduction instead to achieve what you're looking for:",
                    "tf.reduce_max(input_tensor, reduction_indices=[3], keep_dims=True)"
                ]
            },
            {
                "title": "43576298",
                "h": "tf.reduce_max",
                "t": "tf.nn.max_pool",
                "sentences":
                [
                    "It seems like you simply want to find the largest value over one of the dimensions which may be of dynamic size.",
                    "If that is the case, you are probably better off using the tf.reduce_max() function instead of tf.nn.max_pool()."
                ]
            }
        ]
    },
    "tf.py_func":
    {
        "tf.map_fn":
        [
            {
                "title": "50359422",
                "h": "tf.py_func",
                "t": "tf.map_fn",
                "sentences":
                [
                    "So the main issue is that you have to use tf.py_func() to map a Python function (scipy.integrate.quad() in this case) on a tensor.",
                    "tf.map_fn() will map other TensorFlow operations and passes and expects tensors as operands."
                ]
            }
        ],
        "tf.train.string_input_producer":
        [
            {
                "title": "38726143",
                "h": "tf.py_func",
                "t": "tf.train.string_input_producer",
                "sentences":
                [
                    "The tf.py_func op inserts a call to regular python code inside of the TensorFlow graph, we need to specify the inputs and the number and types of the outputs.",
                    "The tf.train.string_input_producer creates a shuffled queue with the given keys."
                ]
            }
        ],
        "tf.train.batch":
        [
            {
                "title": "38726143",
                "h": "tf.py_func",
                "t": "tf.train.batch",
                "sentences":
                [
                    "The tf.py_func op inserts a call to regular python code inside of the TensorFlow graph, we need to specify the inputs and the number and types of the outputs.",
                    "The tf.train.batch op create another queue that contains batches of data."
                ]
            }
        ]
    },
    "tf.keras.optimizers.adam":
    {
        "tf.train.adamoptimizer":
        [
            {
                "title": "53749704",
                "h": "tf.keras.optimizers.adam",
                "t": "tf.train.adamoptimizer",
                "sentences":
                [
                    "Optimizer like tf.keras.optimizers.Adam() will be saved, and tf.train.AdamOptimizer() will not be saved on model.save()."
                ]
            }
        ]
    },
    "tf.train.adamoptimizer":
    {
        "tf.keras.optimizers.adam":
        [
            {
                "title": "53749704",
                "h": "tf.keras.optimizers.adam",
                "t": "tf.train.adamoptimizer",
                "sentences":
                [
                    "Optimizer like tf.keras.optimizers.Adam() will be saved, and tf.train.AdamOptimizer() will not be saved on model.save()."
                ]
            }
        ]
    },
    "tf.contrib.rnn.dropoutwrapper":
    {
        "tf.compat.v1.nn.rnn_cell.dropoutwrapper":
        [
            {
                "title": "61823969",
                "h": "tf.contrib.rnn.dropoutwrapper",
                "t": "tf.compat.v1.nn.rnn_cell.dropoutwrapper",
                "sentences":
                [
                    "Coming to the alternative solution,migarting code from Tensorflow 1 to Tensorflow 2 will not happen automatically, which you have to change manually.",
                    "tf.contrib.rnn.DropoutWrapper you can change it to tf.compat.v1.nn.rnn_cell.DropoutWrapper"
                ]
            }
        ]
    },
    "tf.compat.v1.nn.rnn_cell.dropoutwrapper":
    {
        "tf.contrib.rnn.dropoutwrapper":
        [
            {
                "title": "61823969",
                "h": "tf.contrib.rnn.dropoutwrapper",
                "t": "tf.compat.v1.nn.rnn_cell.dropoutwrapper",
                "sentences":
                [
                    "Coming to the alternative solution,migarting code from Tensorflow 1 to Tensorflow 2 will not happen automatically, which you have to change manually.",
                    "tf.contrib.rnn.DropoutWrapper you can change it to tf.compat.v1.nn.rnn_cell.DropoutWrapper"
                ]
            }
        ]
    },
    "tf.contrib.layers.softmax":
    {
        "tf.nn.softmax":
        [
            {
                "title": "46686881",
                "h": "tf.nn.softmax",
                "t": "tf.contrib.layers.softmax",
                "sentences":
                [
                    "Once that was done, you could use tf.nn.softmax, applying it separately to both of your logit tensors.",
                    "There is also a tf.contrib.layers.softmax which allows you to apply the softmax on the final axis of a tensor with greater than 2 dimensions, but it doesn't look like you need anything like that.",
                    "tf.nn.softmax should work here."
                ]
            }
        ]
    },
    "tf.losses.sigmoid_cross_entropy":
    {
        "tf.nn.weighted_cross_entropy_with_logits":
        [
            {
                "title": "49813762",
                "h": "tf.losses.sigmoid_cross_entropy",
                "t": "tf.nn.weighted_cross_entropy_with_logits",
                "sentences":
                [
                    "If your classes are heavily skewed, and you want to balance it at the calculation of loss, then you have to specify a tensor as weight, as described in the manual for tf.losses.sigmoid_cross_entropy():",
                    "That is make the weights tensor 1.0 for class 0, and maybe 10 for class 1, and now \"false negative\" losses will be much more heavily counted.",
                    "An alternative to achieve the same thing is using tf.nn.weighted_cross_entropy_with_logits(), which has a pos_weight argument for the exact same purpose.",
                    "But it's in tf.nn not tf.losses so you have to manually add it to the losses collection."
                ]
            }
        ]
    },
    "tf.nn.weighted_cross_entropy_with_logits":
    {
        "tf.losses.sigmoid_cross_entropy":
        [
            {
                "title": "49813762",
                "h": "tf.losses.sigmoid_cross_entropy",
                "t": "tf.nn.weighted_cross_entropy_with_logits",
                "sentences":
                [
                    "If your classes are heavily skewed, and you want to balance it at the calculation of loss, then you have to specify a tensor as weight, as described in the manual for tf.losses.sigmoid_cross_entropy():",
                    "That is make the weights tensor 1.0 for class 0, and maybe 10 for class 1, and now \"false negative\" losses will be much more heavily counted.",
                    "An alternative to achieve the same thing is using tf.nn.weighted_cross_entropy_with_logits(), which has a pos_weight argument for the exact same purpose.",
                    "But it's in tf.nn not tf.losses so you have to manually add it to the losses collection."
                ]
            }
        ],
        "tf.nn.softmax_cross_entropy_with_logits":
        [
            {
                "title": "39783314",
                "h": "tf.nn.softmax_cross_entropy_with_logits",
                "t": "tf.nn.weighted_cross_entropy_with_logits",
                "sentences":
                [
                    "The warning just informs you that tf.nn.softmax_cross_entropy_with_logits will apply a softmax on the input logits, before computing cross-entropy.",
                    "This warning seems really to avoid applying softmax twice, as the cross-entropy results would be very different.",
                    "As the warning states, this implementation is for improving performance, with the caveat that you should not put your own softmax layer as input (which is somewhat convenient, in practice).",
                    "If the forced softmax hinders your computation, perhaps another API could help: tf.nn.sigmoid_cross_entropy_with_logits or maybe tf.nn.weighted_cross_entropy_with_logits.",
                    "The implementation does not seem to indicate, though, that any scaling will impact the result.",
                    "I guess a linear scaling function should be fine, as long as it preserves the original logits repartition.",
                    "But whatever is applied on the input logits, tf.nn.softmax_cross_entropy_with_logits will apply a softmax before computing the cross-entropy."
                ]
            }
        ]
    },
    "tf.contrib.eager.defun":
    {
        "tf.contrib.eager.py_func":
        [
            {
                "title": "50161581",
                "h": "tf.contrib.eager.defun",
                "t": "tf.contrib.eager.py_func",
                "sentences":
                [
                    "tf.contrib.eager.defun: Is useful when you have eager execution enabled but want to \"compile\" some computation into a graph to benefit from memory and/or performance optimizations..",
                    "tf.contrib.eager.py_func: Is useful when do not have eager execution enabled but want to execute some computation in the graph as a Python function.."
                ]
            }
        ]
    },
    "tf.contrib.eager.py_func":
    {
        "tf.contrib.eager.defun":
        [
            {
                "title": "50161581",
                "h": "tf.contrib.eager.defun",
                "t": "tf.contrib.eager.py_func",
                "sentences":
                [
                    "tf.contrib.eager.defun: Is useful when you have eager execution enabled but want to \"compile\" some computation into a graph to benefit from memory and/or performance optimizations..",
                    "tf.contrib.eager.py_func: Is useful when do not have eager execution enabled but want to execute some computation in the graph as a Python function.."
                ]
            }
        ]
    },
    "tf.train.batch":
    {
        "tf.train.string_input_producer":
        [
            {
                "title": "38726143",
                "h": "tf.train.string_input_producer",
                "t": "tf.train.batch",
                "sentences":
                [
                    "The tf.train.string_input_producer creates a shuffled queue with the given keys.",
                    "The tf.train.batch op create another queue that contains batches of data."
                ]
            }
        ],
        "tf.py_func":
        [
            {
                "title": "38726143",
                "h": "tf.py_func",
                "t": "tf.train.batch",
                "sentences":
                [
                    "The tf.py_func op inserts a call to regular python code inside of the TensorFlow graph, we need to specify the inputs and the number and types of the outputs.",
                    "The tf.train.batch op create another queue that contains batches of data."
                ]
            }
        ]
    },
    "tf.configproto":
    {
        "tf.compat.v1.configproto":
        [
            {
                "title": "56994817",
                "h": "tf.configproto",
                "t": "tf.compat.v1.configproto",
                "sentences":
                [
                    "ConfigProto disappeared in tf 2.0, so an elegant solution is:",
                    "and then replace:",
                    "tf.ConfigProto by tf.compat.v1.ConfigProto"
                ]
            }
        ]
    },
    "tf.compat.v1.configproto":
    {
        "tf.configproto":
        [
            {
                "title": "56994817",
                "h": "tf.configproto",
                "t": "tf.compat.v1.configproto",
                "sentences":
                [
                    "ConfigProto disappeared in tf 2.0, so an elegant solution is:",
                    "and then replace:",
                    "tf.ConfigProto by tf.compat.v1.ConfigProto"
                ]
            }
        ]
    },
    "tf.compat.v1.session":
    {
        "tf.session":
        [
            {
                "title": "59390988",
                "h": "tf.compat.v1.session",
                "t": "tf.session",
                "sentences":
                [
                    "you need to use tf.compat.v1.Session() as tf.session is deprecated."
                ]
            },
            {
                "title": "61896836",
                "h": "tf.compat.v1.session",
                "t": "tf.session",
                "sentences":
                [
                    "Eager Execution is enabled by default in TF 2.0, so you need to call .numpy() on the Tensor object.",
                    "If Eager Execution is disabled, you can build a graph and then run it through tf.compat.v1.Session in TF 2.x and tf.Session in TF 1.x"
                ]
            }
        ]
    },
    "tf.initialize_all_variables":
    {
        "tf.global_variables_initializer":
        [
            {
                "title": "41489484",
                "h": "tf.initialize_all_variables",
                "t": "tf.global_variables_initializer",
                "sentences":
                [
                    "In the newer version of TensorFlow:",
                    "tf.initialize_all_variables() is deprecated.",
                    "They mention that you have to use:",
                    "tf.global_variables_initializer()"
                ]
            },
            {
                "title": "41943422",
                "h": "tf.global_variables_initializer",
                "t": "tf.initialize_all_variables",
                "sentences":
                [
                    "The two statements are equivalent: both tf.global_variables_initializer() and tf.initialize_all_variables() return a tf.Operation that, when run, will initialize the global variables in a model.",
                    "Passing an operation to sess.run() or calling operation.run() are equivalent when you have created a tf.InteractiveSession, or are in a with tf.Session(): block.",
                    "The tf.initialize_all_variables() function has been deprecated (and will be removed from TensorFlow 1.0) because its name is confusing: it does not initialize all variables (i.e.",
                    "local variables must be initialized separately, using tf.local_variables_initializer()), and it doesn't immediately initialize the variables (instead it returns an operation that you have to run yourself)."
                ]
            }
        ]
    },
    "tf.global_variables_initializer":
    {
        "tf.initialize_all_variables":
        [
            {
                "title": "41489484",
                "h": "tf.initialize_all_variables",
                "t": "tf.global_variables_initializer",
                "sentences":
                [
                    "In the newer version of TensorFlow:",
                    "tf.initialize_all_variables() is deprecated.",
                    "They mention that you have to use:",
                    "tf.global_variables_initializer()"
                ]
            },
            {
                "title": "41943422",
                "h": "tf.global_variables_initializer",
                "t": "tf.initialize_all_variables",
                "sentences":
                [
                    "The two statements are equivalent: both tf.global_variables_initializer() and tf.initialize_all_variables() return a tf.Operation that, when run, will initialize the global variables in a model.",
                    "Passing an operation to sess.run() or calling operation.run() are equivalent when you have created a tf.InteractiveSession, or are in a with tf.Session(): block.",
                    "The tf.initialize_all_variables() function has been deprecated (and will be removed from TensorFlow 1.0) because its name is confusing: it does not initialize all variables (i.e.",
                    "local variables must be initialized separately, using tf.local_variables_initializer()), and it doesn't immediately initialize the variables (instead it returns an operation that you have to run yourself)."
                ]
            }
        ],
        "tf.constant_initializer":
        [
            {
                "title": "57441807",
                "h": "tf.global_variables_initializer",
                "t": "tf.constant_initializer",
                "sentences":
                [
                    "As for the distinction between tf.global_variables_initializer and tf.constant_initializer, they are something completely different:",
                    "tf.global_variables_initializer is an operation that you execute to initialize all variables in your graph.",
                    "It doesn't matter whether the variables will get initialized with tf.constant_initializer, tf.random_normal_initializer or tf.glorot_uniform_initializer.",
                    "You just pass that operation to a tf.Session so that the graph variables will get initialized.",
                    "tf.constant_initializer on the other hand, is just an initializer that you pass to the tf.Variable of your graph.",
                    "Then, when a tf.Session runs the operation tf.global_variables_initializer, the TF Graph will use the tf.constant_initializer to initialize the corresponding tf.Variable with the constant values provided."
                ]
            }
        ]
    },
    "tf.contrib.distribute":
    {
        "tf.distribute":
        [
            {
                "title": "57227547",
                "h": "tf.contrib.distribute",
                "t": "tf.distribute",
                "sentences":
                [
                    "If you are using tensorflow versions older than 2.0, you will use tf.contrib.distribute:",
                    "After 2.0, you only need to use tf.distribute!"
                ]
            }
        ]
    },
    "tf.distribute":
    {
        "tf.contrib.distribute":
        [
            {
                "title": "57227547",
                "h": "tf.contrib.distribute",
                "t": "tf.distribute",
                "sentences":
                [
                    "If you are using tensorflow versions older than 2.0, you will use tf.contrib.distribute:",
                    "After 2.0, you only need to use tf.distribute!"
                ]
            }
        ]
    },
    "tf.compat.v1.reset_default_graph":
    {
        "tf.reset_default_graph":
        [
            {
                "title": "58428014",
                "h": "tf.reset_default_graph",
                "t": "tf.compat.v1.reset_default_graph",
                "sentences":
                [
                    "Defining a new model is not enough to reset the incrementing index, because all models end up on the same underlying graph.",
                    "To reset the index, you must reset the underlying graph.",
                    "In TF 1.X, this is done via tf.reset_default_graph().",
                    "In TF 2.0, you can do this via the v1 compatibility API: tf.compat.v1.reset_default_graph() (the latter will also solve some deprecation warnings you might get with the latest versions of TF 1.X)"
                ]
            }
        ]
    },
    "tf.keras.layers.conv2dtranspose":
    {
        "tf.nn.conv2d_transpose":
        [
            {
                "title": "55199043",
                "h": "tf.keras.layers.conv2dtranspose",
                "t": "tf.nn.conv2d_transpose",
                "sentences":
                [
                    "TensoFlow 2.0 has tf.keras.layers.Conv2DTranspose.",
                    "Keras is the default high level API for TF 2.0, but it still have tf.nn.conv2d_transpose for low level applications"
                ]
            }
        ]
    },
    "tf.contrib.layers.apply_regularization":
    {
        "tf.contrib.layers.l2_regularizer":
        [
            {
                "title": "46219309",
                "h": "tf.contrib.layers.apply_regularization",
                "t": "tf.contrib.layers.l2_regularizer",
                "sentences":
                [
                    "tf.contrib.layers.apply_regularization allows you to combine a regularizer and a set of tensors on which it should be applied..",
                    "tf.contrib.layers.l2_regularizer allows you to defines the scope on which the l2 should be applied..",
                    "But in essence a regularizer is just something to be added to the cost function, so any function (tensor) which you add to the cost function can be considered as a regularizer and will be taken into account.."
                ]
            }
        ]
    },
    "tf.contrib.layers.l2_regularizer":
    {
        "tf.contrib.layers.apply_regularization":
        [
            {
                "title": "46219309",
                "h": "tf.contrib.layers.apply_regularization",
                "t": "tf.contrib.layers.l2_regularizer",
                "sentences":
                [
                    "tf.contrib.layers.apply_regularization allows you to combine a regularizer and a set of tensors on which it should be applied..",
                    "tf.contrib.layers.l2_regularizer allows you to defines the scope on which the l2 should be applied..",
                    "But in essence a regularizer is just something to be added to the cost function, so any function (tensor) which you add to the cost function can be considered as a regularizer and will be taken into account.."
                ]
            }
        ]
    },
    "tf.int32":
    {
        "tf.float32":
        [
            {
                "title": "39133562",
                "h": "tf.int32",
                "t": "tf.float32",
                "sentences":
                [
                    "This error arises because tf.Variable(0, ...) defines a variable of element type tf.int32, and there is no kernel that implements int32 variables on GPU in the standard TensorFlow distribution.",
                    "When you use tf.Variable(tf.zeros([1])), you're defining a variable of element type tf.float32, which is supported on GPU."
                ]
            }
        ]
    },
    "tf.float32":
    {
        "tf.int32":
        [
            {
                "title": "39133562",
                "h": "tf.int32",
                "t": "tf.float32",
                "sentences":
                [
                    "This error arises because tf.Variable(0, ...) defines a variable of element type tf.int32, and there is no kernel that implements int32 variables on GPU in the standard TensorFlow distribution.",
                    "When you use tf.Variable(tf.zeros([1])), you're defining a variable of element type tf.float32, which is supported on GPU."
                ]
            }
        ],
        "tf.float64":
        [
            {
                "title": "35727708",
                "h": "tf.float64",
                "t": "tf.float32",
                "sentences":
                [
                    "(The lack of support for tf.float64 is a known issue.)",
                    "The optimizers require that all of the tf.Variable objects that you are trying to optimize must also have type tf.float32."
                ]
            }
        ]
    },
    "tf.expand_dims":
    {
        "tf.reshape":
        [
            {
                "title": "42708799",
                "h": "tf.expand_dims",
                "t": "tf.reshape",
                "sentences":
                [
                    "You can use tf.expand_dims() to add a new dimension.",
                    "You can also use tf.reshape() for this, but would recommend you to use expand_dims, as this will also carry some values to new dimension if new shape can be satisfied."
                ]
            }
        ]
    },
    "tf.reshape":
    {
        "tf.squeeze": [
          {
            "title": "47658937",
            "h": "tf.squeeze",
            "t": "tf.reshape",
            "prob": "0.99",
            "sentences": 
            [
              "In order to remove 1 dimensional axis from tensor, tf.squeeze is the correct operation.",
              "But you can achieve your desired work with tf.reshape as well though I will suggest you to make use of tf.squeeze."
            ]
          }
        ],
        "tf.expand_dims":
        [
            {
                "title": "42708799",
                "h": "tf.expand_dims",
                "t": "tf.reshape",
                "sentences":
                [
                    "You can use tf.expand_dims() to add a new dimension.",
                    "You can also use tf.reshape() for this, but would recommend you to use expand_dims, as this will also carry some values to new dimension if new shape can be satisfied."
                ]
            }
        ],
        "tf.transpose":
        [
            {
                "title": "36088778",
                "h": "tf.reshape",
                "t": "tf.transpose",
                "sentences":
                [
                    "Note that, since this uses tf.reshape() and not tf.transpose(), it doesn't need to modify the (potentially large) data in the logits tensor, so it should be fairly efficient."
                ]
            }
        ]
    },
    "tf.float64":
    {
        "tf.float32":
        [
            {
                "title": "35727708",
                "h": "tf.float64",
                "t": "tf.float32",
                "sentences":
                [
                    "(The lack of support for tf.float64 is a known issue.)",
                    "The optimizers require that all of the tf.Variable objects that you are trying to optimize must also have type tf.float32."
                ]
            }
        ]
    },
    "tf.sparse_placeholder":
    {
        "tf.constant":
        [
            {
                "title": "54372304",
                "h": "tf.constant",
                "t": "tf.sparse_placeholder",
                "sentences":
                [
                    "In this code you construct a dictionary where the values are tensors.",
                    "Like you said, this won't work for a VarLenFeature.",
                    "Instead of using tf.constant try using tf.placeholder for a a FixedLenFeature and tf.sparse_placeholder for a VarLenFeature."
                ]
            }
        ]
    },
    "tf.contrib.tpu.tpuestimator":
    {
        "tf.estimator.estimator":
        [
            {
                "title": "54742449",
                "h": "tf.estimator.estimator",
                "t": "tf.contrib.tpu.tpuestimator",
                "sentences":
                [
                    "If you actually don't need TPU inference support you can create a tf.estimator.Estimator instead of a tf.contrib.tpu.TPUEstimator one, using the same model_fn and trained model."
                ]
            }
        ]
    },
    "tf.nn.max_pool ":
    {
        "tf.layers.max_pooling1d":
        [
            {
                "title": "48815730",
                "h": "tf.nn.max_pool ",
                "t": "tf.layers.max_pooling1d",
                "sentences":
                [
                    "tf.nn.max_pool is for 2D pooling, i.e., it expects input tensor of rank 4 (yours is rank 3).",
                    "You should either expand the dimensions of the input or simply use tf.layers.max_pooling1d:"
                ]
            }
        ]
    },
    "tf.layers.max_pooling1d":
    {
        "tf.nn.max_pool ":
        [
            {
                "title": "48815730",
                "h": "tf.nn.max_pool ",
                "t": "tf.layers.max_pooling1d",
                "sentences":
                [
                    "tf.nn.max_pool is for 2D pooling, i.e., it expects input tensor of rank 4 (yours is rank 3).",
                    "You should either expand the dimensions of the input or simply use tf.layers.max_pooling1d:"
                ]
            }
        ]
    },
    "tf.multiply":
    {
        "tf.reduce_sum":
        [
            {
                "title": "47583685",
                "h": "tf.multiply",
                "t": "tf.reduce_sum",
                "sentences":
                [
                    "What tf.multiply(X, X) does is essentially multiplying each element of the matrix with itself, like",
                    "whereas tf.reduce_sum(_, axis=1) takes a sum of each row, so the result for the previous example will be"
                ]
            }
        ]
    },
    "tf.saved_model":
    {
        "tf.train.saver":
        [
            {
                "title": "47235448",
                "h": "tf.train.saver",
                "t": "tf.saved_model",
                "sentences":
                [
                    "Though there have been many solutions, most of them is based on tf.train.Saver.",
                    "When we load a .ckpt saved by Saver, we have to either redefine the tensorflow network or use some weird and hard-remembered name, e.g.",
                    "'placehold_0:0','dense/Adam/Weight:0'.",
                    "Here I recommend to use tf.saved_model, one simplest example given below, your can learn more from Serving a TensorFlow Model:"
                ]
            }
        ]
    },
    "tf.clip_by_norm":
    {
        "tf.nn.dropout":
        [
            {
                "title": "39646025",
                "h": "tf.nn.dropout",
                "t": "tf.clip_by_norm",
                "sentences":
                [
                    "tf.nn.dropout does not impose any norm constraint.",
                    "I believe what you're looking for is to \"process the gradients before applying them\" using tf.clip_by_norm."
                ]
            }
        ]
    },
    "tf.contrib.rnn":
    {
        "tf.contrib.rnn.lstmcell":
        [
            {
                "title": "42811536",
                "h": "tf.contrib.rnn",
                "t": "tf.contrib.rnn.lstmcell",
                "sentences":
                [
                    "In particular, tf.contrib.rnn is the TensorFlow module which contains all the RNN cells.",
                    "If you want to use an LSTM cell you need to use an tf.contrib.rnn.LSTMCell."
                ]
            }
        ]
    },
    "tf.contrib.rnn.lstmcell":
    {
        "tf.contrib.rnn":
        [
            {
                "title": "42811536",
                "h": "tf.contrib.rnn",
                "t": "tf.contrib.rnn.lstmcell",
                "sentences":
                [
                    "In particular, tf.contrib.rnn is the TensorFlow module which contains all the RNN cells.",
                    "If you want to use an LSTM cell you need to use an tf.contrib.rnn.LSTMCell."
                ]
            }
        ]
    },
    "tf.contrib.learn.datasets.base.load_csv":
    {
        "tf.contrib.learn.datasets.base.load_csv_with_header":
        [
            {
                "title": "40032112",
                "h": "tf.contrib.learn.datasets.base.load_csv",
                "t": "tf.contrib.learn.datasets.base.load_csv_with_header",
                "sentences":
                [
                    "The function tf.contrib.learn.datasets.base.load_csv() was removed in TensorFlow release 0.11.",
                    "Depending on whether the file has a header or not (and the Iris dataset does have a header), the replacement functions are:",
                    "tf.contrib.learn.datasets.base.load_csv_with_header()."
                ]
            }
        ],
        "tf.contrib.learn.datasets.base.load_csv_without_header":
        [
            {
                "title": "40032112",
                "h": "tf.contrib.learn.datasets.base.load_csv",
                "t": "tf.contrib.learn.datasets.base.load_csv_without_header",
                "sentences":
                [
                    "The function tf.contrib.learn.datasets.base.load_csv() was removed in TensorFlow release 0.11.",
                    "Depending on whether the file has a header or not (and the Iris dataset does have a header), the replacement functions are:",
                    "tf.contrib.learn.datasets.base.load_csv_without_header()."
                ]
            }
        ]
    },
    "tf.contrib.learn.datasets.base.load_csv_with_header":
    {
        "tf.contrib.learn.datasets.base.load_csv":
        [
            {
                "title": "40032112",
                "h": "tf.contrib.learn.datasets.base.load_csv",
                "t": "tf.contrib.learn.datasets.base.load_csv_with_header",
                "sentences":
                [
                    "The function tf.contrib.learn.datasets.base.load_csv() was removed in TensorFlow release 0.11.",
                    "Depending on whether the file has a header or not (and the Iris dataset does have a header), the replacement functions are:",
                    "tf.contrib.learn.datasets.base.load_csv_with_header()."
                ]
            }
        ]
    },
    "tf.contrib.learn.datasets.base.load_csv_without_header":
    {
        "tf.contrib.learn.datasets.base.load_csv":
        [
            {
                "title": "40032112",
                "h": "tf.contrib.learn.datasets.base.load_csv",
                "t": "tf.contrib.learn.datasets.base.load_csv_without_header",
                "sentences":
                [
                    "The function tf.contrib.learn.datasets.base.load_csv() was removed in TensorFlow release 0.11.",
                    "Depending on whether the file has a header or not (and the Iris dataset does have a header), the replacement functions are:",
                    "tf.contrib.learn.datasets.base.load_csv_without_header()."
                ]
            }
        ]
    },
    "tf.estimator.linearclassifier":
    {
        "tf.estimator.linearregressor":
        [
            {
                "title": "60777752",
                "h": "tf.estimator.linearclassifier",
                "t": "tf.estimator.linearregressor",
                "sentences":
                [
                    "It was a simple mistake, I was using tf.estimator.LinearClassifier where in fact I should have been using tf.estimator.LinearRegressor as it was a float value I was trying to predict."
                ]
            }
        ]
    },
    "tf.estimator.linearregressor":
    {
        "tf.estimator.linearclassifier":
        [
            {
                "title": "60777752",
                "h": "tf.estimator.linearclassifier",
                "t": "tf.estimator.linearregressor",
                "sentences":
                [
                    "It was a simple mistake, I was using tf.estimator.LinearClassifier where in fact I should have been using tf.estimator.LinearRegressor as it was a float value I was trying to predict."
                ]
            }
        ]
    },
    "tf.image.decode_png":
    {
        "tf.image.decode_image":
        [
            {
                "title": "48347663",
                "h": "tf.image.decode_image",
                "t": "tf.image.decode_png",
                "sentences":
                [
                    "The issue here actually comes from the fact that tf.image.decode_image doesn't return the shape of the image.",
                    "The problem comes from the fact that tf.image.decode_image also handles .gif, which returns a 4D tensor, whereas .jpg and .png return 3D images.",
                    "Therefore, the correct shape cannot be returned.",
                    "The solution is to simply use tf.image.decode_jpeg or tf.image.decode_png (both work the same and can be used on .png and .jpg images)."
                ]
            }
        ]
    },
    "tf.squeeze":
    {
        "tf.reshape": [
            {
              "title": "47658937",
              "h": "tf.squeeze",
              "t": "tf.reshape",
              "prob": "0.99",
              "sentences": 
              [
                  "In order to remove 1 dimensional axis from tensor, tf.squeeze is the correct operation.",
                  "But you can achieve your desired work with tf.reshape as well though I will suggest you to make use of tf.squeeze."
              ]
            }
          ]
    },
    "tf.transpose":
    {
        "tf.reshape":
        [
            {
                "title": "36088778",
                "h": "tf.reshape",
                "t": "tf.transpose",
                "sentences":
                [
                    "Note that, since this uses tf.reshape() and not tf.transpose(), it doesn't need to modify the (potentially large) data in the logits tensor, so it should be fairly efficient."
                ]
            }
        ]
    },
    "tf.compat.v1.py_func":
    {
        "tf.py_function":
        [
            {
                "title": "56040520",
                "h": "tf.compat.v1.py_func",
                "t": "tf.py_function",
                "sentences":
                [
                    "In case you need to use py_func directly, the most straightforward way is to use tf.compat.v1.py_func.",
                    "TF2.0 has it's own wrapper, called tf.py_function."
                ]
            }
        ]
    },
    "tf.py_function":
    {
        "tf.compat.v1.py_func":
        [
            {
                "title": "56040520",
                "h": "tf.compat.v1.py_func",
                "t": "tf.py_function",
                "sentences":
                [
                    "In case you need to use py_func directly, the most straightforward way is to use tf.compat.v1.py_func.",
                    "TF2.0 has it's own wrapper, called tf.py_function."
                ]
            }
        ]
    },
    "tf.slice":
    {
        "tf.image.extract_glimpse":
        [
            {
                "title": "41600007",
                "h": "tf.slice",
                "t": "tf.image.extract_glimpse",
                "sentences":
                [
                    "Instead of using tf.slice (which doesn't let you operate on a batch), I recommend using tf.image.extract_glimpse."
                ]
            }
        ],
        "tf.image.crop_and_resize":
        [
            {
                "title": "41600007",
                "h": "tf.slice",
                "t": "tf.image.crop_and_resize",
                "sentences":
                [
                    "Instead of using tf.slice (which doesn't let you operate on a batch), I recommend using tf.image.extract_glimpse.",
                    "If you would like to extract multiple sizes (and even multiple glimpses per image), check out tf.image.crop_and_resize."
                ]
            }
        ]
    },
    "tf.image.extract_glimpse":
    {
        "tf.slice":
        [
            {
                "title": "41600007",
                "h": "tf.slice",
                "t": "tf.image.extract_glimpse",
                "sentences":
                [
                    "Instead of using tf.slice (which doesn't let you operate on a batch), I recommend using tf.image.extract_glimpse."
                ]
            }
        ]
    },
    "tf.image.crop_and_resize":
    {
        "tf.slice":
        [
            {
                "title": "41600007",
                "h": "tf.slice",
                "t": "tf.image.crop_and_resize",
                "sentences":
                [
                    "Instead of using tf.slice (which doesn't let you operate on a batch), I recommend using tf.image.extract_glimpse.",
                    "If you would like to extract multiple sizes (and even multiple glimpses per image), check out tf.image.crop_and_resize."
                ]
            }
        ]
    },
    "tf.set_random_seed":
    {
        "tf.random.set_seed":
        [
            {
                "title": "57043917",
                "h": "tf.set_random_seed",
                "t": "tf.random.set_seed",
                "sentences":
                [
                    "In TensorFlow 2.0 tf.set_random_seed(42) has changed to tf.random.set_seed(42)."
                ]
            }
        ]
    },
    "tf.sigmoid ":
    {
        "tf.nn.softmax":
        [
            {
                "title": "47761718",
                "h": "tf.nn.softmax",
                "t": "tf.sigmoid ",
                "sentences":
                [
                    "Instead of tf.nn.softmax, you could also use tf.sigmoid on a single logit, then set the other output to one minus that."
                ]
            }
        ]
    },
    "tf.nn.bias_add":
    {
        "tf.add":
        [
            {
                "title": "43135048",
                "h": "tf.add",
                "t": "tf.nn.bias_add",
                "sentences":
                [
                    "Unlike tf.add, the type of bias is allowed to differ from value in the case where both types are quantized.",
                    "tf.add is a general addition operation, while tf.nn.bias_add is to be used specifically for adding bias to the weights, which raises an exception if the dtypes aren't same."
                ]
            }
        ]
    },
    " tf.keras.layers":
    {
        "tf.layers":
        [
            {
                "title": "55413120",
                "h": "tf.layers",
                "t": " tf.keras.layers",
                "sentences":
                [
                    "As per official docs, tf.layers are wrappers around tf.keras.layers."
                ]
            }
        ]
    },
    "tf.zeros":
    {
        "tf.ones":
        [
            {
                "title": "40366326",
                "h": "tf.zeros",
                "t": "tf.ones",
                "sentences":
                [
                    "As the notebook is a bit long, they have a simple LSTM model where they use tf.truncated_normal to initialize weights and tf.zeros to initialize biases (although I have tried using tf.ones to initialize biases before, seem to also work)."
                ]
            }
        ]
    },
    "tf.ones":
    {
        "tf.zeros":
        [
            {
                "title": "40366326",
                "h": "tf.zeros",
                "t": "tf.ones",
                "sentences":
                [
                    "As the notebook is a bit long, they have a simple LSTM model where they use tf.truncated_normal to initialize weights and tf.zeros to initialize biases (although I have tried using tf.ones to initialize biases before, seem to also work)."
                ]
            }
        ]
    },
    "tf.regex_replace":
    {
        "tf.strings.regex_replace":
        [
            {
                "title": "58195903",
                "h": "tf.regex_replace",
                "t": "tf.strings.regex_replace",
                "sentences":
                [
                    "We can use tf.regex.replace to rename string.",
                    "So, in place of python string replacement, use:file_path_mask = tf.regex_replace(file_path, \"img\", \"mask\").",
                    "For TF 2.0, use tf.strings.regex_replace."
                ]
            }
        ]
    },
    "tf.strings.regex_replace":
    {
        "tf.regex_replace":
        [
            {
                "title": "58195903",
                "h": "tf.regex_replace",
                "t": "tf.strings.regex_replace",
                "sentences":
                [
                    "We can use tf.regex.replace to rename string.",
                    "So, in place of python string replacement, use:file_path_mask = tf.regex_replace(file_path, \"img\", \"mask\").",
                    "For TF 2.0, use tf.strings.regex_replace."
                ]
            }
        ]
    },
    "tf.nn.bidirectional_dynamic_rnn":
    {
        "tf.contrib.rnn.stack_bidirectional_dynamic_rnn":
        [
            {
                "title": "51928900",
                "h": "tf.nn.bidirectional_dynamic_rnn",
                "t": "tf.contrib.rnn.stack_bidirectional_dynamic_rnn",
                "sentences":
                [
                    "(1) tf.nn.bidirectional_dynamic_rnn()",
                    "(2) tf.contrib.rnn.stack_bidirectional_dynamic_rnn().",
                    "All previous answers only capture (1), so I give some details on (2), in particular since it usually outperforms (1)."
                ]
            }
        ]
    },
    "tf.contrib.rnn.stack_bidirectional_dynamic_rnn":
    {
        "tf.nn.bidirectional_dynamic_rnn":
        [
            {
                "title": "51928900",
                "h": "tf.nn.bidirectional_dynamic_rnn",
                "t": "tf.contrib.rnn.stack_bidirectional_dynamic_rnn",
                "sentences":
                [
                    "(1) tf.nn.bidirectional_dynamic_rnn()",
                    "(2) tf.contrib.rnn.stack_bidirectional_dynamic_rnn().",
                    "All previous answers only capture (1), so I give some details on (2), in particular since it usually outperforms (1)."
                ]
            }
        ]
    },
    "tf.contrib.learn.linearclassifier":
    {
        "tf.contrib.learn.linearregressor":
        [
            {
                "title": "41351418",
                "h": "tf.contrib.learn.linearclassifier",
                "t": "tf.contrib.learn.linearregressor",
                "sentences":
                [
                    "In that tutorial they are using logistic regression, that is a linear binary classifier.",
                    "They use the class tf.contrib.learn.LinearClassifier as their model.",
                    "If you use class tf.contrib.learn.LinearRegressor then you can do linear regression instead of classification."
                ]
            }
        ]
    },
    "tf.contrib.learn.linearregressor":
    {
        "tf.contrib.learn.linearclassifier":
        [
            {
                "title": "41351418",
                "h": "tf.contrib.learn.linearclassifier",
                "t": "tf.contrib.learn.linearregressor",
                "sentences":
                [
                    "In that tutorial they are using logistic regression, that is a linear binary classifier.",
                    "They use the class tf.contrib.learn.LinearClassifier as their model.",
                    "If you use class tf.contrib.learn.LinearRegressor then you can do linear regression instead of classification."
                ]
            }
        ]
    },
    "tf.debugging.enable_check_numerics":
    {
        "tf.add_check_numerics_ops":
        [
            {
                "title": "59239728",
                "h": "tf.debugging.enable_check_numerics",
                "t": "tf.add_check_numerics_ops",
                "sentences":
                [
                    "tf.debugging.enable_check_numerics() is the successor to the old API in TF1 called tf.add_check_numerics_ops(), which is not supported in TF2."
                ]
            }
        ]
    },
    "tf.add_check_numerics_ops":
    {
        "tf.debugging.enable_check_numerics":
        [
            {
                "title": "59239728",
                "h": "tf.debugging.enable_check_numerics",
                "t": "tf.add_check_numerics_ops",
                "sentences":
                [
                    "tf.debugging.enable_check_numerics() is the successor to the old API in TF1 called tf.add_check_numerics_ops(), which is not supported in TF2."
                ]
            }
        ]
    },
    "tf.gfile.open":
    {
        "tf.gfile.gfile":
        [
            {
                "title": "51329871",
                "h": "tf.gfile.open",
                "t": "tf.gfile.gfile",
                "sentences":
                [
                    "tf.gfile.Open is an alias for the class tf.gfile.GFile.",
                    "So the line tf.gfile.Open(<foo>) is invoking tf.gfile.GFile.__init__ which expects the first argument to be a Python string, not tf.Tensor of strings (which is what tf.placeholder(tf.string) returns)."
                ]
            }
        ]
    },
    "tf.gfile.gfile":
    {
        "tf.gfile.open":
        [
            {
                "title": "51329871",
                "h": "tf.gfile.open",
                "t": "tf.gfile.gfile",
                "sentences":
                [
                    "tf.gfile.Open is an alias for the class tf.gfile.GFile.",
                    "So the line tf.gfile.Open(<foo>) is invoking tf.gfile.GFile.__init__ which expects the first argument to be a Python string, not tf.Tensor of strings (which is what tf.placeholder(tf.string) returns)."
                ]
            }
        ]
    },
    "tf.keras.model.build":
    {
        "tf.keras.model.fit":
        [
            {
                "title": "56823828",
                "h": "tf.keras.model.build",
                "t": "tf.keras.model.fit",
                "sentences":
                [
                    "You need to call the tf.keras.Model.build method before you try to save a subclassed model weights.",
                    "An alternative to this would be calling tf.keras.Model.fit or tf.keras.Model.fit.call on some inputs before you try to save your model weights."
                ]
            }
        ],
        "tf.keras.model.fit.call":
        [
            {
                "title": "56823828",
                "h": "tf.keras.model.build",
                "t": "tf.keras.model.fit.call",
                "sentences":
                [
                    "You need to call the tf.keras.Model.build method before you try to save a subclassed model weights.",
                    "An alternative to this would be calling tf.keras.Model.fit or tf.keras.Model.fit.call on some inputs before you try to save your model weights."
                ]
            }
        ]
    },
    "tf.keras.model.fit":
    {
        "tf.keras.model.build":
        [
            {
                "title": "56823828",
                "h": "tf.keras.model.build",
                "t": "tf.keras.model.fit",
                "sentences":
                [
                    "You need to call the tf.keras.Model.build method before you try to save a subclassed model weights.",
                    "An alternative to this would be calling tf.keras.Model.fit or tf.keras.Model.fit.call on some inputs before you try to save your model weights."
                ]
            }
        ]
    },
    "tf.keras.model.fit.call":
    {
        "tf.keras.model.build":
        [
            {
                "title": "56823828",
                "h": "tf.keras.model.build",
                "t": "tf.keras.model.fit.call",
                "sentences":
                [
                    "You need to call the tf.keras.Model.build method before you try to save a subclassed model weights.",
                    "An alternative to this would be calling tf.keras.Model.fit or tf.keras.Model.fit.call on some inputs before you try to save your model weights."
                ]
            }
        ]
    },
    "tf.train.example":
    {
        "tf.io.parse_example":
        [
            {
                "title": "57776691",
                "h": "tf.train.example",
                "t": "tf.io.parse_example",
                "sentences":
                [
                    "It looks like your serialized_tf_example placeholder has shape=[] which is a single example.",
                    "You should pass a single tf.train.Example, serialized as a string:",
                    "If you want to feed a batch of examples instead of a single serialized example, you need to use shape=[None] and tf.io.parse_example instead."
                ]
            }
        ]
    },
    "tf.boolean_mask":
    {
        "tf.where":
        [
            {
                "title": "51586823",
                "h": "tf.boolean_mask",
                "t": "tf.where",
                "sentences":
                [
                    "You can also shorten this a bit and use tf.boolean_mask instead of tf.where and tf.gather:"
                ]
            }
        ],
        "tf.gather":
        [
            {
                "title": "51586823",
                "h": "tf.boolean_mask",
                "t": "tf.gather",
                "sentences":
                [
                    "You can also shorten this a bit and use tf.boolean_mask instead of tf.where and tf.gather:"
                ]
            }
        ]
    },
    "tf.summary.merge_all":
    {
        "tf.summary.merge":
        [
            {
                "title": "47723110",
                "h": "tf.summary.merge_all",
                "t": "tf.summary.merge",
                "sentences":
                [
                    "Instead of merging all summaries by merged_summary = tf.summary.merge_all(), you can merge the ops that you wanted like merged_summary_group1 = tf.summary.merge([op1, op2, ...])."
                ]
            }
        ],
        "tf.merge_all_summaries":
        [
            {
                "title": "43603466",
                "h": "tf.merge_all_summaries",
                "t": "tf.summary.merge_all",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.merge_all_summaries should be renamed to tf.summary.merge_all ."
                ]
            }
        ]
    },
    "tf.summary.merge":
    {
        "tf.summary.merge_all":
        [
            {
                "title": "47723110",
                "h": "tf.summary.merge_all",
                "t": "tf.summary.merge",
                "sentences":
                [
                    "Instead of merging all summaries by merged_summary = tf.summary.merge_all(), you can merge the ops that you wanted like merged_summary_group1 = tf.summary.merge([op1, op2, ...])."
                ]
            }
        ]
    },
    "tf.data.dataset.from_tensor_slices":
    {
        "tf.data.dataset.from_generator":
        [
            {
                "title": "50337385",
                "h": "tf.data.dataset.from_tensor_slices",
                "t": "tf.data.dataset.from_generator",
                "sentences":
                [
                    "As you have noticed, tf.data.Dataset.from_tensor_slices() only works on objects that can be converted to a (dense) tf.Tensor or a tf.SparseTensor.",
                    "The easiest way to get variable-length NumPy data into a Dataset is to use tf.data.Dataset.from_generator(), as follows:"
                ]
            }
        ]
    },
    "tf.data.dataset.from_generator":
    {
        "tf.data.dataset.from_tensor_slices":
        [
            {
                "title": "50337385",
                "h": "tf.data.dataset.from_tensor_slices",
                "t": "tf.data.dataset.from_generator",
                "sentences":
                [
                    "As you have noticed, tf.data.Dataset.from_tensor_slices() only works on objects that can be converted to a (dense) tf.Tensor or a tf.SparseTensor.",
                    "The easiest way to get variable-length NumPy data into a Dataset is to use tf.data.Dataset.from_generator(), as follows:"
                ]
            }
        ]
    },
    "tf.interactivesession":
    {
        "tf.session":
        [
            {
                "title": "33633839",
                "h": "tf.session",
                "t": "tf.interactivesession",
                "sentences":
                [
                    "The easiest[A] way to evaluate the actual value of a Tensor object is to pass it to the Session.run() method, or call Tensor.eval() when you have a default session (i.e.",
                    "in a with tf.Session(): block, or see below).",
                    "In general[B], you cannot print the value of a tensor without running some code in a session.",
                    "If you are experimenting with the programming model, and want an easy way to evaluate tensors, the tf.InteractiveSession lets you open a session at the start of your program, and then use that session for all Tensor.eval() (and Operation.run()) calls.",
                    "This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a Session object everywhere."
                ]
            }
        ]
    },
    " tf.nn.static_rnn":
    {
        "tf.nn.dynamic_rnn":
        [
            {
                "title": "44479970",
                "h": " tf.nn.static_rnn",
                "t": "tf.nn.dynamic_rnn",
                "sentences":
                [
                    "tf.nn.static_rnn vs. tf.nn.dynamic_rnn.",
                    "Internally, tf.nn.static_rnn creates an unrolled graph for a fixed RNN length.",
                    "That means that, if you call tf.nn.static_rnn with inputs having 200 time-steps you are creating a static graph with 200 RNN steps.",
                    "First, graph creation is slow.",
                    "Second, you’re unable to pass in longer sequences (> 200) than you've originally specified.",
                    "tf.nn.dynamic_rnn solves this.",
                    "It uses a tf.while_loop to dynamically construct the graph when it is executed.",
                    "That means graph creation is faster and you can feed batches of variable size.",
                    "What about performance?.",
                    "You may think the tf.nn.static_rnn is faster than its dynamic counterpart because it pre-builds the graph.",
                    "Please note, it is strongly encouraged to use tf.nn.dynamic_rnn."
                ]
            }
        ]
    },
    "tf.contrib.deprecated.scalar_summary":
    {
        "tf.summary.scalar":
        [
            {
                "title": "43603466",
                "h": "tf.contrib.deprecated.scalar_summary",
                "t": "tf.summary.scalar",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.contrib.deprecated.scalar_summary should be renamed to tf.summary.scalar."
                ]
            }
        ]
    },
    "tf.summary.scalar":
    {
        "tf.contrib.deprecated.scalar_summary":
        [
            {
                "title": "43603466",
                "h": "tf.contrib.deprecated.scalar_summary",
                "t": "tf.summary.scalar",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.contrib.deprecated.scalar_summary should be renamed to tf.summary.scalar."
                ]
            }
        ],
        "tf.scalar_summary":
        [
            {
                "title": "43603466",
                "h": "tf.scalar_summary",
                "t": "tf.summary.scalar",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.scalar_summary should be renamed to tf.summary.scalar ."
                ]
            }
        ]
    },
    "tf.contrib.deprecated.histogram_summary":
    {
        "tf.summary.histogram":
        [
            {
                "title": "43603466",
                "h": "tf.contrib.deprecated.histogram_summary",
                "t": "tf.summary.histogram",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.contrib.deprecated.histogram_summary should be renamed to tf.summary.histogram ."
                ]
            }
        ]
    },
    "tf.summary.histogram":
    {
        "tf.contrib.deprecated.histogram_summary":
        [
            {
                "title": "43603466",
                "h": "tf.contrib.deprecated.histogram_summary",
                "t": "tf.summary.histogram",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.contrib.deprecated.histogram_summary should be renamed to tf.summary.histogram ."
                ]
            }
        ],
        "tf.histogram_summary":
        [
            {
                "title": "43603466",
                "h": "tf.histogram_summary",
                "t": "tf.summary.histogram",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.histogram_summary should be renamed to tf.summary.histogram."
                ]
            }
        ]
    },
    "tf.merge_all_summaries":
    {
        "tf.summary.merge_all":
        [
            {
                "title": "43603466",
                "h": "tf.merge_all_summaries",
                "t": "tf.summary.merge_all",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.merge_all_summaries should be renamed to tf.summary.merge_all ."
                ]
            }
        ]
    },
    "tf.image_summary":
    {
        "tf.summary.image":
        [
            {
                "title": "43603466",
                "h": "tf.image_summary",
                "t": "tf.summary.image",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.image_summary should be renamed to tf.summary.image ."
                ]
            }
        ]
    },
    "tf.summary.image":
    {
        "tf.image_summary":
        [
            {
                "title": "43603466",
                "h": "tf.image_summary",
                "t": "tf.summary.image",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.image_summary should be renamed to tf.summary.image ."
                ]
            }
        ]
    },
    "tf.histogram_summary":
    {
        "tf.summary.histogram":
        [
            {
                "title": "43603466",
                "h": "tf.histogram_summary",
                "t": "tf.summary.histogram",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.histogram_summary should be renamed to tf.summary.histogram."
                ]
            }
        ]
    },
    "tf.audio_summary":
    {
        "tf.summary.audio":
        [
            {
                "title": "43603466",
                "h": "tf.audio_summary",
                "t": "tf.summary.audio",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.audio_summary should be renamed to tf.summary.audio ."
                ]
            }
        ]
    },
    "tf.summary.audio":
    {
        "tf.audio_summary":
        [
            {
                "title": "43603466",
                "h": "tf.audio_summary",
                "t": "tf.summary.audio",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.audio_summary should be renamed to tf.summary.audio ."
                ]
            }
        ]
    },
    "tf.scalar_summary":
    {
        "tf.summary.scalar":
        [
            {
                "title": "43603466",
                "h": "tf.scalar_summary",
                "t": "tf.summary.scalar",
                "sentences":
                [
                    "In a new version of TF, all summary functions were renamed.",
                    "tf.scalar_summary should be renamed to tf.summary.scalar ."
                ]
            }
        ]
    },
    "tf.nn.rnn_cell.basicrnncell":
    {
        "tf.contrib.rnn.basicrnncell":
        [
            {
                "title": "48932957",
                "h": "tf.nn.rnn_cell.basicrnncell",
                "t": "tf.contrib.rnn.basicrnncell",
                "sentences":
                [
                    "By the way, do we use tf.nn.rnn_cell.BasicRNNCell or tf.contrib.rnn.BasicRNNCell?",
                    "Yes, they are synonyms, but I prefer to use tf.nn.rnn_cell package, because everything in tf.contrib is sort of experimental and can be changed in 1.x versions."
                ]
            }
        ]
    },
    "tf.contrib.rnn.basicrnncell":
    {
        "tf.nn.rnn_cell.basicrnncell":
        [
            {
                "title": "48932957",
                "h": "tf.nn.rnn_cell.basicrnncell",
                "t": "tf.contrib.rnn.basicrnncell",
                "sentences":
                [
                    "By the way, do we use tf.nn.rnn_cell.BasicRNNCell or tf.contrib.rnn.BasicRNNCell?",
                    "Yes, they are synonyms, but I prefer to use tf.nn.rnn_cell package, because everything in tf.contrib is sort of experimental and can be changed in 1.x versions."
                ]
            }
        ]
    },
    "tf.concat":
    {
        "tf.stack":
        [
            {
                "title": "49896013",
                "h": "tf.stack",
                "t": "tf.concat",
                "sentences":
                [
                    "tf.stack concatenate the given tensors along a new dimension.",
                    "If you want to concatenate across an existing dimension, use tf.concat:"
                ]
            }
        ]
    },
    "tf.nn.softmax_cross_entropy":
    {
        "tf.nn.cross_entropy":
        [
            {
                "title": "45183635",
                "h": "tf.nn.softmax_cross_entropy",
                "t": "tf.nn.cross_entropy",
                "sentences":
                [
                    "Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions.",
                    "Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()"
                ]
            }
        ],
        "tf.nn.softmax":
        [
            {
                "title": "45183635",
                "h": "tf.nn.softmax_cross_entropy",
                "t": "tf.nn.softmax",
                "sentences":
                [
                    "Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions.",
                    "Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()"
                ]
            }
        ]
    },
    "tf.nn.cross_entropy":
    {
        "tf.nn.softmax_cross_entropy":
        [
            {
                "title": "45183635",
                "h": "tf.nn.softmax_cross_entropy",
                "t": "tf.nn.cross_entropy",
                "sentences":
                [
                    "Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions.",
                    "Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()"
                ]
            }
        ]
    },
    "tf.train.optimizer.compute_gradients":
    {
        "tf.gradients":
        [
            {
                "title": "45347628",
                "h": "tf.gradients",
                "t": "tf.train.optimizer.compute_gradients",
                "sentences":
                [
                    "tf.gradients does not allow you to compute Jacobians, it aggregates the gradients of each input for every output (something like the summation of each column of the actual Jacobian matrix).",
                    "With respect to tf.train.Optimizer.compute_gradients, yes, its result is basically the same, but taking care of some details automatically and with slightly more convenient output format.",
                    "If you look at the implementation, you will see that, at its core, is a call to tf.gradients (in this case aliased to gradients.gradients), but it is useful for optimizer implementations to have the surrounding logic already implemented.",
                    "Also, having it as a method allows for extensible behaviour in subclasses, either to implement some kind of optimization strategy (not very likely at the compute_gradients step, really) or for auxiliary purposes, like tracing or debugging."
                ]
            }
        ]
    },
    "tf.metric.mean_squared_error":
    {
        "tf.metrics.accuracy":
        [
            {
                "title": "47526013",
                "h": "tf.metrics.accuracy",
                "t": "tf.metric.mean_squared_error",
                "sentences":
                [
                    "Turns out, since this is a multi-class Linear Regression problem, and not a classification problem, that tf.metrics.accuracy is not the right approach.",
                    "Instead of displaying the accuracy of my model in terms of percentage, I instead focused on reducing the Mean Square Error (MSE) instead.",
                    "From looking at other examples, tf.metrics.accuracy is never used for Linear Regression, and only classification.",
                    "Normally tf.metric.mean_squared_error is the right approach."
                ]
            }
        ]
    },
    "tf.data.dataset":
    {
        "tf.data.dataset.cache":
        [
            {
                "title": "59531820",
                "h": "tf.data.dataset",
                "t": "tf.data.dataset.cache",
                "sentences":
                [
                    "tf.data.Dataset is a more abstract object whose job is to define the data pipeline.",
                    "If you want to save intermediate results to speed up your data pipeline you can use tf.data.Dataset.cache() or tf.data.Dataset.prefetch() (more on it here)"
                ]
            }
        ],
        "tf.data.dataset.prefetch":
        [
            {
                "title": "59531820",
                "h": "tf.data.dataset",
                "t": "tf.data.dataset.prefetch",
                "sentences":
                [
                    "tf.data.Dataset is a more abstract object whose job is to define the data pipeline.",
                    "If you want to save intermediate results to speed up your data pipeline you can use tf.data.Dataset.cache() or tf.data.Dataset.prefetch() (more on it here)"
                ]
            }
        ]
    },
    "tf.data.dataset.cache":
    {
        "tf.data.dataset":
        [
            {
                "title": "59531820",
                "h": "tf.data.dataset",
                "t": "tf.data.dataset.cache",
                "sentences":
                [
                    "tf.data.Dataset is a more abstract object whose job is to define the data pipeline.",
                    "If you want to save intermediate results to speed up your data pipeline you can use tf.data.Dataset.cache() or tf.data.Dataset.prefetch() (more on it here)"
                ]
            }
        ]
    },
    "tf.data.dataset.prefetch":
    {
        "tf.data.dataset":
        [
            {
                "title": "59531820",
                "h": "tf.data.dataset",
                "t": "tf.data.dataset.prefetch",
                "sentences":
                [
                    "tf.data.Dataset is a more abstract object whose job is to define the data pipeline.",
                    "If you want to save intermediate results to speed up your data pipeline you can use tf.data.Dataset.cache() or tf.data.Dataset.prefetch() (more on it here)"
                ]
            }
        ]
    }
}